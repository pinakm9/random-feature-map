{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646653a5-f2b6-47ef-af57-e22da1bd6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.integrate import odeint\n",
    "import os, sys, warnings\n",
    "from pathlib import Path\n",
    "from os.path import dirname, realpath\n",
    "script_dir = Path(dirname(realpath('.')))\n",
    "module_dir = str(script_dir)\n",
    "sys.path.insert(0, module_dir + '/modules')\n",
    "import utility as ut\n",
    "import surrogate_nn as srnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02818a22-e0ab-4f51-8f27-dae86c82ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 1573993.147849 time elapsed: 0.0024 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 10 loss: 1562485.526607 time elapsed: 0.0163 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 20 loss: 1551064.813645 time elapsed: 0.0272 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 30 loss: 1539738.344035 time elapsed: 0.0381 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 40 loss: 1528508.663452 time elapsed: 0.0490 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 50 loss: 1517374.874523 time elapsed: 0.0597 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 60 loss: 1506333.827075 time elapsed: 0.0714 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 70 loss: 1495381.468050 time elapsed: 0.0834 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 80 loss: 1484513.660793 time elapsed: 0.0945 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 90 loss: 1473726.597138 time elapsed: 0.1047 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 100 loss: 1463016.885841 time elapsed: 0.1153 learning rate: 0.000100, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 110 loss: 1452381.455419 time elapsed: 0.1284 learning rate: 0.000100, scenario: 0, slope: -1100.0829979514947, fluctuations: 0.0\n",
      "step: 120 loss: 1441817.421496 time elapsed: 0.1406 learning rate: 0.000100, scenario: 0, slope: -1091.4656044832432, fluctuations: 0.0\n",
      "step: 130 loss: 1431321.997807 time elapsed: 0.1529 learning rate: 0.000100, scenario: 0, slope: -1083.171389211041, fluctuations: 0.0\n",
      "step: 140 loss: 1420892.482739 time elapsed: 0.1655 learning rate: 0.000100, scenario: 0, slope: -1075.2168549826238, fluctuations: 0.0\n",
      "step: 150 loss: 1410526.330843 time elapsed: 0.1779 learning rate: 0.000100, scenario: 0, slope: -1067.5939332493267, fluctuations: 0.0\n",
      "step: 160 loss: 1400221.302676 time elapsed: 0.1905 learning rate: 0.000100, scenario: 0, slope: -1060.2836647914528, fluctuations: 0.0\n",
      "step: 170 loss: 1389975.666489 time elapsed: 0.2029 learning rate: 0.000100, scenario: 0, slope: -1053.2636830476035, fluctuations: 0.0\n",
      "step: 180 loss: 1379788.406168 time elapsed: 0.2155 learning rate: 0.000100, scenario: 0, slope: -1046.5097197716252, fluctuations: 0.0\n",
      "step: 190 loss: 1369659.381085 time elapsed: 0.2292 learning rate: 0.000100, scenario: 0, slope: -1039.9934742310006, fluctuations: 0.0\n",
      "step: 200 loss: 1359589.390713 time elapsed: 0.2422 learning rate: 0.000100, scenario: 0, slope: -1034.3026977244172, fluctuations: 0.0\n",
      "step: 210 loss: 1349580.118912 time elapsed: 0.2575 learning rate: 0.000100, scenario: 0, slope: -1027.5206362816684, fluctuations: 0.0\n",
      "step: 220 loss: 1339633.960323 time elapsed: 0.2703 learning rate: 0.000100, scenario: 0, slope: -1021.4612588605235, fluctuations: 0.0\n",
      "step: 230 loss: 1329753.753324 time elapsed: 0.2830 learning rate: 0.000100, scenario: 0, slope: -1015.4356303108855, fluctuations: 0.0\n",
      "step: 240 loss: 1319942.457522 time elapsed: 0.2955 learning rate: 0.000100, scenario: 0, slope: -1009.3744003458477, fluctuations: 0.0\n",
      "step: 250 loss: 1310202.822189 time elapsed: 0.3080 learning rate: 0.000100, scenario: 0, slope: -1003.2106440864327, fluctuations: 0.0\n",
      "step: 260 loss: 1300537.094706 time elapsed: 0.3207 learning rate: 0.000100, scenario: 0, slope: -996.8869940844012, fluctuations: 0.0\n",
      "step: 270 loss: 1290946.810938 time elapsed: 0.3331 learning rate: 0.000100, scenario: 0, slope: -990.362187567986, fluctuations: 0.0\n",
      "step: 280 loss: 1281432.691636 time elapsed: 0.3455 learning rate: 0.000100, scenario: 0, slope: -983.615796014949, fluctuations: 0.0\n",
      "step: 290 loss: 1271994.645702 time elapsed: 0.3577 learning rate: 0.000100, scenario: 0, slope: -976.6502673696233, fluctuations: 0.0\n",
      "step: 300 loss: 1262631.860893 time elapsed: 0.3699 learning rate: 0.000100, scenario: 0, slope: -970.2136714979944, fluctuations: 0.0\n",
      "step: 310 loss: 1253342.951381 time elapsed: 0.3828 learning rate: 0.000100, scenario: 0, slope: -962.1775400783441, fluctuations: 0.0\n",
      "step: 320 loss: 1244126.130562 time elapsed: 0.3955 learning rate: 0.000100, scenario: 0, slope: -954.768217781906, fluctuations: 0.0\n",
      "step: 330 loss: 1234979.383736 time elapsed: 0.4079 learning rate: 0.000100, scenario: 0, slope: -947.3232559380336, fluctuations: 0.0\n",
      "step: 340 loss: 1225900.624128 time elapsed: 0.4204 learning rate: 0.000100, scenario: 0, slope: -939.9033333817637, fluctuations: 0.0\n",
      "step: 350 loss: 1216887.824007 time elapsed: 0.4328 learning rate: 0.000100, scenario: 0, slope: -932.5629123379426, fluctuations: 0.0\n",
      "step: 360 loss: 1207939.118247 time elapsed: 0.4453 learning rate: 0.000100, scenario: 0, slope: -925.3461203541198, fluctuations: 0.0\n",
      "step: 370 loss: 1199052.880543 time elapsed: 0.4580 learning rate: 0.000100, scenario: 0, slope: -918.2844045676667, fluctuations: 0.0\n",
      "step: 380 loss: 1190227.772947 time elapsed: 0.4708 learning rate: 0.000100, scenario: 0, slope: -911.3958712304224, fluctuations: 0.0\n",
      "step: 390 loss: 1181462.768737 time elapsed: 0.4833 learning rate: 0.000100, scenario: 0, slope: -904.6859951957995, fluctuations: 0.0\n",
      "step: 400 loss: 1172757.147832 time elapsed: 0.4957 learning rate: 0.000100, scenario: 0, slope: -898.7955189148258, fluctuations: 0.0\n",
      "step: 410 loss: 1164110.464452 time elapsed: 0.5087 learning rate: 0.000100, scenario: 0, slope: -891.7715924297793, fluctuations: 0.0\n",
      "step: 420 loss: 1155522.489287 time elapsed: 0.5213 learning rate: 0.000100, scenario: 0, slope: -885.5325768362542, fluctuations: 0.0\n",
      "step: 430 loss: 1146993.132926 time elapsed: 0.5339 learning rate: 0.000100, scenario: 0, slope: -879.4084498045588, fluctuations: 0.0\n",
      "step: 440 loss: 1138522.361935 time elapsed: 0.5464 learning rate: 0.000100, scenario: 0, slope: -873.3745085504619, fluctuations: 0.0\n",
      "step: 450 loss: 1130110.120822 time elapsed: 0.5590 learning rate: 0.000100, scenario: 0, slope: -867.4075035184213, fluctuations: 0.0\n",
      "step: 460 loss: 1121756.270703 time elapsed: 0.5713 learning rate: 0.000100, scenario: 0, slope: -861.4876134568855, fluctuations: 0.0\n",
      "step: 470 loss: 1113460.549218 time elapsed: 0.5839 learning rate: 0.000100, scenario: 0, slope: -855.5998893367914, fluctuations: 0.0\n",
      "step: 480 loss: 1105222.548795 time elapsed: 0.5963 learning rate: 0.000100, scenario: 0, slope: -849.7350702196969, fluctuations: 0.0\n",
      "step: 490 loss: 1097041.704238 time elapsed: 0.6086 learning rate: 0.000100, scenario: 0, slope: -843.8897639644023, fluctuations: 0.0\n",
      "step: 500 loss: 1088917.277568 time elapsed: 0.6212 learning rate: 0.000100, scenario: 0, slope: -838.6473255758851, fluctuations: 0.0\n",
      "step: 510 loss: 1080848.328601 time elapsed: 0.6341 learning rate: 0.000100, scenario: 0, slope: -832.2710411052869, fluctuations: 0.0\n",
      "step: 520 loss: 1072833.663718 time elapsed: 0.6469 learning rate: 0.000100, scenario: 0, slope: -826.5156986583801, fluctuations: 0.0\n",
      "step: 530 loss: 1064871.762937 time elapsed: 0.6594 learning rate: 0.000100, scenario: 0, slope: -820.814814992733, fluctuations: 0.0\n",
      "step: 540 loss: 1056960.698037 time elapsed: 0.6719 learning rate: 0.000100, scenario: 0, slope: -815.1867020248501, fluctuations: 0.0\n",
      "step: 550 loss: 1049098.075632 time elapsed: 0.6844 learning rate: 0.000100, scenario: 0, slope: -809.6535291061631, fluctuations: 0.0\n",
      "step: 560 loss: 1041281.072235 time elapsed: 0.6970 learning rate: 0.000100, scenario: 0, slope: -804.2415913418198, fluctuations: 0.0\n",
      "step: 570 loss: 1033506.669879 time elapsed: 0.7094 learning rate: 0.000100, scenario: 0, slope: -798.9806797237511, fluctuations: 0.0\n",
      "step: 580 loss: 1025772.221996 time elapsed: 0.7220 learning rate: 0.000100, scenario: 0, slope: -793.9011555169527, fluctuations: 0.0\n",
      "step: 590 loss: 1018076.403266 time elapsed: 0.7344 learning rate: 0.000100, scenario: 0, slope: -789.0271421510537, fluctuations: 0.0\n",
      "step: 600 loss: 1010420.321396 time elapsed: 0.7470 learning rate: 0.000100, scenario: 0, slope: -784.8222538774612, fluctuations: 0.0\n",
      "step: 610 loss: 1002808.134336 time elapsed: 0.7601 learning rate: 0.000100, scenario: 0, slope: -779.8908433614316, fluctuations: 0.0\n",
      "step: 620 loss: 995246.373107 time elapsed: 0.7728 learning rate: 0.000100, scenario: 0, slope: -775.5392628445192, fluctuations: 0.0\n",
      "step: 630 loss: 987741.916798 time elapsed: 0.7854 learning rate: 0.000100, scenario: 0, slope: -771.2082707790012, fluctuations: 0.0\n",
      "step: 640 loss: 980299.790012 time elapsed: 0.7978 learning rate: 0.000100, scenario: 0, slope: -766.7758270743371, fluctuations: 0.0\n",
      "step: 650 loss: 972922.148629 time elapsed: 0.8103 learning rate: 0.000100, scenario: 0, slope: -762.1260787093487, fluctuations: 0.0\n",
      "step: 660 loss: 965608.674043 time elapsed: 0.8226 learning rate: 0.000100, scenario: 0, slope: -757.1739286089388, fluctuations: 0.0\n",
      "step: 670 loss: 958357.602603 time elapsed: 0.8352 learning rate: 0.000100, scenario: 0, slope: -751.8815041479637, fluctuations: 0.0\n",
      "step: 680 loss: 951166.641124 time elapsed: 0.8485 learning rate: 0.000100, scenario: 0, slope: -746.2646145107991, fluctuations: 0.0\n",
      "step: 690 loss: 944033.506138 time elapsed: 0.8615 learning rate: 0.000100, scenario: 0, slope: -740.3890873070108, fluctuations: 0.0\n",
      "step: 700 loss: 936956.149248 time elapsed: 0.8739 learning rate: 0.000100, scenario: 0, slope: -734.9638297759957, fluctuations: 0.0\n",
      "step: 710 loss: 929932.806890 time elapsed: 0.8867 learning rate: 0.000100, scenario: 0, slope: -728.2832287014895, fluctuations: 0.0\n",
      "step: 720 loss: 922961.977622 time elapsed: 0.8992 learning rate: 0.000100, scenario: 0, slope: -722.2711986939865, fluctuations: 0.0\n",
      "step: 730 loss: 916042.382919 time elapsed: 0.9119 learning rate: 0.000100, scenario: 0, slope: -716.39247775093, fluctuations: 0.0\n",
      "step: 740 loss: 909172.937116 time elapsed: 0.9247 learning rate: 0.000100, scenario: 0, slope: -710.6822542481157, fluctuations: 0.0\n",
      "step: 750 loss: 902352.732105 time elapsed: 0.9374 learning rate: 0.000100, scenario: 0, slope: -705.1462714904047, fluctuations: 0.0\n",
      "step: 760 loss: 895581.026789 time elapsed: 0.9499 learning rate: 0.000100, scenario: 0, slope: -699.7727105791879, fluctuations: 0.0\n",
      "step: 770 loss: 888857.231707 time elapsed: 0.9627 learning rate: 0.000100, scenario: 0, slope: -694.5422864589808, fluctuations: 0.0\n",
      "step: 780 loss: 882180.889488 time elapsed: 0.9751 learning rate: 0.000100, scenario: 0, slope: -689.4342653805818, fluctuations: 0.0\n",
      "step: 790 loss: 875551.653508 time elapsed: 0.9875 learning rate: 0.000100, scenario: 0, slope: -684.4291205020178, fluctuations: 0.0\n",
      "step: 800 loss: 868969.265099 time elapsed: 0.9999 learning rate: 0.000100, scenario: 0, slope: -679.9979139413753, fluctuations: 0.0\n",
      "step: 810 loss: 862433.529955 time elapsed: 1.0129 learning rate: 0.000100, scenario: 0, slope: -674.6593589724994, fluctuations: 0.0\n",
      "step: 820 loss: 855944.294662 time elapsed: 1.0252 learning rate: 0.000100, scenario: 0, slope: -669.8657002581589, fluctuations: 0.0\n",
      "step: 830 loss: 849501.423921 time elapsed: 1.0377 learning rate: 0.000100, scenario: 0, slope: -665.1167362601773, fluctuations: 0.0\n",
      "step: 840 loss: 843104.779503 time elapsed: 1.0504 learning rate: 0.000100, scenario: 0, slope: -660.4028972410613, fluctuations: 0.0\n",
      "step: 850 loss: 836754.203639 time elapsed: 1.0629 learning rate: 0.000100, scenario: 0, slope: -655.7167476347364, fluctuations: 0.0\n",
      "step: 860 loss: 830449.510005 time elapsed: 1.0753 learning rate: 0.000100, scenario: 0, slope: -651.052994502414, fluctuations: 0.0\n",
      "step: 870 loss: 824190.483261 time elapsed: 1.0879 learning rate: 0.000100, scenario: 0, slope: -646.4083659780503, fluctuations: 0.0\n",
      "step: 880 loss: 817976.884674 time elapsed: 1.1003 learning rate: 0.000100, scenario: 0, slope: -641.7813636415325, fluctuations: 0.0\n",
      "step: 890 loss: 811808.459424 time elapsed: 1.1129 learning rate: 0.000100, scenario: 0, slope: -637.1719124800077, fluctuations: 0.0\n",
      "step: 900 loss: 805684.941911 time elapsed: 1.1256 learning rate: 0.000100, scenario: 0, slope: -633.0391770233044, fluctuations: 0.0\n",
      "step: 910 loss: 799606.057825 time elapsed: 1.1388 learning rate: 0.000100, scenario: 0, slope: -628.0100450549132, fluctuations: 0.0\n",
      "step: 920 loss: 793571.523952 time elapsed: 1.1517 learning rate: 0.000100, scenario: 0, slope: -623.4609974660613, fluctuations: 0.0\n",
      "step: 930 loss: 787581.047635 time elapsed: 1.1645 learning rate: 0.000100, scenario: 0, slope: -618.9356147383465, fluctuations: 0.0\n",
      "step: 940 loss: 781634.327522 time elapsed: 1.1771 learning rate: 0.000100, scenario: 0, slope: -614.4355183006425, fluctuations: 0.0\n",
      "step: 950 loss: 775731.056336 time elapsed: 1.1896 learning rate: 0.000100, scenario: 0, slope: -609.9620742078026, fluctuations: 0.0\n",
      "step: 960 loss: 769870.925640 time elapsed: 1.2022 learning rate: 0.000100, scenario: 0, slope: -605.5163884051149, fluctuations: 0.0\n",
      "step: 970 loss: 764053.631960 time elapsed: 1.2151 learning rate: 0.000100, scenario: 0, slope: -601.099326721118, fluctuations: 0.0\n",
      "step: 980 loss: 758278.883425 time elapsed: 1.2279 learning rate: 0.000100, scenario: 0, slope: -596.7115171176654, fluctuations: 0.0\n",
      "step: 990 loss: 752546.405958 time elapsed: 1.2406 learning rate: 0.000100, scenario: 0, slope: -592.3533133748386, fluctuations: 0.0\n",
      "step: 1000 loss: 746855.948240 time elapsed: 1.2534 learning rate: 0.000100, scenario: 0, slope: -588.4562612103929, fluctuations: 0.0\n",
      "step: 1010 loss: 741207.284805 time elapsed: 1.2662 learning rate: 0.000100, scenario: 0, slope: -583.7253574893366, fluctuations: 0.0\n",
      "step: 1020 loss: 735600.216895 time elapsed: 1.2789 learning rate: 0.000100, scenario: 0, slope: -579.4543414047446, fluctuations: 0.0\n",
      "step: 1030 loss: 730034.570880 time elapsed: 1.2915 learning rate: 0.000100, scenario: 0, slope: -575.2103598706485, fluctuations: 0.0\n",
      "step: 1040 loss: 724510.194206 time elapsed: 1.3040 learning rate: 0.000100, scenario: 0, slope: -570.9916969262777, fluctuations: 0.0\n",
      "step: 1050 loss: 719026.949059 time elapsed: 1.3167 learning rate: 0.000100, scenario: 0, slope: -566.7963566738173, fluctuations: 0.0\n",
      "step: 1060 loss: 713584.704218 time elapsed: 1.3291 learning rate: 0.000100, scenario: 0, slope: -562.6222246327022, fluctuations: 0.0\n",
      "step: 1070 loss: 708183.326105 time elapsed: 1.3417 learning rate: 0.000100, scenario: 0, slope: -558.4672560338613, fluctuations: 0.0\n",
      "step: 1080 loss: 702822.670647 time elapsed: 1.3541 learning rate: 0.000100, scenario: 0, slope: -554.3296675338028, fluctuations: 0.0\n",
      "step: 1090 loss: 697502.578157 time elapsed: 1.3665 learning rate: 0.000100, scenario: 0, slope: -550.2081032065407, fluctuations: 0.0\n",
      "step: 1100 loss: 692222.873616 time elapsed: 1.3785 learning rate: 0.000100, scenario: 0, slope: -546.5117051016744, fluctuations: 0.0\n",
      "step: 1110 loss: 686983.374067 time elapsed: 1.3915 learning rate: 0.000100, scenario: 0, slope: -542.0103068597496, fluctuations: 0.0\n",
      "step: 1120 loss: 681783.902995 time elapsed: 1.4040 learning rate: 0.000100, scenario: 0, slope: -537.9339679297723, fluctuations: 0.0\n",
      "step: 1130 loss: 676624.308985 time elapsed: 1.4165 learning rate: 0.000100, scenario: 0, slope: -533.8731068566825, fluctuations: 0.0\n",
      "step: 1140 loss: 671504.483882 time elapsed: 1.4290 learning rate: 0.000100, scenario: 0, slope: -529.8279998854632, fluctuations: 0.0\n",
      "step: 1150 loss: 666424.375485 time elapsed: 1.4415 learning rate: 0.000100, scenario: 0, slope: -525.7984623650176, fluctuations: 0.0\n",
      "step: 1160 loss: 661383.991254 time elapsed: 1.4541 learning rate: 0.000100, scenario: 0, slope: -521.7835403292898, fluctuations: 0.0\n",
      "step: 1170 loss: 656383.391959 time elapsed: 1.4665 learning rate: 0.000100, scenario: 0, slope: -517.7813287150649, fluctuations: 0.0\n",
      "step: 1180 loss: 651422.678438 time elapsed: 1.4790 learning rate: 0.000100, scenario: 0, slope: -513.7889710850262, fluctuations: 0.0\n",
      "step: 1190 loss: 646501.977085 time elapsed: 1.4913 learning rate: 0.000100, scenario: 0, slope: -509.80284490460616, fluctuations: 0.0\n",
      "step: 1200 loss: 641621.421275 time elapsed: 1.5038 learning rate: 0.000100, scenario: 0, slope: -506.21731595368595, fluctuations: 0.0\n",
      "step: 1210 loss: 636781.111823 time elapsed: 1.5169 learning rate: 0.000100, scenario: 0, slope: -501.83318716174364, fluctuations: 0.0\n",
      "step: 1220 loss: 631981.047476 time elapsed: 1.5295 learning rate: 0.000100, scenario: 0, slope: -497.84256592873805, fluctuations: 0.0\n",
      "step: 1230 loss: 627221.053032 time elapsed: 1.5419 learning rate: 0.000100, scenario: 0, slope: -493.84569417609174, fluctuations: 0.0\n",
      "step: 1240 loss: 622500.754387 time elapsed: 1.5543 learning rate: 0.000100, scenario: 0, slope: -489.84391058860405, fluctuations: 0.0\n",
      "step: 1250 loss: 617819.625221 time elapsed: 1.5668 learning rate: 0.000100, scenario: 0, slope: -485.8416775259004, fluctuations: 0.0\n",
      "step: 1260 loss: 613177.089640 time elapsed: 1.5793 learning rate: 0.000100, scenario: 0, slope: -481.84625420250427, fluctuations: 0.0\n",
      "step: 1270 loss: 608572.651506 time elapsed: 1.5917 learning rate: 0.000100, scenario: 0, slope: -477.8665441407592, fluctuations: 0.0\n",
      "step: 1280 loss: 604006.028914 time elapsed: 1.6042 learning rate: 0.000100, scenario: 0, slope: -473.9112450015591, fluctuations: 0.0\n",
      "step: 1290 loss: 599477.265877 time elapsed: 1.6167 learning rate: 0.000100, scenario: 0, slope: -469.98654844350006, fluctuations: 0.0\n",
      "step: 1300 loss: 594986.771789 time elapsed: 1.6290 learning rate: 0.000100, scenario: 0, slope: -466.48175624428717, fluctuations: 0.0\n",
      "step: 1310 loss: 590535.245234 time elapsed: 1.6421 learning rate: 0.000100, scenario: 0, slope: -462.22809248254777, fluctuations: 0.0\n",
      "step: 1320 loss: 586123.500884 time elapsed: 1.6546 learning rate: 0.000100, scenario: 0, slope: -458.3778612691205, fluctuations: 0.0\n",
      "step: 1330 loss: 581752.278793 time elapsed: 1.6670 learning rate: 0.000100, scenario: 0, slope: -454.52701182540625, fluctuations: 0.0\n",
      "step: 1340 loss: 577422.098723 time elapsed: 1.6793 learning rate: 0.000100, scenario: 0, slope: -450.6579535730038, fluctuations: 0.0\n",
      "step: 1350 loss: 573133.185700 time elapsed: 1.6919 learning rate: 0.000100, scenario: 0, slope: -446.75537139954827, fluctuations: 0.0\n",
      "step: 1360 loss: 568885.479698 time elapsed: 1.7043 learning rate: 0.000100, scenario: 0, slope: -442.8094778791387, fluctuations: 0.0\n",
      "step: 1370 loss: 564678.705876 time elapsed: 1.7168 learning rate: 0.000100, scenario: 0, slope: -438.81798452650713, fluctuations: 0.0\n",
      "step: 1380 loss: 560512.453882 time elapsed: 1.7292 learning rate: 0.000100, scenario: 0, slope: -434.78643824658934, fluctuations: 0.0\n",
      "step: 1390 loss: 556386.235747 time elapsed: 1.7416 learning rate: 0.000100, scenario: 0, slope: -430.726972642773, fluctuations: 0.0\n",
      "step: 1400 loss: 552299.524072 time elapsed: 1.7539 learning rate: 0.000100, scenario: 0, slope: -427.0629799209158, fluctuations: 0.0\n",
      "step: 1410 loss: 548251.781172 time elapsed: 1.7669 learning rate: 0.000100, scenario: 0, slope: -422.59042111449565, fluctuations: 0.0\n",
      "step: 1420 loss: 544242.483507 time elapsed: 1.7795 learning rate: 0.000100, scenario: 0, slope: -418.5465168112893, fluctuations: 0.0\n",
      "step: 1430 loss: 540271.140342 time elapsed: 1.7922 learning rate: 0.000100, scenario: 0, slope: -414.5365753654444, fluctuations: 0.0\n",
      "step: 1440 loss: 536337.304812 time elapsed: 1.8048 learning rate: 0.000100, scenario: 0, slope: -410.56895587684164, fluctuations: 0.0\n",
      "step: 1450 loss: 532440.576996 time elapsed: 1.8172 learning rate: 0.000100, scenario: 0, slope: -406.6481270670072, fluctuations: 0.0\n",
      "step: 1460 loss: 528580.600058 time elapsed: 1.8297 learning rate: 0.000100, scenario: 0, slope: -402.7754412142074, fluctuations: 0.0\n",
      "step: 1470 loss: 524757.051469 time elapsed: 1.8420 learning rate: 0.000100, scenario: 0, slope: -398.9500517588044, fluctuations: 0.0\n",
      "step: 1480 loss: 520969.631757 time elapsed: 1.8544 learning rate: 0.000100, scenario: 0, slope: -395.1697102012839, fluctuations: 0.0\n",
      "step: 1490 loss: 517218.053045 time elapsed: 1.8668 learning rate: 0.000100, scenario: 0, slope: -391.4314091633341, fluctuations: 0.0\n",
      "step: 1500 loss: 513502.029136 time elapsed: 1.8788 learning rate: 0.000100, scenario: 0, slope: -388.1002104204492, fluctuations: 0.0\n",
      "step: 1510 loss: 509821.268244 time elapsed: 1.8920 learning rate: 0.000100, scenario: 0, slope: -384.0682260866782, fluctuations: 0.0\n",
      "step: 1520 loss: 506175.468897 time elapsed: 1.9046 learning rate: 0.000100, scenario: 0, slope: -380.4378557466709, fluctuations: 0.0\n",
      "step: 1530 loss: 502564.319023 time elapsed: 1.9172 learning rate: 0.000100, scenario: 0, slope: -376.8390435192283, fluctuations: 0.0\n",
      "step: 1540 loss: 498987.497671 time elapsed: 1.9297 learning rate: 0.000100, scenario: 0, slope: -373.27076931746006, fluctuations: 0.0\n",
      "step: 1550 loss: 495444.678410 time elapsed: 1.9421 learning rate: 0.000100, scenario: 0, slope: -369.73267039016827, fluctuations: 0.0\n",
      "step: 1560 loss: 491935.533206 time elapsed: 1.9547 learning rate: 0.000100, scenario: 0, slope: -366.2248754593441, fluctuations: 0.0\n",
      "step: 1570 loss: 488459.735707 time elapsed: 1.9671 learning rate: 0.000100, scenario: 0, slope: -362.7478050561781, fluctuations: 0.0\n",
      "step: 1580 loss: 485016.963168 time elapsed: 1.9795 learning rate: 0.000100, scenario: 0, slope: -359.30198014424366, fluctuations: 0.0\n",
      "step: 1590 loss: 481606.896803 time elapsed: 1.9920 learning rate: 0.000100, scenario: 0, slope: -355.8878708900722, fluctuations: 0.0\n",
      "step: 1600 loss: 478229.220736 time elapsed: 2.0041 learning rate: 0.000100, scenario: 0, slope: -352.842562832779, fluctuations: 0.0\n",
      "step: 1610 loss: 474883.620110 time elapsed: 2.0169 learning rate: 0.000100, scenario: 0, slope: -349.1559301250277, fluctuations: 0.0\n",
      "step: 1620 loss: 471569.778949 time elapsed: 2.0294 learning rate: 0.000100, scenario: 0, slope: -345.83824987615, fluctuations: 0.0\n",
      "step: 1630 loss: 468287.378324 time elapsed: 2.0418 learning rate: 0.000100, scenario: 0, slope: -342.5526690753091, fluctuations: 0.0\n",
      "step: 1640 loss: 465036.095162 time elapsed: 2.0543 learning rate: 0.000100, scenario: 0, slope: -339.29907334148834, fluctuations: 0.0\n",
      "step: 1650 loss: 461815.601814 time elapsed: 2.0668 learning rate: 0.000100, scenario: 0, slope: -336.0773942581503, fluctuations: 0.0\n",
      "step: 1660 loss: 458625.566349 time elapsed: 2.0790 learning rate: 0.000100, scenario: 0, slope: -332.88765432717605, fluctuations: 0.0\n",
      "step: 1670 loss: 455465.653445 time elapsed: 2.0914 learning rate: 0.000100, scenario: 0, slope: -329.7299830749469, fluctuations: 0.0\n",
      "step: 1680 loss: 452335.525692 time elapsed: 2.1037 learning rate: 0.000100, scenario: 0, slope: -326.60460488274754, fluctuations: 0.0\n",
      "step: 1690 loss: 449234.845154 time elapsed: 2.1162 learning rate: 0.000100, scenario: 0, slope: -323.5118050757878, fluctuations: 0.0\n",
      "step: 1700 loss: 446163.275026 time elapsed: 2.1286 learning rate: 0.000100, scenario: 0, slope: -320.75638837767247, fluctuations: 0.0\n",
      "step: 1710 loss: 443120.481259 time elapsed: 2.1415 learning rate: 0.000100, scenario: 0, slope: -317.42510790433107, fluctuations: 0.0\n",
      "step: 1720 loss: 440106.134051 time elapsed: 2.1543 learning rate: 0.000100, scenario: 0, slope: -314.4316672635917, fluctuations: 0.0\n",
      "step: 1730 loss: 437119.909130 time elapsed: 2.1667 learning rate: 0.000100, scenario: 0, slope: -311.47164276361644, fluctuations: 0.0\n",
      "step: 1740 loss: 434161.488749 time elapsed: 2.1793 learning rate: 0.000100, scenario: 0, slope: -308.54498576017033, fluctuations: 0.0\n",
      "step: 1750 loss: 431230.562388 time elapsed: 2.1917 learning rate: 0.000100, scenario: 0, slope: -305.6515103819597, fluctuations: 0.0\n",
      "step: 1760 loss: 428326.827114 time elapsed: 2.2039 learning rate: 0.000100, scenario: 0, slope: -302.79089690789107, fluctuations: 0.0\n",
      "step: 1770 loss: 425449.987635 time elapsed: 2.2165 learning rate: 0.000100, scenario: 0, slope: -299.9627043245772, fluctuations: 0.0\n",
      "step: 1780 loss: 422599.756063 time elapsed: 2.2289 learning rate: 0.000100, scenario: 0, slope: -297.16638984188046, fluctuations: 0.0\n",
      "step: 1790 loss: 419775.851461 time elapsed: 2.2413 learning rate: 0.000100, scenario: 0, slope: -294.4013330589295, fluctuations: 0.0\n",
      "step: 1800 loss: 416977.999243 time elapsed: 2.2538 learning rate: 0.000100, scenario: 0, slope: -291.93895289945215, fluctuations: 0.0\n",
      "step: 1810 loss: 414205.930529 time elapsed: 2.2666 learning rate: 0.000100, scenario: 0, slope: -288.9622818960894, fluctuations: 0.0\n",
      "step: 1820 loss: 411459.381553 time elapsed: 2.2788 learning rate: 0.000100, scenario: 0, slope: -286.28689496249154, fluctuations: 0.0\n",
      "step: 1830 loss: 408738.093206 time elapsed: 2.2910 learning rate: 0.000100, scenario: 0, slope: -283.64002507848244, fluctuations: 0.0\n",
      "step: 1840 loss: 406041.810782 time elapsed: 2.3033 learning rate: 0.000100, scenario: 0, slope: -281.0210298875518, fluctuations: 0.0\n",
      "step: 1850 loss: 403370.283922 time elapsed: 2.3161 learning rate: 0.000100, scenario: 0, slope: -278.42930890819594, fluctuations: 0.0\n",
      "step: 1860 loss: 400723.266744 time elapsed: 2.3286 learning rate: 0.000100, scenario: 0, slope: -275.8643043718517, fluctuations: 0.0\n",
      "step: 1870 loss: 398100.518065 time elapsed: 2.3410 learning rate: 0.000100, scenario: 0, slope: -273.32549609786787, fluctuations: 0.0\n",
      "step: 1880 loss: 395501.801617 time elapsed: 2.3534 learning rate: 0.000100, scenario: 0, slope: -270.8123921767823, fluctuations: 0.0\n",
      "step: 1890 loss: 392926.886140 time elapsed: 2.3658 learning rate: 0.000100, scenario: 0, slope: -268.32451792521516, fluctuations: 0.0\n",
      "step: 1900 loss: 390375.545265 time elapsed: 2.3784 learning rate: 0.000100, scenario: 0, slope: -266.10661608431326, fluctuations: 0.0\n",
      "step: 1910 loss: 387847.557119 time elapsed: 2.3915 learning rate: 0.000100, scenario: 0, slope: -263.42258950429255, fluctuations: 0.0\n",
      "step: 1920 loss: 385342.703669 time elapsed: 2.4039 learning rate: 0.000100, scenario: 0, slope: -261.0076022573385, fluctuations: 0.0\n",
      "step: 1930 loss: 382860.769932 time elapsed: 2.4164 learning rate: 0.000100, scenario: 0, slope: -258.6159829011793, fluctuations: 0.0\n",
      "step: 1940 loss: 380401.543283 time elapsed: 2.4287 learning rate: 0.000100, scenario: 0, slope: -256.2472853674946, fluctuations: 0.0\n",
      "step: 1950 loss: 377964.813179 time elapsed: 2.4411 learning rate: 0.000100, scenario: 0, slope: -253.90109003311088, fluctuations: 0.0\n",
      "step: 1960 loss: 375550.371617 time elapsed: 2.4536 learning rate: 0.000100, scenario: 0, slope: -251.5770113692385, fluctuations: 0.0\n",
      "step: 1970 loss: 373158.014486 time elapsed: 2.4661 learning rate: 0.000100, scenario: 0, slope: -249.27469660106507, fluctuations: 0.0\n",
      "step: 1980 loss: 370787.543710 time elapsed: 2.4783 learning rate: 0.000100, scenario: 0, slope: -246.99381135922516, fluctuations: 0.0\n",
      "step: 1990 loss: 368438.769796 time elapsed: 2.4907 learning rate: 0.000100, scenario: 0, slope: -244.7340114579988, fluctuations: 0.0\n",
      "step: 2000 loss: 366111.514172 time elapsed: 2.5031 learning rate: 0.000100, scenario: 0, slope: -242.7178966925828, fluctuations: 0.0\n",
      "step: 2010 loss: 363805.610621 time elapsed: 2.5162 learning rate: 0.000100, scenario: 0, slope: -240.27600769256122, fluctuations: 0.0\n",
      "step: 2020 loss: 361520.905237 time elapsed: 2.5287 learning rate: 0.000100, scenario: 0, slope: -238.07671764338872, fluctuations: 0.0\n",
      "step: 2030 loss: 359257.254593 time elapsed: 2.5414 learning rate: 0.000100, scenario: 0, slope: -235.8962963197121, fluctuations: 0.0\n",
      "step: 2040 loss: 357014.522260 time elapsed: 2.5541 learning rate: 0.000100, scenario: 0, slope: -233.73389011110822, fluctuations: 0.0\n",
      "step: 2050 loss: 354792.574216 time elapsed: 2.5666 learning rate: 0.000100, scenario: 0, slope: -231.5885806300479, fluctuations: 0.0\n",
      "step: 2060 loss: 352591.274005 time elapsed: 2.5792 learning rate: 0.000100, scenario: 0, slope: -229.45946391694045, fluctuations: 0.0\n",
      "step: 2070 loss: 350410.478558 time elapsed: 2.5918 learning rate: 0.000100, scenario: 0, slope: -227.34574494898365, fluctuations: 0.0\n",
      "step: 2080 loss: 348250.035426 time elapsed: 2.6043 learning rate: 0.000100, scenario: 0, slope: -225.246829154113, fluctuations: 0.0\n",
      "step: 2090 loss: 346109.781860 time elapsed: 2.6167 learning rate: 0.000100, scenario: 0, slope: -223.16239149937417, fluctuations: 0.0\n",
      "step: 2100 loss: 343989.545836 time elapsed: 2.6289 learning rate: 0.000100, scenario: 0, slope: -221.29874981441614, fluctuations: 0.0\n",
      "step: 2110 loss: 341889.148906 time elapsed: 2.6417 learning rate: 0.000100, scenario: 0, slope: -219.03713449847635, fluctuations: 0.0\n",
      "step: 2120 loss: 339808.410646 time elapsed: 2.6542 learning rate: 0.000100, scenario: 0, slope: -216.99704972798008, fluctuations: 0.0\n",
      "step: 2130 loss: 337747.154414 time elapsed: 2.6668 learning rate: 0.000100, scenario: 0, slope: -214.97273974851115, fluctuations: 0.0\n",
      "step: 2140 loss: 335705.213743 time elapsed: 2.6792 learning rate: 0.000100, scenario: 0, slope: -212.9647674280964, fluctuations: 0.0\n",
      "step: 2150 loss: 333682.438175 time elapsed: 2.6916 learning rate: 0.000100, scenario: 0, slope: -210.9735289613964, fluctuations: 0.0\n",
      "step: 2160 loss: 331678.696980 time elapsed: 2.7040 learning rate: 0.000100, scenario: 0, slope: -208.99912801573637, fluctuations: 0.0\n",
      "step: 2170 loss: 329693.879603 time elapsed: 2.7165 learning rate: 0.000100, scenario: 0, slope: -207.04129185520787, fluctuations: 0.0\n",
      "step: 2180 loss: 327727.892308 time elapsed: 2.7292 learning rate: 0.000100, scenario: 0, slope: -205.09934966766468, fluctuations: 0.0\n",
      "step: 2190 loss: 325780.651099 time elapsed: 2.7415 learning rate: 0.000100, scenario: 0, slope: -203.17228480974526, fluctuations: 0.0\n",
      "step: 2200 loss: 323852.072265 time elapsed: 2.7538 learning rate: 0.000100, scenario: 0, slope: -201.44962643861552, fluctuations: 0.0\n",
      "step: 2210 loss: 321942.065385 time elapsed: 2.7669 learning rate: 0.000100, scenario: 0, slope: -199.35779838723155, fluctuations: 0.0\n",
      "step: 2220 loss: 320050.532358 time elapsed: 2.7794 learning rate: 0.000100, scenario: 0, slope: -197.46794961650042, fluctuations: 0.0\n",
      "step: 2230 loss: 318177.366172 time elapsed: 2.7919 learning rate: 0.000100, scenario: 0, slope: -195.58843792269414, fluctuations: 0.0\n",
      "step: 2240 loss: 316322.446596 time elapsed: 2.8044 learning rate: 0.000100, scenario: 0, slope: -193.71876285488952, fluctuations: 0.0\n",
      "step: 2250 loss: 314485.638881 time elapsed: 2.8168 learning rate: 0.000100, scenario: 0, slope: -191.85885828914033, fluctuations: 0.0\n",
      "step: 2260 loss: 312666.796306 time elapsed: 2.8293 learning rate: 0.000100, scenario: 0, slope: -190.00907956565047, fluctuations: 0.0\n",
      "step: 2270 loss: 310865.764653 time elapsed: 2.8415 learning rate: 0.000100, scenario: 0, slope: -188.17011907573485, fluctuations: 0.0\n",
      "step: 2280 loss: 309082.388195 time elapsed: 2.8539 learning rate: 0.000100, scenario: 0, slope: -186.34286743220622, fluctuations: 0.0\n",
      "step: 2290 loss: 307316.516657 time elapsed: 2.8664 learning rate: 0.000100, scenario: 0, slope: -184.5282467866902, fluctuations: 0.0\n",
      "step: 2300 loss: 305568.011428 time elapsed: 2.8787 learning rate: 0.000100, scenario: 0, slope: -182.90654887079404, fluctuations: 0.0\n",
      "step: 2310 loss: 303836.747935 time elapsed: 2.8916 learning rate: 0.000100, scenario: 0, slope: -180.93982702966247, fluctuations: 0.0\n",
      "step: 2320 loss: 302122.609632 time elapsed: 2.9040 learning rate: 0.000100, scenario: 0, slope: -179.16680683510785, fluctuations: 0.0\n",
      "step: 2330 loss: 300425.470014 time elapsed: 2.9165 learning rate: 0.000100, scenario: 0, slope: -177.40794091974428, fluctuations: 0.0\n",
      "step: 2340 loss: 298745.169091 time elapsed: 2.9287 learning rate: 0.000100, scenario: 0, slope: -175.6630774766673, fluctuations: 0.0\n",
      "step: 2350 loss: 297081.509977 time elapsed: 2.9410 learning rate: 0.000100, scenario: 0, slope: -173.93222906461614, fluctuations: 0.0\n",
      "step: 2360 loss: 295434.290453 time elapsed: 2.9533 learning rate: 0.000100, scenario: 0, slope: -172.21570996921628, fluctuations: 0.0\n",
      "step: 2370 loss: 293803.332270 time elapsed: 2.9658 learning rate: 0.000100, scenario: 0, slope: -170.51405362843502, fluctuations: 0.0\n",
      "step: 2380 loss: 292188.482432 time elapsed: 2.9782 learning rate: 0.000100, scenario: 0, slope: -168.8278522897747, fluctuations: 0.0\n",
      "step: 2390 loss: 290589.606174 time elapsed: 2.9907 learning rate: 0.000100, scenario: 0, slope: -167.15761893488505, fluctuations: 0.0\n",
      "step: 2400 loss: 289006.584199 time elapsed: 3.0030 learning rate: 0.000100, scenario: 0, slope: -165.66831593677333, fluctuations: 0.0\n",
      "step: 2410 loss: 287439.312519 time elapsed: 3.0159 learning rate: 0.000100, scenario: 0, slope: -163.86589149635378, fluctuations: 0.0\n",
      "step: 2420 loss: 285887.702415 time elapsed: 3.0282 learning rate: 0.000100, scenario: 0, slope: -162.24377146206965, fluctuations: 0.0\n",
      "step: 2430 loss: 284351.679711 time elapsed: 3.0404 learning rate: 0.000100, scenario: 0, slope: -160.63618301271993, fluctuations: 0.0\n",
      "step: 2440 loss: 282831.183004 time elapsed: 3.0527 learning rate: 0.000100, scenario: 0, slope: -159.04159328947054, fluctuations: 0.0\n",
      "step: 2450 loss: 281326.159484 time elapsed: 3.0651 learning rate: 0.000100, scenario: 0, slope: -157.4583331229209, fluctuations: 0.0\n",
      "step: 2460 loss: 279836.555820 time elapsed: 3.0774 learning rate: 0.000100, scenario: 0, slope: -155.88487199293192, fluctuations: 0.0\n",
      "step: 2470 loss: 278362.303207 time elapsed: 3.0900 learning rate: 0.000100, scenario: 0, slope: -154.3199632144587, fluctuations: 0.0\n",
      "step: 2480 loss: 276903.301182 time elapsed: 3.1025 learning rate: 0.000100, scenario: 0, slope: -152.76275972284583, fluctuations: 0.0\n",
      "step: 2490 loss: 275459.409579 time elapsed: 3.1149 learning rate: 0.000100, scenario: 0, slope: -151.2129691153712, fluctuations: 0.0\n",
      "step: 2500 loss: 274030.454685 time elapsed: 3.1272 learning rate: 0.000100, scenario: 0, slope: -149.82479545673803, fluctuations: 0.0\n",
      "step: 2510 loss: 272616.243715 time elapsed: 3.1402 learning rate: 0.000100, scenario: 0, slope: -148.13781554604273, fluctuations: 0.0\n",
      "step: 2520 loss: 271216.575104 time elapsed: 3.1528 learning rate: 0.000100, scenario: 0, slope: -146.61509612788745, fluctuations: 0.0\n",
      "step: 2530 loss: 269831.241125 time elapsed: 3.1652 learning rate: 0.000100, scenario: 0, slope: -145.10476904383833, fluctuations: 0.0\n",
      "step: 2540 loss: 268460.028554 time elapsed: 3.1776 learning rate: 0.000100, scenario: 0, slope: -143.60894325172345, fluctuations: 0.0\n",
      "step: 2550 loss: 267102.722031 time elapsed: 3.1902 learning rate: 0.000100, scenario: 0, slope: -142.12965763771783, fluctuations: 0.0\n",
      "step: 2560 loss: 265759.111072 time elapsed: 3.2026 learning rate: 0.000100, scenario: 0, slope: -140.66865391175085, fluctuations: 0.0\n",
      "step: 2570 loss: 264428.997419 time elapsed: 3.2150 learning rate: 0.000100, scenario: 0, slope: -139.22718682909564, fluctuations: 0.0\n",
      "step: 2580 loss: 263112.198719 time elapsed: 3.2273 learning rate: 0.000100, scenario: 0, slope: -137.8059332714883, fluctuations: 0.0\n",
      "step: 2590 loss: 261808.548431 time elapsed: 3.2395 learning rate: 0.000100, scenario: 0, slope: -136.4050243249434, fluctuations: 0.0\n",
      "step: 2600 loss: 260517.893956 time elapsed: 3.2516 learning rate: 0.000100, scenario: 0, slope: -135.1613532156687, fluctuations: 0.0\n",
      "step: 2610 loss: 259240.094029 time elapsed: 3.2643 learning rate: 0.000100, scenario: 0, slope: -133.66267143681574, fluctuations: 0.0\n",
      "step: 2620 loss: 257975.015345 time elapsed: 3.2766 learning rate: 0.000100, scenario: 0, slope: -132.3196870243943, fluctuations: 0.0\n",
      "step: 2630 loss: 256722.528241 time elapsed: 3.2887 learning rate: 0.000100, scenario: 0, slope: -130.99413641865084, fluctuations: 0.0\n",
      "step: 2640 loss: 255482.501431 time elapsed: 3.3015 learning rate: 0.000100, scenario: 0, slope: -129.68491716713598, fluctuations: 0.0\n",
      "step: 2650 loss: 254254.796155 time elapsed: 3.3140 learning rate: 0.000100, scenario: 0, slope: -128.39103632748137, fluctuations: 0.0\n",
      "step: 2660 loss: 253039.261268 time elapsed: 3.3264 learning rate: 0.000100, scenario: 0, slope: -127.11175370262008, fluctuations: 0.0\n",
      "step: 2670 loss: 251835.732227 time elapsed: 3.3387 learning rate: 0.000100, scenario: 0, slope: -125.8466764620439, fluctuations: 0.0\n",
      "step: 2680 loss: 250644.034383 time elapsed: 3.3512 learning rate: 0.000100, scenario: 0, slope: -124.59578105856227, fluctuations: 0.0\n",
      "step: 2690 loss: 249463.985982 time elapsed: 3.3637 learning rate: 0.000100, scenario: 0, slope: -123.35937431265047, fluctuations: 0.0\n",
      "step: 2700 loss: 248295.398909 time elapsed: 3.3761 learning rate: 0.000100, scenario: 0, slope: -122.25946160164004, fluctuations: 0.0\n",
      "step: 2710 loss: 247138.079268 time elapsed: 3.3892 learning rate: 0.000100, scenario: 0, slope: -120.9324789462448, fluctuations: 0.0\n",
      "step: 2720 loss: 245991.828018 time elapsed: 3.4017 learning rate: 0.000100, scenario: 0, slope: -119.74357557729327, fluctuations: 0.0\n",
      "step: 2730 loss: 244856.440662 time elapsed: 3.4142 learning rate: 0.000100, scenario: 0, slope: -118.57216087930652, fluctuations: 0.0\n",
      "step: 2740 loss: 243731.705386 time elapsed: 3.4265 learning rate: 0.000100, scenario: 0, slope: -117.41903296815025, fluctuations: 0.0\n",
      "step: 2750 loss: 242617.397778 time elapsed: 3.4393 learning rate: 0.000100, scenario: 0, slope: -116.2849332590625, fluctuations: 0.0\n",
      "step: 2760 loss: 241513.278651 time elapsed: 3.4519 learning rate: 0.000100, scenario: 0, slope: -115.17059369854589, fluctuations: 0.0\n",
      "step: 2770 loss: 240419.099471 time elapsed: 3.4641 learning rate: 0.000100, scenario: 0, slope: -114.07678388300248, fluctuations: 0.0\n",
      "step: 2780 loss: 239334.611615 time elapsed: 3.4766 learning rate: 0.000100, scenario: 0, slope: -113.00431197607621, fluctuations: 0.0\n",
      "step: 2790 loss: 238259.587156 time elapsed: 3.4890 learning rate: 0.000100, scenario: 0, slope: -111.95392561262595, fluctuations: 0.0\n",
      "step: 2800 loss: 237193.846538 time elapsed: 3.5013 learning rate: 0.000100, scenario: 0, slope: -111.02785435783738, fluctuations: 0.0\n",
      "step: 2810 loss: 236137.285814 time elapsed: 3.5147 learning rate: 0.000100, scenario: 0, slope: -109.92064563514057, fluctuations: 0.0\n",
      "step: 2820 loss: 235089.879332 time elapsed: 3.5271 learning rate: 0.000100, scenario: 0, slope: -108.93649191672692, fluctuations: 0.0\n",
      "step: 2830 loss: 234051.576155 time elapsed: 3.5395 learning rate: 0.000100, scenario: 0, slope: -107.9715852866881, fluctuations: 0.0\n",
      "step: 2840 loss: 233022.103169 time elapsed: 3.5518 learning rate: 0.000100, scenario: 0, slope: -107.0239841104122, fluctuations: 0.0\n",
      "step: 2850 loss: 232001.135848 time elapsed: 3.5640 learning rate: 0.000100, scenario: 0, slope: -106.0928657741027, fluctuations: 0.0\n",
      "step: 2860 loss: 230988.514926 time elapsed: 3.5765 learning rate: 0.000100, scenario: 0, slope: -105.17788925366776, fluctuations: 0.0\n",
      "step: 2870 loss: 229984.075203 time elapsed: 3.5904 learning rate: 0.000100, scenario: 0, slope: -104.2788720898952, fluctuations: 0.0\n",
      "step: 2880 loss: 228987.585458 time elapsed: 3.6035 learning rate: 0.000100, scenario: 0, slope: -103.39623228933382, fluctuations: 0.0\n",
      "step: 2890 loss: 227998.780130 time elapsed: 3.6161 learning rate: 0.000100, scenario: 0, slope: -102.53124977331527, fluctuations: 0.0\n",
      "step: 2900 loss: 227017.400840 time elapsed: 3.6285 learning rate: 0.000100, scenario: 0, slope: -101.76946676609288, fluctuations: 0.0\n",
      "step: 2910 loss: 226043.214765 time elapsed: 3.6416 learning rate: 0.000100, scenario: 0, slope: -100.86211050593933, fluctuations: 0.0\n",
      "step: 2920 loss: 225076.015311 time elapsed: 3.6542 learning rate: 0.000100, scenario: 0, slope: -100.0612435248133, fluctuations: 0.0\n",
      "step: 2930 loss: 224115.632570 time elapsed: 3.6668 learning rate: 0.000100, scenario: 0, slope: -99.28321013825004, fluctuations: 0.0\n",
      "step: 2940 loss: 223161.944629 time elapsed: 3.6795 learning rate: 0.000100, scenario: 0, slope: -98.52708414296852, fluctuations: 0.0\n",
      "step: 2950 loss: 222214.878005 time elapsed: 3.6920 learning rate: 0.000100, scenario: 0, slope: -97.79197844046385, fluctuations: 0.0\n",
      "step: 2960 loss: 221274.399891 time elapsed: 3.7043 learning rate: 0.000100, scenario: 0, slope: -97.076411475064, fluctuations: 0.0\n",
      "step: 2970 loss: 220340.511618 time elapsed: 3.7167 learning rate: 0.000100, scenario: 0, slope: -96.37778447713761, fluctuations: 0.0\n",
      "step: 2980 loss: 219413.253262 time elapsed: 3.7289 learning rate: 0.000100, scenario: 0, slope: -95.69269756848736, fluctuations: 0.0\n",
      "step: 2990 loss: 218492.705888 time elapsed: 3.7412 learning rate: 0.000100, scenario: 0, slope: -95.01733540343012, fluctuations: 0.0\n",
      "step: 3000 loss: 217578.971550 time elapsed: 3.7536 learning rate: 0.000100, scenario: 0, slope: -94.4145630740886, fluctuations: 0.0\n",
      "step: 3010 loss: 216672.145139 time elapsed: 3.7664 learning rate: 0.000100, scenario: 0, slope: -93.68015851812788, fluctuations: 0.0\n",
      "step: 3020 loss: 215772.291564 time elapsed: 3.7789 learning rate: 0.000100, scenario: 0, slope: -93.011253375508, fluctuations: 0.0\n",
      "step: 3030 loss: 214879.423849 time elapsed: 3.7914 learning rate: 0.000100, scenario: 0, slope: -92.33863004345382, fluctuations: 0.0\n",
      "step: 3040 loss: 213993.487051 time elapsed: 3.8038 learning rate: 0.000100, scenario: 0, slope: -91.66108255010523, fluctuations: 0.0\n",
      "step: 3050 loss: 213114.347478 time elapsed: 3.8163 learning rate: 0.000100, scenario: 0, slope: -90.97882990629506, fluctuations: 0.0\n",
      "step: 3060 loss: 212241.786082 time elapsed: 3.8288 learning rate: 0.000100, scenario: 0, slope: -90.29366599914313, fluctuations: 0.0\n",
      "step: 3070 loss: 211375.498984 time elapsed: 3.8415 learning rate: 0.000100, scenario: 0, slope: -89.60902581128062, fluctuations: 0.0\n",
      "step: 3080 loss: 210515.107781 time elapsed: 3.8539 learning rate: 0.000100, scenario: 0, slope: -88.92987774015697, fluctuations: 0.0\n",
      "step: 3090 loss: 209660.179150 time elapsed: 3.8667 learning rate: 0.000100, scenario: 0, slope: -88.26235491995078, fluctuations: 0.0\n",
      "step: 3100 loss: 208810.250712 time elapsed: 3.8792 learning rate: 0.000100, scenario: 0, slope: -87.67708950397002, fluctuations: 0.0\n",
      "step: 3110 loss: 207964.859674 time elapsed: 3.8923 learning rate: 0.000100, scenario: 0, slope: -86.98911272611494, fluctuations: 0.0\n",
      "step: 3120 loss: 207123.569106 time elapsed: 3.9049 learning rate: 0.000100, scenario: 0, slope: -86.3961778303486, fluctuations: 0.0\n",
      "step: 3130 loss: 206285.985095 time elapsed: 3.9176 learning rate: 0.000100, scenario: 0, slope: -85.83924927170818, fluctuations: 0.0\n",
      "step: 3140 loss: 205451.762735 time elapsed: 3.9302 learning rate: 0.000100, scenario: 0, slope: -85.32158504970916, fluctuations: 0.0\n",
      "step: 3150 loss: 204620.606552 time elapsed: 3.9428 learning rate: 0.000100, scenario: 0, slope: -84.84465064473933, fluctuations: 0.0\n",
      "step: 3160 loss: 203792.271061 time elapsed: 3.9556 learning rate: 0.000100, scenario: 0, slope: -84.40813271358606, fluctuations: 0.0\n",
      "step: 3170 loss: 202966.561652 time elapsed: 3.9682 learning rate: 0.000100, scenario: 0, slope: -84.0101271293481, fluctuations: 0.0\n",
      "step: 3180 loss: 202143.333354 time elapsed: 3.9807 learning rate: 0.000100, scenario: 0, slope: -83.64745405570135, fluctuations: 0.0\n",
      "step: 3190 loss: 201322.485214 time elapsed: 3.9932 learning rate: 0.000100, scenario: 0, slope: -83.31604667322242, fluctuations: 0.0\n",
      "step: 3200 loss: 200503.948499 time elapsed: 4.0061 learning rate: 0.000100, scenario: 0, slope: -83.04076634160485, fluctuations: 0.0\n",
      "step: 3210 loss: 199687.671116 time elapsed: 4.0189 learning rate: 0.000100, scenario: 0, slope: -82.72882536946322, fluctuations: 0.0\n",
      "step: 3220 loss: 198873.608523 time elapsed: 4.0316 learning rate: 0.000100, scenario: 0, slope: -82.46409692722717, fluctuations: 0.0\n",
      "step: 3230 loss: 198061.730977 time elapsed: 4.0442 learning rate: 0.000100, scenario: 0, slope: -82.21333397550822, fluctuations: 0.0\n",
      "step: 3240 loss: 197252.046106 time elapsed: 4.0568 learning rate: 0.000100, scenario: 0, slope: -81.97315259587191, fluctuations: 0.0\n",
      "step: 3250 loss: 196444.629089 time elapsed: 4.0693 learning rate: 0.000100, scenario: 0, slope: -81.74044249870728, fluctuations: 0.0\n",
      "step: 3260 loss: 195639.649426 time elapsed: 4.0817 learning rate: 0.000100, scenario: 0, slope: -81.51201955120065, fluctuations: 0.0\n",
      "step: 3270 loss: 194837.377651 time elapsed: 4.0942 learning rate: 0.000100, scenario: 0, slope: -81.2842216834473, fluctuations: 0.0\n",
      "step: 3280 loss: 194038.158734 time elapsed: 4.1069 learning rate: 0.000100, scenario: 0, slope: -81.05262853746727, fluctuations: 0.0\n",
      "step: 3290 loss: 193242.363360 time elapsed: 4.1195 learning rate: 0.000100, scenario: 0, slope: -80.8120749797139, fluctuations: 0.0\n",
      "step: 3300 loss: 192450.359365 time elapsed: 4.1318 learning rate: 0.000100, scenario: 0, slope: -80.58327717606964, fluctuations: 0.0\n",
      "step: 3310 loss: 191662.514030 time elapsed: 4.1447 learning rate: 0.000100, scenario: 0, slope: -80.2815689762503, fluctuations: 0.0\n",
      "step: 3320 loss: 190879.174810 time elapsed: 4.1574 learning rate: 0.000100, scenario: 0, slope: -79.98070571823101, fluctuations: 0.0\n",
      "step: 3330 loss: 190100.619652 time elapsed: 4.1701 learning rate: 0.000100, scenario: 0, slope: -79.65022294265763, fluctuations: 0.0\n",
      "step: 3340 loss: 189327.023049 time elapsed: 4.1828 learning rate: 0.000100, scenario: 0, slope: -79.28784375248458, fluctuations: 0.0\n",
      "step: 3350 loss: 188558.471564 time elapsed: 4.1955 learning rate: 0.000100, scenario: 0, slope: -78.8936102088872, fluctuations: 0.0\n",
      "step: 3360 loss: 187795.017639 time elapsed: 4.2083 learning rate: 0.000100, scenario: 0, slope: -78.46975091235434, fluctuations: 0.0\n",
      "step: 3370 loss: 187036.728924 time elapsed: 4.2207 learning rate: 0.000100, scenario: 0, slope: -78.01996664871297, fluctuations: 0.0\n",
      "step: 3380 loss: 186283.708357 time elapsed: 4.2337 learning rate: 0.000100, scenario: 0, slope: -77.54847875194028, fluctuations: 0.0\n",
      "step: 3390 loss: 185536.083080 time elapsed: 4.2463 learning rate: 0.000100, scenario: 0, slope: -77.05926440459909, fluctuations: 0.0\n",
      "step: 3400 loss: 184793.971601 time elapsed: 4.2585 learning rate: 0.000100, scenario: 0, slope: -76.60660480143599, fluctuations: 0.0\n",
      "step: 3410 loss: 184057.447430 time elapsed: 4.2716 learning rate: 0.000100, scenario: 0, slope: -76.04034649177457, fluctuations: 0.0\n",
      "step: 3420 loss: 183326.517897 time elapsed: 4.2845 learning rate: 0.000100, scenario: 0, slope: -75.51518714803437, fluctuations: 0.0\n",
      "step: 3430 loss: 182601.127928 time elapsed: 4.2971 learning rate: 0.000100, scenario: 0, slope: -74.98170241681636, fluctuations: 0.0\n",
      "step: 3440 loss: 181881.185499 time elapsed: 4.3095 learning rate: 0.000100, scenario: 0, slope: -74.44142405917414, fluctuations: 0.0\n",
      "step: 3450 loss: 181166.594911 time elapsed: 4.3220 learning rate: 0.000100, scenario: 0, slope: -73.8963098362446, fluctuations: 0.0\n",
      "step: 3460 loss: 180457.281390 time elapsed: 4.3345 learning rate: 0.000100, scenario: 0, slope: -73.34881784973113, fluctuations: 0.0\n",
      "step: 3470 loss: 179753.197072 time elapsed: 4.3471 learning rate: 0.000100, scenario: 0, slope: -72.80162674236857, fluctuations: 0.0\n",
      "step: 3480 loss: 179054.309338 time elapsed: 4.3597 learning rate: 0.000100, scenario: 0, slope: -72.25718971060257, fluctuations: 0.0\n",
      "step: 3490 loss: 178360.579894 time elapsed: 4.3721 learning rate: 0.000100, scenario: 0, slope: -71.71735548756355, fluctuations: 0.0\n",
      "step: 3500 loss: 177671.944150 time elapsed: 4.3844 learning rate: 0.000100, scenario: 0, slope: -71.23636776690672, fluctuations: 0.0\n",
      "step: 3510 loss: 176988.297438 time elapsed: 4.3977 learning rate: 0.000100, scenario: 0, slope: -70.6553422432596, fluctuations: 0.0\n",
      "step: 3520 loss: 176309.491066 time elapsed: 4.4106 learning rate: 0.000100, scenario: 0, slope: -70.13404125300754, fluctuations: 0.0\n",
      "step: 3530 loss: 175635.338794 time elapsed: 4.4228 learning rate: 0.000100, scenario: 0, slope: -69.61995739022564, fluctuations: 0.0\n",
      "step: 3540 loss: 174965.632624 time elapsed: 4.4352 learning rate: 0.000100, scenario: 0, slope: -69.11432001243806, fluctuations: 0.0\n",
      "step: 3550 loss: 174300.164595 time elapsed: 4.4477 learning rate: 0.000100, scenario: 0, slope: -68.61898965564595, fluctuations: 0.0\n",
      "step: 3560 loss: 173638.749673 time elapsed: 4.4601 learning rate: 0.000100, scenario: 0, slope: -68.13617998487177, fluctuations: 0.0\n",
      "step: 3570 loss: 172981.245731 time elapsed: 4.4728 learning rate: 0.000100, scenario: 0, slope: -67.66796646456977, fluctuations: 0.0\n",
      "step: 3580 loss: 172327.568320 time elapsed: 4.4854 learning rate: 0.000100, scenario: 0, slope: -67.2157385466092, fluctuations: 0.0\n",
      "step: 3590 loss: 171677.699400 time elapsed: 4.4982 learning rate: 0.000100, scenario: 0, slope: -66.7797415308614, fluctuations: 0.0\n",
      "step: 3600 loss: 171031.689948 time elapsed: 4.5106 learning rate: 0.000100, scenario: 0, slope: -66.40028304379004, fluctuations: 0.0\n",
      "step: 3610 loss: 170389.656672 time elapsed: 4.5234 learning rate: 0.000100, scenario: 0, slope: -65.95027771423356, fluctuations: 0.0\n",
      "step: 3620 loss: 169751.772448 time elapsed: 4.5363 learning rate: 0.000100, scenario: 0, slope: -65.55021638746861, fluctuations: 0.0\n",
      "step: 3630 loss: 169118.249625 time elapsed: 4.5491 learning rate: 0.000100, scenario: 0, slope: -65.15371192629755, fluctuations: 0.0\n",
      "step: 3640 loss: 168489.315414 time elapsed: 4.5619 learning rate: 0.000100, scenario: 0, slope: -64.75539709658442, fluctuations: 0.0\n",
      "step: 3650 loss: 167865.179364 time elapsed: 4.5745 learning rate: 0.000100, scenario: 0, slope: -64.35003636722473, fluctuations: 0.0\n",
      "step: 3660 loss: 167245.995581 time elapsed: 4.5871 learning rate: 0.000100, scenario: 0, slope: -63.93317083535736, fluctuations: 0.0\n",
      "step: 3670 loss: 166631.826610 time elapsed: 4.5999 learning rate: 0.000100, scenario: 0, slope: -63.501762451972205, fluctuations: 0.0\n",
      "step: 3680 loss: 166022.619630 time elapsed: 4.6128 learning rate: 0.000100, scenario: 0, slope: -63.05475416910704, fluctuations: 0.0\n",
      "step: 3690 loss: 165418.205497 time elapsed: 4.6249 learning rate: 0.000100, scenario: 0, slope: -62.59342669968861, fluctuations: 0.0\n",
      "step: 3700 loss: 164818.324675 time elapsed: 4.6373 learning rate: 0.000100, scenario: 0, slope: -62.16895705840637, fluctuations: 0.0\n",
      "step: 3710 loss: 164222.672758 time elapsed: 4.6505 learning rate: 0.000100, scenario: 0, slope: -61.64437026661385, fluctuations: 0.0\n",
      "step: 3720 loss: 163630.948855 time elapsed: 4.6632 learning rate: 0.000100, scenario: 0, slope: -61.169104278087765, fluctuations: 0.0\n",
      "step: 3730 loss: 163042.890069 time elapsed: 4.6758 learning rate: 0.000100, scenario: 0, slope: -60.702685626815224, fluctuations: 0.0\n",
      "step: 3740 loss: 162458.284830 time elapsed: 4.6884 learning rate: 0.000100, scenario: 0, slope: -60.251386564177345, fluctuations: 0.0\n",
      "step: 3750 loss: 161876.969085 time elapsed: 4.7009 learning rate: 0.000100, scenario: 0, slope: -59.81988437209889, fluctuations: 0.0\n",
      "step: 3760 loss: 161298.814615 time elapsed: 4.7134 learning rate: 0.000100, scenario: 0, slope: -59.41080150526982, fluctuations: 0.0\n",
      "step: 3770 loss: 160723.717145 time elapsed: 4.7259 learning rate: 0.000100, scenario: 0, slope: -59.02465595906352, fluctuations: 0.0\n",
      "step: 3780 loss: 160151.587656 time elapsed: 4.7384 learning rate: 0.000100, scenario: 0, slope: -58.660193911012165, fluctuations: 0.0\n",
      "step: 3790 loss: 159582.347091 time elapsed: 4.7510 learning rate: 0.000100, scenario: 0, slope: -58.31498969901229, fluctuations: 0.0\n",
      "step: 3800 loss: 159015.923409 time elapsed: 4.7634 learning rate: 0.000100, scenario: 0, slope: -58.01836644331625, fluctuations: 0.0\n",
      "step: 3810 loss: 158452.249938 time elapsed: 4.7763 learning rate: 0.000100, scenario: 0, slope: -57.670818728823924, fluctuations: 0.0\n",
      "step: 3820 loss: 157891.264396 time elapsed: 4.7889 learning rate: 0.000100, scenario: 0, slope: -57.36669976430169, fluctuations: 0.0\n",
      "step: 3830 loss: 157332.908256 time elapsed: 4.8014 learning rate: 0.000100, scenario: 0, slope: -57.072006275204224, fluctuations: 0.0\n",
      "step: 3840 loss: 156777.126333 time elapsed: 4.8139 learning rate: 0.000100, scenario: 0, slope: -56.78548040064755, fluctuations: 0.0\n",
      "step: 3850 loss: 156223.866545 time elapsed: 4.8264 learning rate: 0.000100, scenario: 0, slope: -56.50624126483699, fluctuations: 0.0\n",
      "step: 3860 loss: 155673.079812 time elapsed: 4.8390 learning rate: 0.000100, scenario: 0, slope: -56.23365180393658, fluctuations: 0.0\n",
      "step: 3870 loss: 155124.720076 time elapsed: 4.8517 learning rate: 0.000100, scenario: 0, slope: -55.96722067565563, fluctuations: 0.0\n",
      "step: 3880 loss: 154578.744386 time elapsed: 4.8642 learning rate: 0.000100, scenario: 0, slope: -55.70654047636354, fluctuations: 0.0\n",
      "step: 3890 loss: 154035.113002 time elapsed: 4.8767 learning rate: 0.000100, scenario: 0, slope: -55.451251409769164, fluctuations: 0.0\n",
      "step: 3900 loss: 153493.789443 time elapsed: 4.8890 learning rate: 0.000100, scenario: 0, slope: -55.225824589692394, fluctuations: 0.0\n",
      "step: 3910 loss: 152954.740430 time elapsed: 4.9020 learning rate: 0.000100, scenario: 0, slope: -54.95552520605033, fluctuations: 0.0\n",
      "step: 3920 loss: 152417.935657 time elapsed: 4.9145 learning rate: 0.000100, scenario: 0, slope: -54.714452944871226, fluctuations: 0.0\n",
      "step: 3930 loss: 151883.347359 time elapsed: 4.9269 learning rate: 0.000100, scenario: 0, slope: -54.477491981020506, fluctuations: 0.0\n",
      "step: 3940 loss: 151350.949674 time elapsed: 4.9395 learning rate: 0.000100, scenario: 0, slope: -54.24433732545301, fluctuations: 0.0\n",
      "step: 3950 loss: 150820.717811 time elapsed: 4.9521 learning rate: 0.000100, scenario: 0, slope: -54.01469714048421, fluctuations: 0.0\n",
      "step: 3960 loss: 150292.627108 time elapsed: 4.9646 learning rate: 0.000100, scenario: 0, slope: -53.78830368098463, fluctuations: 0.0\n",
      "step: 3970 loss: 149766.652031 time elapsed: 4.9772 learning rate: 0.000100, scenario: 0, slope: -53.56492678930554, fluctuations: 0.0\n",
      "step: 3980 loss: 149242.765231 time elapsed: 4.9897 learning rate: 0.000100, scenario: 0, slope: -53.34438814833666, fluctuations: 0.0\n",
      "step: 3990 loss: 148720.936736 time elapsed: 5.0023 learning rate: 0.000100, scenario: 0, slope: -53.12657424143728, fluctuations: 0.0\n",
      "step: 4000 loss: 148201.133333 time elapsed: 5.0145 learning rate: 0.000100, scenario: 0, slope: -52.93283749101426, fluctuations: 0.0\n",
      "step: 4010 loss: 147683.318161 time elapsed: 5.0276 learning rate: 0.000100, scenario: 0, slope: -52.69904399912679, fluctuations: 0.0\n",
      "step: 4020 loss: 147167.450487 time elapsed: 5.0402 learning rate: 0.000100, scenario: 0, slope: -52.48948737898672, fluctuations: 0.0\n",
      "step: 4030 loss: 146653.485605 time elapsed: 5.0529 learning rate: 0.000100, scenario: 0, slope: -52.28296857706145, fluctuations: 0.0\n",
      "step: 4040 loss: 146141.374766 time elapsed: 5.0658 learning rate: 0.000100, scenario: 0, slope: -52.079743246249976, fluctuations: 0.0\n",
      "step: 4050 loss: 145631.065070 time elapsed: 5.0786 learning rate: 0.000100, scenario: 0, slope: -51.88011840087941, fluctuations: 0.0\n",
      "step: 4060 loss: 145122.499240 time elapsed: 5.0912 learning rate: 0.000100, scenario: 0, slope: -51.6844406903374, fluctuations: 0.0\n",
      "step: 4070 loss: 144615.615277 time elapsed: 5.1039 learning rate: 0.000100, scenario: 0, slope: -51.49308694872034, fluctuations: 0.0\n",
      "step: 4080 loss: 144110.345997 time elapsed: 5.1167 learning rate: 0.000100, scenario: 0, slope: -51.30645847631739, fluctuations: 0.0\n",
      "step: 4090 loss: 143606.618506 time elapsed: 5.1293 learning rate: 0.000100, scenario: 0, slope: -51.12497954430879, fluctuations: 0.0\n",
      "step: 4100 loss: 143104.353693 time elapsed: 5.1417 learning rate: 0.000100, scenario: 0, slope: -50.9664222084656, fluctuations: 0.0\n",
      "step: 4110 loss: 142603.465815 time elapsed: 5.1547 learning rate: 0.000100, scenario: 0, slope: -50.77929742263603, fluctuations: 0.0\n",
      "step: 4120 loss: 142103.862227 time elapsed: 5.1673 learning rate: 0.000100, scenario: 0, slope: -50.6160861745273, fluctuations: 0.0\n",
      "step: 4130 loss: 141605.443279 time elapsed: 5.1800 learning rate: 0.000100, scenario: 0, slope: -50.460015936448386, fluctuations: 0.0\n",
      "step: 4140 loss: 141108.102356 time elapsed: 5.1925 learning rate: 0.000100, scenario: 0, slope: -50.31167383607465, fluctuations: 0.0\n",
      "step: 4150 loss: 140611.725985 time elapsed: 5.2051 learning rate: 0.000100, scenario: 0, slope: -50.17168044627014, fluctuations: 0.0\n",
      "step: 4160 loss: 140116.193890 time elapsed: 5.2177 learning rate: 0.000100, scenario: 0, slope: -50.04068346325679, fluctuations: 0.0\n",
      "step: 4170 loss: 139621.378868 time elapsed: 5.2302 learning rate: 0.000100, scenario: 0, slope: -49.9193504227274, fluctuations: 0.0\n",
      "step: 4180 loss: 139127.146328 time elapsed: 5.2427 learning rate: 0.000100, scenario: 0, slope: -49.80836303571337, fluctuations: 0.0\n",
      "step: 4190 loss: 138633.353406 time elapsed: 5.2549 learning rate: 0.000100, scenario: 0, slope: -49.708415977784725, fluctuations: 0.0\n",
      "step: 4200 loss: 138139.847618 time elapsed: 5.2670 learning rate: 0.000100, scenario: 0, slope: -49.62849202589836, fluctuations: 0.0\n",
      "step: 4210 loss: 137646.465230 time elapsed: 5.2801 learning rate: 0.000100, scenario: 0, slope: -49.544527752443464, fluctuations: 0.0\n",
      "step: 4220 loss: 137153.029868 time elapsed: 5.2927 learning rate: 0.000100, scenario: 0, slope: -49.482128036036926, fluctuations: 0.0\n",
      "step: 4230 loss: 136659.352683 time elapsed: 5.3051 learning rate: 0.000100, scenario: 0, slope: -49.43388834905237, fluctuations: 0.0\n",
      "step: 4240 loss: 136165.236909 time elapsed: 5.3174 learning rate: 0.000100, scenario: 0, slope: -49.40074054158002, fluctuations: 0.0\n",
      "step: 4250 loss: 135670.491315 time elapsed: 5.3302 learning rate: 0.000100, scenario: 0, slope: -49.38362686373815, fluctuations: 0.0\n",
      "step: 4260 loss: 135174.954455 time elapsed: 5.3428 learning rate: 0.000100, scenario: 0, slope: -49.383342321837745, fluctuations: 0.0\n",
      "step: 4270 loss: 134678.516282 time elapsed: 5.3553 learning rate: 0.000100, scenario: 0, slope: -49.400273676636296, fluctuations: 0.0\n",
      "step: 4280 loss: 134181.113338 time elapsed: 5.3676 learning rate: 0.000100, scenario: 0, slope: -49.4341578671626, fluctuations: 0.0\n",
      "step: 4290 loss: 133682.703297 time elapsed: 5.3800 learning rate: 0.000100, scenario: 0, slope: -49.48401525621349, fluctuations: 0.0\n",
      "step: 4300 loss: 133183.242862 time elapsed: 5.3925 learning rate: 0.000100, scenario: 0, slope: -49.54125355440927, fluctuations: 0.0\n",
      "step: 4310 loss: 132682.674670 time elapsed: 5.4058 learning rate: 0.000100, scenario: 0, slope: -49.62501713128473, fluctuations: 0.0\n",
      "step: 4320 loss: 132180.926422 time elapsed: 5.4185 learning rate: 0.000100, scenario: 0, slope: -49.71227693771064, fluctuations: 0.0\n",
      "step: 4330 loss: 131677.936700 time elapsed: 5.4312 learning rate: 0.000100, scenario: 0, slope: -49.80826133052062, fluctuations: 0.0\n",
      "step: 4340 loss: 131173.698260 time elapsed: 5.4444 learning rate: 0.000100, scenario: 0, slope: -49.91136711142014, fluctuations: 0.0\n",
      "step: 4350 loss: 130668.285035 time elapsed: 5.4568 learning rate: 0.000100, scenario: 0, slope: -50.01999478487204, fluctuations: 0.0\n",
      "step: 4360 loss: 130161.858828 time elapsed: 5.4695 learning rate: 0.000100, scenario: 0, slope: -50.13224493283018, fluctuations: 0.0\n",
      "step: 4370 loss: 129654.672105 time elapsed: 5.4822 learning rate: 0.000100, scenario: 0, slope: -50.24551291659397, fluctuations: 0.0\n",
      "step: 4380 loss: 129147.071342 time elapsed: 5.4950 learning rate: 0.000100, scenario: 0, slope: -50.35606365312479, fluctuations: 0.0\n",
      "step: 4390 loss: 128639.497098 time elapsed: 5.5076 learning rate: 0.000100, scenario: 0, slope: -50.458763830239974, fluctuations: 0.0\n",
      "step: 4400 loss: 128132.476542 time elapsed: 5.5201 learning rate: 0.000100, scenario: 0, slope: -50.53910271483689, fluctuations: 0.0\n",
      "step: 4410 loss: 127626.604944 time elapsed: 5.5330 learning rate: 0.000100, scenario: 0, slope: -50.61332795549531, fluctuations: 0.0\n",
      "step: 4420 loss: 127122.513577 time elapsed: 5.5455 learning rate: 0.000100, scenario: 0, slope: -50.649299965450936, fluctuations: 0.0\n",
      "step: 4430 loss: 126620.823230 time elapsed: 5.5584 learning rate: 0.000100, scenario: 0, slope: -50.646996839626496, fluctuations: 0.0\n",
      "step: 4440 loss: 126122.085859 time elapsed: 5.5709 learning rate: 0.000100, scenario: 0, slope: -50.59950230927185, fluctuations: 0.0\n",
      "step: 4450 loss: 125626.721780 time elapsed: 5.5832 learning rate: 0.000100, scenario: 0, slope: -50.501822667444685, fluctuations: 0.0\n",
      "step: 4460 loss: 125134.965084 time elapsed: 5.5956 learning rate: 0.000100, scenario: 0, slope: -50.351744667936316, fluctuations: 0.0\n",
      "step: 4470 loss: 124646.833129 time elapsed: 5.6084 learning rate: 0.000100, scenario: 0, slope: -50.150599153592935, fluctuations: 0.0\n",
      "step: 4480 loss: 124162.133284 time elapsed: 5.6212 learning rate: 0.000100, scenario: 0, slope: -49.90370381629846, fluctuations: 0.0\n",
      "step: 4490 loss: 123680.509791 time elapsed: 5.6338 learning rate: 0.000100, scenario: 0, slope: -49.62026049735643, fluctuations: 0.0\n",
      "step: 4500 loss: 123201.518697 time elapsed: 5.6465 learning rate: 0.000100, scenario: 0, slope: -49.34405285335539, fluctuations: 0.0\n",
      "step: 4510 loss: 122724.707354 time elapsed: 5.6592 learning rate: 0.000100, scenario: 0, slope: -48.99477025590372, fluctuations: 0.0\n",
      "step: 4520 loss: 122249.675086 time elapsed: 5.6721 learning rate: 0.000100, scenario: 0, slope: -48.68090840842431, fluctuations: 0.0\n",
      "step: 4530 loss: 121776.103010 time elapsed: 5.6848 learning rate: 0.000100, scenario: 0, slope: -48.38342433458216, fluctuations: 0.0\n",
      "step: 4540 loss: 121303.755617 time elapsed: 5.6973 learning rate: 0.000100, scenario: 0, slope: -48.11167657029564, fluctuations: 0.0\n",
      "step: 4550 loss: 120832.465537 time elapsed: 5.7099 learning rate: 0.000100, scenario: 0, slope: -47.87119905764174, fluctuations: 0.0\n",
      "step: 4560 loss: 120362.113410 time elapsed: 5.7225 learning rate: 0.000100, scenario: 0, slope: -47.663647961541216, fluctuations: 0.0\n",
      "step: 4570 loss: 119892.610266 time elapsed: 5.7349 learning rate: 0.000100, scenario: 0, slope: -47.48739966488414, fluctuations: 0.0\n",
      "step: 4580 loss: 119423.885042 time elapsed: 5.7471 learning rate: 0.000100, scenario: 0, slope: -47.33859580195919, fluctuations: 0.0\n",
      "step: 4590 loss: 118955.876808 time elapsed: 5.7597 learning rate: 0.000100, scenario: 0, slope: -47.2123395941638, fluctuations: 0.0\n",
      "step: 4600 loss: 118488.530323 time elapsed: 5.7720 learning rate: 0.000100, scenario: 0, slope: -47.113936182257625, fluctuations: 0.0\n",
      "step: 4610 loss: 118021.793557 time elapsed: 5.7850 learning rate: 0.000100, scenario: 0, slope: -47.008635003910726, fluctuations: 0.0\n",
      "step: 4620 loss: 117555.616234 time elapsed: 5.7977 learning rate: 0.000100, scenario: 0, slope: -46.92383947921888, fluctuations: 0.0\n",
      "step: 4630 loss: 117089.948823 time elapsed: 5.8105 learning rate: 0.000100, scenario: 0, slope: -46.84717469913119, fluctuations: 0.0\n",
      "step: 4640 loss: 116624.741643 time elapsed: 5.8232 learning rate: 0.000100, scenario: 0, slope: -46.77725257538418, fluctuations: 0.0\n",
      "step: 4650 loss: 116159.943881 time elapsed: 5.8360 learning rate: 0.000100, scenario: 0, slope: -46.71324943785048, fluctuations: 0.0\n",
      "step: 4660 loss: 115695.502443 time elapsed: 5.8488 learning rate: 0.000100, scenario: 0, slope: -46.65471227230498, fluctuations: 0.0\n",
      "step: 4670 loss: 115231.360714 time elapsed: 5.8612 learning rate: 0.000100, scenario: 0, slope: -46.601427534907785, fluctuations: 0.0\n",
      "step: 4680 loss: 114767.457457 time elapsed: 5.8740 learning rate: 0.000100, scenario: 0, slope: -46.553345813940496, fluctuations: 0.0\n",
      "step: 4690 loss: 114303.726069 time elapsed: 5.8869 learning rate: 0.000100, scenario: 0, slope: -46.51054381962987, fluctuations: 0.0\n",
      "step: 4700 loss: 113840.094138 time elapsed: 5.8992 learning rate: 0.000100, scenario: 0, slope: -46.47668663494097, fluctuations: 0.0\n",
      "step: 4710 loss: 113376.483092 time elapsed: 5.9123 learning rate: 0.000100, scenario: 0, slope: -46.44161461160361, fluctuations: 0.0\n",
      "step: 4720 loss: 112912.807928 time elapsed: 5.9249 learning rate: 0.000100, scenario: 0, slope: -46.41614067291617, fluctuations: 0.0\n",
      "step: 4730 loss: 112448.977306 time elapsed: 5.9377 learning rate: 0.000100, scenario: 0, slope: -46.39723401776531, fluctuations: 0.0\n",
      "step: 4740 loss: 111984.894255 time elapsed: 5.9504 learning rate: 0.000100, scenario: 0, slope: -46.38540712520058, fluctuations: 0.0\n",
      "step: 4750 loss: 111520.457677 time elapsed: 5.9628 learning rate: 0.000100, scenario: 0, slope: -46.38121172822903, fluctuations: 0.0\n",
      "step: 4760 loss: 111055.564744 time elapsed: 5.9754 learning rate: 0.000100, scenario: 0, slope: -46.3852065765601, fluctuations: 0.0\n",
      "step: 4770 loss: 110590.114332 time elapsed: 5.9882 learning rate: 0.000100, scenario: 0, slope: -46.39791577670483, fluctuations: 0.0\n",
      "step: 4780 loss: 110124.011652 time elapsed: 6.0008 learning rate: 0.000100, scenario: 0, slope: -46.41977648012806, fluctuations: 0.0\n",
      "step: 4790 loss: 109657.174271 time elapsed: 6.0134 learning rate: 0.000100, scenario: 0, slope: -46.45107338675714, fluctuations: 0.0\n",
      "step: 4800 loss: 109189.539696 time elapsed: 6.0257 learning rate: 0.000100, scenario: 0, slope: -46.48735687156484, fluctuations: 0.0\n",
      "step: 4810 loss: 108721.074597 time elapsed: 6.0386 learning rate: 0.000100, scenario: 0, slope: -46.54184630288946, fluctuations: 0.0\n",
      "step: 4820 loss: 108251.785577 time elapsed: 6.0510 learning rate: 0.000100, scenario: 0, slope: -46.60031231506063, fluctuations: 0.0\n",
      "step: 4830 loss: 107781.731138 time elapsed: 6.0633 learning rate: 0.000100, scenario: 0, slope: -46.66595465855488, fluctuations: 0.0\n",
      "step: 4840 loss: 107311.034014 time elapsed: 6.0759 learning rate: 0.000100, scenario: 0, slope: -46.73676905677053, fluctuations: 0.0\n",
      "step: 4850 loss: 106839.892453 time elapsed: 6.0905 learning rate: 0.000100, scenario: 0, slope: -46.809922339173724, fluctuations: 0.0\n",
      "step: 4860 loss: 106368.588259 time elapsed: 6.1033 learning rate: 0.000100, scenario: 0, slope: -46.88165262359545, fluctuations: 0.0\n",
      "step: 4870 loss: 105897.488502 time elapsed: 6.1163 learning rate: 0.000100, scenario: 0, slope: -46.94722442081579, fluctuations: 0.0\n",
      "step: 4880 loss: 105427.037082 time elapsed: 6.1291 learning rate: 0.000100, scenario: 0, slope: -47.0009791878256, fluctuations: 0.0\n",
      "step: 4890 loss: 104957.732125 time elapsed: 6.1418 learning rate: 0.000100, scenario: 0, slope: -47.036530881339964, fluctuations: 0.0\n",
      "step: 4900 loss: 104490.086287 time elapsed: 6.1543 learning rate: 0.000100, scenario: 0, slope: -47.04740566526529, fluctuations: 0.0\n",
      "step: 4910 loss: 104024.570408 time elapsed: 6.1677 learning rate: 0.000100, scenario: 0, slope: -47.02642473707848, fluctuations: 0.0\n",
      "step: 4920 loss: 103561.547128 time elapsed: 6.1803 learning rate: 0.000100, scenario: 0, slope: -46.96903438356277, fluctuations: 0.0\n",
      "step: 4930 loss: 103101.209201 time elapsed: 6.1929 learning rate: 0.000100, scenario: 0, slope: -46.871814447329626, fluctuations: 0.0\n",
      "step: 4940 loss: 102643.543427 time elapsed: 6.2054 learning rate: 0.000100, scenario: 0, slope: -46.73466443456068, fluctuations: 0.0\n",
      "step: 4950 loss: 102188.338800 time elapsed: 6.2179 learning rate: 0.000100, scenario: 0, slope: -46.56118533948985, fluctuations: 0.0\n",
      "step: 4960 loss: 101735.242487 time elapsed: 6.2305 learning rate: 0.000100, scenario: 0, slope: -46.358719536821724, fluctuations: 0.0\n",
      "step: 4970 loss: 101283.844889 time elapsed: 6.2431 learning rate: 0.000100, scenario: 0, slope: -46.13765263304364, fluctuations: 0.0\n",
      "step: 4980 loss: 100833.760791 time elapsed: 6.2558 learning rate: 0.000100, scenario: 0, slope: -45.910057697354446, fluctuations: 0.0\n",
      "step: 4990 loss: 100384.679731 time elapsed: 6.2711 learning rate: 0.000100, scenario: 0, slope: -45.68798110404237, fluctuations: 0.0\n",
      "step: 5000 loss: 99936.379591 time elapsed: 6.2846 learning rate: 0.000100, scenario: 0, slope: -45.50143774203436, fluctuations: 0.0\n",
      "step: 5010 loss: 99488.715414 time elapsed: 6.2980 learning rate: 0.000100, scenario: 0, slope: -45.29877866417644, fluctuations: 0.0\n",
      "step: 5020 loss: 99041.599741 time elapsed: 6.3110 learning rate: 0.000100, scenario: 0, slope: -45.14273664848025, fluctuations: 0.0\n",
      "step: 5030 loss: 98594.984980 time elapsed: 6.3235 learning rate: 0.000100, scenario: 0, slope: -45.01377515078899, fluctuations: 0.0\n",
      "step: 5040 loss: 98148.850948 time elapsed: 6.3361 learning rate: 0.000100, scenario: 0, slope: -44.909127936547264, fluctuations: 0.0\n",
      "step: 5050 loss: 97703.196764 time elapsed: 6.3488 learning rate: 0.000100, scenario: 0, slope: -44.82423024314254, fluctuations: 0.0\n",
      "step: 5060 loss: 97258.035344 time elapsed: 6.3614 learning rate: 0.000100, scenario: 0, slope: -44.753902468862606, fluctuations: 0.0\n",
      "step: 5070 loss: 96813.389227 time elapsed: 6.3739 learning rate: 0.000100, scenario: 0, slope: -44.6932950161871, fluctuations: 0.0\n",
      "step: 5080 loss: 96369.287146 time elapsed: 6.3865 learning rate: 0.000100, scenario: 0, slope: -44.63841605683062, fluctuations: 0.0\n",
      "step: 5090 loss: 95925.761111 time elapsed: 6.3993 learning rate: 0.000100, scenario: 0, slope: -44.5862612840682, fluctuations: 0.0\n",
      "step: 5100 loss: 95482.843953 time elapsed: 6.4117 learning rate: 0.000100, scenario: 0, slope: -44.53987314774912, fluctuations: 0.0\n",
      "step: 5110 loss: 95040.567284 time elapsed: 6.4245 learning rate: 0.000100, scenario: 0, slope: -44.48226457306875, fluctuations: 0.0\n",
      "step: 5120 loss: 94598.959834 time elapsed: 6.4369 learning rate: 0.000100, scenario: 0, slope: -44.42800063866776, fluctuations: 0.0\n",
      "step: 5130 loss: 94158.046132 time elapsed: 6.4492 learning rate: 0.000100, scenario: 0, slope: -44.371315828765034, fluctuations: 0.0\n",
      "step: 5140 loss: 93717.845507 time elapsed: 6.4618 learning rate: 0.000100, scenario: 0, slope: -44.31191161284557, fluctuations: 0.0\n",
      "step: 5150 loss: 93278.371411 time elapsed: 6.4742 learning rate: 0.000100, scenario: 0, slope: -44.249724669398844, fluctuations: 0.0\n",
      "step: 5160 loss: 92839.631034 time elapsed: 6.4870 learning rate: 0.000100, scenario: 0, slope: -44.18488318391286, fluctuations: 0.0\n",
      "step: 5170 loss: 92401.625213 time elapsed: 6.4998 learning rate: 0.000100, scenario: 0, slope: -44.11766876961223, fluctuations: 0.0\n",
      "step: 5180 loss: 91964.348547 time elapsed: 6.5125 learning rate: 0.000100, scenario: 0, slope: -44.048481631361135, fluctuations: 0.0\n",
      "step: 5190 loss: 91527.789683 time elapsed: 6.5251 learning rate: 0.000100, scenario: 0, slope: -43.977808522813206, fluctuations: 0.0\n",
      "step: 5200 loss: 91091.931694 time elapsed: 6.5374 learning rate: 0.000100, scenario: 0, slope: -43.91338127945378, fluctuations: 0.0\n",
      "step: 5210 loss: 90656.752512 time elapsed: 6.5506 learning rate: 0.000100, scenario: 0, slope: -43.83421316058986, fluctuations: 0.0\n",
      "step: 5220 loss: 90222.225412 time elapsed: 6.5631 learning rate: 0.000100, scenario: 0, slope: -43.76245259139401, fluctuations: 0.0\n",
      "step: 5230 loss: 89788.319590 time elapsed: 6.5756 learning rate: 0.000100, scenario: 0, slope: -43.691488802755316, fluctuations: 0.0\n",
      "step: 5240 loss: 89355.000892 time elapsed: 6.5881 learning rate: 0.000100, scenario: 0, slope: -43.621874432732405, fluctuations: 0.0\n",
      "step: 5250 loss: 88922.232821 time elapsed: 6.6010 learning rate: 0.000100, scenario: 0, slope: -43.554124623647446, fluctuations: 0.0\n",
      "step: 5260 loss: 88489.977953 time elapsed: 6.6138 learning rate: 0.000100, scenario: 0, slope: -43.48870382722539, fluctuations: 0.0\n",
      "step: 5270 loss: 88058.199884 time elapsed: 6.6265 learning rate: 0.000100, scenario: 0, slope: -43.42601028262059, fluctuations: 0.0\n",
      "step: 5280 loss: 87626.865799 time elapsed: 6.6391 learning rate: 0.000100, scenario: 0, slope: -43.36635563614105, fluctuations: 0.0\n",
      "step: 5290 loss: 87195.949626 time elapsed: 6.6515 learning rate: 0.000100, scenario: 0, slope: -43.309937859253324, fluctuations: 0.0\n",
      "step: 5300 loss: 86765.435531 time elapsed: 6.6640 learning rate: 0.000100, scenario: 0, slope: -43.26197469324031, fluctuations: 0.0\n",
      "step: 5310 loss: 86335.321242 time elapsed: 6.6780 learning rate: 0.000100, scenario: 0, slope: -43.206828933896766, fluctuations: 0.0\n",
      "step: 5320 loss: 85905.620426 time elapsed: 6.6908 learning rate: 0.000100, scenario: 0, slope: -43.15964642293981, fluctuations: 0.0\n",
      "step: 5330 loss: 85476.363242 time elapsed: 6.7032 learning rate: 0.000100, scenario: 0, slope: -43.114659804245285, fluctuations: 0.0\n",
      "step: 5340 loss: 85047.594423 time elapsed: 6.7157 learning rate: 0.000100, scenario: 0, slope: -43.07102757922174, fluctuations: 0.0\n",
      "step: 5350 loss: 84619.368823 time elapsed: 6.7285 learning rate: 0.000100, scenario: 0, slope: -43.02770483456748, fluctuations: 0.0\n",
      "step: 5360 loss: 84191.745178 time elapsed: 6.7413 learning rate: 0.000100, scenario: 0, slope: -42.98352150552988, fluctuations: 0.0\n",
      "step: 5370 loss: 83764.779425 time elapsed: 6.7538 learning rate: 0.000100, scenario: 0, slope: -42.93729527868446, fluctuations: 0.0\n",
      "step: 5380 loss: 83338.519045 time elapsed: 6.7667 learning rate: 0.000100, scenario: 0, slope: -42.88796308041405, fluctuations: 0.0\n",
      "step: 5390 loss: 82912.999446 time elapsed: 6.7793 learning rate: 0.000100, scenario: 0, slope: -42.834707940927224, fluctuations: 0.0\n",
      "step: 5400 loss: 82488.242668 time elapsed: 6.7918 learning rate: 0.000100, scenario: 0, slope: -42.78302522334655, fluctuations: 0.0\n",
      "step: 5410 loss: 82064.258010 time elapsed: 6.8048 learning rate: 0.000100, scenario: 0, slope: -42.71493097333856, fluctuations: 0.0\n",
      "step: 5420 loss: 81641.043869 time elapsed: 6.8174 learning rate: 0.000100, scenario: 0, slope: -42.64863819998378, fluctuations: 0.0\n",
      "step: 5430 loss: 81218.590014 time elapsed: 6.8299 learning rate: 0.000100, scenario: 0, slope: -42.578813368050426, fluctuations: 0.0\n",
      "step: 5440 loss: 80796.879752 time elapsed: 6.8426 learning rate: 0.000100, scenario: 0, slope: -42.506319067756046, fluctuations: 0.0\n",
      "step: 5450 loss: 80375.891635 time elapsed: 6.8553 learning rate: 0.000100, scenario: 0, slope: -42.4321304957031, fluctuations: 0.0\n",
      "step: 5460 loss: 79955.600615 time elapsed: 6.8682 learning rate: 0.000100, scenario: 0, slope: -42.357227675661164, fluctuations: 0.0\n",
      "step: 5470 loss: 79535.978639 time elapsed: 6.8806 learning rate: 0.000100, scenario: 0, slope: -42.28251374409181, fluctuations: 0.0\n",
      "step: 5480 loss: 79116.994790 time elapsed: 6.8935 learning rate: 0.000100, scenario: 0, slope: -42.20876856760288, fluctuations: 0.0\n",
      "step: 5490 loss: 78698.615098 time elapsed: 6.9061 learning rate: 0.000100, scenario: 0, slope: -42.13663695769251, fluctuations: 0.0\n",
      "step: 5500 loss: 78280.802140 time elapsed: 6.9185 learning rate: 0.000100, scenario: 0, slope: -42.07353367530635, fluctuations: 0.0\n",
      "step: 5510 loss: 77863.514567 time elapsed: 6.9313 learning rate: 0.000100, scenario: 0, slope: -41.999222717197846, fluctuations: 0.0\n",
      "step: 5520 loss: 77446.706688 time elapsed: 6.9437 learning rate: 0.000100, scenario: 0, slope: -41.93475455791114, fluctuations: 0.0\n",
      "step: 5530 loss: 77030.328243 time elapsed: 6.9563 learning rate: 0.000100, scenario: 0, slope: -41.873596185190856, fluctuations: 0.0\n",
      "step: 5540 loss: 76614.324824 time elapsed: 6.9688 learning rate: 0.000100, scenario: 0, slope: -41.81610541683098, fluctuations: 0.0\n",
      "step: 5550 loss: 76198.637170 time elapsed: 6.9811 learning rate: 0.000100, scenario: 0, slope: -41.762652233700074, fluctuations: 0.0\n",
      "step: 5560 loss: 75783.204734 time elapsed: 6.9937 learning rate: 0.000100, scenario: 0, slope: -41.71362301938851, fluctuations: 0.0\n",
      "step: 5570 loss: 75367.966460 time elapsed: 7.0063 learning rate: 0.000100, scenario: 0, slope: -41.669395982017804, fluctuations: 0.0\n",
      "step: 5580 loss: 74952.864328 time elapsed: 7.0188 learning rate: 0.000100, scenario: 0, slope: -41.63031509508046, fluctuations: 0.0\n",
      "step: 5590 loss: 74537.848773 time elapsed: 7.0314 learning rate: 0.000100, scenario: 0, slope: -41.596640094580835, fluctuations: 0.0\n",
      "step: 5600 loss: 74122.885254 time elapsed: 7.0440 learning rate: 0.000100, scenario: 0, slope: -41.571047400481746, fluctuations: 0.0\n",
      "step: 5610 loss: 73707.962877 time elapsed: 7.0571 learning rate: 0.000100, scenario: 0, slope: -41.545692096496076, fluctuations: 0.0\n",
      "step: 5620 loss: 73293.104853 time elapsed: 7.0699 learning rate: 0.000100, scenario: 0, slope: -41.52779364450531, fluctuations: 0.0\n",
      "step: 5630 loss: 72878.380293 time elapsed: 7.0822 learning rate: 0.000100, scenario: 0, slope: -41.5138095815737, fluctuations: 0.0\n",
      "step: 5640 loss: 72463.916146 time elapsed: 7.0946 learning rate: 0.000100, scenario: 0, slope: -41.50214342117621, fluctuations: 0.0\n",
      "step: 5650 loss: 72049.907114 time elapsed: 7.1071 learning rate: 0.000100, scenario: 0, slope: -41.49044017177371, fluctuations: 0.0\n",
      "step: 5660 loss: 71636.620230 time elapsed: 7.1196 learning rate: 0.000100, scenario: 0, slope: -41.4754954991695, fluctuations: 0.0\n",
      "step: 5670 loss: 71224.389953 time elapsed: 7.1319 learning rate: 0.000100, scenario: 0, slope: -41.45322748841732, fluctuations: 0.0\n",
      "step: 5680 loss: 70813.599776 time elapsed: 7.1444 learning rate: 0.000100, scenario: 0, slope: -41.41879684929604, fluctuations: 0.0\n",
      "step: 5690 loss: 70404.648565 time elapsed: 7.1568 learning rate: 0.000100, scenario: 0, slope: -41.36690761301919, fluctuations: 0.0\n",
      "step: 5700 loss: 69997.904796 time elapsed: 7.1690 learning rate: 0.000100, scenario: 0, slope: -41.30093927915681, fluctuations: 0.0\n",
      "step: 5710 loss: 69593.658773 time elapsed: 7.1821 learning rate: 0.000100, scenario: 0, slope: -41.19056003114947, fluctuations: 0.0\n",
      "step: 5720 loss: 69192.088464 time elapsed: 7.1950 learning rate: 0.000100, scenario: 0, slope: -41.058663382982004, fluctuations: 0.0\n",
      "step: 5730 loss: 68793.253776 time elapsed: 7.2075 learning rate: 0.000100, scenario: 0, slope: -40.89583878244956, fluctuations: 0.0\n",
      "step: 5740 loss: 68397.123823 time elapsed: 7.2201 learning rate: 0.000100, scenario: 0, slope: -40.70375598624633, fluctuations: 0.0\n",
      "step: 5750 loss: 68003.625997 time elapsed: 7.2327 learning rate: 0.000100, scenario: 0, slope: -40.486362050770204, fluctuations: 0.0\n",
      "step: 5760 loss: 67612.694749 time elapsed: 7.2453 learning rate: 0.000100, scenario: 0, slope: -40.24921808506813, fluctuations: 0.0\n",
      "step: 5770 loss: 67224.300293 time elapsed: 7.2579 learning rate: 0.000100, scenario: 0, slope: -39.998526991018885, fluctuations: 0.0\n",
      "step: 5780 loss: 66838.450659 time elapsed: 7.2705 learning rate: 0.000100, scenario: 0, slope: -39.740112000083045, fluctuations: 0.0\n",
      "step: 5790 loss: 66455.174132 time elapsed: 7.2830 learning rate: 0.000100, scenario: 0, slope: -39.478601385511894, fluctuations: 0.0\n",
      "step: 5800 loss: 66074.494871 time elapsed: 7.2964 learning rate: 0.000100, scenario: 0, slope: -39.24311297756501, fluctuations: 0.0\n",
      "step: 5810 loss: 65696.412188 time elapsed: 7.3100 learning rate: 0.000100, scenario: 0, slope: -38.95672071901532, fluctuations: 0.0\n",
      "step: 5820 loss: 65320.888452 time elapsed: 7.3226 learning rate: 0.000100, scenario: 0, slope: -38.698027568743484, fluctuations: 0.0\n",
      "step: 5830 loss: 64947.845914 time elapsed: 7.3352 learning rate: 0.000100, scenario: 0, slope: -38.44072803657405, fluctuations: 0.0\n",
      "step: 5840 loss: 64577.170413 time elapsed: 7.3478 learning rate: 0.000100, scenario: 0, slope: -38.184874033646416, fluctuations: 0.0\n",
      "step: 5850 loss: 64208.719310 time elapsed: 7.3603 learning rate: 0.000100, scenario: 0, slope: -37.93124218280728, fluctuations: 0.0\n",
      "step: 5860 loss: 63842.331267 time elapsed: 7.3729 learning rate: 0.000100, scenario: 0, slope: -37.68146774125856, fluctuations: 0.0\n",
      "step: 5870 loss: 63477.836057 time elapsed: 7.3855 learning rate: 0.000100, scenario: 0, slope: -37.43785482553679, fluctuations: 0.0\n",
      "step: 5880 loss: 63115.063140 time elapsed: 7.3982 learning rate: 0.000100, scenario: 0, slope: -37.202996397311196, fluctuations: 0.0\n",
      "step: 5890 loss: 62753.848293 time elapsed: 7.4110 learning rate: 0.000100, scenario: 0, slope: -36.979364218914064, fluctuations: 0.0\n",
      "step: 5900 loss: 62394.038094 time elapsed: 7.4235 learning rate: 0.000100, scenario: 0, slope: -36.789381962783224, fluctuations: 0.0\n",
      "step: 5910 loss: 62035.492441 time elapsed: 7.4365 learning rate: 0.000100, scenario: 0, slope: -36.57325754739137, fluctuations: 0.0\n",
      "step: 5920 loss: 61678.085630 time elapsed: 7.4492 learning rate: 0.000100, scenario: 0, slope: -36.39289209804515, fluctuations: 0.0\n",
      "step: 5930 loss: 61321.706619 time elapsed: 7.4618 learning rate: 0.000100, scenario: 0, slope: -36.22796393954401, fluctuations: 0.0\n",
      "step: 5940 loss: 60966.259172 time elapsed: 7.4744 learning rate: 0.000100, scenario: 0, slope: -36.07801561793953, fluctuations: 0.0\n",
      "step: 5950 loss: 60611.662404 time elapsed: 7.4869 learning rate: 0.000100, scenario: 0, slope: -35.942184700058135, fluctuations: 0.0\n",
      "step: 5960 loss: 60257.852077 time elapsed: 7.4993 learning rate: 0.000100, scenario: 0, slope: -35.819322420666985, fluctuations: 0.0\n",
      "step: 5970 loss: 59904.782678 time elapsed: 7.5123 learning rate: 0.000100, scenario: 0, slope: -35.7080843671532, fluctuations: 0.0\n",
      "step: 5980 loss: 59552.429960 time elapsed: 7.5249 learning rate: 0.000100, scenario: 0, slope: -35.60698456845165, fluctuations: 0.0\n",
      "step: 5990 loss: 59200.793357 time elapsed: 7.5376 learning rate: 0.000100, scenario: 0, slope: -35.514414933313965, fluctuations: 0.0\n",
      "step: 6000 loss: 58849.897792 time elapsed: 7.5501 learning rate: 0.000100, scenario: 0, slope: -35.43696422468652, fluctuations: 0.0\n",
      "step: 6010 loss: 58499.795027 time elapsed: 7.5633 learning rate: 0.000100, scenario: 0, slope: -35.34777917153331, fluctuations: 0.0\n",
      "step: 6020 loss: 58150.565364 time elapsed: 7.5759 learning rate: 0.000100, scenario: 0, slope: -35.26979346470953, fluctuations: 0.0\n",
      "step: 6030 loss: 57802.319783 time elapsed: 7.5884 learning rate: 0.000100, scenario: 0, slope: -35.1924591776656, fluctuations: 0.0\n",
      "step: 6040 loss: 57455.203311 time elapsed: 7.6009 learning rate: 0.000100, scenario: 0, slope: -35.11335642331674, fluctuations: 0.0\n",
      "step: 6050 loss: 57109.392687 time elapsed: 7.6135 learning rate: 0.000100, scenario: 0, slope: -35.029870747188106, fluctuations: 0.0\n",
      "step: 6060 loss: 56765.091330 time elapsed: 7.6260 learning rate: 0.000100, scenario: 0, slope: -34.939228661857655, fluctuations: 0.0\n",
      "step: 6070 loss: 56422.516622 time elapsed: 7.6385 learning rate: 0.000100, scenario: 0, slope: -34.83859011726184, fluctuations: 0.0\n",
      "step: 6080 loss: 56081.880598 time elapsed: 7.6510 learning rate: 0.000100, scenario: 0, slope: -34.72521676930805, fluctuations: 0.0\n",
      "step: 6090 loss: 55743.365478 time elapsed: 7.6635 learning rate: 0.000100, scenario: 0, slope: -34.59672571851773, fluctuations: 0.0\n",
      "step: 6100 loss: 55407.098657 time elapsed: 7.6759 learning rate: 0.000100, scenario: 0, slope: -34.46673285692674, fluctuations: 0.0\n",
      "step: 6110 loss: 55073.134497 time elapsed: 7.6891 learning rate: 0.000100, scenario: 0, slope: -34.288621405653444, fluctuations: 0.0\n",
      "step: 6120 loss: 54741.450105 time elapsed: 7.7013 learning rate: 0.000100, scenario: 0, slope: -34.10900346532461, fluctuations: 0.0\n",
      "step: 6130 loss: 54411.958152 time elapsed: 7.7145 learning rate: 0.000100, scenario: 0, slope: -33.91466961558715, fluctuations: 0.0\n",
      "step: 6140 loss: 54084.532611 time elapsed: 7.7272 learning rate: 0.000100, scenario: 0, slope: -33.709043667254875, fluctuations: 0.0\n",
      "step: 6150 loss: 53759.037396 time elapsed: 7.7396 learning rate: 0.000100, scenario: 0, slope: -33.496479543293994, fluctuations: 0.0\n",
      "step: 6160 loss: 53435.347700 time elapsed: 7.7521 learning rate: 0.000100, scenario: 0, slope: -33.2816933143996, fluctuations: 0.0\n",
      "step: 6170 loss: 53113.359592 time elapsed: 7.7647 learning rate: 0.000100, scenario: 0, slope: -33.06914523845182, fluctuations: 0.0\n",
      "step: 6180 loss: 52792.990071 time elapsed: 7.7772 learning rate: 0.000100, scenario: 0, slope: -32.8625047244741, fluctuations: 0.0\n",
      "step: 6190 loss: 52474.172686 time elapsed: 7.7899 learning rate: 0.000100, scenario: 0, slope: -32.66429947464502, fluctuations: 0.0\n",
      "step: 6200 loss: 52156.852783 time elapsed: 7.8022 learning rate: 0.000100, scenario: 0, slope: -32.4942033691087, fluctuations: 0.0\n",
      "step: 6210 loss: 51840.984071 time elapsed: 7.8158 learning rate: 0.000100, scenario: 0, slope: -32.29715399767975, fluctuations: 0.0\n",
      "step: 6220 loss: 51526.526547 time elapsed: 7.8285 learning rate: 0.000100, scenario: 0, slope: -32.12766622611191, fluctuations: 0.0\n",
      "step: 6230 loss: 51213.445306 time elapsed: 7.8415 learning rate: 0.000100, scenario: 0, slope: -31.96619932892181, fluctuations: 0.0\n",
      "step: 6240 loss: 50901.709809 time elapsed: 7.8542 learning rate: 0.000100, scenario: 0, slope: -31.811504241446556, fluctuations: 0.0\n",
      "step: 6250 loss: 50591.293380 time elapsed: 7.8671 learning rate: 0.000100, scenario: 0, slope: -31.662447505951423, fluctuations: 0.0\n",
      "step: 6260 loss: 50282.172818 time elapsed: 7.8801 learning rate: 0.000100, scenario: 0, slope: -31.518103159839523, fluctuations: 0.0\n",
      "step: 6270 loss: 49974.328079 time elapsed: 7.8929 learning rate: 0.000100, scenario: 0, slope: -31.377747963497225, fluctuations: 0.0\n",
      "step: 6280 loss: 49667.742005 time elapsed: 7.9054 learning rate: 0.000100, scenario: 0, slope: -31.24081397541672, fluctuations: 0.0\n",
      "step: 6290 loss: 49362.400080 time elapsed: 7.9193 learning rate: 0.000100, scenario: 0, slope: -31.106838442635503, fluctuations: 0.0\n",
      "step: 6300 loss: 49058.290190 time elapsed: 7.9319 learning rate: 0.000100, scenario: 0, slope: -30.988463485898155, fluctuations: 0.0\n",
      "step: 6310 loss: 48755.402407 time elapsed: 7.9450 learning rate: 0.000100, scenario: 0, slope: -30.846235415608504, fluctuations: 0.0\n",
      "step: 6320 loss: 48453.728759 time elapsed: 7.9577 learning rate: 0.000100, scenario: 0, slope: -30.718951542454427, fluctuations: 0.0\n",
      "step: 6330 loss: 48153.262999 time elapsed: 7.9705 learning rate: 0.000100, scenario: 0, slope: -30.593295595210794, fluctuations: 0.0\n",
      "step: 6340 loss: 47854.000382 time elapsed: 7.9834 learning rate: 0.000100, scenario: 0, slope: -30.46901355592437, fluctuations: 0.0\n",
      "step: 6350 loss: 47555.937429 time elapsed: 7.9961 learning rate: 0.000100, scenario: 0, slope: -30.34587594577648, fluctuations: 0.0\n",
      "step: 6360 loss: 47259.071710 time elapsed: 8.0087 learning rate: 0.000100, scenario: 0, slope: -30.22367679660193, fluctuations: 0.0\n",
      "step: 6370 loss: 46963.401644 time elapsed: 8.0214 learning rate: 0.000100, scenario: 0, slope: -30.102233057058395, fluctuations: 0.0\n",
      "step: 6380 loss: 46668.926313 time elapsed: 8.0341 learning rate: 0.000100, scenario: 0, slope: -29.981384109882942, fluctuations: 0.0\n",
      "step: 6390 loss: 46375.645336 time elapsed: 8.0468 learning rate: 0.000100, scenario: 0, slope: -29.860991119870715, fluctuations: 0.0\n",
      "step: 6400 loss: 46083.559325 time elapsed: 8.0591 learning rate: 0.000100, scenario: 0, slope: -29.752928895842377, fluctuations: 0.0\n",
      "step: 6410 loss: 45792.667096 time elapsed: 8.0721 learning rate: 0.000100, scenario: 0, slope: -29.621116460096797, fluctuations: 0.0\n",
      "step: 6420 loss: 45502.971235 time elapsed: 8.0847 learning rate: 0.000100, scenario: 0, slope: -29.501456259118644, fluctuations: 0.0\n",
      "step: 6430 loss: 45214.472057 time elapsed: 8.0975 learning rate: 0.000100, scenario: 0, slope: -29.381884667516708, fluctuations: 0.0\n",
      "step: 6440 loss: 44927.171782 time elapsed: 8.1104 learning rate: 0.000100, scenario: 0, slope: -29.262343038170947, fluctuations: 0.0\n",
      "step: 6450 loss: 44641.072649 time elapsed: 8.1233 learning rate: 0.000100, scenario: 0, slope: -29.142779676860265, fluctuations: 0.0\n",
      "step: 6460 loss: 44356.177850 time elapsed: 8.1358 learning rate: 0.000100, scenario: 0, slope: -29.02314540961394, fluctuations: 0.0\n",
      "step: 6470 loss: 44072.491329 time elapsed: 8.1486 learning rate: 0.000100, scenario: 0, slope: -28.903390098692565, fluctuations: 0.0\n",
      "step: 6480 loss: 43790.017910 time elapsed: 8.1613 learning rate: 0.000100, scenario: 0, slope: -28.783459744307596, fluctuations: 0.0\n",
      "step: 6490 loss: 43508.763257 time elapsed: 8.1740 learning rate: 0.000100, scenario: 0, slope: -28.663294726227868, fluctuations: 0.0\n",
      "step: 6500 loss: 43228.733820 time elapsed: 8.1865 learning rate: 0.000100, scenario: 0, slope: -28.554891000922257, fluctuations: 0.0\n",
      "step: 6510 loss: 42949.936705 time elapsed: 8.1994 learning rate: 0.000100, scenario: 0, slope: -28.421988375874978, fluctuations: 0.0\n",
      "step: 6520 loss: 42672.379493 time elapsed: 8.2117 learning rate: 0.000100, scenario: 0, slope: -28.30070455222279, fluctuations: 0.0\n",
      "step: 6530 loss: 42396.070020 time elapsed: 8.2240 learning rate: 0.000100, scenario: 0, slope: -28.178902676870965, fluctuations: 0.0\n",
      "step: 6540 loss: 42121.016126 time elapsed: 8.2364 learning rate: 0.000100, scenario: 0, slope: -28.056513885521895, fluctuations: 0.0\n",
      "step: 6550 loss: 41847.225405 time elapsed: 8.2490 learning rate: 0.000100, scenario: 0, slope: -27.933479063644445, fluctuations: 0.0\n",
      "step: 6560 loss: 41574.704948 time elapsed: 8.2617 learning rate: 0.000100, scenario: 0, slope: -27.809752710615655, fluctuations: 0.0\n",
      "step: 6570 loss: 41303.461107 time elapsed: 8.2740 learning rate: 0.000100, scenario: 0, slope: -27.685306776873876, fluctuations: 0.0\n",
      "step: 6580 loss: 41033.499292 time elapsed: 8.2867 learning rate: 0.000100, scenario: 0, slope: -27.560133432642754, fluctuations: 0.0\n",
      "step: 6590 loss: 40764.823787 time elapsed: 8.2993 learning rate: 0.000100, scenario: 0, slope: -27.434246792964494, fluctuations: 0.0\n",
      "step: 6600 loss: 40497.437610 time elapsed: 8.3116 learning rate: 0.000100, scenario: 0, slope: -27.320368733926887, fluctuations: 0.0\n",
      "step: 6610 loss: 40231.342404 time elapsed: 8.3257 learning rate: 0.000100, scenario: 0, slope: -27.1805018992761, fluctuations: 0.0\n",
      "step: 6620 loss: 39966.538362 time elapsed: 8.3385 learning rate: 0.000100, scenario: 0, slope: -27.052781036393156, fluctuations: 0.0\n",
      "step: 6630 loss: 39703.024183 time elapsed: 8.3514 learning rate: 0.000100, scenario: 0, slope: -26.924617904698874, fluctuations: 0.0\n",
      "step: 6640 loss: 39440.797058 time elapsed: 8.3641 learning rate: 0.000100, scenario: 0, slope: -26.796124971913777, fluctuations: 0.0\n",
      "step: 6650 loss: 39179.852670 time elapsed: 8.3768 learning rate: 0.000100, scenario: 0, slope: -26.66742714826778, fluctuations: 0.0\n",
      "step: 6660 loss: 38920.185218 time elapsed: 8.3897 learning rate: 0.000100, scenario: 0, slope: -26.538658770636136, fluctuations: 0.0\n",
      "step: 6670 loss: 38661.787436 time elapsed: 8.4026 learning rate: 0.000100, scenario: 0, slope: -26.409960744072844, fluctuations: 0.0\n",
      "step: 6680 loss: 38404.650619 time elapsed: 8.4151 learning rate: 0.000100, scenario: 0, slope: -26.281478008638416, fluctuations: 0.0\n",
      "step: 6690 loss: 38148.764630 time elapsed: 8.4281 learning rate: 0.000100, scenario: 0, slope: -26.153357475764217, fluctuations: 0.0\n",
      "step: 6700 loss: 37894.117895 time elapsed: 8.4408 learning rate: 0.000100, scenario: 0, slope: -26.03848051642572, fluctuations: 0.0\n",
      "step: 6710 loss: 37640.697367 time elapsed: 8.4540 learning rate: 0.000100, scenario: 0, slope: -25.898792342099544, fluctuations: 0.0\n",
      "step: 6720 loss: 37388.488465 time elapsed: 8.4668 learning rate: 0.000100, scenario: 0, slope: -25.772641545143976, fluctuations: 0.0\n",
      "step: 6730 loss: 37137.475000 time elapsed: 8.4795 learning rate: 0.000100, scenario: 0, slope: -25.64744102200783, fluctuations: 0.0\n",
      "step: 6740 loss: 36887.639121 time elapsed: 8.4921 learning rate: 0.000100, scenario: 0, slope: -25.523338791333014, fluctuations: 0.0\n",
      "step: 6750 loss: 36638.961475 time elapsed: 8.5047 learning rate: 0.000100, scenario: 0, slope: -25.40048483275235, fluctuations: 0.0\n",
      "step: 6760 loss: 36391.421495 time elapsed: 8.5175 learning rate: 0.000100, scenario: 0, slope: -25.279028161970725, fluctuations: 0.0\n",
      "step: 6770 loss: 36144.996086 time elapsed: 8.5306 learning rate: 0.000100, scenario: 0, slope: -25.159126432753062, fluctuations: 0.0\n",
      "step: 6780 loss: 35899.665939 time elapsed: 8.5436 learning rate: 0.000100, scenario: 0, slope: -25.04091991909879, fluctuations: 0.0\n",
      "step: 6790 loss: 35655.413390 time elapsed: 8.5561 learning rate: 0.000100, scenario: 0, slope: -24.924522912763123, fluctuations: 0.0\n",
      "step: 6800 loss: 35412.228858 time elapsed: 8.5682 learning rate: 0.000100, scenario: 0, slope: -24.82135925399109, fluctuations: 0.0\n",
      "step: 6810 loss: 35170.116444 time elapsed: 8.5813 learning rate: 0.000100, scenario: 0, slope: -24.697258474703574, fluctuations: 0.0\n",
      "step: 6820 loss: 34929.097964 time elapsed: 8.5937 learning rate: 0.000100, scenario: 0, slope: -24.586090375682552, fluctuations: 0.0\n",
      "step: 6830 loss: 34689.208768 time elapsed: 8.6062 learning rate: 0.000100, scenario: 0, slope: -24.476018735998437, fluctuations: 0.0\n",
      "step: 6840 loss: 34450.473739 time elapsed: 8.6187 learning rate: 0.000100, scenario: 0, slope: -24.366407488407955, fluctuations: 0.0\n",
      "step: 6850 loss: 34212.868293 time elapsed: 8.6310 learning rate: 0.000100, scenario: 0, slope: -24.256698013913606, fluctuations: 0.0\n",
      "step: 6860 loss: 33976.314739 time elapsed: 8.6434 learning rate: 0.000100, scenario: 0, slope: -24.146747025037662, fluctuations: 0.0\n",
      "step: 6870 loss: 33740.738538 time elapsed: 8.6558 learning rate: 0.000100, scenario: 0, slope: -24.036930608807456, fluctuations: 0.0\n",
      "step: 6880 loss: 33506.098276 time elapsed: 8.6684 learning rate: 0.000100, scenario: 0, slope: -23.927916131190276, fluctuations: 0.0\n",
      "step: 6890 loss: 33272.365109 time elapsed: 8.6811 learning rate: 0.000100, scenario: 0, slope: -23.820490920416795, fluctuations: 0.0\n",
      "step: 6900 loss: 33039.507670 time elapsed: 8.6935 learning rate: 0.000100, scenario: 0, slope: -23.725861790479925, fluctuations: 0.0\n",
      "step: 6910 loss: 32807.491683 time elapsed: 8.7066 learning rate: 0.000100, scenario: 0, slope: -23.61375076026442, fluctuations: 0.0\n",
      "step: 6920 loss: 32576.282271 time elapsed: 8.7189 learning rate: 0.000100, scenario: 0, slope: -23.515921636262682, fluctuations: 0.0\n",
      "step: 6930 loss: 32345.845878 time elapsed: 8.7324 learning rate: 0.000100, scenario: 0, slope: -23.422347931259356, fluctuations: 0.0\n",
      "step: 6940 loss: 32116.152846 time elapsed: 8.7450 learning rate: 0.000100, scenario: 0, slope: -23.332933899805187, fluctuations: 0.0\n",
      "step: 6950 loss: 31887.181327 time elapsed: 8.7578 learning rate: 0.000100, scenario: 0, slope: -23.247246423229434, fluctuations: 0.0\n",
      "step: 6960 loss: 31658.922574 time elapsed: 8.7702 learning rate: 0.000100, scenario: 0, slope: -23.164802325542226, fluctuations: 0.0\n",
      "step: 6970 loss: 31431.387035 time elapsed: 8.7828 learning rate: 0.000100, scenario: 0, slope: -23.08519655752512, fluctuations: 0.0\n",
      "step: 6980 loss: 31204.609601 time elapsed: 8.7953 learning rate: 0.000100, scenario: 0, slope: -23.007916573390713, fluctuations: 0.0\n",
      "step: 6990 loss: 30978.650536 time elapsed: 8.8077 learning rate: 0.000100, scenario: 0, slope: -22.932142782943362, fluctuations: 0.0\n",
      "step: 7000 loss: 30753.586898 time elapsed: 8.8201 learning rate: 0.000100, scenario: 0, slope: -22.864278177575496, fluctuations: 0.0\n",
      "step: 7010 loss: 30529.490279 time elapsed: 8.8329 learning rate: 0.000100, scenario: 0, slope: -22.780229644886663, fluctuations: 0.0\n",
      "step: 7020 loss: 30306.394327 time elapsed: 8.8455 learning rate: 0.000100, scenario: 0, slope: -22.701343647043664, fluctuations: 0.0\n",
      "step: 7030 loss: 30084.270224 time elapsed: 8.8579 learning rate: 0.000100, scenario: 0, slope: -22.619133110579583, fluctuations: 0.0\n",
      "step: 7040 loss: 29863.034832 time elapsed: 8.8702 learning rate: 0.000100, scenario: 0, slope: -22.53346496426426, fluctuations: 0.0\n",
      "step: 7050 loss: 29642.592666 time elapsed: 8.8826 learning rate: 0.000100, scenario: 0, slope: -22.44509317544431, fluctuations: 0.0\n",
      "step: 7060 loss: 29422.876594 time elapsed: 8.8951 learning rate: 0.000100, scenario: 0, slope: -22.35543898696568, fluctuations: 0.0\n",
      "step: 7070 loss: 29203.857057 time elapsed: 8.9077 learning rate: 0.000100, scenario: 0, slope: -22.266206389701466, fluctuations: 0.0\n",
      "step: 7080 loss: 28985.528220 time elapsed: 8.9201 learning rate: 0.000100, scenario: 0, slope: -22.179013960549902, fluctuations: 0.0\n",
      "step: 7090 loss: 28767.893454 time elapsed: 8.9334 learning rate: 0.000100, scenario: 0, slope: -22.095099615243385, fluctuations: 0.0\n",
      "step: 7100 loss: 28550.960040 time elapsed: 8.9458 learning rate: 0.000100, scenario: 0, slope: -22.022912280853642, fluctuations: 0.0\n",
      "step: 7110 loss: 28334.730001 time elapsed: 8.9586 learning rate: 0.000100, scenario: 0, slope: -21.938882959857455, fluctuations: 0.0\n",
      "step: 7120 loss: 28119.212554 time elapsed: 8.9711 learning rate: 0.000100, scenario: 0, slope: -21.865706574140507, fluctuations: 0.0\n",
      "step: 7130 loss: 27904.411897 time elapsed: 8.9836 learning rate: 0.000100, scenario: 0, slope: -21.794411380636067, fluctuations: 0.0\n",
      "step: 7140 loss: 27690.330535 time elapsed: 8.9963 learning rate: 0.000100, scenario: 0, slope: -21.723896632595967, fluctuations: 0.0\n",
      "step: 7150 loss: 27476.967857 time elapsed: 9.0087 learning rate: 0.000100, scenario: 0, slope: -21.65342333460031, fluctuations: 0.0\n",
      "step: 7160 loss: 27264.319460 time elapsed: 9.0215 learning rate: 0.000100, scenario: 0, slope: -21.582672766122002, fluctuations: 0.0\n",
      "step: 7170 loss: 27052.376426 time elapsed: 9.0338 learning rate: 0.000100, scenario: 0, slope: -21.51162678293665, fluctuations: 0.0\n",
      "step: 7180 loss: 26841.125199 time elapsed: 9.0463 learning rate: 0.000100, scenario: 0, slope: -21.44043459646364, fluctuations: 0.0\n",
      "step: 7190 loss: 26630.547982 time elapsed: 9.0588 learning rate: 0.000100, scenario: 0, slope: -21.369339517621288, fluctuations: 0.0\n",
      "step: 7200 loss: 26420.623774 time elapsed: 9.0710 learning rate: 0.000100, scenario: 0, slope: -21.30568627075055, fluctuations: 0.0\n",
      "step: 7210 loss: 26211.330174 time elapsed: 9.0839 learning rate: 0.000100, scenario: 0, slope: -21.228693346551058, fluctuations: 0.0\n",
      "step: 7220 loss: 26002.646091 time elapsed: 9.0962 learning rate: 0.000100, scenario: 0, slope: -21.15983564945272, fluctuations: 0.0\n",
      "step: 7230 loss: 25794.555537 time elapsed: 9.1088 learning rate: 0.000100, scenario: 0, slope: -21.092372546912824, fluctuations: 0.0\n",
      "step: 7240 loss: 25587.052660 time elapsed: 9.1213 learning rate: 0.000100, scenario: 0, slope: -21.026502409492192, fluctuations: 0.0\n",
      "step: 7250 loss: 25380.148127 time elapsed: 9.1338 learning rate: 0.000100, scenario: 0, slope: -20.962246249808263, fluctuations: 0.0\n",
      "step: 7260 loss: 25173.876848 time elapsed: 9.1470 learning rate: 0.000100, scenario: 0, slope: -20.899360622965748, fluctuations: 0.0\n",
      "step: 7270 loss: 24968.306710 time elapsed: 9.1599 learning rate: 0.000100, scenario: 0, slope: -20.837239483950473, fluctuations: 0.0\n",
      "step: 7280 loss: 24763.547499 time elapsed: 9.1724 learning rate: 0.000100, scenario: 0, slope: -20.77480993427377, fluctuations: 0.0\n",
      "step: 7290 loss: 24559.758347 time elapsed: 9.1849 learning rate: 0.000100, scenario: 0, slope: -20.71043350799865, fluctuations: 0.0\n",
      "step: 7300 loss: 24357.151157 time elapsed: 9.1971 learning rate: 0.000100, scenario: 0, slope: -20.648962744883608, fluctuations: 0.0\n",
      "step: 7310 loss: 24155.986939 time elapsed: 9.2102 learning rate: 0.000100, scenario: 0, slope: -20.566086084415065, fluctuations: 0.0\n",
      "step: 7320 loss: 23956.562651 time elapsed: 9.2228 learning rate: 0.000100, scenario: 0, slope: -20.479697086293665, fluctuations: 0.0\n",
      "step: 7330 loss: 23759.188273 time elapsed: 9.2353 learning rate: 0.000100, scenario: 0, slope: -20.378826017829567, fluctuations: 0.0\n",
      "step: 7340 loss: 23564.156902 time elapsed: 9.2478 learning rate: 0.000100, scenario: 0, slope: -20.259640805514902, fluctuations: 0.0\n",
      "step: 7350 loss: 23371.713300 time elapsed: 9.2604 learning rate: 0.000100, scenario: 0, slope: -20.118795458210514, fluctuations: 0.0\n",
      "step: 7360 loss: 23182.027451 time elapsed: 9.2728 learning rate: 0.000100, scenario: 0, slope: -19.953959946532514, fluctuations: 0.0\n",
      "step: 7370 loss: 22995.179058 time elapsed: 9.2853 learning rate: 0.000100, scenario: 0, slope: -19.764305583958777, fluctuations: 0.0\n",
      "step: 7380 loss: 22811.156502 time elapsed: 9.2977 learning rate: 0.000100, scenario: 0, slope: -19.550835521738094, fluctuations: 0.0\n",
      "step: 7390 loss: 22629.870095 time elapsed: 9.3102 learning rate: 0.000100, scenario: 0, slope: -19.316465134124414, fluctuations: 0.0\n",
      "step: 7400 loss: 22451.175693 time elapsed: 9.3225 learning rate: 0.000100, scenario: 0, slope: -19.091442937262936, fluctuations: 0.0\n",
      "step: 7410 loss: 22274.902148 time elapsed: 9.3359 learning rate: 0.000100, scenario: 0, slope: -18.80464579174087, fluctuations: 0.0\n",
      "step: 7420 loss: 22100.875948 time elapsed: 9.3481 learning rate: 0.000100, scenario: 0, slope: -18.539287675789314, fluctuations: 0.0\n",
      "step: 7430 loss: 21928.938477 time elapsed: 9.3616 learning rate: 0.000100, scenario: 0, slope: -18.275761234250822, fluctuations: 0.0\n",
      "step: 7440 loss: 21758.954593 time elapsed: 9.3748 learning rate: 0.000100, scenario: 0, slope: -18.019179735669034, fluctuations: 0.0\n",
      "step: 7450 loss: 21590.814221 time elapsed: 9.3875 learning rate: 0.000100, scenario: 0, slope: -17.773286727350452, fluctuations: 0.0\n",
      "step: 7460 loss: 21424.429358 time elapsed: 9.4002 learning rate: 0.000100, scenario: 0, slope: -17.54027343491953, fluctuations: 0.0\n",
      "step: 7470 loss: 21259.727719 time elapsed: 9.4129 learning rate: 0.000100, scenario: 0, slope: -17.320883880689312, fluctuations: 0.0\n",
      "step: 7480 loss: 21096.651954 time elapsed: 9.4256 learning rate: 0.000100, scenario: 0, slope: -17.114679777389366, fluctuations: 0.0\n",
      "step: 7490 loss: 20935.153066 time elapsed: 9.4385 learning rate: 0.000100, scenario: 0, slope: -16.920448054128386, fluctuations: 0.0\n",
      "step: 7500 loss: 20775.188424 time elapsed: 9.4511 learning rate: 0.000100, scenario: 0, slope: -16.754563934575597, fluctuations: 0.0\n",
      "step: 7510 loss: 20616.720225 time elapsed: 9.4643 learning rate: 0.000100, scenario: 0, slope: -16.56148872748963, fluctuations: 0.0\n",
      "step: 7520 loss: 20459.714444 time elapsed: 9.4771 learning rate: 0.000100, scenario: 0, slope: -16.393634136419777, fluctuations: 0.0\n",
      "step: 7530 loss: 20304.139934 time elapsed: 9.4899 learning rate: 0.000100, scenario: 0, slope: -16.231808278828566, fluctuations: 0.0\n",
      "step: 7540 loss: 20149.967892 time elapsed: 9.5030 learning rate: 0.000100, scenario: 0, slope: -16.075055904390453, fluctuations: 0.0\n",
      "step: 7550 loss: 19997.171454 time elapsed: 9.5159 learning rate: 0.000100, scenario: 0, slope: -15.922657845244904, fluctuations: 0.0\n",
      "step: 7560 loss: 19845.725389 time elapsed: 9.5295 learning rate: 0.000100, scenario: 0, slope: -15.774072667165935, fluctuations: 0.0\n",
      "step: 7570 loss: 19695.605831 time elapsed: 9.5448 learning rate: 0.000100, scenario: 0, slope: -15.628906242359522, fluctuations: 0.0\n",
      "step: 7580 loss: 19546.790058 time elapsed: 9.5598 learning rate: 0.000100, scenario: 0, slope: -15.48684793101377, fluctuations: 0.0\n",
      "step: 7590 loss: 19399.256296 time elapsed: 9.5751 learning rate: 0.000100, scenario: 0, slope: -15.347653524243421, fluctuations: 0.0\n",
      "step: 7600 loss: 19252.983537 time elapsed: 9.5885 learning rate: 0.000100, scenario: 0, slope: -15.224664025427744, fluctuations: 0.0\n",
      "step: 7610 loss: 19107.951376 time elapsed: 9.6020 learning rate: 0.000100, scenario: 0, slope: -15.07710621225382, fluctuations: 0.0\n",
      "step: 7620 loss: 18964.139857 time elapsed: 9.6150 learning rate: 0.000100, scenario: 0, slope: -14.945459903940796, fluctuations: 0.0\n",
      "step: 7630 loss: 18821.529328 time elapsed: 9.6277 learning rate: 0.000100, scenario: 0, slope: -14.816078396329909, fluctuations: 0.0\n",
      "step: 7640 loss: 18680.100311 time elapsed: 9.6422 learning rate: 0.000100, scenario: 0, slope: -14.688872752395863, fluctuations: 0.0\n",
      "step: 7650 loss: 18539.833373 time elapsed: 9.6550 learning rate: 0.000100, scenario: 0, slope: -14.563772102114129, fluctuations: 0.0\n",
      "step: 7660 loss: 18400.709022 time elapsed: 9.6678 learning rate: 0.000100, scenario: 0, slope: -14.440721962278161, fluctuations: 0.0\n",
      "step: 7670 loss: 18262.707620 time elapsed: 9.6806 learning rate: 0.000100, scenario: 0, slope: -14.319682819504488, fluctuations: 0.0\n",
      "step: 7680 loss: 18125.809330 time elapsed: 9.6930 learning rate: 0.000100, scenario: 0, slope: -14.20062874885407, fluctuations: 0.0\n",
      "step: 7690 loss: 17989.994123 time elapsed: 9.7056 learning rate: 0.000100, scenario: 0, slope: -14.083545828571364, fluctuations: 0.0\n",
      "step: 7700 loss: 17855.241859 time elapsed: 9.7180 learning rate: 0.000100, scenario: 0, slope: -13.97985306421501, fluctuations: 0.0\n",
      "step: 7710 loss: 17721.532496 time elapsed: 9.7308 learning rate: 0.000100, scenario: 0, slope: -13.8552843678879, fluctuations: 0.0\n",
      "step: 7720 loss: 17588.846470 time elapsed: 9.7430 learning rate: 0.000100, scenario: 0, slope: -13.744114280417877, fluctuations: 0.0\n",
      "step: 7730 loss: 17457.165299 time elapsed: 9.7555 learning rate: 0.000100, scenario: 0, slope: -13.634921497115645, fluctuations: 0.0\n",
      "step: 7740 loss: 17326.472451 time elapsed: 9.7692 learning rate: 0.000100, scenario: 0, slope: -13.527694889323076, fluctuations: 0.0\n",
      "step: 7750 loss: 17196.754440 time elapsed: 9.7819 learning rate: 0.000100, scenario: 0, slope: -13.422398447740564, fluctuations: 0.0\n",
      "step: 7760 loss: 17068.001986 time elapsed: 9.7945 learning rate: 0.000100, scenario: 0, slope: -13.318956688306361, fluctuations: 0.0\n",
      "step: 7770 loss: 16940.210733 time elapsed: 9.8072 learning rate: 0.000100, scenario: 0, slope: -13.217239940412862, fluctuations: 0.0\n",
      "step: 7780 loss: 16813.380759 time elapsed: 9.8198 learning rate: 0.000100, scenario: 0, slope: -13.117055301509765, fluctuations: 0.0\n",
      "step: 7790 loss: 16687.514313 time elapsed: 9.8323 learning rate: 0.000100, scenario: 0, slope: -13.018146783519759, fluctuations: 0.0\n",
      "step: 7800 loss: 16562.608256 time elapsed: 9.8445 learning rate: 0.000100, scenario: 0, slope: -12.930002994978581, fluctuations: 0.0\n",
      "step: 7810 loss: 16438.654453 time elapsed: 9.8574 learning rate: 0.000100, scenario: 0, slope: -12.823124425238785, fluctuations: 0.0\n",
      "step: 7820 loss: 16315.637854 time elapsed: 9.8697 learning rate: 0.000100, scenario: 0, slope: -12.726609055994434, fluctuations: 0.0\n",
      "step: 7830 loss: 16193.543667 time elapsed: 9.8822 learning rate: 0.000100, scenario: 0, slope: -12.630654016798086, fluctuations: 0.0\n",
      "step: 7840 loss: 16072.364632 time elapsed: 9.8952 learning rate: 0.000100, scenario: 0, slope: -12.53528058222273, fluctuations: 0.0\n",
      "step: 7850 loss: 15952.103013 time elapsed: 9.9081 learning rate: 0.000100, scenario: 0, slope: -12.440525474906996, fluctuations: 0.0\n",
      "step: 7860 loss: 15832.769457 time elapsed: 9.9206 learning rate: 0.000100, scenario: 0, slope: -12.346383527116418, fluctuations: 0.0\n",
      "step: 7870 loss: 15714.381164 time elapsed: 9.9335 learning rate: 0.000100, scenario: 0, slope: -12.25275915889592, fluctuations: 0.0\n",
      "step: 7880 loss: 15596.959981 time elapsed: 9.9463 learning rate: 0.000100, scenario: 0, slope: -12.159430289064083, fluctuations: 0.0\n",
      "step: 7890 loss: 15480.530072 time elapsed: 9.9590 learning rate: 0.000100, scenario: 0, slope: -12.066028238232345, fluctuations: 0.0\n",
      "step: 7900 loss: 15365.114822 time elapsed: 9.9718 learning rate: 0.000100, scenario: 0, slope: -11.98152355959625, fluctuations: 0.0\n",
      "step: 7910 loss: 15250.732908 time elapsed: 9.9863 learning rate: 0.000100, scenario: 0, slope: -11.877118239844558, fluctuations: 0.0\n",
      "step: 7920 loss: 15137.393575 time elapsed: 9.9993 learning rate: 0.000100, scenario: 0, slope: -11.78070139463621, fluctuations: 0.0\n",
      "step: 7930 loss: 15025.090757 time elapsed: 10.0123 learning rate: 0.000100, scenario: 0, slope: -11.682644605431715, fluctuations: 0.0\n",
      "step: 7940 loss: 14913.794132 time elapsed: 10.0250 learning rate: 0.000100, scenario: 0, slope: -11.583061432210716, fluctuations: 0.0\n",
      "step: 7950 loss: 14803.430151 time elapsed: 10.0375 learning rate: 0.000100, scenario: 0, slope: -11.482489088551668, fluctuations: 0.0\n",
      "step: 7960 loss: 14693.829530 time elapsed: 10.0502 learning rate: 0.000100, scenario: 0, slope: -11.382138597731487, fluctuations: 0.0\n",
      "step: 7970 loss: 14584.559001 time elapsed: 10.0630 learning rate: 0.000100, scenario: 0, slope: -11.28464056798017, fluctuations: 0.0\n",
      "step: 7980 loss: 14474.386756 time elapsed: 10.0756 learning rate: 0.000100, scenario: 0, slope: -11.196460087613373, fluctuations: 0.0\n",
      "step: 7990 loss: 14360.330036 time elapsed: 10.0882 learning rate: 0.000100, scenario: 0, slope: -11.134198837087947, fluctuations: 0.0\n",
      "step: 8000 loss: 14239.367524 time elapsed: 10.1004 learning rate: 0.000100, scenario: 0, slope: -11.125709228570596, fluctuations: 0.0\n",
      "step: 8010 loss: 14113.710550 time elapsed: 10.1136 learning rate: 0.000100, scenario: 0, slope: -11.203844259160087, fluctuations: 0.0\n",
      "step: 8020 loss: 13988.574043 time elapsed: 10.1263 learning rate: 0.000100, scenario: 0, slope: -11.351219352845247, fluctuations: 0.0\n",
      "step: 8030 loss: 13866.802419 time elapsed: 10.1388 learning rate: 0.000100, scenario: 0, slope: -11.536578396148965, fluctuations: 0.0\n",
      "step: 8040 loss: 13748.770093 time elapsed: 10.1513 learning rate: 0.000100, scenario: 0, slope: -11.720406508971669, fluctuations: 0.0\n",
      "step: 8050 loss: 13633.970693 time elapsed: 10.1641 learning rate: 0.000100, scenario: 0, slope: -11.867971796113554, fluctuations: 0.0\n",
      "step: 8060 loss: 13521.830686 time elapsed: 10.1766 learning rate: 0.000100, scenario: 0, slope: -11.951474590544935, fluctuations: 0.0\n",
      "step: 8070 loss: 13411.932889 time elapsed: 10.1900 learning rate: 0.000100, scenario: 0, slope: -11.950117027111633, fluctuations: 0.0\n",
      "step: 8080 loss: 13304.000341 time elapsed: 10.2029 learning rate: 0.000100, scenario: 0, slope: -11.852344232512289, fluctuations: 0.0\n",
      "step: 8090 loss: 13197.843200 time elapsed: 10.2155 learning rate: 0.000100, scenario: 0, slope: -11.664532955998535, fluctuations: 0.0\n",
      "step: 8100 loss: 13093.321393 time elapsed: 10.2279 learning rate: 0.000100, scenario: 0, slope: -11.446808930059676, fluctuations: 0.0\n",
      "step: 8110 loss: 12990.324460 time elapsed: 10.2413 learning rate: 0.000100, scenario: 0, slope: -11.17075477559528, fluctuations: 0.0\n",
      "step: 8120 loss: 12888.761466 time elapsed: 10.2540 learning rate: 0.000100, scenario: 0, slope: -10.94115599972544, fluctuations: 0.0\n",
      "step: 8130 loss: 12788.555689 time elapsed: 10.2664 learning rate: 0.000100, scenario: 0, slope: -10.738142490507594, fluctuations: 0.0\n",
      "step: 8140 loss: 12689.645616 time elapsed: 10.2788 learning rate: 0.000100, scenario: 0, slope: -10.557279638007603, fluctuations: 0.0\n",
      "step: 8150 loss: 12591.963060 time elapsed: 10.2913 learning rate: 0.000100, scenario: 0, slope: -10.392952659834455, fluctuations: 0.0\n",
      "step: 8160 loss: 12495.467607 time elapsed: 10.3043 learning rate: 0.000100, scenario: 0, slope: -10.240967650970488, fluctuations: 0.0\n",
      "step: 8170 loss: 12400.113788 time elapsed: 10.3175 learning rate: 0.000100, scenario: 0, slope: -10.098597476533397, fluctuations: 0.0\n",
      "step: 8180 loss: 12305.862505 time elapsed: 10.3300 learning rate: 0.000100, scenario: 0, slope: -9.96405954868487, fluctuations: 0.0\n",
      "step: 8190 loss: 12212.678627 time elapsed: 10.3427 learning rate: 0.000100, scenario: 0, slope: -9.836110278240486, fluctuations: 0.0\n",
      "step: 8200 loss: 12120.530738 time elapsed: 10.3552 learning rate: 0.000100, scenario: 0, slope: -9.725815037540105, fluctuations: 0.0\n",
      "step: 8210 loss: 12029.390332 time elapsed: 10.3688 learning rate: 0.000100, scenario: 0, slope: -9.596450559784202, fluctuations: 0.0\n",
      "step: 8220 loss: 11939.231318 time elapsed: 10.3819 learning rate: 0.000100, scenario: 0, slope: -9.483413648540543, fluctuations: 0.0\n",
      "step: 8230 loss: 11850.029675 time elapsed: 10.3962 learning rate: 0.000100, scenario: 0, slope: -9.374219703551997, fluctuations: 0.0\n",
      "step: 8240 loss: 11761.763159 time elapsed: 10.4100 learning rate: 0.000100, scenario: 0, slope: -9.268454539938727, fluctuations: 0.0\n",
      "step: 8250 loss: 11674.411071 time elapsed: 10.4242 learning rate: 0.000100, scenario: 0, slope: -9.165789120566899, fluctuations: 0.0\n",
      "step: 8260 loss: 11587.954059 time elapsed: 10.4371 learning rate: 0.000100, scenario: 0, slope: -9.065941019024837, fluctuations: 0.0\n",
      "step: 8270 loss: 11502.373976 time elapsed: 10.4500 learning rate: 0.000100, scenario: 0, slope: -8.968666918211987, fluctuations: 0.0\n",
      "step: 8280 loss: 11417.653754 time elapsed: 10.4627 learning rate: 0.000100, scenario: 0, slope: -8.8737601541815, fluctuations: 0.0\n",
      "step: 8290 loss: 11333.777320 time elapsed: 10.4756 learning rate: 0.000100, scenario: 0, slope: -8.781043557612875, fluctuations: 0.0\n",
      "step: 8300 loss: 11250.729531 time elapsed: 10.4878 learning rate: 0.000100, scenario: 0, slope: -8.699343589800767, fluctuations: 0.0\n",
      "step: 8310 loss: 11168.496127 time elapsed: 10.5008 learning rate: 0.000100, scenario: 0, slope: -8.601583606327285, fluctuations: 0.0\n",
      "step: 8320 loss: 11087.063711 time elapsed: 10.5137 learning rate: 0.000100, scenario: 0, slope: -8.514583904366528, fluctuations: 0.0\n",
      "step: 8330 loss: 11006.419722 time elapsed: 10.5264 learning rate: 0.000100, scenario: 0, slope: -8.42925449572752, fluctuations: 0.0\n",
      "step: 8340 loss: 10926.552437 time elapsed: 10.5387 learning rate: 0.000100, scenario: 0, slope: -8.345494339456357, fluctuations: 0.0\n",
      "step: 8350 loss: 10847.450958 time elapsed: 10.5513 learning rate: 0.000100, scenario: 0, slope: -8.263208701129011, fluctuations: 0.0\n",
      "step: 8360 loss: 10769.105199 time elapsed: 10.5639 learning rate: 0.000100, scenario: 0, slope: -8.182307376523596, fluctuations: 0.0\n",
      "step: 8370 loss: 10691.505869 time elapsed: 10.5765 learning rate: 0.000100, scenario: 0, slope: -8.102703385489525, fluctuations: 0.0\n",
      "step: 8380 loss: 10614.644430 time elapsed: 10.5907 learning rate: 0.000100, scenario: 0, slope: -8.02431214718351, fluctuations: 0.0\n",
      "step: 8390 loss: 10538.513050 time elapsed: 10.6042 learning rate: 0.000100, scenario: 0, slope: -7.9470511454354895, fluctuations: 0.0\n",
      "step: 8400 loss: 10463.104521 time elapsed: 10.6169 learning rate: 0.000100, scenario: 0, slope: -7.878416171918607, fluctuations: 0.0\n",
      "step: 8410 loss: 10388.412176 time elapsed: 10.6303 learning rate: 0.000100, scenario: 0, slope: -7.795601454673965, fluctuations: 0.0\n",
      "step: 8420 loss: 10314.429778 time elapsed: 10.6433 learning rate: 0.000100, scenario: 0, slope: -7.721261582025581, fluctuations: 0.0\n",
      "step: 8430 loss: 10241.151403 time elapsed: 10.6563 learning rate: 0.000100, scenario: 0, slope: -7.6477518227857635, fluctuations: 0.0\n",
      "step: 8440 loss: 10168.571321 time elapsed: 10.6690 learning rate: 0.000100, scenario: 0, slope: -7.575010004776587, fluctuations: 0.0\n",
      "step: 8450 loss: 10096.683876 time elapsed: 10.6817 learning rate: 0.000100, scenario: 0, slope: -7.502981821238963, fluctuations: 0.0\n",
      "step: 8460 loss: 10025.483391 time elapsed: 10.6948 learning rate: 0.000100, scenario: 0, slope: -7.431622049283187, fluctuations: 0.0\n",
      "step: 8470 loss: 9954.968896 time elapsed: 10.7078 learning rate: 0.000100, scenario: 0, slope: -7.360888173765982, fluctuations: 0.0\n",
      "step: 8480 loss: 9885.120102 time elapsed: 10.7209 learning rate: 0.000100, scenario: 0, slope: -7.290760101403817, fluctuations: 0.0\n",
      "step: 8490 loss: 9815.945094 time elapsed: 10.7336 learning rate: 0.000100, scenario: 0, slope: -7.221234641908683, fluctuations: 0.0\n",
      "step: 8500 loss: 9747.432258 time elapsed: 10.7458 learning rate: 0.000100, scenario: 0, slope: -7.159167617300864, fluctuations: 0.0\n",
      "step: 8510 loss: 9679.574581 time elapsed: 10.7588 learning rate: 0.000100, scenario: 0, slope: -7.083963081557992, fluctuations: 0.0\n",
      "step: 8520 loss: 9612.364891 time elapsed: 10.7714 learning rate: 0.000100, scenario: 0, slope: -7.016234413944905, fluctuations: 0.0\n",
      "step: 8530 loss: 9545.795662 time elapsed: 10.7840 learning rate: 0.000100, scenario: 0, slope: -6.949134910921278, fluctuations: 0.0\n",
      "step: 8540 loss: 9479.859085 time elapsed: 10.7965 learning rate: 0.000100, scenario: 0, slope: -6.882689218731104, fluctuations: 0.0\n",
      "step: 8550 loss: 9414.547147 time elapsed: 10.8099 learning rate: 0.000100, scenario: 0, slope: -6.816924675784243, fluctuations: 0.0\n",
      "step: 8560 loss: 9349.851681 time elapsed: 10.8226 learning rate: 0.000100, scenario: 0, slope: -6.75186945080396, fluctuations: 0.0\n",
      "step: 8570 loss: 9285.764423 time elapsed: 10.8353 learning rate: 0.000100, scenario: 0, slope: -6.687542980229494, fluctuations: 0.0\n",
      "step: 8580 loss: 9222.277070 time elapsed: 10.8478 learning rate: 0.000100, scenario: 0, slope: -6.623969419469257, fluctuations: 0.0\n",
      "step: 8590 loss: 9159.381337 time elapsed: 10.8606 learning rate: 0.000100, scenario: 0, slope: -6.56118380560414, fluctuations: 0.0\n",
      "step: 8600 loss: 9097.069013 time elapsed: 10.8731 learning rate: 0.000100, scenario: 0, slope: -6.505359376294855, fluctuations: 0.0\n",
      "step: 8610 loss: 9035.332003 time elapsed: 10.8860 learning rate: 0.000100, scenario: 0, slope: -6.438019497080532, fluctuations: 0.0\n",
      "step: 8620 loss: 8974.162372 time elapsed: 10.8987 learning rate: 0.000100, scenario: 0, slope: -6.377657543864832, fluctuations: 0.0\n",
      "step: 8630 loss: 8913.552374 time elapsed: 10.9112 learning rate: 0.000100, scenario: 0, slope: -6.318112368807024, fluctuations: 0.0\n",
      "step: 8640 loss: 8853.494470 time elapsed: 10.9234 learning rate: 0.000100, scenario: 0, slope: -6.259380321855898, fluctuations: 0.0\n",
      "step: 8650 loss: 8793.981341 time elapsed: 10.9358 learning rate: 0.000100, scenario: 0, slope: -6.201453219543534, fluctuations: 0.0\n",
      "step: 8660 loss: 8735.005883 time elapsed: 10.9485 learning rate: 0.000100, scenario: 0, slope: -6.144318939718034, fluctuations: 0.0\n",
      "step: 8670 loss: 8676.561200 time elapsed: 10.9611 learning rate: 0.000100, scenario: 0, slope: -6.087962177953611, fluctuations: 0.0\n",
      "step: 8680 loss: 8618.640584 time elapsed: 10.9733 learning rate: 0.000100, scenario: 0, slope: -6.032365309392521, fluctuations: 0.0\n",
      "step: 8690 loss: 8561.237487 time elapsed: 10.9859 learning rate: 0.000100, scenario: 0, slope: -5.977509306032298, fluctuations: 0.0\n",
      "step: 8700 loss: 8504.345493 time elapsed: 10.9979 learning rate: 0.000100, scenario: 0, slope: -5.928756215623196, fluctuations: 0.0\n",
      "step: 8710 loss: 8447.958283 time elapsed: 11.0123 learning rate: 0.000100, scenario: 0, slope: -5.869942270579339, fluctuations: 0.0\n",
      "step: 8720 loss: 8392.069602 time elapsed: 11.0252 learning rate: 0.000100, scenario: 0, slope: -5.817194233251986, fluctuations: 0.0\n",
      "step: 8730 loss: 8336.673224 time elapsed: 11.0379 learning rate: 0.000100, scenario: 0, slope: -5.7651145224667335, fluctuations: 0.0\n",
      "step: 8740 loss: 8281.762926 time elapsed: 11.0507 learning rate: 0.000100, scenario: 0, slope: -5.7136895121768365, fluctuations: 0.0\n",
      "step: 8750 loss: 8227.332458 time elapsed: 11.0633 learning rate: 0.000100, scenario: 0, slope: -5.662908337277284, fluctuations: 0.0\n",
      "step: 8760 loss: 8173.375535 time elapsed: 11.0757 learning rate: 0.000100, scenario: 0, slope: -5.612763066273509, fluctuations: 0.0\n",
      "step: 8770 loss: 8119.890978 time elapsed: 11.0882 learning rate: 0.000100, scenario: 0, slope: -5.563241254569961, fluctuations: 0.0\n",
      "step: 8780 loss: 8066.857804 time elapsed: 11.1007 learning rate: 0.000100, scenario: 0, slope: -5.514339050669812, fluctuations: 0.0\n",
      "step: 8790 loss: 8014.282704 time elapsed: 11.1133 learning rate: 0.000100, scenario: 0, slope: -5.466082774615339, fluctuations: 0.0\n",
      "step: 8800 loss: 7962.155199 time elapsed: 11.1259 learning rate: 0.000100, scenario: 0, slope: -5.4231955272511065, fluctuations: 0.0\n",
      "step: 8810 loss: 7910.468471 time elapsed: 11.1390 learning rate: 0.000100, scenario: 0, slope: -5.371480721550108, fluctuations: 0.0\n",
      "step: 8820 loss: 7859.215348 time elapsed: 11.1516 learning rate: 0.000100, scenario: 0, slope: -5.325146082782444, fluctuations: 0.0\n",
      "step: 8830 loss: 7808.388565 time elapsed: 11.1642 learning rate: 0.000100, scenario: 0, slope: -5.279468450279525, fluctuations: 0.0\n",
      "step: 8840 loss: 7757.980666 time elapsed: 11.1767 learning rate: 0.000100, scenario: 0, slope: -5.234460333336971, fluctuations: 0.0\n",
      "step: 8850 loss: 7707.983935 time elapsed: 11.1894 learning rate: 0.000100, scenario: 0, slope: -5.1901365243433935, fluctuations: 0.0\n",
      "step: 8860 loss: 7658.390336 time elapsed: 11.2018 learning rate: 0.000100, scenario: 0, slope: -5.146514284074288, fluctuations: 0.0\n",
      "step: 8870 loss: 7609.191450 time elapsed: 11.2155 learning rate: 0.000100, scenario: 0, slope: -5.103605629549734, fluctuations: 0.0\n",
      "step: 8880 loss: 7560.378418 time elapsed: 11.2285 learning rate: 0.000100, scenario: 0, slope: -5.06142473718906, fluctuations: 0.0\n",
      "step: 8890 loss: 7511.941879 time elapsed: 11.2412 learning rate: 0.000100, scenario: 0, slope: -5.0200280682128975, fluctuations: 0.0\n",
      "step: 8900 loss: 7463.871920 time elapsed: 11.2538 learning rate: 0.000100, scenario: 0, slope: -4.98345774080802, fluctuations: 0.0\n",
      "step: 8910 loss: 7416.158070 time elapsed: 11.2668 learning rate: 0.000100, scenario: 0, slope: -4.939681842621404, fluctuations: 0.0\n",
      "step: 8920 loss: 7368.789376 time elapsed: 11.2795 learning rate: 0.000100, scenario: 0, slope: -4.900806930787579, fluctuations: 0.0\n",
      "step: 8930 loss: 7321.754613 time elapsed: 11.2921 learning rate: 0.000100, scenario: 0, slope: -4.862854668100066, fluctuations: 0.0\n",
      "step: 8940 loss: 7275.042699 time elapsed: 11.3047 learning rate: 0.000100, scenario: 0, slope: -4.825869062164021, fluctuations: 0.0\n",
      "step: 8950 loss: 7228.643378 time elapsed: 11.3172 learning rate: 0.000100, scenario: 0, slope: -4.789888557946091, fluctuations: 0.0\n",
      "step: 8960 loss: 7182.548167 time elapsed: 11.3300 learning rate: 0.000100, scenario: 0, slope: -4.75493698863543, fluctuations: 0.0\n",
      "step: 8970 loss: 7136.751465 time elapsed: 11.3425 learning rate: 0.000100, scenario: 0, slope: -4.721011153231454, fluctuations: 0.0\n",
      "step: 8980 loss: 7091.251585 time elapsed: 11.3547 learning rate: 0.000100, scenario: 0, slope: -4.688066188576968, fluctuations: 0.0\n",
      "step: 8990 loss: 7046.051351 time elapsed: 11.3672 learning rate: 0.000100, scenario: 0, slope: -4.656001632572919, fluctuations: 0.0\n",
      "step: 9000 loss: 7001.157940 time elapsed: 11.3796 learning rate: 0.000100, scenario: 0, slope: -4.627761324794268, fluctuations: 0.0\n",
      "step: 9010 loss: 6956.581864 time elapsed: 11.3926 learning rate: 0.000100, scenario: 0, slope: -4.593790437380502, fluctuations: 0.0\n",
      "step: 9020 loss: 6912.335307 time elapsed: 11.4049 learning rate: 0.000100, scenario: 0, slope: -4.563136730397591, fluctuations: 0.0\n",
      "step: 9030 loss: 6868.430256 time elapsed: 11.4183 learning rate: 0.000100, scenario: 0, slope: -4.532388967644757, fluctuations: 0.0\n",
      "step: 9040 loss: 6824.876995 time elapsed: 11.4317 learning rate: 0.000100, scenario: 0, slope: -4.501255248779725, fluctuations: 0.0\n",
      "step: 9050 loss: 6781.693363 time elapsed: 11.4444 learning rate: 0.000100, scenario: 0, slope: -4.469471689732901, fluctuations: 0.0\n",
      "step: 9060 loss: 6738.852318 time elapsed: 11.4569 learning rate: 0.000100, scenario: 0, slope: -4.436896338227596, fluctuations: 0.0\n",
      "step: 9070 loss: 6696.386749 time elapsed: 11.4695 learning rate: 0.000100, scenario: 0, slope: -4.403469304992616, fluctuations: 0.0\n",
      "step: 9080 loss: 6654.284839 time elapsed: 11.4821 learning rate: 0.000100, scenario: 0, slope: -4.369211731159489, fluctuations: 0.0\n",
      "step: 9090 loss: 6612.543514 time elapsed: 11.4947 learning rate: 0.000100, scenario: 0, slope: -4.3342402415142365, fluctuations: 0.0\n",
      "step: 9100 loss: 6571.158382 time elapsed: 11.5071 learning rate: 0.000100, scenario: 0, slope: -4.30230554861834, fluctuations: 0.0\n",
      "step: 9110 loss: 6530.124183 time elapsed: 11.5202 learning rate: 0.000100, scenario: 0, slope: -4.262919090921446, fluctuations: 0.0\n",
      "step: 9120 loss: 6489.435184 time elapsed: 11.5328 learning rate: 0.000100, scenario: 0, slope: -4.227004479428855, fluctuations: 0.0\n",
      "step: 9130 loss: 6449.085424 time elapsed: 11.5457 learning rate: 0.000100, scenario: 0, slope: -4.191192439178356, fluctuations: 0.0\n",
      "step: 9140 loss: 6409.068893 time elapsed: 11.5583 learning rate: 0.000100, scenario: 0, slope: -4.15564748106923, fluctuations: 0.0\n",
      "step: 9150 loss: 6369.379660 time elapsed: 11.5711 learning rate: 0.000100, scenario: 0, slope: -4.120473714568172, fluctuations: 0.0\n",
      "step: 9160 loss: 6330.011953 time elapsed: 11.5839 learning rate: 0.000100, scenario: 0, slope: -4.085772879419872, fluctuations: 0.0\n",
      "step: 9170 loss: 6290.960170 time elapsed: 11.5966 learning rate: 0.000100, scenario: 0, slope: -4.051616117230418, fluctuations: 0.0\n",
      "step: 9180 loss: 6252.218897 time elapsed: 11.6090 learning rate: 0.000100, scenario: 0, slope: -4.01802481966065, fluctuations: 0.0\n",
      "step: 9190 loss: 6213.782897 time elapsed: 11.6229 learning rate: 0.000100, scenario: 0, slope: -3.985007363008056, fluctuations: 0.0\n",
      "step: 9200 loss: 6175.647090 time elapsed: 11.6359 learning rate: 0.000100, scenario: 0, slope: -3.955780621627285, fluctuations: 0.0\n",
      "step: 9210 loss: 6137.806525 time elapsed: 11.6494 learning rate: 0.000100, scenario: 0, slope: -3.9206779432897054, fluctuations: 0.0\n",
      "step: 9220 loss: 6100.256352 time elapsed: 11.6622 learning rate: 0.000100, scenario: 0, slope: -3.889343381104646, fluctuations: 0.0\n",
      "step: 9230 loss: 6062.991781 time elapsed: 11.6748 learning rate: 0.000100, scenario: 0, slope: -3.8585431969422403, fluctuations: 0.0\n",
      "step: 9240 loss: 6026.008041 time elapsed: 11.6872 learning rate: 0.000100, scenario: 0, slope: -3.8282631502725186, fluctuations: 0.0\n",
      "step: 9250 loss: 5989.300330 time elapsed: 11.6997 learning rate: 0.000100, scenario: 0, slope: -3.79849068687495, fluctuations: 0.0\n",
      "step: 9260 loss: 5952.863754 time elapsed: 11.7123 learning rate: 0.000100, scenario: 0, slope: -3.7692159230307127, fluctuations: 0.0\n",
      "step: 9270 loss: 5916.693259 time elapsed: 11.7250 learning rate: 0.000100, scenario: 0, slope: -3.7404324590825286, fluctuations: 0.0\n",
      "step: 9280 loss: 5880.783528 time elapsed: 11.7377 learning rate: 0.000100, scenario: 0, slope: -3.7121381791234778, fluctuations: 0.0\n",
      "step: 9290 loss: 5845.128858 time elapsed: 11.7503 learning rate: 0.000100, scenario: 0, slope: -3.684336200155913, fluctuations: 0.0\n",
      "step: 9300 loss: 5809.722975 time elapsed: 11.7628 learning rate: 0.000100, scenario: 0, slope: -3.659743136951007, fluctuations: 0.0\n",
      "step: 9310 loss: 5774.558852 time elapsed: 11.7760 learning rate: 0.000100, scenario: 0, slope: -3.6302559698675037, fluctuations: 0.0\n",
      "step: 9320 loss: 5739.639716 time elapsed: 11.7886 learning rate: 0.000100, scenario: 0, slope: -3.604004846565456, fluctuations: 0.0\n",
      "step: 9330 loss: 5704.921516 time elapsed: 11.8014 learning rate: 0.000100, scenario: 0, slope: -3.578345309364514, fluctuations: 0.0\n",
      "step: 9340 loss: 5670.424752 time elapsed: 11.8136 learning rate: 0.000100, scenario: 0, slope: -3.5533699534666994, fluctuations: 0.0\n",
      "step: 9350 loss: 5636.119330 time elapsed: 11.8274 learning rate: 0.000100, scenario: 0, slope: -3.5291645315749927, fluctuations: 0.0\n",
      "step: 9360 loss: 5601.980140 time elapsed: 11.8403 learning rate: 0.000100, scenario: 0, slope: -3.505888440334443, fluctuations: 0.0\n",
      "step: 9370 loss: 5567.966696 time elapsed: 11.8531 learning rate: 0.000100, scenario: 0, slope: -3.4838047734911406, fluctuations: 0.0\n",
      "step: 9380 loss: 5534.012313 time elapsed: 11.8656 learning rate: 0.000100, scenario: 0, slope: -3.4633557275702973, fluctuations: 0.0\n",
      "step: 9390 loss: 5500.002337 time elapsed: 11.8783 learning rate: 0.000100, scenario: 0, slope: -3.4453088289327143, fluctuations: 0.0\n",
      "step: 9400 loss: 5465.739483 time elapsed: 11.8909 learning rate: 0.000100, scenario: 0, slope: -3.4322219174807844, fluctuations: 0.0\n",
      "step: 9410 loss: 5430.916092 time elapsed: 11.9038 learning rate: 0.000100, scenario: 0, slope: -3.422797164397649, fluctuations: 0.0\n",
      "step: 9420 loss: 5395.178551 time elapsed: 11.9163 learning rate: 0.000100, scenario: 0, slope: -3.4240654510623094, fluctuations: 0.0\n",
      "step: 9430 loss: 5358.371851 time elapsed: 11.9289 learning rate: 0.000100, scenario: 0, slope: -3.438574344614157, fluctuations: 0.0\n",
      "step: 9440 loss: 5320.768325 time elapsed: 11.9415 learning rate: 0.000100, scenario: 0, slope: -3.4682324411740137, fluctuations: 0.0\n",
      "step: 9450 loss: 5282.936872 time elapsed: 11.9542 learning rate: 0.000100, scenario: 0, slope: -3.511074064013985, fluctuations: 0.0\n",
      "step: 9460 loss: 5245.395743 time elapsed: 11.9665 learning rate: 0.000100, scenario: 0, slope: -3.5614020868640175, fluctuations: 0.0\n",
      "step: 9470 loss: 5208.426397 time elapsed: 11.9788 learning rate: 0.000100, scenario: 0, slope: -3.6115175132962207, fluctuations: 0.0\n",
      "step: 9480 loss: 5172.107874 time elapsed: 11.9916 learning rate: 0.000100, scenario: 0, slope: -3.6536021210053353, fluctuations: 0.0\n",
      "step: 9490 loss: 5136.423228 time elapsed: 12.0046 learning rate: 0.000100, scenario: 0, slope: -3.6809854010187304, fluctuations: 0.0\n",
      "step: 9500 loss: 5101.331527 time elapsed: 12.0166 learning rate: 0.000100, scenario: 0, slope: -3.689133977706846, fluctuations: 0.0\n",
      "step: 9510 loss: 5066.793356 time elapsed: 12.0307 learning rate: 0.000100, scenario: 0, slope: -3.675737072745624, fluctuations: 0.0\n",
      "step: 9520 loss: 5032.774960 time elapsed: 12.0473 learning rate: 0.000100, scenario: 0, slope: -3.6430492132637937, fluctuations: 0.0\n",
      "step: 9530 loss: 4999.247438 time elapsed: 12.0617 learning rate: 0.000100, scenario: 0, slope: -3.5962390206927837, fluctuations: 0.0\n",
      "step: 9540 loss: 4966.185532 time elapsed: 12.0755 learning rate: 0.000100, scenario: 0, slope: -3.542171222342711, fluctuations: 0.0\n",
      "step: 9550 loss: 4933.566746 time elapsed: 12.0896 learning rate: 0.000100, scenario: 0, slope: -3.486537697608425, fluctuations: 0.0\n",
      "step: 9560 loss: 4901.370784 time elapsed: 12.1039 learning rate: 0.000100, scenario: 0, slope: -3.4325322816578443, fluctuations: 0.0\n",
      "step: 9570 loss: 4869.579163 time elapsed: 12.1175 learning rate: 0.000100, scenario: 0, slope: -3.3812680388036203, fluctuations: 0.0\n",
      "step: 9580 loss: 4838.174937 time elapsed: 12.1308 learning rate: 0.000100, scenario: 0, slope: -3.3328009144739887, fluctuations: 0.0\n",
      "step: 9590 loss: 4807.142488 time elapsed: 12.1453 learning rate: 0.000100, scenario: 0, slope: -3.2868666483971696, fluctuations: 0.0\n",
      "step: 9600 loss: 4776.467368 time elapsed: 12.1592 learning rate: 0.000100, scenario: 0, slope: -3.2474568705657405, fluctuations: 0.0\n",
      "step: 9610 loss: 4746.136188 time elapsed: 12.1732 learning rate: 0.000100, scenario: 0, slope: -3.20151388773336, fluctuations: 0.0\n",
      "step: 9620 loss: 4716.142365 time elapsed: 12.1861 learning rate: 0.000100, scenario: 0, slope: -3.1616623628910965, fluctuations: 0.0\n",
      "step: 9630 loss: 4686.467402 time elapsed: 12.1988 learning rate: 0.000100, scenario: 0, slope: -3.123425764860297, fluctuations: 0.0\n",
      "step: 9640 loss: 4657.089889 time elapsed: 12.2128 learning rate: 0.000100, scenario: 0, slope: -3.0867785497830598, fluctuations: 0.0\n",
      "step: 9650 loss: 4628.015886 time elapsed: 12.2280 learning rate: 0.000100, scenario: 0, slope: -3.051558221745002, fluctuations: 0.0\n",
      "step: 9660 loss: 4599.233094 time elapsed: 12.2419 learning rate: 0.000100, scenario: 0, slope: -3.017652683623243, fluctuations: 0.0\n",
      "step: 9670 loss: 4570.731456 time elapsed: 12.2556 learning rate: 0.000100, scenario: 0, slope: -2.98496659660012, fluctuations: 0.0\n",
      "step: 9680 loss: 4542.502168 time elapsed: 12.2690 learning rate: 0.000100, scenario: 0, slope: -2.953415851900727, fluctuations: 0.0\n",
      "step: 9690 loss: 4514.537280 time elapsed: 12.2818 learning rate: 0.000100, scenario: 0, slope: -2.922924771288161, fluctuations: 0.0\n",
      "step: 9700 loss: 4486.829377 time elapsed: 12.2947 learning rate: 0.000100, scenario: 0, slope: -2.896331945323334, fluctuations: 0.0\n",
      "step: 9710 loss: 4459.371476 time elapsed: 12.3105 learning rate: 0.000100, scenario: 0, slope: -2.8648531548279683, fluctuations: 0.0\n",
      "step: 9720 loss: 4432.156971 time elapsed: 12.3241 learning rate: 0.000100, scenario: 0, slope: -2.8371450773514755, fluctuations: 0.0\n",
      "step: 9730 loss: 4405.179595 time elapsed: 12.3378 learning rate: 0.000100, scenario: 0, slope: -2.8101856659787683, fluctuations: 0.0\n",
      "step: 9740 loss: 4378.433397 time elapsed: 12.3509 learning rate: 0.000100, scenario: 0, slope: -2.7840400709546205, fluctuations: 0.0\n",
      "step: 9750 loss: 4351.912711 time elapsed: 12.3639 learning rate: 0.000100, scenario: 0, slope: -2.7586292282390583, fluctuations: 0.0\n",
      "step: 9760 loss: 4325.612151 time elapsed: 12.3771 learning rate: 0.000100, scenario: 0, slope: -2.7339070766337232, fluctuations: 0.0\n",
      "step: 9770 loss: 4299.526589 time elapsed: 12.3898 learning rate: 0.000100, scenario: 0, slope: -2.7098348246569084, fluctuations: 0.0\n",
      "step: 9780 loss: 4273.651163 time elapsed: 12.4028 learning rate: 0.000100, scenario: 0, slope: -2.6863780612578108, fluctuations: 0.0\n",
      "step: 9790 loss: 4247.981272 time elapsed: 12.4161 learning rate: 0.000100, scenario: 0, slope: -2.663505171104752, fluctuations: 0.0\n",
      "step: 9800 loss: 4222.512601 time elapsed: 12.4312 learning rate: 0.000100, scenario: 0, slope: -2.643394216618068, fluctuations: 0.0\n",
      "step: 9810 loss: 4197.241131 time elapsed: 12.4463 learning rate: 0.000100, scenario: 0, slope: -2.619393458587332, fluctuations: 0.0\n",
      "step: 9820 loss: 4172.163176 time elapsed: 12.4606 learning rate: 0.000100, scenario: 0, slope: -2.5980981660182585, fluctuations: 0.0\n",
      "step: 9830 loss: 4147.275405 time elapsed: 12.4749 learning rate: 0.000100, scenario: 0, slope: -2.5772723922004985, fluctuations: 0.0\n",
      "step: 9840 loss: 4122.574878 time elapsed: 12.4892 learning rate: 0.000100, scenario: 0, slope: -2.5568870583531265, fluctuations: 0.0\n",
      "step: 9850 loss: 4098.059065 time elapsed: 12.5021 learning rate: 0.000100, scenario: 0, slope: -2.5369116081017817, fluctuations: 0.0\n",
      "step: 9860 loss: 4073.725865 time elapsed: 12.5147 learning rate: 0.000100, scenario: 0, slope: -2.5173135632415935, fluctuations: 0.0\n",
      "step: 9870 loss: 4049.573611 time elapsed: 12.5276 learning rate: 0.000100, scenario: 0, slope: -2.498058289263646, fluctuations: 0.0\n",
      "step: 9880 loss: 4025.601052 time elapsed: 12.5408 learning rate: 0.000100, scenario: 0, slope: -2.4791090271193332, fluctuations: 0.0\n",
      "step: 9890 loss: 4001.807434 time elapsed: 12.5535 learning rate: 0.000100, scenario: 0, slope: -2.4604270885281774, fluctuations: 0.0\n",
      "step: 9900 loss: 3978.216397 time elapsed: 12.5662 learning rate: 0.000100, scenario: 0, slope: -2.4437856121295365, fluctuations: 0.0\n",
      "step: 9910 loss: 3954.755647 time elapsed: 12.5796 learning rate: 0.000100, scenario: 0, slope: -2.4236153815051864, fluctuations: 0.0\n",
      "step: 9920 loss: 3931.496757 time elapsed: 12.5922 learning rate: 0.000100, scenario: 0, slope: -2.405499730291367, fluctuations: 0.0\n",
      "step: 9930 loss: 3908.415994 time elapsed: 12.6048 learning rate: 0.000100, scenario: 0, slope: -2.3875087149218968, fluctuations: 0.0\n",
      "step: 9940 loss: 3885.513812 time elapsed: 12.6175 learning rate: 0.000100, scenario: 0, slope: -2.3696001219578897, fluctuations: 0.0\n",
      "step: 9950 loss: 3862.790140 time elapsed: 12.6303 learning rate: 0.000100, scenario: 0, slope: -2.3517433708982174, fluctuations: 0.0\n",
      "step: 9960 loss: 3840.244944 time elapsed: 12.6442 learning rate: 0.000100, scenario: 0, slope: -2.3339158317692377, fluctuations: 0.0\n",
      "step: 9970 loss: 3817.878058 time elapsed: 12.6574 learning rate: 0.000100, scenario: 0, slope: -2.3161022695928195, fluctuations: 0.0\n",
      "step: 9980 loss: 3795.689117 time elapsed: 12.6703 learning rate: 0.000100, scenario: 0, slope: -2.2982947885994895, fluctuations: 0.0\n",
      "step: 9990 loss: 3773.677517 time elapsed: 12.6828 learning rate: 0.000100, scenario: 0, slope: -2.2804925251769537, fluctuations: 0.0\n",
      "step: 10000 loss: 3751.842379 time elapsed: 12.6951 learning rate: 0.000100, scenario: 0, slope: -2.2644544741595602, fluctuations: 0.0\n",
      "step: 10010 loss: 3730.182553 time elapsed: 12.7082 learning rate: 0.000100, scenario: 0, slope: -2.2448050661759233, fluctuations: 0.0\n",
      "step: 10020 loss: 3708.696647 time elapsed: 12.7205 learning rate: 0.000100, scenario: 0, slope: -2.2270300844643236, fluctuations: 0.0\n",
      "step: 10030 loss: 3687.383041 time elapsed: 12.7330 learning rate: 0.000100, scenario: 0, slope: -2.209321091061622, fluctuations: 0.0\n",
      "step: 10040 loss: 3666.239909 time elapsed: 12.7451 learning rate: 0.000100, scenario: 0, slope: -2.1916933696434158, fluctuations: 0.0\n",
      "step: 10050 loss: 3645.265257 time elapsed: 12.7577 learning rate: 0.000100, scenario: 0, slope: -2.174168084445627, fluctuations: 0.0\n",
      "step: 10060 loss: 3624.456950 time elapsed: 12.7703 learning rate: 0.000100, scenario: 0, slope: -2.1567676758080805, fluctuations: 0.0\n",
      "step: 10070 loss: 3603.812745 time elapsed: 12.7829 learning rate: 0.000100, scenario: 0, slope: -2.1395139989092553, fluctuations: 0.0\n",
      "step: 10080 loss: 3583.330314 time elapsed: 12.7954 learning rate: 0.000100, scenario: 0, slope: -2.1224272646365256, fluctuations: 0.0\n",
      "step: 10090 loss: 3563.007279 time elapsed: 12.8080 learning rate: 0.000100, scenario: 0, slope: -2.105525415981833, fluctuations: 0.0\n",
      "step: 10100 loss: 3542.841224 time elapsed: 12.8205 learning rate: 0.000100, scenario: 0, slope: -2.090484564552159, fluctuations: 0.0\n",
      "step: 10110 loss: 3522.829719 time elapsed: 12.8338 learning rate: 0.000100, scenario: 0, slope: -2.0723349988220097, fluctuations: 0.0\n",
      "step: 10120 loss: 3502.970337 time elapsed: 12.8462 learning rate: 0.000100, scenario: 0, slope: -2.0560689449283305, fluctuations: 0.0\n",
      "step: 10130 loss: 3483.260662 time elapsed: 12.8595 learning rate: 0.000100, scenario: 0, slope: -2.0400330157208564, fluctuations: 0.0\n",
      "step: 10140 loss: 3463.698301 time elapsed: 12.8724 learning rate: 0.000100, scenario: 0, slope: -2.024232298832139, fluctuations: 0.0\n",
      "step: 10150 loss: 3444.280896 time elapsed: 12.8852 learning rate: 0.000100, scenario: 0, slope: -2.0086698561398912, fluctuations: 0.0\n",
      "step: 10160 loss: 3425.007179 time elapsed: 12.8977 learning rate: 0.000100, scenario: 0, slope: -1.9933456570315065, fluctuations: 0.0\n",
      "step: 10170 loss: 3405.885994 time elapsed: 12.9100 learning rate: 0.000100, scenario: 0, slope: -1.9781464088243548, fluctuations: 0.0\n",
      "step: 10180 loss: 3386.884550 time elapsed: 12.9224 learning rate: 0.000100, scenario: 0, slope: -1.9632769790992424, fluctuations: 0.0\n",
      "step: 10190 loss: 3368.018714 time elapsed: 12.9348 learning rate: 0.000100, scenario: 0, slope: -1.9486813801405576, fluctuations: 0.0\n",
      "step: 10200 loss: 3349.291183 time elapsed: 12.9470 learning rate: 0.000100, scenario: 0, slope: -1.935753764719483, fluctuations: 0.0\n",
      "step: 10210 loss: 3330.696752 time elapsed: 12.9601 learning rate: 0.000100, scenario: 0, slope: -1.920208989245221, fluctuations: 0.0\n",
      "step: 10220 loss: 3312.232212 time elapsed: 12.9728 learning rate: 0.000100, scenario: 0, slope: -1.9063146192220985, fluctuations: 0.0\n",
      "step: 10230 loss: 3293.895316 time elapsed: 12.9853 learning rate: 0.000100, scenario: 0, slope: -1.8926418651460966, fluctuations: 0.0\n",
      "step: 10240 loss: 3275.684059 time elapsed: 12.9978 learning rate: 0.000100, scenario: 0, slope: -1.8791869628391422, fluctuations: 0.0\n",
      "step: 10250 loss: 3257.596514 time elapsed: 13.0105 learning rate: 0.000100, scenario: 0, slope: -1.8659463143110036, fluctuations: 0.0\n",
      "step: 10260 loss: 3239.630802 time elapsed: 13.0230 learning rate: 0.000100, scenario: 0, slope: -1.8529149415250672, fluctuations: 0.0\n",
      "step: 10270 loss: 3221.785086 time elapsed: 13.0357 learning rate: 0.000100, scenario: 0, slope: -1.8399583148254146, fluctuations: 0.0\n",
      "step: 10280 loss: 3204.057562 time elapsed: 13.0480 learning rate: 0.000100, scenario: 0, slope: -1.8272540992565502, fluctuations: 0.0\n",
      "step: 10290 loss: 3186.446460 time elapsed: 13.0616 learning rate: 0.000100, scenario: 0, slope: -1.8147724367204845, fluctuations: 0.0\n",
      "step: 10300 loss: 3168.950044 time elapsed: 13.0743 learning rate: 0.000100, scenario: 0, slope: -1.803709611312506, fluctuations: 0.0\n",
      "step: 10310 loss: 3151.566607 time elapsed: 13.0879 learning rate: 0.000100, scenario: 0, slope: -1.7903972868170264, fluctuations: 0.0\n",
      "step: 10320 loss: 3134.294464 time elapsed: 13.1006 learning rate: 0.000100, scenario: 0, slope: -1.7784881078260255, fluctuations: 0.0\n",
      "step: 10330 loss: 3117.131953 time elapsed: 13.1133 learning rate: 0.000100, scenario: 0, slope: -1.7667583988509805, fluctuations: 0.0\n",
      "step: 10340 loss: 3100.077428 time elapsed: 13.1260 learning rate: 0.000100, scenario: 0, slope: -1.7552046604832858, fluctuations: 0.0\n",
      "step: 10350 loss: 3083.129251 time elapsed: 13.1386 learning rate: 0.000100, scenario: 0, slope: -1.7438238891313325, fluctuations: 0.0\n",
      "step: 10360 loss: 3066.285787 time elapsed: 13.1507 learning rate: 0.000100, scenario: 0, slope: -1.7326135219416583, fluctuations: 0.0\n",
      "step: 10370 loss: 3049.545395 time elapsed: 13.1632 learning rate: 0.000100, scenario: 0, slope: -1.721571458366505, fluctuations: 0.0\n",
      "step: 10380 loss: 3032.906418 time elapsed: 13.1757 learning rate: 0.000100, scenario: 0, slope: -1.7106961197263761, fluctuations: 0.0\n",
      "step: 10390 loss: 3016.367169 time elapsed: 13.1882 learning rate: 0.000100, scenario: 0, slope: -1.699986532030588, fluctuations: 0.0\n",
      "step: 10400 loss: 2999.925918 time elapsed: 13.2007 learning rate: 0.000100, scenario: 0, slope: -1.6904893953411961, fluctuations: 0.0\n",
      "step: 10410 loss: 2983.580866 time elapsed: 13.2138 learning rate: 0.000100, scenario: 0, slope: -1.6790644877191443, fluctuations: 0.0\n",
      "step: 10420 loss: 2967.330120 time elapsed: 13.2264 learning rate: 0.000100, scenario: 0, slope: -1.6688543847334265, fluctuations: 0.0\n",
      "step: 10430 loss: 2951.171661 time elapsed: 13.2391 learning rate: 0.000100, scenario: 0, slope: -1.6588152297903083, fluctuations: 0.0\n",
      "step: 10440 loss: 2935.104615 time elapsed: 13.2514 learning rate: 0.000100, scenario: 0, slope: -1.6489501394823372, fluctuations: 0.0\n",
      "step: 10450 loss: 2919.146356 time elapsed: 13.2654 learning rate: 0.000100, scenario: 0, slope: -1.639122978691287, fluctuations: 0.0\n",
      "step: 10460 loss: 2903.240940 time elapsed: 13.2792 learning rate: 0.000100, scenario: 0, slope: -1.6296060273669768, fluctuations: 0.0\n",
      "step: 10470 loss: 2887.419019 time elapsed: 13.2923 learning rate: 0.000100, scenario: 0, slope: -1.620341312950741, fluctuations: 0.0\n",
      "step: 10480 loss: 2871.680489 time elapsed: 13.3052 learning rate: 0.000100, scenario: 0, slope: -1.6113172416292072, fluctuations: 0.0\n",
      "step: 10490 loss: 2856.016377 time elapsed: 13.3178 learning rate: 0.000100, scenario: 0, slope: -1.6025527496869207, fluctuations: 0.0\n",
      "step: 10500 loss: 2840.418197 time elapsed: 13.3301 learning rate: 0.000100, scenario: 0, slope: -1.5949195776793128, fluctuations: 0.0\n",
      "step: 10510 loss: 2824.875563 time elapsed: 13.3431 learning rate: 0.000100, scenario: 0, slope: -1.5859882082797467, fluctuations: 0.0\n",
      "step: 10520 loss: 2809.373487 time elapsed: 13.3557 learning rate: 0.000100, scenario: 0, slope: -1.578353887273238, fluctuations: 0.0\n",
      "step: 10530 loss: 2793.888336 time elapsed: 13.3683 learning rate: 0.000100, scenario: 0, slope: -1.5713474493908863, fluctuations: 0.0\n",
      "step: 10540 loss: 2778.380407 time elapsed: 13.3807 learning rate: 0.000100, scenario: 0, slope: -1.5652380934294376, fluctuations: 0.0\n",
      "step: 10550 loss: 2762.779712 time elapsed: 13.3928 learning rate: 0.000100, scenario: 0, slope: -1.5603330915234246, fluctuations: 0.0\n",
      "step: 10560 loss: 2746.960960 time elapsed: 13.4064 learning rate: 0.000100, scenario: 0, slope: -1.5577091122906666, fluctuations: 0.0\n",
      "step: 10570 loss: 2730.713338 time elapsed: 13.4212 learning rate: 0.000100, scenario: 0, slope: -1.558834080197365, fluctuations: 0.0\n",
      "step: 10580 loss: 2713.752210 time elapsed: 13.4362 learning rate: 0.000100, scenario: 0, slope: -1.56611650736864, fluctuations: 0.0\n",
      "step: 10590 loss: 2695.854602 time elapsed: 13.4503 learning rate: 0.000100, scenario: 0, slope: -1.5826525465464758, fluctuations: 0.0\n",
      "step: 10600 loss: 2677.062186 time elapsed: 13.4634 learning rate: 0.000100, scenario: 0, slope: -1.6075508475148126, fluctuations: 0.0\n",
      "step: 10610 loss: 2657.717989 time elapsed: 13.4771 learning rate: 0.000100, scenario: 0, slope: -1.651115425709543, fluctuations: 0.0\n",
      "step: 10620 loss: 2638.275389 time elapsed: 13.4904 learning rate: 0.000100, scenario: 0, slope: -1.7001734124139076, fluctuations: 0.0\n",
      "step: 10630 loss: 2619.084074 time elapsed: 13.5036 learning rate: 0.000100, scenario: 0, slope: -1.7526051859997256, fluctuations: 0.0\n",
      "step: 10640 loss: 2600.319304 time elapsed: 13.5181 learning rate: 0.000100, scenario: 0, slope: -1.8018574612030953, fluctuations: 0.0\n",
      "step: 10650 loss: 2582.028331 time elapsed: 13.5314 learning rate: 0.000100, scenario: 0, slope: -1.8416509024126926, fluctuations: 0.0\n",
      "step: 10660 loss: 2564.198147 time elapsed: 13.5452 learning rate: 0.000100, scenario: 0, slope: -1.8669230799947738, fluctuations: 0.0\n",
      "step: 10670 loss: 2546.796937 time elapsed: 13.5592 learning rate: 0.000100, scenario: 0, slope: -1.8746172947673128, fluctuations: 0.0\n",
      "step: 10680 loss: 2529.790375 time elapsed: 13.5736 learning rate: 0.000100, scenario: 0, slope: -1.864469952407048, fluctuations: 0.0\n",
      "step: 10690 loss: 2513.145941 time elapsed: 13.5881 learning rate: 0.000100, scenario: 0, slope: -1.8393624125101602, fluctuations: 0.0\n",
      "step: 10700 loss: 2496.833617 time elapsed: 13.6022 learning rate: 0.000100, scenario: 0, slope: -1.8082138834712913, fluctuations: 0.0\n",
      "step: 10710 loss: 2480.825940 time elapsed: 13.6159 learning rate: 0.000100, scenario: 0, slope: -1.7651648044819617, fluctuations: 0.0\n",
      "step: 10720 loss: 2465.098019 time elapsed: 13.6297 learning rate: 0.000100, scenario: 0, slope: -1.7255818752584025, fluctuations: 0.0\n",
      "step: 10730 loss: 2449.627525 time elapsed: 13.6439 learning rate: 0.000100, scenario: 0, slope: -1.687873533039718, fluctuations: 0.0\n",
      "step: 10740 loss: 2434.408657 time elapsed: 13.6590 learning rate: 0.000100, scenario: 0, slope: -1.652816854064979, fluctuations: 0.0\n",
      "step: 10750 loss: 2419.421421 time elapsed: 13.6741 learning rate: 0.000100, scenario: 0, slope: -1.6203258473744153, fluctuations: 0.0\n",
      "step: 10760 loss: 2404.589166 time elapsed: 13.6881 learning rate: 0.000100, scenario: 0, slope: -1.5906219261584622, fluctuations: 0.0\n",
      "step: 10770 loss: 2389.962289 time elapsed: 13.7024 learning rate: 0.000100, scenario: 0, slope: -1.563331722214283, fluctuations: 0.0\n",
      "step: 10780 loss: 2375.520915 time elapsed: 13.7171 learning rate: 0.000100, scenario: 0, slope: -1.538186922750867, fluctuations: 0.0\n",
      "step: 10790 loss: 2361.249232 time elapsed: 13.7318 learning rate: 0.000100, scenario: 0, slope: -1.5149561520901873, fluctuations: 0.0\n",
      "step: 10800 loss: 2347.135120 time elapsed: 13.7470 learning rate: 0.000100, scenario: 0, slope: -1.4955134584896157, fluctuations: 0.0\n",
      "step: 10810 loss: 2333.169818 time elapsed: 13.7619 learning rate: 0.000100, scenario: 0, slope: -1.4734281110636205, fluctuations: 0.0\n",
      "step: 10820 loss: 2319.345868 time elapsed: 13.7762 learning rate: 0.000100, scenario: 0, slope: -1.4547740019644806, fluctuations: 0.0\n",
      "step: 10830 loss: 2305.656644 time elapsed: 13.7897 learning rate: 0.000100, scenario: 0, slope: -1.437319100288063, fluctuations: 0.0\n",
      "step: 10840 loss: 2292.096268 time elapsed: 13.8028 learning rate: 0.000100, scenario: 0, slope: -1.420910435993015, fluctuations: 0.0\n",
      "step: 10850 loss: 2278.659520 time elapsed: 13.8172 learning rate: 0.000100, scenario: 0, slope: -1.4051901322481029, fluctuations: 0.0\n",
      "step: 10860 loss: 2265.341757 time elapsed: 13.8299 learning rate: 0.000100, scenario: 0, slope: -1.3904408137096136, fluctuations: 0.0\n",
      "step: 10870 loss: 2252.138836 time elapsed: 13.8427 learning rate: 0.000100, scenario: 0, slope: -1.376461393610472, fluctuations: 0.0\n",
      "step: 10880 loss: 2239.047038 time elapsed: 13.8552 learning rate: 0.000100, scenario: 0, slope: -1.3631406454774206, fluctuations: 0.0\n",
      "step: 10890 loss: 2226.063010 time elapsed: 13.8678 learning rate: 0.000100, scenario: 0, slope: -1.3503963025575731, fluctuations: 0.0\n",
      "step: 10900 loss: 2213.183705 time elapsed: 13.8809 learning rate: 0.000100, scenario: 0, slope: -1.3393645553400897, fluctuations: 0.0\n",
      "step: 10910 loss: 2200.406339 time elapsed: 13.8981 learning rate: 0.000100, scenario: 0, slope: -1.3263834202996991, fluctuations: 0.0\n",
      "step: 10920 loss: 2187.728348 time elapsed: 13.9120 learning rate: 0.000100, scenario: 0, slope: -1.3150109344575667, fluctuations: 0.0\n",
      "step: 10930 loss: 2175.147345 time elapsed: 13.9256 learning rate: 0.000100, scenario: 0, slope: -1.3040040130779016, fluctuations: 0.0\n",
      "step: 10940 loss: 2162.661099 time elapsed: 13.9388 learning rate: 0.000100, scenario: 0, slope: -1.293327617499111, fluctuations: 0.0\n",
      "step: 10950 loss: 2150.267496 time elapsed: 13.9513 learning rate: 0.000100, scenario: 0, slope: -1.2829520006171462, fluctuations: 0.0\n",
      "step: 10960 loss: 2137.964523 time elapsed: 13.9642 learning rate: 0.000100, scenario: 0, slope: -1.2728520569834187, fluctuations: 0.0\n",
      "step: 10970 loss: 2125.750245 time elapsed: 13.9771 learning rate: 0.000100, scenario: 0, slope: -1.2630067450850282, fluctuations: 0.0\n",
      "step: 10980 loss: 2113.622787 time elapsed: 13.9898 learning rate: 0.000100, scenario: 0, slope: -1.2533985700723973, fluctuations: 0.0\n",
      "step: 10990 loss: 2101.580318 time elapsed: 14.0024 learning rate: 0.000100, scenario: 0, slope: -1.2440131192401531, fluctuations: 0.0\n",
      "step: 11000 loss: 2089.621038 time elapsed: 14.0146 learning rate: 0.000100, scenario: 0, slope: -1.2357468919780605, fluctuations: 0.0\n",
      "step: 11010 loss: 2077.743167 time elapsed: 14.0278 learning rate: 0.000100, scenario: 0, slope: -1.225865759246021, fluctuations: 0.0\n",
      "step: 11020 loss: 2065.945033 time elapsed: 14.0405 learning rate: 0.000100, scenario: 0, slope: -1.2170868982461465, fluctuations: 0.0\n",
      "step: 11030 loss: 2054.263586 time elapsed: 14.0532 learning rate: 0.000100, scenario: 0, slope: -1.208445009566334, fluctuations: 0.0\n",
      "step: 11040 loss: 2042.612376 time elapsed: 14.0658 learning rate: 0.000100, scenario: 0, slope: -1.1997871499152224, fluctuations: 0.0\n",
      "step: 11050 loss: 2031.023279 time elapsed: 14.0784 learning rate: 0.000100, scenario: 0, slope: -1.191548688072155, fluctuations: 0.0\n",
      "step: 11060 loss: 2019.524317 time elapsed: 14.0921 learning rate: 0.000100, scenario: 0, slope: -1.1835230289988747, fluctuations: 0.0\n",
      "step: 11070 loss: 2008.102184 time elapsed: 14.1057 learning rate: 0.000100, scenario: 0, slope: -1.1756789590932055, fluctuations: 0.0\n",
      "step: 11080 loss: 1996.748803 time elapsed: 14.1192 learning rate: 0.000100, scenario: 0, slope: -1.1680120692666032, fluctuations: 0.0\n",
      "step: 11090 loss: 1985.461784 time elapsed: 14.1321 learning rate: 0.000100, scenario: 0, slope: -1.1605261278316845, fluctuations: 0.0\n",
      "step: 11100 loss: 1974.239215 time elapsed: 14.1449 learning rate: 0.000100, scenario: 0, slope: -1.1539496641214024, fluctuations: 0.0\n",
      "step: 11110 loss: 1963.078833 time elapsed: 14.1578 learning rate: 0.000100, scenario: 0, slope: -1.146129730396928, fluctuations: 0.0\n",
      "step: 11120 loss: 1951.978196 time elapsed: 14.1704 learning rate: 0.000100, scenario: 0, slope: -1.139241288857839, fluctuations: 0.0\n",
      "step: 11130 loss: 1940.934708 time elapsed: 14.1828 learning rate: 0.000100, scenario: 0, slope: -1.1325221251723598, fluctuations: 0.0\n",
      "step: 11140 loss: 1929.945603 time elapsed: 14.1954 learning rate: 0.000100, scenario: 0, slope: -1.125751977707748, fluctuations: 0.0\n",
      "step: 11150 loss: 1919.007925 time elapsed: 14.2079 learning rate: 0.000100, scenario: 0, slope: -1.1194129519538107, fluctuations: 0.0\n",
      "step: 11160 loss: 1908.118525 time elapsed: 14.2202 learning rate: 0.000100, scenario: 0, slope: -1.1133493236099812, fluctuations: 0.0\n",
      "step: 11170 loss: 1897.274067 time elapsed: 14.2329 learning rate: 0.000100, scenario: 0, slope: -1.107551565963957, fluctuations: 0.0\n",
      "step: 11180 loss: 1886.471064 time elapsed: 14.2453 learning rate: 0.000100, scenario: 0, slope: -1.1020290475036811, fluctuations: 0.0\n",
      "step: 11190 loss: 1875.705943 time elapsed: 14.2573 learning rate: 0.000100, scenario: 0, slope: -1.096795947571803, fluctuations: 0.0\n",
      "step: 11200 loss: 1864.975145 time elapsed: 14.2700 learning rate: 0.000100, scenario: 0, slope: -1.0923461855318697, fluctuations: 0.0\n",
      "step: 11210 loss: 1854.275269 time elapsed: 14.2845 learning rate: 0.000100, scenario: 0, slope: -1.087257551270284, fluctuations: 0.0\n",
      "step: 11220 loss: 1843.603249 time elapsed: 14.2988 learning rate: 0.000100, scenario: 0, slope: -1.0829765893657057, fluctuations: 0.0\n",
      "step: 11230 loss: 1832.956550 time elapsed: 14.3145 learning rate: 0.000100, scenario: 0, slope: -1.0790285933666386, fluctuations: 0.0\n",
      "step: 11240 loss: 1822.333368 time elapsed: 14.3284 learning rate: 0.000100, scenario: 0, slope: -1.0754085151830712, fluctuations: 0.0\n",
      "step: 11250 loss: 1811.732784 time elapsed: 14.3417 learning rate: 0.000100, scenario: 0, slope: -1.0720996357841102, fluctuations: 0.0\n",
      "step: 11260 loss: 1801.154858 time elapsed: 14.3548 learning rate: 0.000100, scenario: 0, slope: -1.0690715895789973, fluctuations: 0.0\n",
      "step: 11270 loss: 1790.600629 time elapsed: 14.3677 learning rate: 0.000100, scenario: 0, slope: -1.0662795208615918, fluctuations: 0.0\n",
      "step: 11280 loss: 1780.072018 time elapsed: 14.3803 learning rate: 0.000100, scenario: 0, slope: -1.0636648030808866, fluctuations: 0.0\n",
      "step: 11290 loss: 1769.571646 time elapsed: 14.3930 learning rate: 0.000100, scenario: 0, slope: -1.0611575394029764, fluctuations: 0.0\n",
      "step: 11300 loss: 1759.102609 time elapsed: 14.4053 learning rate: 0.000100, scenario: 0, slope: -1.058929303538425, fluctuations: 0.0\n",
      "step: 11310 loss: 1748.668239 time elapsed: 14.4182 learning rate: 0.000100, scenario: 0, slope: -1.0561556862522699, fluctuations: 0.0\n",
      "step: 11320 loss: 1738.273766 time elapsed: 14.4307 learning rate: 0.000100, scenario: 0, slope: -1.0535053713041316, fluctuations: 0.0\n",
      "step: 11330 loss: 1727.982303 time elapsed: 14.4432 learning rate: 0.000100, scenario: 0, slope: -1.0503780307067645, fluctuations: 0.0\n",
      "step: 11340 loss: 1717.637233 time elapsed: 14.4557 learning rate: 0.000100, scenario: 0, slope: -1.047225687444758, fluctuations: 0.0\n",
      "step: 11350 loss: 1707.357466 time elapsed: 14.4682 learning rate: 0.000100, scenario: 0, slope: -1.0438833773502432, fluctuations: 0.0\n",
      "step: 11360 loss: 1697.143715 time elapsed: 14.4806 learning rate: 0.000100, scenario: 0, slope: -1.0402632314250413, fluctuations: 0.0\n",
      "step: 11370 loss: 1686.986865 time elapsed: 14.4930 learning rate: 0.000100, scenario: 0, slope: -1.036339276698847, fluctuations: 0.0\n",
      "step: 11380 loss: 1676.884521 time elapsed: 14.5060 learning rate: 0.000100, scenario: 0, slope: -1.0321149706587263, fluctuations: 0.0\n",
      "step: 11390 loss: 1666.838376 time elapsed: 14.5193 learning rate: 0.000100, scenario: 0, slope: -1.0276060984499924, fluctuations: 0.0\n",
      "step: 11400 loss: 1656.850393 time elapsed: 14.5318 learning rate: 0.000100, scenario: 0, slope: -1.0233228451647876, fluctuations: 0.0\n",
      "step: 11410 loss: 1646.922069 time elapsed: 14.5461 learning rate: 0.000100, scenario: 0, slope: -1.0178257032647613, fluctuations: 0.0\n",
      "step: 11420 loss: 1637.054354 time elapsed: 14.5593 learning rate: 0.000100, scenario: 0, slope: -1.0126048754680321, fluctuations: 0.0\n",
      "step: 11430 loss: 1627.247524 time elapsed: 14.5718 learning rate: 0.000100, scenario: 0, slope: -1.006878443394014, fluctuations: 0.0\n",
      "step: 11440 loss: 1617.501080 time elapsed: 14.5847 learning rate: 0.000100, scenario: 0, slope: -1.001109978112575, fluctuations: 0.0\n",
      "step: 11450 loss: 1607.813702 time elapsed: 14.5975 learning rate: 0.000100, scenario: 0, slope: -0.9952806274734284, fluctuations: 0.0\n",
      "step: 11460 loss: 1598.183274 time elapsed: 14.6102 learning rate: 0.000100, scenario: 0, slope: -0.9893884069086868, fluctuations: 0.0\n",
      "step: 11470 loss: 1588.606971 time elapsed: 14.6230 learning rate: 0.000100, scenario: 0, slope: -0.9834707202920375, fluctuations: 0.0\n",
      "step: 11480 loss: 1579.081399 time elapsed: 14.6359 learning rate: 0.000100, scenario: 0, slope: -0.9775852740291917, fluctuations: 0.0\n",
      "step: 11490 loss: 1569.602782 time elapsed: 14.6487 learning rate: 0.000100, scenario: 0, slope: -0.9717970381244772, fluctuations: 0.0\n",
      "step: 11500 loss: 1560.167227 time elapsed: 14.6612 learning rate: 0.000100, scenario: 0, slope: -0.9667252757475826, fluctuations: 0.0\n",
      "step: 11510 loss: 1550.771097 time elapsed: 14.6745 learning rate: 0.000100, scenario: 0, slope: -0.9607707043332324, fluctuations: 0.0\n",
      "step: 11520 loss: 1541.411553 time elapsed: 14.6874 learning rate: 0.000100, scenario: 0, slope: -0.9556423201407349, fluctuations: 0.0\n",
      "step: 11530 loss: 1532.087290 time elapsed: 14.7005 learning rate: 0.000100, scenario: 0, slope: -0.950815739951733, fluctuations: 0.0\n",
      "step: 11540 loss: 1522.799438 time elapsed: 14.7135 learning rate: 0.000100, scenario: 0, slope: -0.946290649961136, fluctuations: 0.0\n",
      "step: 11550 loss: 1513.552411 time elapsed: 14.7273 learning rate: 0.000100, scenario: 0, slope: -0.942027044140905, fluctuations: 0.0\n",
      "step: 11560 loss: 1504.354359 time elapsed: 14.7414 learning rate: 0.000100, scenario: 0, slope: -0.9379357822154354, fluctuations: 0.0\n",
      "step: 11570 loss: 1495.216757 time elapsed: 14.7554 learning rate: 0.000100, scenario: 0, slope: -0.9338740582658415, fluctuations: 0.0\n",
      "step: 11580 loss: 1486.152908 time elapsed: 14.7690 learning rate: 0.000100, scenario: 0, slope: -0.9296508688997076, fluctuations: 0.0\n",
      "step: 11590 loss: 1477.175626 time elapsed: 14.7821 learning rate: 0.000100, scenario: 0, slope: -0.925046126716773, fluctuations: 0.0\n",
      "step: 11600 loss: 1468.294883 time elapsed: 14.7956 learning rate: 0.000100, scenario: 0, slope: -0.9203953999566469, fluctuations: 0.0\n",
      "step: 11610 loss: 1459.516299 time elapsed: 14.8097 learning rate: 0.000100, scenario: 0, slope: -0.9138642266295532, fluctuations: 0.0\n",
      "step: 11620 loss: 1450.840879 time elapsed: 14.8226 learning rate: 0.000100, scenario: 0, slope: -0.9070113438523686, fluctuations: 0.0\n",
      "step: 11630 loss: 1442.265845 time elapsed: 14.8363 learning rate: 0.000100, scenario: 0, slope: -0.8992830989568469, fluctuations: 0.0\n",
      "step: 11640 loss: 1433.788122 time elapsed: 14.8495 learning rate: 0.000100, scenario: 0, slope: -0.8907790964069092, fluctuations: 0.0\n",
      "step: 11650 loss: 1425.460536 time elapsed: 14.8627 learning rate: 0.000100, scenario: 0, slope: -0.8813425098730614, fluctuations: 0.0\n",
      "step: 11660 loss: 1417.111002 time elapsed: 14.8756 learning rate: 0.000100, scenario: 0, slope: -0.8718368568469175, fluctuations: 0.0\n",
      "step: 11670 loss: 1408.861028 time elapsed: 14.8885 learning rate: 0.000100, scenario: 0, slope: -0.8623769017433255, fluctuations: 0.0\n",
      "step: 11680 loss: 1400.703339 time elapsed: 14.9014 learning rate: 0.000100, scenario: 0, slope: -0.8531127853377727, fluctuations: 0.0\n",
      "step: 11690 loss: 1392.610128 time elapsed: 14.9163 learning rate: 0.000100, scenario: 0, slope: -0.8442117953185728, fluctuations: 0.0\n",
      "step: 11700 loss: 1384.577327 time elapsed: 14.9301 learning rate: 0.000100, scenario: 0, slope: -0.8366082919402881, fluctuations: 0.0\n",
      "step: 11710 loss: 1376.603134 time elapsed: 14.9438 learning rate: 0.000100, scenario: 0, slope: -0.8278996322630452, fluctuations: 0.0\n",
      "step: 11720 loss: 1368.684283 time elapsed: 14.9566 learning rate: 0.000100, scenario: 0, slope: -0.8205470542191636, fluctuations: 0.0\n",
      "step: 11730 loss: 1360.818037 time elapsed: 14.9693 learning rate: 0.000100, scenario: 0, slope: -0.8137032183851591, fluctuations: 0.0\n",
      "step: 11740 loss: 1353.002235 time elapsed: 14.9818 learning rate: 0.000100, scenario: 0, slope: -0.8073191599845728, fluctuations: 0.0\n",
      "step: 11750 loss: 1345.235071 time elapsed: 14.9945 learning rate: 0.000100, scenario: 0, slope: -0.8009476891911539, fluctuations: 0.0\n",
      "step: 11760 loss: 1337.515006 time elapsed: 15.0068 learning rate: 0.000100, scenario: 0, slope: -0.7950645801797978, fluctuations: 0.0\n",
      "step: 11770 loss: 1329.840720 time elapsed: 15.0191 learning rate: 0.000100, scenario: 0, slope: -0.7895588659570442, fluctuations: 0.0\n",
      "step: 11780 loss: 1322.211075 time elapsed: 15.0317 learning rate: 0.000100, scenario: 0, slope: -0.7843219690982476, fluctuations: 0.0\n",
      "step: 11790 loss: 1314.625085 time elapsed: 15.0441 learning rate: 0.000100, scenario: 0, slope: -0.7793024216686173, fluctuations: 0.0\n",
      "step: 11800 loss: 1307.081890 time elapsed: 15.0562 learning rate: 0.000100, scenario: 0, slope: -0.7749410704813104, fluctuations: 0.0\n",
      "step: 11810 loss: 1299.580739 time elapsed: 15.0690 learning rate: 0.000100, scenario: 0, slope: -0.7697791374692189, fluctuations: 0.0\n",
      "step: 11820 loss: 1292.120968 time elapsed: 15.0818 learning rate: 0.000100, scenario: 0, slope: -0.7652240534238899, fluctuations: 0.0\n",
      "step: 11830 loss: 1284.701990 time elapsed: 15.0943 learning rate: 0.000100, scenario: 0, slope: -0.7607810595642275, fluctuations: 0.0\n",
      "step: 11840 loss: 1277.323280 time elapsed: 15.1067 learning rate: 0.000100, scenario: 0, slope: -0.756435131980987, fluctuations: 0.0\n",
      "step: 11850 loss: 1269.984369 time elapsed: 15.1199 learning rate: 0.000100, scenario: 0, slope: -0.7521737339124455, fluctuations: 0.0\n",
      "step: 11860 loss: 1262.684829 time elapsed: 15.1330 learning rate: 0.000100, scenario: 0, slope: -0.7479863749838349, fluctuations: 0.0\n",
      "step: 11870 loss: 1255.424276 time elapsed: 15.1470 learning rate: 0.000100, scenario: 0, slope: -0.7438642566944952, fluctuations: 0.0\n",
      "step: 11880 loss: 1248.202355 time elapsed: 15.1604 learning rate: 0.000100, scenario: 0, slope: -0.7397999821995718, fluctuations: 0.0\n",
      "step: 11890 loss: 1241.018740 time elapsed: 15.1740 learning rate: 0.000100, scenario: 0, slope: -0.7357873174176981, fluctuations: 0.0\n",
      "step: 11900 loss: 1233.873132 time elapsed: 15.1864 learning rate: 0.000100, scenario: 0, slope: -0.7322156734389703, fluctuations: 0.0\n",
      "step: 11910 loss: 1226.765250 time elapsed: 15.1998 learning rate: 0.000100, scenario: 0, slope: -0.727896542805393, fluctuations: 0.0\n",
      "step: 11920 loss: 1219.694833 time elapsed: 15.2126 learning rate: 0.000100, scenario: 0, slope: -0.724010165173822, fluctuations: 0.0\n",
      "step: 11930 loss: 1212.661634 time elapsed: 15.2253 learning rate: 0.000100, scenario: 0, slope: -0.7201586161145299, fluctuations: 0.0\n",
      "step: 11940 loss: 1205.665423 time elapsed: 15.2379 learning rate: 0.000100, scenario: 0, slope: -0.7163391158093184, fluctuations: 0.0\n",
      "step: 11950 loss: 1198.705979 time elapsed: 15.2507 learning rate: 0.000100, scenario: 0, slope: -0.7125492727041317, fluctuations: 0.0\n",
      "step: 11960 loss: 1191.783390 time elapsed: 15.2635 learning rate: 0.000100, scenario: 0, slope: -0.7087866442257373, fluctuations: 0.0\n",
      "step: 11970 loss: 1185.013037 time elapsed: 15.2763 learning rate: 0.000100, scenario: 0, slope: -0.7048860802056159, fluctuations: 0.0\n",
      "step: 11980 loss: 1178.048166 time elapsed: 15.2888 learning rate: 0.000100, scenario: 0, slope: -0.7008694172648208, fluctuations: 0.0\n",
      "step: 11990 loss: 1171.242739 time elapsed: 15.3010 learning rate: 0.000100, scenario: 0, slope: -0.6971828461219988, fluctuations: 0.0\n",
      "step: 12000 loss: 1164.467265 time elapsed: 15.3134 learning rate: 0.000100, scenario: 0, slope: -0.6939446320331459, fluctuations: 0.0\n",
      "step: 12010 loss: 1157.721839 time elapsed: 15.3270 learning rate: 0.000100, scenario: 0, slope: -0.6900329129711094, fluctuations: 0.0\n",
      "step: 12020 loss: 1151.015583 time elapsed: 15.3399 learning rate: 0.000100, scenario: 0, slope: -0.6865055229475789, fluctuations: 0.0\n",
      "step: 12030 loss: 1144.346088 time elapsed: 15.3540 learning rate: 0.000100, scenario: 0, slope: -0.6829990609531345, fluctuations: 0.0\n",
      "step: 12040 loss: 1137.711714 time elapsed: 15.3669 learning rate: 0.000100, scenario: 0, slope: -0.6795146343692715, fluctuations: 0.0\n",
      "step: 12050 loss: 1131.112225 time elapsed: 15.3808 learning rate: 0.000100, scenario: 0, slope: -0.676054473089201, fluctuations: 0.0\n",
      "step: 12060 loss: 1124.547510 time elapsed: 15.3936 learning rate: 0.000100, scenario: 0, slope: -0.6726208195260543, fluctuations: 0.0\n",
      "step: 12070 loss: 1118.017413 time elapsed: 15.4063 learning rate: 0.000100, scenario: 0, slope: -0.6690409063598719, fluctuations: 0.0\n",
      "step: 12080 loss: 1111.521775 time elapsed: 15.4188 learning rate: 0.000100, scenario: 0, slope: -0.6651912654675772, fluctuations: 0.0\n",
      "step: 12090 loss: 1105.060440 time elapsed: 15.4313 learning rate: 0.000100, scenario: 0, slope: -0.6616038347396792, fluctuations: 0.0\n",
      "step: 12100 loss: 1098.633250 time elapsed: 15.4435 learning rate: 0.000100, scenario: 0, slope: -0.6584377385891791, fluctuations: 0.0\n",
      "step: 12110 loss: 1092.240051 time elapsed: 15.4568 learning rate: 0.000100, scenario: 0, slope: -0.6546079454380435, fluctuations: 0.0\n",
      "step: 12120 loss: 1085.880686 time elapsed: 15.4693 learning rate: 0.000100, scenario: 0, slope: -0.6511522540281881, fluctuations: 0.0\n",
      "step: 12130 loss: 1079.554999 time elapsed: 15.4818 learning rate: 0.000100, scenario: 0, slope: -0.6477139775350076, fluctuations: 0.0\n",
      "step: 12140 loss: 1073.262830 time elapsed: 15.4941 learning rate: 0.000100, scenario: 0, slope: -0.64429201735696, fluctuations: 0.0\n",
      "step: 12150 loss: 1067.004021 time elapsed: 15.5064 learning rate: 0.000100, scenario: 0, slope: -0.6408859579386237, fluctuations: 0.0\n",
      "step: 12160 loss: 1060.778407 time elapsed: 15.5190 learning rate: 0.000100, scenario: 0, slope: -0.6374956536273029, fluctuations: 0.0\n",
      "step: 12170 loss: 1054.585826 time elapsed: 15.5316 learning rate: 0.000100, scenario: 0, slope: -0.6341211216033731, fluctuations: 0.0\n",
      "step: 12180 loss: 1048.426109 time elapsed: 15.5440 learning rate: 0.000100, scenario: 0, slope: -0.6307624615285102, fluctuations: 0.0\n",
      "step: 12190 loss: 1042.299087 time elapsed: 15.5579 learning rate: 0.000100, scenario: 0, slope: -0.6274198217818772, fluctuations: 0.0\n",
      "step: 12200 loss: 1036.204587 time elapsed: 15.5710 learning rate: 0.000100, scenario: 0, slope: -0.6244252931805463, fluctuations: 0.0\n",
      "step: 12210 loss: 1030.142434 time elapsed: 15.5843 learning rate: 0.000100, scenario: 0, slope: -0.6207833575961054, fluctuations: 0.0\n",
      "step: 12220 loss: 1024.112449 time elapsed: 15.5969 learning rate: 0.000100, scenario: 0, slope: -0.6174899728510858, fluctuations: 0.0\n",
      "step: 12230 loss: 1018.114452 time elapsed: 15.6095 learning rate: 0.000100, scenario: 0, slope: -0.6142134806809809, fluctuations: 0.0\n",
      "step: 12240 loss: 1012.148256 time elapsed: 15.6221 learning rate: 0.000100, scenario: 0, slope: -0.6109541477178027, fluctuations: 0.0\n",
      "step: 12250 loss: 1006.213675 time elapsed: 15.6345 learning rate: 0.000100, scenario: 0, slope: -0.6077122532809632, fluctuations: 0.0\n",
      "step: 12260 loss: 1000.310519 time elapsed: 15.6470 learning rate: 0.000100, scenario: 0, slope: -0.604488086041345, fluctuations: 0.0\n",
      "step: 12270 loss: 994.438593 time elapsed: 15.6595 learning rate: 0.000100, scenario: 0, slope: -0.6012819400598121, fluctuations: 0.0\n",
      "step: 12280 loss: 988.597758 time elapsed: 15.6722 learning rate: 0.000100, scenario: 0, slope: -0.598094036924871, fluctuations: 0.0\n",
      "step: 12290 loss: 982.814499 time elapsed: 15.6848 learning rate: 0.000100, scenario: 0, slope: -0.5948912449673516, fluctuations: 0.0\n",
      "step: 12300 loss: 977.094972 time elapsed: 15.6973 learning rate: 0.000100, scenario: 0, slope: -0.5916236797320115, fluctuations: 0.0\n",
      "step: 12310 loss: 971.293487 time elapsed: 15.7103 learning rate: 0.000100, scenario: 0, slope: -0.5880909152943004, fluctuations: 0.0\n",
      "step: 12320 loss: 965.550406 time elapsed: 15.7228 learning rate: 0.000100, scenario: 0, slope: -0.5850579188482508, fluctuations: 0.0\n",
      "step: 12330 loss: 959.860926 time elapsed: 15.7354 learning rate: 0.000100, scenario: 0, slope: -0.5820762700536215, fluctuations: 0.0\n",
      "step: 12340 loss: 954.205484 time elapsed: 15.7477 learning rate: 0.000100, scenario: 0, slope: -0.5791212000185814, fluctuations: 0.0\n",
      "step: 12350 loss: 948.578386 time elapsed: 15.7611 learning rate: 0.000100, scenario: 0, slope: -0.576189802420358, fluctuations: 0.0\n",
      "step: 12360 loss: 942.980784 time elapsed: 15.7737 learning rate: 0.000100, scenario: 0, slope: -0.5732821523110018, fluctuations: 0.0\n",
      "step: 12370 loss: 937.412601 time elapsed: 15.7863 learning rate: 0.000100, scenario: 0, slope: -0.5704003226460354, fluctuations: 0.0\n",
      "step: 12380 loss: 931.873460 time elapsed: 15.7988 learning rate: 0.000100, scenario: 0, slope: -0.5675472310247262, fluctuations: 0.0\n",
      "step: 12390 loss: 926.363100 time elapsed: 15.8113 learning rate: 0.000100, scenario: 0, slope: -0.5646903495365756, fluctuations: 0.0\n",
      "step: 12400 loss: 920.881317 time elapsed: 15.8236 learning rate: 0.000100, scenario: 0, slope: -0.5616367013653835, fluctuations: 0.0\n",
      "step: 12410 loss: 915.427919 time elapsed: 15.8363 learning rate: 0.000100, scenario: 0, slope: -0.5582313370003721, fluctuations: 0.0\n",
      "step: 12420 loss: 910.002725 time elapsed: 15.8486 learning rate: 0.000100, scenario: 0, slope: -0.5552844367084051, fluctuations: 0.0\n",
      "step: 12430 loss: 904.605560 time elapsed: 15.8611 learning rate: 0.000100, scenario: 0, slope: -0.5523845331537861, fluctuations: 0.0\n",
      "step: 12440 loss: 899.236256 time elapsed: 15.8735 learning rate: 0.000100, scenario: 0, slope: -0.549511359570488, fluctuations: 0.0\n",
      "step: 12450 loss: 893.894650 time elapsed: 15.8860 learning rate: 0.000100, scenario: 0, slope: -0.5466609449931711, fluctuations: 0.0\n",
      "step: 12460 loss: 888.580589 time elapsed: 15.8987 learning rate: 0.000100, scenario: 0, slope: -0.5438303275886316, fluctuations: 0.0\n",
      "step: 12470 loss: 883.293926 time elapsed: 15.9112 learning rate: 0.000100, scenario: 0, slope: -0.5410178459544909, fluctuations: 0.0\n",
      "step: 12480 loss: 878.034523 time elapsed: 15.9239 learning rate: 0.000100, scenario: 0, slope: -0.5382225618671677, fluctuations: 0.0\n",
      "step: 12490 loss: 872.802249 time elapsed: 15.9365 learning rate: 0.000100, scenario: 0, slope: -0.5354437127947204, fluctuations: 0.0\n",
      "step: 12500 loss: 867.596979 time elapsed: 15.9486 learning rate: 0.000100, scenario: 0, slope: -0.5329561917258006, fluctuations: 0.0\n",
      "step: 12510 loss: 862.418598 time elapsed: 15.9624 learning rate: 0.000100, scenario: 0, slope: -0.5299323705808892, fluctuations: 0.0\n",
      "step: 12520 loss: 857.266995 time elapsed: 15.9753 learning rate: 0.000100, scenario: 0, slope: -0.5271983984833042, fluctuations: 0.0\n",
      "step: 12530 loss: 852.142067 time elapsed: 15.9881 learning rate: 0.000100, scenario: 0, slope: -0.5244779103905044, fluctuations: 0.0\n",
      "step: 12540 loss: 847.043714 time elapsed: 16.0006 learning rate: 0.000100, scenario: 0, slope: -0.5217701865886865, fluctuations: 0.0\n",
      "step: 12550 loss: 841.971844 time elapsed: 16.0132 learning rate: 0.000100, scenario: 0, slope: -0.519074532739767, fluctuations: 0.0\n",
      "step: 12560 loss: 836.926367 time elapsed: 16.0258 learning rate: 0.000100, scenario: 0, slope: -0.5163902890959524, fluctuations: 0.0\n",
      "step: 12570 loss: 831.907198 time elapsed: 16.0383 learning rate: 0.000100, scenario: 0, slope: -0.5137168385936941, fluctuations: 0.0\n",
      "step: 12580 loss: 826.914254 time elapsed: 16.0508 learning rate: 0.000100, scenario: 0, slope: -0.5110536126259795, fluctuations: 0.0\n",
      "step: 12590 loss: 821.947456 time elapsed: 16.0631 learning rate: 0.000100, scenario: 0, slope: -0.5084000947871924, fluctuations: 0.0\n",
      "step: 12600 loss: 817.006729 time elapsed: 16.0756 learning rate: 0.000100, scenario: 0, slope: -0.5060198442664839, fluctuations: 0.0\n",
      "step: 12610 loss: 812.092553 time elapsed: 16.0886 learning rate: 0.000100, scenario: 0, slope: -0.5031196648539766, fluctuations: 0.0\n",
      "step: 12620 loss: 807.379109 time elapsed: 16.1019 learning rate: 0.000100, scenario: 0, slope: -0.5002257549378529, fluctuations: 0.0\n",
      "step: 12630 loss: 802.352132 time elapsed: 16.1147 learning rate: 0.000100, scenario: 0, slope: -0.4972880001871937, fluctuations: 0.0\n",
      "step: 12640 loss: 797.524198 time elapsed: 16.1269 learning rate: 0.000100, scenario: 0, slope: -0.49468557790244794, fluctuations: 0.0\n",
      "step: 12650 loss: 792.706573 time elapsed: 16.1392 learning rate: 0.000100, scenario: 0, slope: -0.492182209302366, fluctuations: 0.0\n",
      "step: 12660 loss: 787.916545 time elapsed: 16.1521 learning rate: 0.000100, scenario: 0, slope: -0.48971588555245976, fluctuations: 0.0\n",
      "step: 12670 loss: 783.158423 time elapsed: 16.1646 learning rate: 0.000100, scenario: 0, slope: -0.487262494853901, fluctuations: 0.0\n",
      "step: 12680 loss: 778.426621 time elapsed: 16.1784 learning rate: 0.000100, scenario: 0, slope: -0.48481843592737567, fluctuations: 0.0\n",
      "step: 12690 loss: 773.720080 time elapsed: 16.1918 learning rate: 0.000100, scenario: 0, slope: -0.48238500870728535, fluctuations: 0.0\n",
      "step: 12700 loss: 769.038910 time elapsed: 16.2041 learning rate: 0.000100, scenario: 0, slope: -0.480205962063573, fluctuations: 0.0\n",
      "step: 12710 loss: 764.383093 time elapsed: 16.2172 learning rate: 0.000100, scenario: 0, slope: -0.47755924075654865, fluctuations: 0.0\n",
      "step: 12720 loss: 759.752558 time elapsed: 16.2299 learning rate: 0.000100, scenario: 0, slope: -0.4748855084962266, fluctuations: 0.0\n",
      "step: 12730 loss: 755.147243 time elapsed: 16.2424 learning rate: 0.000100, scenario: 0, slope: -0.4719735033585822, fluctuations: 0.0\n",
      "step: 12740 loss: 750.567094 time elapsed: 16.2550 learning rate: 0.000100, scenario: 0, slope: -0.4693214934741368, fluctuations: 0.0\n",
      "step: 12750 loss: 746.012058 time elapsed: 16.2677 learning rate: 0.000100, scenario: 0, slope: -0.4667483253338018, fluctuations: 0.0\n",
      "step: 12760 loss: 741.482089 time elapsed: 16.2800 learning rate: 0.000100, scenario: 0, slope: -0.4642082199102172, fluctuations: 0.0\n",
      "step: 12770 loss: 736.977141 time elapsed: 16.2925 learning rate: 0.000100, scenario: 0, slope: -0.46168027268189354, fluctuations: 0.0\n",
      "step: 12780 loss: 732.497169 time elapsed: 16.3050 learning rate: 0.000100, scenario: 0, slope: -0.45915969525790046, fluctuations: 0.0\n",
      "step: 12790 loss: 728.042131 time elapsed: 16.3175 learning rate: 0.000100, scenario: 0, slope: -0.45664513885025293, fluctuations: 0.0\n",
      "step: 12800 loss: 723.611986 time elapsed: 16.3301 learning rate: 0.000100, scenario: 0, slope: -0.45438652955208886, fluctuations: 0.0\n",
      "step: 12810 loss: 719.206690 time elapsed: 16.3433 learning rate: 0.000100, scenario: 0, slope: -0.45163126928349134, fluctuations: 0.0\n",
      "step: 12820 loss: 714.826203 time elapsed: 16.3564 learning rate: 0.000100, scenario: 0, slope: -0.4491313018533556, fluctuations: 0.0\n",
      "step: 12830 loss: 710.470482 time elapsed: 16.3689 learning rate: 0.000100, scenario: 0, slope: -0.44663576098479646, fluctuations: 0.0\n",
      "step: 12840 loss: 706.139483 time elapsed: 16.3845 learning rate: 0.000100, scenario: 0, slope: -0.4441445557816206, fluctuations: 0.0\n",
      "step: 12850 loss: 701.833162 time elapsed: 16.3979 learning rate: 0.000100, scenario: 0, slope: -0.44165763775092987, fluctuations: 0.0\n",
      "step: 12860 loss: 697.551473 time elapsed: 16.4113 learning rate: 0.000100, scenario: 0, slope: -0.4391750002749349, fluctuations: 0.0\n",
      "step: 12870 loss: 693.294365 time elapsed: 16.4250 learning rate: 0.000100, scenario: 0, slope: -0.4366966805284262, fluctuations: 0.0\n",
      "step: 12880 loss: 689.061786 time elapsed: 16.4388 learning rate: 0.000100, scenario: 0, slope: -0.4342227624105282, fluctuations: 0.0\n",
      "step: 12890 loss: 684.853680 time elapsed: 16.4525 learning rate: 0.000100, scenario: 0, slope: -0.4317533787813337, fluctuations: 0.0\n",
      "step: 12900 loss: 680.669986 time elapsed: 16.4651 learning rate: 0.000100, scenario: 0, slope: -0.4295349608936915, fluctuations: 0.0\n",
      "step: 12910 loss: 676.510640 time elapsed: 16.4786 learning rate: 0.000100, scenario: 0, slope: -0.4268289995951463, fluctuations: 0.0\n",
      "step: 12920 loss: 672.375571 time elapsed: 16.4916 learning rate: 0.000100, scenario: 0, slope: -0.42437452576093215, fluctuations: 0.0\n",
      "step: 12930 loss: 668.264710 time elapsed: 16.5041 learning rate: 0.000100, scenario: 0, slope: -0.42192562097753966, fluctuations: 0.0\n",
      "step: 12940 loss: 664.180855 time elapsed: 16.5169 learning rate: 0.000100, scenario: 0, slope: -0.41947907887905933, fluctuations: 0.0\n",
      "step: 12950 loss: 660.248954 time elapsed: 16.5301 learning rate: 0.000100, scenario: 0, slope: -0.4164706889987568, fluctuations: 0.0\n",
      "step: 12960 loss: 656.118505 time elapsed: 16.5428 learning rate: 0.000100, scenario: 0, slope: -0.41390030980477754, fluctuations: 0.0\n",
      "step: 12970 loss: 652.068854 time elapsed: 16.5557 learning rate: 0.000100, scenario: 0, slope: -0.411548807395463, fluctuations: 0.0\n",
      "step: 12980 loss: 648.085326 time elapsed: 16.5685 learning rate: 0.000100, scenario: 0, slope: -0.4092570667397875, fluctuations: 0.0\n",
      "step: 12990 loss: 644.120182 time elapsed: 16.5828 learning rate: 0.000100, scenario: 0, slope: -0.4069911134237112, fluctuations: 0.0\n",
      "step: 13000 loss: 640.177999 time elapsed: 16.5964 learning rate: 0.000100, scenario: 0, slope: -0.40496456699743266, fluctuations: 0.0\n",
      "step: 13010 loss: 636.261090 time elapsed: 16.6102 learning rate: 0.000100, scenario: 0, slope: -0.40250044697839144, fluctuations: 0.0\n",
      "step: 13020 loss: 632.367818 time elapsed: 16.6243 learning rate: 0.000100, scenario: 0, slope: -0.40027543522102454, fluctuations: 0.0\n",
      "step: 13030 loss: 628.497666 time elapsed: 16.6373 learning rate: 0.000100, scenario: 0, slope: -0.39806971567169935, fluctuations: 0.0\n",
      "step: 13040 loss: 624.650501 time elapsed: 16.6501 learning rate: 0.000100, scenario: 0, slope: -0.3958846456095205, fluctuations: 0.0\n",
      "step: 13050 loss: 620.826175 time elapsed: 16.6628 learning rate: 0.000100, scenario: 0, slope: -0.3930883501609604, fluctuations: 0.0\n",
      "step: 13060 loss: 617.024522 time elapsed: 16.6756 learning rate: 0.000100, scenario: 0, slope: -0.3905295711302267, fluctuations: 0.0\n",
      "step: 13070 loss: 613.245365 time elapsed: 16.6882 learning rate: 0.000100, scenario: 0, slope: -0.388138854343085, fluctuations: 0.0\n",
      "step: 13080 loss: 609.488523 time elapsed: 16.7005 learning rate: 0.000100, scenario: 0, slope: -0.3858037776273415, fluctuations: 0.0\n",
      "step: 13090 loss: 605.753806 time elapsed: 16.7132 learning rate: 0.000100, scenario: 0, slope: -0.3835028622386569, fluctuations: 0.0\n",
      "step: 13100 loss: 602.041015 time elapsed: 16.7253 learning rate: 0.000100, scenario: 0, slope: -0.3814535287953289, fluctuations: 0.0\n",
      "step: 13110 loss: 598.349947 time elapsed: 16.7380 learning rate: 0.000100, scenario: 0, slope: -0.3789701848927745, fluctuations: 0.0\n",
      "step: 13120 loss: 594.680391 time elapsed: 16.7505 learning rate: 0.000100, scenario: 0, slope: -0.3767319720916034, fluctuations: 0.0\n",
      "step: 13130 loss: 591.032128 time elapsed: 16.7628 learning rate: 0.000100, scenario: 0, slope: -0.37451262321040474, fluctuations: 0.0\n",
      "step: 13140 loss: 587.404934 time elapsed: 16.7755 learning rate: 0.000100, scenario: 0, slope: -0.3723127041517869, fluctuations: 0.0\n",
      "step: 13150 loss: 583.798579 time elapsed: 16.7883 learning rate: 0.000100, scenario: 0, slope: -0.37013286755487657, fluctuations: 0.0\n",
      "step: 13160 loss: 580.212828 time elapsed: 16.8017 learning rate: 0.000100, scenario: 0, slope: -0.36797378770102995, fluctuations: 0.0\n",
      "step: 13170 loss: 576.647441 time elapsed: 16.8146 learning rate: 0.000100, scenario: 0, slope: -0.36583612330986914, fluctuations: 0.0\n",
      "step: 13180 loss: 573.102176 time elapsed: 16.8273 learning rate: 0.000100, scenario: 0, slope: -0.3637204967375958, fluctuations: 0.0\n",
      "step: 13190 loss: 569.576786 time elapsed: 16.8395 learning rate: 0.000100, scenario: 0, slope: -0.3616274806026651, fluctuations: 0.0\n",
      "step: 13200 loss: 566.071026 time elapsed: 16.8518 learning rate: 0.000100, scenario: 0, slope: -0.3597635225076328, fluctuations: 0.0\n",
      "step: 13210 loss: 562.584648 time elapsed: 16.8650 learning rate: 0.000100, scenario: 0, slope: -0.3575112561489585, fluctuations: 0.0\n",
      "step: 13220 loss: 559.117406 time elapsed: 16.8778 learning rate: 0.000100, scenario: 0, slope: -0.35548884795312546, fluctuations: 0.0\n",
      "step: 13230 loss: 555.669057 time elapsed: 16.8905 learning rate: 0.000100, scenario: 0, slope: -0.3534906291275791, fluctuations: 0.0\n",
      "step: 13240 loss: 552.239361 time elapsed: 16.9032 learning rate: 0.000100, scenario: 0, slope: -0.35151676370422646, fluctuations: 0.0\n",
      "step: 13250 loss: 548.828084 time elapsed: 16.9159 learning rate: 0.000100, scenario: 0, slope: -0.3495673025051599, fluctuations: 0.0\n",
      "step: 13260 loss: 545.434999 time elapsed: 16.9282 learning rate: 0.000100, scenario: 0, slope: -0.34764217328777497, fluctuations: 0.0\n",
      "step: 13270 loss: 542.059899 time elapsed: 16.9408 learning rate: 0.000100, scenario: 0, slope: -0.34574115614360457, fluctuations: 0.0\n",
      "step: 13280 loss: 538.707217 time elapsed: 16.9535 learning rate: 0.000100, scenario: 0, slope: -0.3438580707200574, fluctuations: 0.0\n",
      "step: 13290 loss: 535.443852 time elapsed: 16.9658 learning rate: 0.000100, scenario: 0, slope: -0.3413887680101471, fluctuations: 0.0\n",
      "step: 13300 loss: 532.073926 time elapsed: 16.9784 learning rate: 0.000100, scenario: 0, slope: -0.33959664574155035, fluctuations: 0.0\n",
      "step: 13310 loss: 528.749317 time elapsed: 16.9912 learning rate: 0.000100, scenario: 0, slope: -0.33765878254037296, fluctuations: 0.0\n",
      "step: 13320 loss: 525.472659 time elapsed: 17.0053 learning rate: 0.000100, scenario: 0, slope: -0.3359651477177032, fluctuations: 0.0\n",
      "step: 13330 loss: 522.207760 time elapsed: 17.0191 learning rate: 0.000100, scenario: 0, slope: -0.3342990136194878, fluctuations: 0.0\n",
      "step: 13340 loss: 518.959995 time elapsed: 17.0321 learning rate: 0.000100, scenario: 0, slope: -0.33265001288846224, fluctuations: 0.0\n",
      "step: 13350 loss: 515.730289 time elapsed: 17.0449 learning rate: 0.000100, scenario: 0, slope: -0.33101735504128804, fluctuations: 0.0\n",
      "step: 13360 loss: 512.517834 time elapsed: 17.0575 learning rate: 0.000100, scenario: 0, slope: -0.3294060806221211, fluctuations: 0.0\n",
      "step: 13370 loss: 509.322244 time elapsed: 17.0700 learning rate: 0.000100, scenario: 0, slope: -0.327823067230938, fluctuations: 0.0\n",
      "step: 13380 loss: 506.143349 time elapsed: 17.0826 learning rate: 0.000100, scenario: 0, slope: -0.32626975054923374, fluctuations: 0.0\n",
      "step: 13390 loss: 502.981107 time elapsed: 17.0950 learning rate: 0.000100, scenario: 0, slope: -0.3240588365506118, fluctuations: 0.0\n",
      "step: 13400 loss: 499.835505 time elapsed: 17.1076 learning rate: 0.000100, scenario: 0, slope: -0.3223171676186078, fluctuations: 0.0\n",
      "step: 13410 loss: 496.706551 time elapsed: 17.1208 learning rate: 0.000100, scenario: 0, slope: -0.3203773067862562, fluctuations: 0.0\n",
      "step: 13420 loss: 493.594276 time elapsed: 17.1335 learning rate: 0.000100, scenario: 0, slope: -0.3186690131911234, fluctuations: 0.0\n",
      "step: 13430 loss: 490.498738 time elapsed: 17.1460 learning rate: 0.000100, scenario: 0, slope: -0.31698475829194817, fluctuations: 0.0\n",
      "step: 13440 loss: 487.420017 time elapsed: 17.1588 learning rate: 0.000100, scenario: 0, slope: -0.31531115947782873, fluctuations: 0.0\n",
      "step: 13450 loss: 484.358216 time elapsed: 17.1713 learning rate: 0.000100, scenario: 0, slope: -0.31364009967784623, fluctuations: 0.0\n",
      "step: 13460 loss: 481.313460 time elapsed: 17.1841 learning rate: 0.000100, scenario: 0, slope: -0.31196761421695185, fluctuations: 0.0\n",
      "step: 13470 loss: 478.285894 time elapsed: 17.1983 learning rate: 0.000100, scenario: 0, slope: -0.3102908500809922, fluctuations: 0.0\n",
      "step: 13480 loss: 475.275682 time elapsed: 17.2114 learning rate: 0.000100, scenario: 0, slope: -0.3086073581069458, fluctuations: 0.0\n",
      "step: 13490 loss: 472.283001 time elapsed: 17.2239 learning rate: 0.000100, scenario: 0, slope: -0.30691484927882956, fluctuations: 0.0\n",
      "step: 13500 loss: 469.308040 time elapsed: 17.2363 learning rate: 0.000100, scenario: 0, slope: -0.30538210846924974, fluctuations: 0.0\n",
      "step: 13510 loss: 466.350994 time elapsed: 17.2498 learning rate: 0.000100, scenario: 0, slope: -0.30349439307723564, fluctuations: 0.0\n",
      "step: 13520 loss: 463.412061 time elapsed: 17.2625 learning rate: 0.000100, scenario: 0, slope: -0.30176274147956367, fluctuations: 0.0\n",
      "step: 13530 loss: 460.491436 time elapsed: 17.2753 learning rate: 0.000100, scenario: 0, slope: -0.3000147631904705, fluctuations: 0.0\n",
      "step: 13540 loss: 457.589304 time elapsed: 17.2878 learning rate: 0.000100, scenario: 0, slope: -0.29824932687172506, fluctuations: 0.0\n",
      "step: 13550 loss: 454.705836 time elapsed: 17.3003 learning rate: 0.000100, scenario: 0, slope: -0.2964656876522492, fluctuations: 0.0\n",
      "step: 13560 loss: 451.841183 time elapsed: 17.3128 learning rate: 0.000100, scenario: 0, slope: -0.29466353639563897, fluctuations: 0.0\n",
      "step: 13570 loss: 448.995470 time elapsed: 17.3254 learning rate: 0.000100, scenario: 0, slope: -0.29284304168509756, fluctuations: 0.0\n",
      "step: 13580 loss: 446.168792 time elapsed: 17.3379 learning rate: 0.000100, scenario: 0, slope: -0.291004882676318, fluctuations: 0.0\n",
      "step: 13590 loss: 443.361205 time elapsed: 17.3506 learning rate: 0.000100, scenario: 0, slope: -0.28915027282831457, fluctuations: 0.0\n",
      "step: 13600 loss: 440.572723 time elapsed: 17.3629 learning rate: 0.000100, scenario: 0, slope: -0.28746850421582926, fluctuations: 0.0\n",
      "step: 13610 loss: 437.803332 time elapsed: 17.3763 learning rate: 0.000100, scenario: 0, slope: -0.285399284894577, fluctuations: 0.0\n",
      "step: 13620 loss: 435.061434 time elapsed: 17.3886 learning rate: 0.000100, scenario: 0, slope: -0.28349740317527267, fluctuations: 0.0\n",
      "step: 13630 loss: 432.364531 time elapsed: 17.4018 learning rate: 0.000100, scenario: 0, slope: -0.2809837518003728, fluctuations: 0.0\n",
      "step: 13640 loss: 429.644981 time elapsed: 17.4157 learning rate: 0.000100, scenario: 0, slope: -0.2789453362972284, fluctuations: 0.0\n",
      "step: 13650 loss: 426.926214 time elapsed: 17.4288 learning rate: 0.000100, scenario: 0, slope: -0.2771190637921388, fluctuations: 0.0\n",
      "step: 13660 loss: 424.258700 time elapsed: 17.4416 learning rate: 0.000100, scenario: 0, slope: -0.275342737908942, fluctuations: 0.0\n",
      "step: 13670 loss: 421.602406 time elapsed: 17.4543 learning rate: 0.000100, scenario: 0, slope: -0.2735953388158212, fluctuations: 0.0\n",
      "step: 13680 loss: 418.965365 time elapsed: 17.4669 learning rate: 0.000100, scenario: 0, slope: -0.2718716226352719, fluctuations: 0.0\n",
      "step: 13690 loss: 416.343916 time elapsed: 17.4795 learning rate: 0.000100, scenario: 0, slope: -0.2701837150590946, fluctuations: 0.0\n",
      "step: 13700 loss: 413.737273 time elapsed: 17.4918 learning rate: 0.000100, scenario: 0, slope: -0.26870787762465137, fluctuations: 0.0\n",
      "step: 13710 loss: 411.143630 time elapsed: 17.5048 learning rate: 0.000100, scenario: 0, slope: -0.26698198737121354, fluctuations: 0.0\n",
      "step: 13720 loss: 408.560849 time elapsed: 17.5173 learning rate: 0.000100, scenario: 0, slope: -0.26550182378620557, fluctuations: 0.0\n",
      "step: 13730 loss: 405.985777 time elapsed: 17.5298 learning rate: 0.000100, scenario: 0, slope: -0.2634293723133956, fluctuations: 0.0\n",
      "step: 13740 loss: 403.413997 time elapsed: 17.5425 learning rate: 0.000100, scenario: 0, slope: -0.2617767678997088, fluctuations: 0.0\n",
      "step: 13750 loss: 400.839083 time elapsed: 17.5550 learning rate: 0.000100, scenario: 0, slope: -0.26047475178503005, fluctuations: 0.0\n",
      "step: 13760 loss: 398.251768 time elapsed: 17.5672 learning rate: 0.000100, scenario: 0, slope: -0.2594685489563867, fluctuations: 0.0\n",
      "step: 13770 loss: 395.639089 time elapsed: 17.5795 learning rate: 0.000100, scenario: 0, slope: -0.2588448821348193, fluctuations: 0.0\n",
      "step: 13780 loss: 392.984394 time elapsed: 17.5921 learning rate: 0.000100, scenario: 0, slope: -0.2587371901044131, fluctuations: 0.0\n",
      "step: 13790 loss: 390.270312 time elapsed: 17.6041 learning rate: 0.000100, scenario: 0, slope: -0.25932982143942424, fluctuations: 0.0\n",
      "step: 13800 loss: 387.487773 time elapsed: 17.6180 learning rate: 0.000100, scenario: 0, slope: -0.2606153155097862, fluctuations: 0.0\n",
      "step: 13810 loss: 384.650507 time elapsed: 17.6313 learning rate: 0.000100, scenario: 0, slope: -0.2632506923913719, fluctuations: 0.0\n",
      "step: 13820 loss: 381.802536 time elapsed: 17.6439 learning rate: 0.000100, scenario: 0, slope: -0.2664968094513517, fluctuations: 0.0\n",
      "step: 13830 loss: 379.001633 time elapsed: 17.6565 learning rate: 0.000100, scenario: 0, slope: -0.27005093939197317, fluctuations: 0.0\n",
      "step: 13840 loss: 376.288311 time elapsed: 17.6688 learning rate: 0.000100, scenario: 0, slope: -0.2731872759252519, fluctuations: 0.0\n",
      "step: 13850 loss: 373.672097 time elapsed: 17.6811 learning rate: 0.000100, scenario: 0, slope: -0.2751721825827763, fluctuations: 0.0\n",
      "step: 13860 loss: 371.141338 time elapsed: 17.6937 learning rate: 0.000100, scenario: 0, slope: -0.2754587717686061, fluctuations: 0.0\n",
      "step: 13870 loss: 368.677754 time elapsed: 17.7063 learning rate: 0.000100, scenario: 0, slope: -0.27378221924512686, fluctuations: 0.0\n",
      "step: 13880 loss: 366.264637 time elapsed: 17.7188 learning rate: 0.000100, scenario: 0, slope: -0.2701864434985343, fluctuations: 0.0\n",
      "step: 13890 loss: 363.889358 time elapsed: 17.7314 learning rate: 0.000100, scenario: 0, slope: -0.26501919317556777, fluctuations: 0.0\n",
      "step: 13900 loss: 361.543201 time elapsed: 17.7438 learning rate: 0.000100, scenario: 0, slope: -0.2595252835050973, fluctuations: 0.0\n",
      "step: 13910 loss: 359.220399 time elapsed: 17.7569 learning rate: 0.000100, scenario: 0, slope: -0.2525526465600073, fluctuations: 0.0\n",
      "step: 13920 loss: 356.917189 time elapsed: 17.7697 learning rate: 0.000100, scenario: 0, slope: -0.24669165867203896, fluctuations: 0.0\n",
      "step: 13930 loss: 354.631172 time elapsed: 17.7823 learning rate: 0.000100, scenario: 0, slope: -0.2417183078753942, fluctuations: 0.0\n",
      "step: 13940 loss: 352.384684 time elapsed: 17.7955 learning rate: 0.000100, scenario: 0, slope: -0.237684366968378, fluctuations: 0.0\n",
      "step: 13950 loss: 350.165643 time elapsed: 17.8083 learning rate: 0.000100, scenario: 0, slope: -0.2340265840193826, fluctuations: 0.0\n",
      "step: 13960 loss: 347.889601 time elapsed: 17.8233 learning rate: 0.000100, scenario: 0, slope: -0.2314323295372285, fluctuations: 0.0\n",
      "step: 13970 loss: 345.646506 time elapsed: 17.8379 learning rate: 0.000100, scenario: 0, slope: -0.2293983842636323, fluctuations: 0.0\n",
      "step: 13980 loss: 343.437097 time elapsed: 17.8515 learning rate: 0.000100, scenario: 0, slope: -0.22767143754081073, fluctuations: 0.0\n",
      "step: 13990 loss: 341.237486 time elapsed: 17.8644 learning rate: 0.000100, scenario: 0, slope: -0.22613933555681875, fluctuations: 0.0\n",
      "step: 14000 loss: 339.054708 time elapsed: 17.8780 learning rate: 0.000100, scenario: 0, slope: -0.22486259113071905, fluctuations: 0.0\n",
      "step: 14010 loss: 336.886156 time elapsed: 17.8927 learning rate: 0.000100, scenario: 0, slope: -0.22338148012437217, fluctuations: 0.0\n",
      "step: 14020 loss: 334.732431 time elapsed: 17.9059 learning rate: 0.000100, scenario: 0, slope: -0.22207885033530422, fluctuations: 0.0\n",
      "step: 14030 loss: 332.594180 time elapsed: 17.9197 learning rate: 0.000100, scenario: 0, slope: -0.22079627723834688, fluctuations: 0.0\n",
      "step: 14040 loss: 330.471942 time elapsed: 17.9326 learning rate: 0.000100, scenario: 0, slope: -0.21948699641273678, fluctuations: 0.0\n",
      "step: 14050 loss: 328.366234 time elapsed: 17.9462 learning rate: 0.000100, scenario: 0, slope: -0.2175883961459808, fluctuations: 0.0\n",
      "step: 14060 loss: 326.277548 time elapsed: 17.9595 learning rate: 0.000100, scenario: 0, slope: -0.215971262738261, fluctuations: 0.0\n",
      "step: 14070 loss: 324.206307 time elapsed: 17.9730 learning rate: 0.000100, scenario: 0, slope: -0.21441326583419995, fluctuations: 0.0\n",
      "step: 14080 loss: 322.152841 time elapsed: 17.9859 learning rate: 0.000100, scenario: 0, slope: -0.21282499379615655, fluctuations: 0.0\n",
      "step: 14090 loss: 320.117388 time elapsed: 17.9989 learning rate: 0.000100, scenario: 0, slope: -0.21120022867639685, fluctuations: 0.0\n",
      "step: 14100 loss: 318.100096 time elapsed: 18.0135 learning rate: 0.000100, scenario: 0, slope: -0.20970120514293933, fluctuations: 0.0\n",
      "step: 14110 loss: 316.101025 time elapsed: 18.0285 learning rate: 0.000100, scenario: 0, slope: -0.20782425824840453, fluctuations: 0.0\n",
      "step: 14120 loss: 314.120161 time elapsed: 18.0425 learning rate: 0.000100, scenario: 0, slope: -0.20608152113627679, fluctuations: 0.0\n",
      "step: 14130 loss: 312.157422 time elapsed: 18.0569 learning rate: 0.000100, scenario: 0, slope: -0.20431147524262888, fluctuations: 0.0\n",
      "step: 14140 loss: 310.212673 time elapsed: 18.0709 learning rate: 0.000100, scenario: 0, slope: -0.20252200296240325, fluctuations: 0.0\n",
      "step: 14150 loss: 308.285735 time elapsed: 18.0845 learning rate: 0.000100, scenario: 0, slope: -0.20072100751539837, fluctuations: 0.0\n",
      "step: 14160 loss: 306.376425 time elapsed: 18.0977 learning rate: 0.000100, scenario: 0, slope: -0.19891592612277043, fluctuations: 0.0\n",
      "step: 14170 loss: 304.489055 time elapsed: 18.1106 learning rate: 0.000100, scenario: 0, slope: -0.19710710210846255, fluctuations: 0.0\n",
      "step: 14180 loss: 302.761164 time elapsed: 18.1234 learning rate: 0.000100, scenario: 0, slope: -0.19479317363532087, fluctuations: 0.0\n",
      "step: 14190 loss: 300.790850 time elapsed: 18.1359 learning rate: 0.000100, scenario: 0, slope: -0.19291263275503504, fluctuations: 0.0\n",
      "step: 14200 loss: 298.923594 time elapsed: 18.1484 learning rate: 0.000100, scenario: 0, slope: -0.1913656610276059, fluctuations: 0.0\n",
      "step: 14210 loss: 297.095548 time elapsed: 18.1616 learning rate: 0.000100, scenario: 0, slope: -0.18957130673468953, fluctuations: 0.0\n",
      "step: 14220 loss: 295.288113 time elapsed: 18.1743 learning rate: 0.000100, scenario: 0, slope: -0.18798404776809807, fluctuations: 0.0\n",
      "step: 14230 loss: 293.497653 time elapsed: 18.1868 learning rate: 0.000100, scenario: 0, slope: -0.18642759346119223, fluctuations: 0.0\n",
      "step: 14240 loss: 291.722761 time elapsed: 18.1996 learning rate: 0.000100, scenario: 0, slope: -0.18490123116942736, fluctuations: 0.0\n",
      "step: 14250 loss: 289.962791 time elapsed: 18.2122 learning rate: 0.000100, scenario: 0, slope: -0.1834075719260223, fluctuations: 0.0\n",
      "step: 14260 loss: 288.217589 time elapsed: 18.2266 learning rate: 0.000100, scenario: 0, slope: -0.18195013948737596, fluctuations: 0.0\n",
      "step: 14270 loss: 286.487013 time elapsed: 18.2398 learning rate: 0.000100, scenario: 0, slope: -0.1805258739740073, fluctuations: 0.0\n",
      "step: 14280 loss: 284.770710 time elapsed: 18.2530 learning rate: 0.000100, scenario: 0, slope: -0.17856359980859346, fluctuations: 0.0\n",
      "step: 14290 loss: 283.068412 time elapsed: 18.2659 learning rate: 0.000100, scenario: 0, slope: -0.17686101188294037, fluctuations: 0.0\n",
      "step: 14300 loss: 281.379875 time elapsed: 18.2788 learning rate: 0.000100, scenario: 0, slope: -0.17543793293082524, fluctuations: 0.0\n",
      "step: 14310 loss: 279.704849 time elapsed: 18.2917 learning rate: 0.000100, scenario: 0, slope: -0.17379002414713351, fluctuations: 0.0\n",
      "step: 14320 loss: 278.043094 time elapsed: 18.3044 learning rate: 0.000100, scenario: 0, slope: -0.17233918457312467, fluctuations: 0.0\n",
      "step: 14330 loss: 276.394381 time elapsed: 18.3172 learning rate: 0.000100, scenario: 0, slope: -0.17092125923726426, fluctuations: 0.0\n",
      "step: 14340 loss: 274.758483 time elapsed: 18.3299 learning rate: 0.000100, scenario: 0, slope: -0.16953138577571636, fluctuations: 0.0\n",
      "step: 14350 loss: 273.135182 time elapsed: 18.3424 learning rate: 0.000100, scenario: 0, slope: -0.16816727938469428, fluctuations: 0.0\n",
      "step: 14360 loss: 271.524266 time elapsed: 18.3546 learning rate: 0.000100, scenario: 0, slope: -0.16682758225492228, fluctuations: 0.0\n",
      "step: 14370 loss: 269.925530 time elapsed: 18.3670 learning rate: 0.000100, scenario: 0, slope: -0.16551144828434075, fluctuations: 0.0\n",
      "step: 14380 loss: 268.338775 time elapsed: 18.3801 learning rate: 0.000100, scenario: 0, slope: -0.16421819588375458, fluctuations: 0.0\n",
      "step: 14390 loss: 266.763808 time elapsed: 18.3931 learning rate: 0.000100, scenario: 0, slope: -0.1629471133708293, fluctuations: 0.0\n",
      "step: 14400 loss: 265.200440 time elapsed: 18.4052 learning rate: 0.000100, scenario: 0, slope: -0.1618215530345215, fluctuations: 0.0\n",
      "step: 14410 loss: 263.648491 time elapsed: 18.4184 learning rate: 0.000100, scenario: 0, slope: -0.16046884584186857, fluctuations: 0.0\n",
      "step: 14420 loss: 262.107786 time elapsed: 18.4312 learning rate: 0.000100, scenario: 0, slope: -0.1592603860699928, fluctuations: 0.0\n",
      "step: 14430 loss: 260.578204 time elapsed: 18.4457 learning rate: 0.000100, scenario: 0, slope: -0.15807148014933148, fluctuations: 0.0\n",
      "step: 14440 loss: 259.073435 time elapsed: 18.4591 learning rate: 0.000100, scenario: 0, slope: -0.15688317941162458, fluctuations: 0.0\n",
      "step: 14450 loss: 257.581296 time elapsed: 18.4720 learning rate: 0.000100, scenario: 0, slope: -0.15510166703176276, fluctuations: 0.0\n",
      "step: 14460 loss: 256.082015 time elapsed: 18.4847 learning rate: 0.000100, scenario: 0, slope: -0.15385457186545148, fluctuations: 0.0\n",
      "step: 14470 loss: 254.591217 time elapsed: 18.4972 learning rate: 0.000100, scenario: 0, slope: -0.1528213834772832, fluctuations: 0.0\n",
      "step: 14480 loss: 253.108512 time elapsed: 18.5098 learning rate: 0.000100, scenario: 0, slope: -0.15186278681897936, fluctuations: 0.0\n",
      "step: 14490 loss: 251.641506 time elapsed: 18.5224 learning rate: 0.000100, scenario: 0, slope: -0.15093661336291864, fluctuations: 0.0\n",
      "step: 14500 loss: 250.188826 time elapsed: 18.5347 learning rate: 0.000100, scenario: 0, slope: -0.15011871962168893, fluctuations: 0.0\n",
      "step: 14510 loss: 248.746301 time elapsed: 18.5478 learning rate: 0.000100, scenario: 0, slope: -0.149139905372764, fluctuations: 0.0\n",
      "step: 14520 loss: 247.313427 time elapsed: 18.5604 learning rate: 0.000100, scenario: 0, slope: -0.14827304919869821, fluctuations: 0.0\n",
      "step: 14530 loss: 245.890116 time elapsed: 18.5730 learning rate: 0.000100, scenario: 0, slope: -0.1474323486473228, fluctuations: 0.0\n",
      "step: 14540 loss: 244.476257 time elapsed: 18.5858 learning rate: 0.000100, scenario: 0, slope: -0.14660258643518334, fluctuations: 0.0\n",
      "step: 14550 loss: 243.071723 time elapsed: 18.5982 learning rate: 0.000100, scenario: 0, slope: -0.14506439022355913, fluctuations: 0.0\n",
      "step: 14560 loss: 241.676377 time elapsed: 18.6105 learning rate: 0.000100, scenario: 0, slope: -0.14386260377341323, fluctuations: 0.0\n",
      "step: 14570 loss: 240.290108 time elapsed: 18.6232 learning rate: 0.000100, scenario: 0, slope: -0.14282384919439312, fluctuations: 0.0\n",
      "step: 14580 loss: 238.912805 time elapsed: 18.6357 learning rate: 0.000100, scenario: 0, slope: -0.14184857751636404, fluctuations: 0.0\n",
      "step: 14590 loss: 237.544360 time elapsed: 18.6499 learning rate: 0.000100, scenario: 0, slope: -0.14090417627684257, fluctuations: 0.0\n",
      "step: 14600 loss: 236.184671 time elapsed: 18.6625 learning rate: 0.000100, scenario: 0, slope: -0.14006834186238434, fluctuations: 0.0\n",
      "step: 14610 loss: 234.833635 time elapsed: 18.6760 learning rate: 0.000100, scenario: 0, slope: -0.1390617367686075, fluctuations: 0.0\n",
      "step: 14620 loss: 233.491154 time elapsed: 18.6888 learning rate: 0.000100, scenario: 0, slope: -0.1381591463773143, fluctuations: 0.0\n",
      "step: 14630 loss: 232.157131 time elapsed: 18.7012 learning rate: 0.000100, scenario: 0, slope: -0.13726769343849746, fluctuations: 0.0\n",
      "step: 14640 loss: 230.831474 time elapsed: 18.7137 learning rate: 0.000100, scenario: 0, slope: -0.13638686115716528, fluctuations: 0.0\n",
      "step: 14650 loss: 229.514090 time elapsed: 18.7261 learning rate: 0.000100, scenario: 0, slope: -0.13551632398672306, fluctuations: 0.0\n",
      "step: 14660 loss: 228.204890 time elapsed: 18.7384 learning rate: 0.000100, scenario: 0, slope: -0.13465580147049225, fluctuations: 0.0\n",
      "step: 14670 loss: 226.903787 time elapsed: 18.7511 learning rate: 0.000100, scenario: 0, slope: -0.13380503482016654, fluctuations: 0.0\n",
      "step: 14680 loss: 225.610695 time elapsed: 18.7636 learning rate: 0.000100, scenario: 0, slope: -0.13296378063044023, fluctuations: 0.0\n",
      "step: 14690 loss: 224.325532 time elapsed: 18.7760 learning rate: 0.000100, scenario: 0, slope: -0.1321318048708481, fluctuations: 0.0\n",
      "step: 14700 loss: 223.048346 time elapsed: 18.7883 learning rate: 0.000100, scenario: 0, slope: -0.13139066879191952, fluctuations: 0.0\n",
      "step: 14710 loss: 221.816375 time elapsed: 18.8014 learning rate: 0.000100, scenario: 0, slope: -0.1304444811708721, fluctuations: 0.0\n",
      "step: 14720 loss: 220.584151 time elapsed: 18.8139 learning rate: 0.000100, scenario: 0, slope: -0.12907323837295678, fluctuations: 0.0\n",
      "step: 14730 loss: 219.299864 time elapsed: 18.8267 learning rate: 0.000100, scenario: 0, slope: -0.12822490747064472, fluctuations: 0.0\n",
      "step: 14740 loss: 218.029288 time elapsed: 18.8389 learning rate: 0.000100, scenario: 0, slope: -0.12753876550194895, fluctuations: 0.0\n",
      "step: 14750 loss: 216.786589 time elapsed: 18.8526 learning rate: 0.000100, scenario: 0, slope: -0.12689541075032787, fluctuations: 0.0\n",
      "step: 14760 loss: 215.558515 time elapsed: 18.8655 learning rate: 0.000100, scenario: 0, slope: -0.12626748829514473, fluctuations: 0.0\n",
      "step: 14770 loss: 214.336957 time elapsed: 18.8779 learning rate: 0.000100, scenario: 0, slope: -0.12565070820407692, fluctuations: 0.0\n",
      "step: 14780 loss: 213.122474 time elapsed: 18.8904 learning rate: 0.000100, scenario: 0, slope: -0.12504463189182294, fluctuations: 0.0\n",
      "step: 14790 loss: 211.915365 time elapsed: 18.9028 learning rate: 0.000100, scenario: 0, slope: -0.1244516379872179, fluctuations: 0.0\n",
      "step: 14800 loss: 210.715394 time elapsed: 18.9153 learning rate: 0.000100, scenario: 0, slope: -0.12393164402134846, fluctuations: 0.0\n",
      "step: 14810 loss: 209.522468 time elapsed: 18.9284 learning rate: 0.000100, scenario: 0, slope: -0.1232639489020964, fluctuations: 0.0\n",
      "step: 14820 loss: 208.336507 time elapsed: 18.9410 learning rate: 0.000100, scenario: 0, slope: -0.12199798691650883, fluctuations: 0.0\n",
      "step: 14830 loss: 207.157441 time elapsed: 18.9535 learning rate: 0.000100, scenario: 0, slope: -0.12111136297930135, fluctuations: 0.0\n",
      "step: 14840 loss: 205.985208 time elapsed: 18.9660 learning rate: 0.000100, scenario: 0, slope: -0.12035353284880898, fluctuations: 0.0\n",
      "step: 14850 loss: 204.819749 time elapsed: 18.9786 learning rate: 0.000100, scenario: 0, slope: -0.1196324180980155, fluctuations: 0.0\n",
      "step: 14860 loss: 203.661004 time elapsed: 18.9909 learning rate: 0.000100, scenario: 0, slope: -0.11892685257598219, fluctuations: 0.0\n",
      "step: 14870 loss: 202.508917 time elapsed: 19.0034 learning rate: 0.000100, scenario: 0, slope: -0.11823209543316122, fluctuations: 0.0\n",
      "step: 14880 loss: 201.363433 time elapsed: 19.0157 learning rate: 0.000100, scenario: 0, slope: -0.11754502126162632, fluctuations: 0.0\n",
      "step: 14890 loss: 200.224496 time elapsed: 19.0281 learning rate: 0.000100, scenario: 0, slope: -0.11686453103470214, fluctuations: 0.0\n",
      "step: 14900 loss: 199.092052 time elapsed: 19.0406 learning rate: 0.000100, scenario: 0, slope: -0.11625726079049438, fluctuations: 0.0\n",
      "step: 14910 loss: 197.966049 time elapsed: 19.0551 learning rate: 0.000100, scenario: 0, slope: -0.11552143328034847, fluctuations: 0.0\n",
      "step: 14920 loss: 196.846436 time elapsed: 19.0680 learning rate: 0.000100, scenario: 0, slope: -0.11485844987459232, fluctuations: 0.0\n",
      "step: 14930 loss: 195.733162 time elapsed: 19.0811 learning rate: 0.000100, scenario: 0, slope: -0.1142010018677171, fluctuations: 0.0\n",
      "step: 14940 loss: 194.627085 time elapsed: 19.0933 learning rate: 0.000100, scenario: 0, slope: -0.1135478480637048, fluctuations: 0.0\n",
      "step: 14950 loss: 193.793938 time elapsed: 19.1057 learning rate: 0.000100, scenario: 0, slope: -0.11245778924019578, fluctuations: 0.0\n",
      "step: 14960 loss: 192.473980 time elapsed: 19.1183 learning rate: 0.000100, scenario: 0, slope: -0.1113981153952136, fluctuations: 0.0\n",
      "step: 14970 loss: 191.353103 time elapsed: 19.1308 learning rate: 0.000100, scenario: 0, slope: -0.11079791219702735, fluctuations: 0.0\n",
      "step: 14980 loss: 190.275452 time elapsed: 19.1429 learning rate: 0.000100, scenario: 0, slope: -0.1103150816559712, fluctuations: 0.0\n",
      "step: 14990 loss: 189.201699 time elapsed: 19.1553 learning rate: 0.000100, scenario: 0, slope: -0.10986633316282594, fluctuations: 0.0\n",
      "step: 15000 loss: 188.133719 time elapsed: 19.1675 learning rate: 0.000100, scenario: 0, slope: -0.10947609871316946, fluctuations: 0.0\n",
      "step: 15010 loss: 187.073845 time elapsed: 19.1804 learning rate: 0.000100, scenario: 0, slope: -0.10900893405776306, fluctuations: 0.0\n",
      "step: 15020 loss: 186.019468 time elapsed: 19.1927 learning rate: 0.000100, scenario: 0, slope: -0.1085951496237164, fluctuations: 0.0\n",
      "step: 15030 loss: 184.971019 time elapsed: 19.2050 learning rate: 0.000100, scenario: 0, slope: -0.10819615573037364, fluctuations: 0.0\n",
      "step: 15040 loss: 183.928423 time elapsed: 19.2174 learning rate: 0.000100, scenario: 0, slope: -0.10781544087011917, fluctuations: 0.0\n",
      "step: 15050 loss: 182.891559 time elapsed: 19.2299 learning rate: 0.000100, scenario: 0, slope: -0.10698015778101334, fluctuations: 0.0\n",
      "step: 15060 loss: 181.860387 time elapsed: 19.2427 learning rate: 0.000100, scenario: 0, slope: -0.10590028858449657, fluctuations: 0.0\n",
      "step: 15070 loss: 180.834869 time elapsed: 19.2558 learning rate: 0.000100, scenario: 0, slope: -0.10518125242826575, fluctuations: 0.0\n",
      "step: 15080 loss: 179.814963 time elapsed: 19.2705 learning rate: 0.000100, scenario: 0, slope: -0.10455611558156788, fluctuations: 0.0\n",
      "step: 15090 loss: 178.800632 time elapsed: 19.2839 learning rate: 0.000100, scenario: 0, slope: -0.10396245113192772, fluctuations: 0.0\n",
      "step: 15100 loss: 177.791837 time elapsed: 19.2970 learning rate: 0.000100, scenario: 0, slope: -0.10344284528136387, fluctuations: 0.0\n",
      "step: 15110 loss: 176.788540 time elapsed: 19.3118 learning rate: 0.000100, scenario: 0, slope: -0.1028165142634684, fluctuations: 0.0\n",
      "step: 15120 loss: 175.790705 time elapsed: 19.3262 learning rate: 0.000100, scenario: 0, slope: -0.10225253983787651, fluctuations: 0.0\n",
      "step: 15130 loss: 174.798294 time elapsed: 19.3395 learning rate: 0.000100, scenario: 0, slope: -0.10169301999180644, fluctuations: 0.0\n",
      "step: 15140 loss: 173.811273 time elapsed: 19.3526 learning rate: 0.000100, scenario: 0, slope: -0.10113749776714351, fluctuations: 0.0\n",
      "step: 15150 loss: 172.829606 time elapsed: 19.3656 learning rate: 0.000100, scenario: 0, slope: -0.10058581116942585, fluctuations: 0.0\n",
      "step: 15160 loss: 171.853257 time elapsed: 19.3786 learning rate: 0.000100, scenario: 0, slope: -0.10003786657149748, fluctuations: 0.0\n",
      "step: 15170 loss: 170.882192 time elapsed: 19.3925 learning rate: 0.000100, scenario: 0, slope: -0.09949359907930344, fluctuations: 0.0\n",
      "step: 15180 loss: 169.916378 time elapsed: 19.4060 learning rate: 0.000100, scenario: 0, slope: -0.09895295379538585, fluctuations: 0.0\n",
      "step: 15190 loss: 168.955779 time elapsed: 19.4194 learning rate: 0.000100, scenario: 0, slope: -0.0984158801568973, fluctuations: 0.0\n",
      "step: 15200 loss: 168.000364 time elapsed: 19.4324 learning rate: 0.000100, scenario: 0, slope: -0.09793552967616659, fluctuations: 0.0\n",
      "step: 15210 loss: 167.050098 time elapsed: 19.4481 learning rate: 0.000100, scenario: 0, slope: -0.0973522666000959, fluctuations: 0.0\n",
      "step: 15220 loss: 166.104948 time elapsed: 19.4636 learning rate: 0.000100, scenario: 0, slope: -0.09682564159838483, fluctuations: 0.0\n",
      "step: 15230 loss: 165.164903 time elapsed: 19.4776 learning rate: 0.000100, scenario: 0, slope: -0.09630239016850517, fluctuations: 0.0\n",
      "step: 15240 loss: 164.235568 time elapsed: 19.4913 learning rate: 0.000100, scenario: 0, slope: -0.0957751096511828, fluctuations: 0.0\n",
      "step: 15250 loss: 163.447419 time elapsed: 19.5057 learning rate: 0.000100, scenario: 0, slope: -0.09451429442829877, fluctuations: 0.0\n",
      "step: 15260 loss: 162.434561 time elapsed: 19.5199 learning rate: 0.000100, scenario: 0, slope: -0.09384764867535783, fluctuations: 0.0\n",
      "step: 15270 loss: 161.468236 time elapsed: 19.5341 learning rate: 0.000100, scenario: 0, slope: -0.09343941048569357, fluctuations: 0.0\n",
      "step: 15280 loss: 160.548974 time elapsed: 19.5475 learning rate: 0.000100, scenario: 0, slope: -0.09311418706057277, fluctuations: 0.0\n",
      "step: 15290 loss: 159.643052 time elapsed: 19.5617 learning rate: 0.000100, scenario: 0, slope: -0.09281460510362298, fluctuations: 0.0\n",
      "step: 15300 loss: 158.740389 time elapsed: 19.5755 learning rate: 0.000100, scenario: 0, slope: -0.092555471120132, fluctuations: 0.0\n",
      "step: 15310 loss: 157.842309 time elapsed: 19.5895 learning rate: 0.000100, scenario: 0, slope: -0.09224689727164091, fluctuations: 0.0\n",
      "step: 15320 loss: 156.949363 time elapsed: 19.6025 learning rate: 0.000100, scenario: 0, slope: -0.09197544270158071, fluctuations: 0.0\n",
      "step: 15330 loss: 156.061370 time elapsed: 19.6155 learning rate: 0.000100, scenario: 0, slope: -0.09171518896530816, fluctuations: 0.0\n",
      "step: 15340 loss: 155.178133 time elapsed: 19.6282 learning rate: 0.000100, scenario: 0, slope: -0.09146168566091069, fluctuations: 0.0\n",
      "step: 15350 loss: 154.299599 time elapsed: 19.6408 learning rate: 0.000100, scenario: 0, slope: -0.09038551489658207, fluctuations: 0.0\n",
      "step: 15360 loss: 153.425725 time elapsed: 19.6534 learning rate: 0.000100, scenario: 0, slope: -0.08963616988633621, fluctuations: 0.0\n",
      "step: 15370 loss: 152.556484 time elapsed: 19.6679 learning rate: 0.000100, scenario: 0, slope: -0.0890671552480414, fluctuations: 0.0\n",
      "step: 15380 loss: 151.691849 time elapsed: 19.6818 learning rate: 0.000100, scenario: 0, slope: -0.08856125141516046, fluctuations: 0.0\n",
      "step: 15390 loss: 150.831793 time elapsed: 19.6952 learning rate: 0.000100, scenario: 0, slope: -0.08807752156496562, fluctuations: 0.0\n",
      "step: 15400 loss: 149.976291 time elapsed: 19.7079 learning rate: 0.000100, scenario: 0, slope: -0.0876524329031965, fluctuations: 0.0\n",
      "step: 15410 loss: 149.125318 time elapsed: 19.7214 learning rate: 0.000100, scenario: 0, slope: -0.08713927110826453, fluctuations: 0.0\n",
      "step: 15420 loss: 148.278852 time elapsed: 19.7344 learning rate: 0.000100, scenario: 0, slope: -0.08667698153101086, fluctuations: 0.0\n",
      "step: 15430 loss: 147.436870 time elapsed: 19.7471 learning rate: 0.000100, scenario: 0, slope: -0.08621761787823408, fluctuations: 0.0\n",
      "step: 15440 loss: 146.599350 time elapsed: 19.7595 learning rate: 0.000100, scenario: 0, slope: -0.08576088561469161, fluctuations: 0.0\n",
      "step: 15450 loss: 145.766273 time elapsed: 19.7721 learning rate: 0.000100, scenario: 0, slope: -0.08530662367400794, fluctuations: 0.0\n",
      "step: 15460 loss: 144.937619 time elapsed: 19.7846 learning rate: 0.000100, scenario: 0, slope: -0.08485472559581898, fluctuations: 0.0\n",
      "step: 15470 loss: 144.113491 time elapsed: 19.7970 learning rate: 0.000100, scenario: 0, slope: -0.08440492177036471, fluctuations: 0.0\n",
      "step: 15480 loss: 143.323495 time elapsed: 19.8095 learning rate: 0.000100, scenario: 0, slope: -0.08391678608044056, fluctuations: 0.0\n",
      "step: 15490 loss: 142.532824 time elapsed: 19.8224 learning rate: 0.000100, scenario: 0, slope: -0.08311045647448226, fluctuations: 0.0\n",
      "step: 15500 loss: 141.681505 time elapsed: 19.8348 learning rate: 0.000100, scenario: 0, slope: -0.08267556776287092, fluctuations: 0.0\n",
      "step: 15510 loss: 140.870801 time elapsed: 19.8478 learning rate: 0.000100, scenario: 0, slope: -0.08224230435971436, fluctuations: 0.0\n",
      "step: 15520 loss: 140.069175 time elapsed: 19.8603 learning rate: 0.000100, scenario: 0, slope: -0.08187614650529626, fluctuations: 0.0\n",
      "step: 15530 loss: 139.273388 time elapsed: 19.8729 learning rate: 0.000100, scenario: 0, slope: -0.0815174415779863, fluctuations: 0.0\n",
      "step: 15540 loss: 138.482512 time elapsed: 19.8873 learning rate: 0.000100, scenario: 0, slope: -0.08116228020646572, fluctuations: 0.0\n",
      "step: 15550 loss: 137.696092 time elapsed: 19.9003 learning rate: 0.000100, scenario: 0, slope: -0.08081187727817248, fluctuations: 0.0\n",
      "step: 15560 loss: 136.913886 time elapsed: 19.9130 learning rate: 0.000100, scenario: 0, slope: -0.08046917468668742, fluctuations: 0.0\n",
      "step: 15570 loss: 136.135853 time elapsed: 19.9254 learning rate: 0.000100, scenario: 0, slope: -0.08013747027035775, fluctuations: 0.0\n",
      "step: 15580 loss: 135.362049 time elapsed: 19.9381 learning rate: 0.000100, scenario: 0, slope: -0.0797770416650121, fluctuations: 0.0\n",
      "step: 15590 loss: 134.592450 time elapsed: 19.9506 learning rate: 0.000100, scenario: 0, slope: -0.07900176005654179, fluctuations: 0.0\n",
      "step: 15600 loss: 133.827032 time elapsed: 19.9628 learning rate: 0.000100, scenario: 0, slope: -0.07852075223173506, fluctuations: 0.0\n",
      "step: 15610 loss: 133.065944 time elapsed: 19.9757 learning rate: 0.000100, scenario: 0, slope: -0.07801312063020222, fluctuations: 0.0\n",
      "step: 15620 loss: 132.323795 time elapsed: 19.9883 learning rate: 0.000100, scenario: 0, slope: -0.07755561065137612, fluctuations: 0.0\n",
      "step: 15630 loss: 131.571909 time elapsed: 20.0007 learning rate: 0.000100, scenario: 0, slope: -0.07673871706206767, fluctuations: 0.0\n",
      "step: 15640 loss: 130.824403 time elapsed: 20.0129 learning rate: 0.000100, scenario: 0, slope: -0.07628663330221334, fluctuations: 0.0\n",
      "step: 15650 loss: 130.072355 time elapsed: 20.0253 learning rate: 0.000100, scenario: 0, slope: -0.0759260889141086, fluctuations: 0.0\n",
      "step: 15660 loss: 129.327006 time elapsed: 20.0377 learning rate: 0.000100, scenario: 0, slope: -0.07561163944140513, fluctuations: 0.0\n",
      "step: 15670 loss: 128.591523 time elapsed: 20.0502 learning rate: 0.000100, scenario: 0, slope: -0.0753045949509356, fluctuations: 0.0\n",
      "step: 15680 loss: 127.859882 time elapsed: 20.0627 learning rate: 0.000100, scenario: 0, slope: -0.07500404319773189, fluctuations: 0.0\n",
      "step: 15690 loss: 127.132754 time elapsed: 20.0748 learning rate: 0.000100, scenario: 0, slope: -0.07470649951563745, fluctuations: 0.0\n",
      "step: 15700 loss: 126.409823 time elapsed: 20.0885 learning rate: 0.000100, scenario: 0, slope: -0.07444191186257315, fluctuations: 0.0\n",
      "step: 15710 loss: 125.690828 time elapsed: 20.1030 learning rate: 0.000100, scenario: 0, slope: -0.07412400495518345, fluctuations: 0.0\n",
      "step: 15720 loss: 124.975857 time elapsed: 20.1160 learning rate: 0.000100, scenario: 0, slope: -0.07381734953707685, fluctuations: 0.0\n",
      "step: 15730 loss: 124.264861 time elapsed: 20.1291 learning rate: 0.000100, scenario: 0, slope: -0.07305965371300308, fluctuations: 0.0\n",
      "step: 15740 loss: 123.557817 time elapsed: 20.1417 learning rate: 0.000100, scenario: 0, slope: -0.07255020535980075, fluctuations: 0.0\n",
      "step: 15750 loss: 122.854706 time elapsed: 20.1546 learning rate: 0.000100, scenario: 0, slope: -0.0721005727349897, fluctuations: 0.0\n",
      "step: 15760 loss: 122.155508 time elapsed: 20.1674 learning rate: 0.000100, scenario: 0, slope: -0.07168934665632322, fluctuations: 0.0\n",
      "step: 15770 loss: 121.460203 time elapsed: 20.1799 learning rate: 0.000100, scenario: 0, slope: -0.07128484687288443, fluctuations: 0.0\n",
      "step: 15780 loss: 120.768778 time elapsed: 20.1921 learning rate: 0.000100, scenario: 0, slope: -0.07088740185707963, fluctuations: 0.0\n",
      "step: 15790 loss: 120.081478 time elapsed: 20.2050 learning rate: 0.000100, scenario: 0, slope: -0.0704923621105632, fluctuations: 0.0\n",
      "step: 15800 loss: 119.425877 time elapsed: 20.2175 learning rate: 0.000100, scenario: 0, slope: -0.07011251085304625, fluctuations: 0.0\n",
      "step: 15810 loss: 118.733645 time elapsed: 20.2306 learning rate: 0.000100, scenario: 0, slope: -0.06917819881772116, fluctuations: 0.0\n",
      "step: 15820 loss: 118.059252 time elapsed: 20.2432 learning rate: 0.000100, scenario: 0, slope: -0.06872804753792576, fluctuations: 0.0\n",
      "step: 15830 loss: 117.386277 time elapsed: 20.2559 learning rate: 0.000100, scenario: 0, slope: -0.06842843134054792, fluctuations: 0.0\n",
      "step: 15840 loss: 116.711647 time elapsed: 20.2686 learning rate: 0.000100, scenario: 0, slope: -0.06816858222555544, fluctuations: 0.0\n",
      "step: 15850 loss: 116.046155 time elapsed: 20.2811 learning rate: 0.000100, scenario: 0, slope: -0.06791976992671193, fluctuations: 0.0\n",
      "step: 15860 loss: 115.387447 time elapsed: 20.2959 learning rate: 0.000100, scenario: 0, slope: -0.06767590233254098, fluctuations: 0.0\n",
      "step: 15870 loss: 114.732496 time elapsed: 20.3093 learning rate: 0.000100, scenario: 0, slope: -0.06743739620246776, fluctuations: 0.0\n",
      "step: 15880 loss: 114.080948 time elapsed: 20.3229 learning rate: 0.000100, scenario: 0, slope: -0.06720559191710052, fluctuations: 0.0\n",
      "step: 15890 loss: 113.433210 time elapsed: 20.3378 learning rate: 0.000100, scenario: 0, slope: -0.06698266542312668, fluctuations: 0.0\n",
      "step: 15900 loss: 112.789013 time elapsed: 20.3511 learning rate: 0.000100, scenario: 0, slope: -0.06676392087895953, fluctuations: 0.0\n",
      "step: 15910 loss: 112.148406 time elapsed: 20.3646 learning rate: 0.000100, scenario: 0, slope: -0.0659046767655331, fluctuations: 0.0\n",
      "step: 15920 loss: 111.511342 time elapsed: 20.3775 learning rate: 0.000100, scenario: 0, slope: -0.06537783837067844, fluctuations: 0.0\n",
      "step: 15930 loss: 110.877806 time elapsed: 20.3901 learning rate: 0.000100, scenario: 0, slope: -0.06496918319232124, fluctuations: 0.0\n",
      "step: 15940 loss: 110.247792 time elapsed: 20.4029 learning rate: 0.000100, scenario: 0, slope: -0.06459381110365786, fluctuations: 0.0\n",
      "step: 15950 loss: 109.622348 time elapsed: 20.4159 learning rate: 0.000100, scenario: 0, slope: -0.0642282178618853, fluctuations: 0.0\n",
      "step: 15960 loss: 109.094071 time elapsed: 20.4285 learning rate: 0.000100, scenario: 0, slope: -0.06370255475973392, fluctuations: 0.0\n",
      "step: 15970 loss: 108.412042 time elapsed: 20.4412 learning rate: 0.000100, scenario: 0, slope: -0.06310486469548536, fluctuations: 0.0\n",
      "step: 15980 loss: 107.777332 time elapsed: 20.4539 learning rate: 0.000100, scenario: 0, slope: -0.06278593944722759, fluctuations: 0.0\n",
      "step: 15990 loss: 107.153488 time elapsed: 20.4669 learning rate: 0.000100, scenario: 0, slope: -0.06251177375293139, fluctuations: 0.0\n",
      "step: 16000 loss: 106.545223 time elapsed: 20.4794 learning rate: 0.000100, scenario: 0, slope: -0.062280509923803906, fluctuations: 0.0\n",
      "step: 16010 loss: 105.940531 time elapsed: 20.4940 learning rate: 0.000100, scenario: 0, slope: -0.06200927799745874, fluctuations: 0.0\n",
      "step: 16020 loss: 105.338406 time elapsed: 20.5079 learning rate: 0.000100, scenario: 0, slope: -0.061766810282792184, fluctuations: 0.0\n",
      "step: 16030 loss: 104.740352 time elapsed: 20.5215 learning rate: 0.000100, scenario: 0, slope: -0.06152817984034087, fluctuations: 0.0\n",
      "step: 16040 loss: 104.145444 time elapsed: 20.5349 learning rate: 0.000100, scenario: 0, slope: -0.06129449837806462, fluctuations: 0.0\n",
      "step: 16050 loss: 103.553942 time elapsed: 20.5478 learning rate: 0.000100, scenario: 0, slope: -0.061065020566381974, fluctuations: 0.0\n",
      "step: 16060 loss: 102.966446 time elapsed: 20.5608 learning rate: 0.000100, scenario: 0, slope: -0.06065966768653909, fluctuations: 0.0\n",
      "step: 16070 loss: 102.418911 time elapsed: 20.5734 learning rate: 0.000100, scenario: 0, slope: -0.05998435336970318, fluctuations: 0.0\n",
      "step: 16080 loss: 101.808851 time elapsed: 20.5857 learning rate: 0.000100, scenario: 0, slope: -0.059368435588589195, fluctuations: 0.0\n",
      "step: 16090 loss: 101.232264 time elapsed: 20.5981 learning rate: 0.000100, scenario: 0, slope: -0.059002704700614805, fluctuations: 0.0\n",
      "step: 16100 loss: 100.652117 time elapsed: 20.6106 learning rate: 0.000100, scenario: 0, slope: -0.05874321713821525, fluctuations: 0.0\n",
      "step: 16110 loss: 100.079323 time elapsed: 20.6235 learning rate: 0.000100, scenario: 0, slope: -0.05845464144147587, fluctuations: 0.0\n",
      "step: 16120 loss: 99.510159 time elapsed: 20.6362 learning rate: 0.000100, scenario: 0, slope: -0.05820362312576504, fluctuations: 0.0\n",
      "step: 16130 loss: 98.945509 time elapsed: 20.6488 learning rate: 0.000100, scenario: 0, slope: -0.05795686376571229, fluctuations: 0.0\n",
      "step: 16140 loss: 98.384057 time elapsed: 20.6614 learning rate: 0.000100, scenario: 0, slope: -0.05771429040081768, fluctuations: 0.0\n",
      "step: 16150 loss: 97.825651 time elapsed: 20.6738 learning rate: 0.000100, scenario: 0, slope: -0.057476860304598004, fluctuations: 0.0\n",
      "step: 16160 loss: 97.270326 time elapsed: 20.6867 learning rate: 0.000100, scenario: 0, slope: -0.05724442611663519, fluctuations: 0.0\n",
      "step: 16170 loss: 96.718081 time elapsed: 20.6996 learning rate: 0.000100, scenario: 0, slope: -0.0569468130275019, fluctuations: 0.0\n",
      "step: 16180 loss: 96.168874 time elapsed: 20.7135 learning rate: 0.000100, scenario: 0, slope: -0.056396898516176325, fluctuations: 0.0\n",
      "step: 16190 loss: 95.622774 time elapsed: 20.7270 learning rate: 0.000100, scenario: 0, slope: -0.05601437243652673, fluctuations: 0.0\n",
      "step: 16200 loss: 95.082907 time elapsed: 20.7400 learning rate: 0.000100, scenario: 0, slope: -0.05571241116597348, fluctuations: 0.0\n",
      "step: 16210 loss: 94.646989 time elapsed: 20.7537 learning rate: 0.000100, scenario: 0, slope: -0.05513004613903567, fluctuations: 0.0\n",
      "step: 16220 loss: 94.042505 time elapsed: 20.7666 learning rate: 0.000100, scenario: 0, slope: -0.05461606351146856, fluctuations: 0.0\n",
      "step: 16230 loss: 93.470248 time elapsed: 20.7792 learning rate: 0.000100, scenario: 0, slope: -0.05435306213977521, fluctuations: 0.0\n",
      "step: 16240 loss: 92.944215 time elapsed: 20.7920 learning rate: 0.000100, scenario: 0, slope: -0.054139876428321965, fluctuations: 0.0\n",
      "step: 16250 loss: 92.412558 time elapsed: 20.8046 learning rate: 0.000100, scenario: 0, slope: -0.05394628230653411, fluctuations: 0.0\n",
      "step: 16260 loss: 91.887792 time elapsed: 20.8171 learning rate: 0.000100, scenario: 0, slope: -0.053761765233641935, fluctuations: 0.0\n",
      "step: 16270 loss: 91.366291 time elapsed: 20.8296 learning rate: 0.000100, scenario: 0, slope: -0.05358066754939432, fluctuations: 0.0\n",
      "step: 16280 loss: 90.847419 time elapsed: 20.8423 learning rate: 0.000100, scenario: 0, slope: -0.053403382128808845, fluctuations: 0.0\n",
      "step: 16290 loss: 90.331502 time elapsed: 20.8548 learning rate: 0.000100, scenario: 0, slope: -0.05323046700109521, fluctuations: 0.0\n",
      "step: 16300 loss: 89.818909 time elapsed: 20.8672 learning rate: 0.000100, scenario: 0, slope: -0.05307449561681764, fluctuations: 0.0\n",
      "step: 16310 loss: 89.327305 time elapsed: 20.8804 learning rate: 0.000100, scenario: 0, slope: -0.052599064364965295, fluctuations: 0.0\n",
      "step: 16320 loss: 88.803956 time elapsed: 20.8934 learning rate: 0.000100, scenario: 0, slope: -0.05182717140946537, fluctuations: 0.0\n",
      "step: 16330 loss: 88.304479 time elapsed: 20.9063 learning rate: 0.000100, scenario: 0, slope: -0.051479349844885815, fluctuations: 0.0\n",
      "step: 16340 loss: 87.802742 time elapsed: 20.9203 learning rate: 0.000100, scenario: 0, slope: -0.05120567586052411, fluctuations: 0.0\n",
      "step: 16350 loss: 87.300016 time elapsed: 20.9338 learning rate: 0.000100, scenario: 0, slope: -0.05097140095647212, fluctuations: 0.0\n",
      "step: 16360 loss: 86.805002 time elapsed: 20.9468 learning rate: 0.000100, scenario: 0, slope: -0.05074937401250697, fluctuations: 0.0\n",
      "step: 16370 loss: 86.312772 time elapsed: 20.9595 learning rate: 0.000100, scenario: 0, slope: -0.050530994244783, fluctuations: 0.0\n",
      "step: 16380 loss: 85.823074 time elapsed: 20.9722 learning rate: 0.000100, scenario: 0, slope: -0.05031681715731298, fluctuations: 0.0\n",
      "step: 16390 loss: 85.336078 time elapsed: 20.9848 learning rate: 0.000100, scenario: 0, slope: -0.05010755058851411, fluctuations: 0.0\n",
      "step: 16400 loss: 84.851771 time elapsed: 20.9971 learning rate: 0.000100, scenario: 0, slope: -0.04992374268798297, fluctuations: 0.0\n",
      "step: 16410 loss: 84.370404 time elapsed: 21.0104 learning rate: 0.000100, scenario: 0, slope: -0.049669940143356184, fluctuations: 0.0\n",
      "step: 16420 loss: 83.913044 time elapsed: 21.0229 learning rate: 0.000100, scenario: 0, slope: -0.049142886534984095, fluctuations: 0.0\n",
      "step: 16430 loss: 83.426151 time elapsed: 21.0355 learning rate: 0.000100, scenario: 0, slope: -0.04846371382410194, fluctuations: 0.0\n",
      "step: 16440 loss: 82.967219 time elapsed: 21.0480 learning rate: 0.000100, scenario: 0, slope: -0.04813047879823264, fluctuations: 0.0\n",
      "step: 16450 loss: 82.475290 time elapsed: 21.0601 learning rate: 0.000100, scenario: 0, slope: -0.04793338350407421, fluctuations: 0.0\n",
      "step: 16460 loss: 82.005432 time elapsed: 21.0723 learning rate: 0.000100, scenario: 0, slope: -0.04776307198111151, fluctuations: 0.0\n",
      "step: 16470 loss: 81.540313 time elapsed: 21.0846 learning rate: 0.000100, scenario: 0, slope: -0.04760842034672302, fluctuations: 0.0\n",
      "step: 16480 loss: 81.076976 time elapsed: 21.0974 learning rate: 0.000100, scenario: 0, slope: -0.047459183645493785, fluctuations: 0.0\n",
      "step: 16490 loss: 80.617123 time elapsed: 21.1097 learning rate: 0.000100, scenario: 0, slope: -0.04731316886311392, fluctuations: 0.0\n",
      "step: 16500 loss: 80.159973 time elapsed: 21.1237 learning rate: 0.000100, scenario: 0, slope: -0.04718420618931708, fluctuations: 0.0\n",
      "step: 16510 loss: 79.713412 time elapsed: 21.1377 learning rate: 0.000100, scenario: 0, slope: -0.0470143330075565, fluctuations: 0.0\n",
      "step: 16520 loss: 79.294874 time elapsed: 21.1507 learning rate: 0.000100, scenario: 0, slope: -0.04664212278833883, fluctuations: 0.0\n",
      "step: 16530 loss: 78.815127 time elapsed: 21.1633 learning rate: 0.000100, scenario: 0, slope: -0.046038031321715245, fluctuations: 0.0\n",
      "step: 16540 loss: 78.359235 time elapsed: 21.1772 learning rate: 0.000100, scenario: 0, slope: -0.0457060499153188, fluctuations: 0.0\n",
      "step: 16550 loss: 77.913001 time elapsed: 21.1915 learning rate: 0.000100, scenario: 0, slope: -0.045476437360532786, fluctuations: 0.0\n",
      "step: 16560 loss: 77.470710 time elapsed: 21.2045 learning rate: 0.000100, scenario: 0, slope: -0.045266934488817245, fluctuations: 0.0\n",
      "step: 16570 loss: 77.031076 time elapsed: 21.2179 learning rate: 0.000100, scenario: 0, slope: -0.045072491836798255, fluctuations: 0.0\n",
      "step: 16580 loss: 76.593758 time elapsed: 21.2302 learning rate: 0.000100, scenario: 0, slope: -0.04488363815353946, fluctuations: 0.0\n",
      "step: 16590 loss: 76.158992 time elapsed: 21.2427 learning rate: 0.000100, scenario: 0, slope: -0.044698401774670486, fluctuations: 0.0\n",
      "step: 16600 loss: 75.726622 time elapsed: 21.2560 learning rate: 0.000100, scenario: 0, slope: -0.04453490477190876, fluctuations: 0.0\n",
      "step: 16610 loss: 75.296636 time elapsed: 21.2690 learning rate: 0.000100, scenario: 0, slope: -0.044321331656253696, fluctuations: 0.0\n",
      "step: 16620 loss: 74.869476 time elapsed: 21.2815 learning rate: 0.000100, scenario: 0, slope: -0.043905112205712186, fluctuations: 0.0\n",
      "step: 16630 loss: 74.463728 time elapsed: 21.2942 learning rate: 0.000100, scenario: 0, slope: -0.04357391603290659, fluctuations: 0.0\n",
      "step: 16640 loss: 74.047647 time elapsed: 21.3074 learning rate: 0.000100, scenario: 0, slope: -0.04298184066508713, fluctuations: 0.0\n",
      "step: 16650 loss: 73.606899 time elapsed: 21.3231 learning rate: 0.000100, scenario: 0, slope: -0.0427150530124426, fluctuations: 0.0\n",
      "step: 16660 loss: 73.192181 time elapsed: 21.3368 learning rate: 0.000100, scenario: 0, slope: -0.04252549733627262, fluctuations: 0.0\n",
      "step: 16670 loss: 72.769799 time elapsed: 21.3500 learning rate: 0.000100, scenario: 0, slope: -0.04238317298078199, fluctuations: 0.0\n",
      "step: 16680 loss: 72.357658 time elapsed: 21.3633 learning rate: 0.000100, scenario: 0, slope: -0.04224797267312367, fluctuations: 0.0\n",
      "step: 16690 loss: 71.946829 time elapsed: 21.3777 learning rate: 0.000100, scenario: 0, slope: -0.04211786925953677, fluctuations: 0.0\n",
      "step: 16700 loss: 71.538420 time elapsed: 21.3913 learning rate: 0.000100, scenario: 0, slope: -0.04200424234051151, fluctuations: 0.0\n",
      "step: 16710 loss: 71.132826 time elapsed: 21.4068 learning rate: 0.000100, scenario: 0, slope: -0.041868771279881346, fluctuations: 0.0\n",
      "step: 16720 loss: 70.750075 time elapsed: 21.4206 learning rate: 0.000100, scenario: 0, slope: -0.04171475256303786, fluctuations: 0.0\n",
      "step: 16730 loss: 70.328041 time elapsed: 21.4342 learning rate: 0.000100, scenario: 0, slope: -0.0412871834393272, fluctuations: 0.0\n",
      "step: 16740 loss: 69.945251 time elapsed: 21.4472 learning rate: 0.000100, scenario: 0, slope: -0.04074316117166567, fluctuations: 0.0\n",
      "step: 16750 loss: 69.533674 time elapsed: 21.4604 learning rate: 0.000100, scenario: 0, slope: -0.04047997667000535, fluctuations: 0.0\n",
      "step: 16760 loss: 69.140237 time elapsed: 21.4733 learning rate: 0.000100, scenario: 0, slope: -0.04029081334990689, fluctuations: 0.0\n",
      "step: 16770 loss: 68.746883 time elapsed: 21.4861 learning rate: 0.000100, scenario: 0, slope: -0.040145154988873874, fluctuations: 0.0\n",
      "step: 16780 loss: 68.356481 time elapsed: 21.4989 learning rate: 0.000100, scenario: 0, slope: -0.040008725710247474, fluctuations: 0.0\n",
      "step: 16790 loss: 67.968731 time elapsed: 21.5117 learning rate: 0.000100, scenario: 0, slope: -0.039878329501049456, fluctuations: 0.0\n",
      "step: 16800 loss: 67.582954 time elapsed: 21.5237 learning rate: 0.000100, scenario: 0, slope: -0.039765021947580886, fluctuations: 0.0\n",
      "step: 16810 loss: 67.199266 time elapsed: 21.5386 learning rate: 0.000100, scenario: 0, slope: -0.03963048492724999, fluctuations: 0.0\n",
      "step: 16820 loss: 66.817700 time elapsed: 21.5524 learning rate: 0.000100, scenario: 0, slope: -0.03947460693548032, fluctuations: 0.0\n",
      "step: 16830 loss: 66.440816 time elapsed: 21.5653 learning rate: 0.000100, scenario: 0, slope: -0.03899639328375853, fluctuations: 0.0\n",
      "step: 16840 loss: 66.193005 time elapsed: 21.5786 learning rate: 0.000100, scenario: 0, slope: -0.03842240133639834, fluctuations: 0.0\n",
      "step: 16850 loss: 65.698379 time elapsed: 21.5916 learning rate: 0.000100, scenario: 0, slope: -0.03801012490696379, fluctuations: 0.0\n",
      "step: 16860 loss: 65.329736 time elapsed: 21.6056 learning rate: 0.000100, scenario: 0, slope: -0.037833874187079806, fluctuations: 0.0\n",
      "step: 16870 loss: 64.946091 time elapsed: 21.6197 learning rate: 0.000100, scenario: 0, slope: -0.03771254102999545, fluctuations: 0.0\n",
      "step: 16880 loss: 64.578389 time elapsed: 21.6328 learning rate: 0.000100, scenario: 0, slope: -0.03760747147555004, fluctuations: 0.0\n",
      "step: 16890 loss: 64.212311 time elapsed: 21.6466 learning rate: 0.000100, scenario: 0, slope: -0.037511410209550525, fluctuations: 0.0\n",
      "step: 16900 loss: 63.848185 time elapsed: 21.6602 learning rate: 0.000100, scenario: 0, slope: -0.03742744768658858, fluctuations: 0.0\n",
      "step: 16910 loss: 63.486397 time elapsed: 21.6747 learning rate: 0.000100, scenario: 0, slope: -0.03732946337106576, fluctuations: 0.0\n",
      "step: 16920 loss: 63.126576 time elapsed: 21.6875 learning rate: 0.000100, scenario: 0, slope: -0.037245887603011854, fluctuations: 0.0\n",
      "step: 16930 loss: 62.768840 time elapsed: 21.7004 learning rate: 0.000100, scenario: 0, slope: -0.0371647841211863, fluctuations: 0.0\n",
      "step: 16940 loss: 62.419331 time elapsed: 21.7140 learning rate: 0.000100, scenario: 0, slope: -0.03676531874517121, fluctuations: 0.0\n",
      "step: 16950 loss: 62.129373 time elapsed: 21.7285 learning rate: 0.000100, scenario: 0, slope: -0.036079397380368874, fluctuations: 0.0\n",
      "step: 16960 loss: 61.714629 time elapsed: 21.7435 learning rate: 0.000100, scenario: 0, slope: -0.03578372585526285, fluctuations: 0.0\n",
      "step: 16970 loss: 61.361607 time elapsed: 21.7577 learning rate: 0.000100, scenario: 0, slope: -0.035616082531291025, fluctuations: 0.0\n",
      "step: 16980 loss: 61.012598 time elapsed: 21.7720 learning rate: 0.000100, scenario: 0, slope: -0.03547939580934601, fluctuations: 0.0\n",
      "step: 16990 loss: 60.664400 time elapsed: 21.7860 learning rate: 0.000100, scenario: 0, slope: -0.03536489109482233, fluctuations: 0.0\n",
      "step: 17000 loss: 60.320489 time elapsed: 21.7994 learning rate: 0.000100, scenario: 0, slope: -0.035265997995285854, fluctuations: 0.0\n",
      "step: 17010 loss: 59.978425 time elapsed: 21.8140 learning rate: 0.000100, scenario: 0, slope: -0.035148672146045425, fluctuations: 0.0\n",
      "step: 17020 loss: 59.638154 time elapsed: 21.8278 learning rate: 0.000100, scenario: 0, slope: -0.03504457790815575, fluctuations: 0.0\n",
      "step: 17030 loss: 59.299758 time elapsed: 21.8421 learning rate: 0.000100, scenario: 0, slope: -0.03494324406186821, fluctuations: 0.0\n",
      "step: 17040 loss: 58.963212 time elapsed: 21.8553 learning rate: 0.000100, scenario: 0, slope: -0.03483288772139424, fluctuations: 0.0\n",
      "step: 17050 loss: 58.628702 time elapsed: 21.8681 learning rate: 0.000100, scenario: 0, slope: -0.03442969381637083, fluctuations: 0.0\n",
      "step: 17060 loss: 58.304416 time elapsed: 21.8814 learning rate: 0.000100, scenario: 0, slope: -0.034137753084894355, fluctuations: 0.0\n",
      "step: 17070 loss: 58.032129 time elapsed: 21.8951 learning rate: 0.000100, scenario: 0, slope: -0.033614842811009425, fluctuations: 0.0\n",
      "step: 17080 loss: 57.637916 time elapsed: 21.9082 learning rate: 0.000100, scenario: 0, slope: -0.03338067451402726, fluctuations: 0.0\n",
      "step: 17090 loss: 57.316748 time elapsed: 21.9227 learning rate: 0.000100, scenario: 0, slope: -0.03324084771579117, fluctuations: 0.0\n",
      "step: 17100 loss: 56.987789 time elapsed: 21.9385 learning rate: 0.000100, scenario: 0, slope: -0.03314050236595721, fluctuations: 0.0\n",
      "step: 17110 loss: 56.663024 time elapsed: 21.9536 learning rate: 0.000100, scenario: 0, slope: -0.03303440911253102, fluctuations: 0.0\n",
      "step: 17120 loss: 56.342534 time elapsed: 21.9677 learning rate: 0.000100, scenario: 0, slope: -0.03294097129247101, fluctuations: 0.0\n",
      "step: 17130 loss: 56.027268 time elapsed: 21.9813 learning rate: 0.000100, scenario: 0, slope: -0.03284188712688286, fluctuations: 0.0\n",
      "step: 17140 loss: 55.752654 time elapsed: 21.9957 learning rate: 0.000100, scenario: 0, slope: -0.03262344218227469, fluctuations: 0.0\n",
      "step: 17150 loss: 55.402315 time elapsed: 22.0101 learning rate: 0.000100, scenario: 0, slope: -0.032481557968325636, fluctuations: 0.0\n",
      "step: 17160 loss: 55.081050 time elapsed: 22.0243 learning rate: 0.000100, scenario: 0, slope: -0.03240636892060203, fluctuations: 0.0\n",
      "step: 17170 loss: 54.765911 time elapsed: 22.0390 learning rate: 0.000100, scenario: 0, slope: -0.0320121525444149, fluctuations: 0.0\n",
      "step: 17180 loss: 54.454459 time elapsed: 22.0530 learning rate: 0.000100, scenario: 0, slope: -0.03179959535460398, fluctuations: 0.0\n",
      "step: 17190 loss: 54.145470 time elapsed: 22.0664 learning rate: 0.000100, scenario: 0, slope: -0.03164773485412493, fluctuations: 0.0\n",
      "step: 17200 loss: 53.838403 time elapsed: 22.0796 learning rate: 0.000100, scenario: 0, slope: -0.03152866715016309, fluctuations: 0.0\n",
      "step: 17210 loss: 53.533228 time elapsed: 22.0944 learning rate: 0.000100, scenario: 0, slope: -0.03139899180183162, fluctuations: 0.0\n",
      "step: 17220 loss: 53.229756 time elapsed: 22.1082 learning rate: 0.000100, scenario: 0, slope: -0.0312840892951933, fluctuations: 0.0\n",
      "step: 17230 loss: 52.930177 time elapsed: 22.1214 learning rate: 0.000100, scenario: 0, slope: -0.031158025444902246, fluctuations: 0.0\n",
      "step: 17240 loss: 52.683300 time elapsed: 22.1365 learning rate: 0.000100, scenario: 0, slope: -0.030774807672809837, fluctuations: 0.0\n",
      "step: 17250 loss: 52.351187 time elapsed: 22.1516 learning rate: 0.000100, scenario: 0, slope: -0.030356442516300323, fluctuations: 0.0\n",
      "step: 17260 loss: 52.054838 time elapsed: 22.1646 learning rate: 0.000100, scenario: 0, slope: -0.030111184246040895, fluctuations: 0.0\n",
      "step: 17270 loss: 51.741891 time elapsed: 22.1787 learning rate: 0.000100, scenario: 0, slope: -0.029984980813845485, fluctuations: 0.0\n",
      "step: 17280 loss: 51.448509 time elapsed: 22.1929 learning rate: 0.000100, scenario: 0, slope: -0.029895087566143484, fluctuations: 0.0\n",
      "step: 17290 loss: 51.157229 time elapsed: 22.2070 learning rate: 0.000100, scenario: 0, slope: -0.029808476775549667, fluctuations: 0.0\n",
      "step: 17300 loss: 50.869382 time elapsed: 22.2212 learning rate: 0.000100, scenario: 0, slope: -0.02973284044459544, fluctuations: 0.0\n",
      "step: 17310 loss: 50.592224 time elapsed: 22.2362 learning rate: 0.000100, scenario: 0, slope: -0.02961384859279366, fluctuations: 0.0\n",
      "step: 17320 loss: 50.298521 time elapsed: 22.2495 learning rate: 0.000100, scenario: 0, slope: -0.029479899455912936, fluctuations: 0.0\n",
      "step: 17330 loss: 50.011085 time elapsed: 22.2636 learning rate: 0.000100, scenario: 0, slope: -0.02943296735238337, fluctuations: 0.0\n",
      "step: 17340 loss: 49.724944 time elapsed: 22.2774 learning rate: 0.000100, scenario: 0, slope: -0.029258117607745337, fluctuations: 0.0\n",
      "step: 17350 loss: 49.443513 time elapsed: 22.2917 learning rate: 0.000100, scenario: 0, slope: -0.028965112068835897, fluctuations: 0.0\n",
      "step: 17360 loss: 49.164864 time elapsed: 22.3051 learning rate: 0.000100, scenario: 0, slope: -0.028731088913421708, fluctuations: 0.0\n",
      "step: 17370 loss: 48.894092 time elapsed: 22.3181 learning rate: 0.000100, scenario: 0, slope: -0.0285495650193035, fluctuations: 0.0\n",
      "step: 17380 loss: 48.633572 time elapsed: 22.3316 learning rate: 0.000100, scenario: 0, slope: -0.02830878419685139, fluctuations: 0.0\n",
      "step: 17390 loss: 48.355527 time elapsed: 22.3456 learning rate: 0.000100, scenario: 0, slope: -0.028126270974410235, fluctuations: 0.0\n",
      "step: 17400 loss: 48.067085 time elapsed: 22.3618 learning rate: 0.000100, scenario: 0, slope: -0.02791473703590666, fluctuations: 0.0\n",
      "step: 17410 loss: 47.792654 time elapsed: 22.3776 learning rate: 0.000100, scenario: 0, slope: -0.02777469681924528, fluctuations: 0.0\n",
      "step: 17420 loss: 47.519142 time elapsed: 22.3922 learning rate: 0.000100, scenario: 0, slope: -0.027608044648405567, fluctuations: 0.0\n",
      "step: 17430 loss: 47.251106 time elapsed: 22.4069 learning rate: 0.000100, scenario: 0, slope: -0.027523896911442098, fluctuations: 0.0\n",
      "step: 17440 loss: 46.982687 time elapsed: 22.4219 learning rate: 0.000100, scenario: 0, slope: -0.02743556795093725, fluctuations: 0.0\n",
      "step: 17450 loss: 46.717436 time elapsed: 22.4360 learning rate: 0.000100, scenario: 0, slope: -0.027363521181948586, fluctuations: 0.0\n",
      "step: 17460 loss: 46.458645 time elapsed: 22.4492 learning rate: 0.000100, scenario: 0, slope: -0.02727637864526462, fluctuations: 0.0\n",
      "step: 17470 loss: 46.249104 time elapsed: 22.4630 learning rate: 0.000100, scenario: 0, slope: -0.027006888227546894, fluctuations: 0.0\n",
      "step: 17480 loss: 45.956525 time elapsed: 22.4781 learning rate: 0.000100, scenario: 0, slope: -0.026712440931092388, fluctuations: 0.0\n",
      "step: 17490 loss: 45.682014 time elapsed: 22.4927 learning rate: 0.000100, scenario: 0, slope: -0.026516708836807075, fluctuations: 0.0\n",
      "step: 17500 loss: 45.415789 time elapsed: 22.5067 learning rate: 0.000100, scenario: 0, slope: -0.02631574457538693, fluctuations: 0.0\n",
      "step: 17510 loss: 45.159175 time elapsed: 22.5207 learning rate: 0.000100, scenario: 0, slope: -0.02620558839271673, fluctuations: 0.0\n",
      "step: 17520 loss: 44.908694 time elapsed: 22.5335 learning rate: 0.000100, scenario: 0, slope: -0.02610781996607513, fluctuations: 0.0\n",
      "step: 17530 loss: 44.671571 time elapsed: 22.5469 learning rate: 0.000100, scenario: 0, slope: -0.025958210217812118, fluctuations: 0.0\n",
      "step: 17540 loss: 44.397648 time elapsed: 22.5624 learning rate: 0.000100, scenario: 0, slope: -0.025867250205538792, fluctuations: 0.0\n",
      "step: 17550 loss: 44.149037 time elapsed: 22.5780 learning rate: 0.000100, scenario: 0, slope: -0.025817411574878532, fluctuations: 0.0\n",
      "step: 17560 loss: 43.898875 time elapsed: 22.5921 learning rate: 0.000100, scenario: 0, slope: -0.02577783602625106, fluctuations: 0.0\n",
      "step: 17570 loss: 43.650886 time elapsed: 22.6059 learning rate: 0.000100, scenario: 0, slope: -0.0255519675842261, fluctuations: 0.0\n",
      "step: 17580 loss: 43.404753 time elapsed: 22.6202 learning rate: 0.000100, scenario: 0, slope: -0.025333833203632604, fluctuations: 0.0\n",
      "step: 17590 loss: 43.167454 time elapsed: 22.6342 learning rate: 0.000100, scenario: 0, slope: -0.025144476310041044, fluctuations: 0.0\n",
      "step: 17600 loss: 42.974423 time elapsed: 22.6473 learning rate: 0.000100, scenario: 0, slope: -0.024867604610423064, fluctuations: 0.0\n",
      "step: 17610 loss: 42.697429 time elapsed: 22.6620 learning rate: 0.000100, scenario: 0, slope: -0.024663579822763716, fluctuations: 0.0\n",
      "step: 17620 loss: 42.449319 time elapsed: 22.6761 learning rate: 0.000100, scenario: 0, slope: -0.024543792624941673, fluctuations: 0.0\n",
      "step: 17630 loss: 42.202384 time elapsed: 22.6894 learning rate: 0.000100, scenario: 0, slope: -0.024373535611503298, fluctuations: 0.0\n",
      "step: 17640 loss: 41.958751 time elapsed: 22.7022 learning rate: 0.000100, scenario: 0, slope: -0.02427037773983881, fluctuations: 0.0\n",
      "step: 17650 loss: 41.722360 time elapsed: 22.7157 learning rate: 0.000100, scenario: 0, slope: -0.024206282351231834, fluctuations: 0.0\n",
      "step: 17660 loss: 41.490657 time elapsed: 22.7284 learning rate: 0.000100, scenario: 0, slope: -0.024150819171366012, fluctuations: 0.0\n",
      "step: 17670 loss: 41.292177 time elapsed: 22.7420 learning rate: 0.000100, scenario: 0, slope: -0.024000012151919475, fluctuations: 0.0\n",
      "step: 17680 loss: 41.025603 time elapsed: 22.7551 learning rate: 0.000100, scenario: 0, slope: -0.02388429176497319, fluctuations: 0.0\n",
      "step: 17690 loss: 40.791838 time elapsed: 22.7684 learning rate: 0.000100, scenario: 0, slope: -0.02384017595438867, fluctuations: 0.0\n",
      "step: 17700 loss: 40.561252 time elapsed: 22.7832 learning rate: 0.000100, scenario: 0, slope: -0.023640332890716622, fluctuations: 0.0\n",
      "step: 17710 loss: 40.333206 time elapsed: 22.7981 learning rate: 0.000100, scenario: 0, slope: -0.023442781795283234, fluctuations: 0.0\n",
      "step: 17720 loss: 40.106899 time elapsed: 22.8119 learning rate: 0.000100, scenario: 0, slope: -0.023311983290424367, fluctuations: 0.0\n",
      "step: 17730 loss: 39.886843 time elapsed: 22.8262 learning rate: 0.000100, scenario: 0, slope: -0.02317790671807255, fluctuations: 0.0\n",
      "step: 17740 loss: 39.723936 time elapsed: 22.8403 learning rate: 0.000100, scenario: 0, slope: -0.022888065375906687, fluctuations: 0.0\n",
      "step: 17750 loss: 39.456377 time elapsed: 22.8542 learning rate: 0.000100, scenario: 0, slope: -0.02274884242171743, fluctuations: 0.0\n",
      "step: 17760 loss: 39.219287 time elapsed: 22.8671 learning rate: 0.000100, scenario: 0, slope: -0.02269929620317785, fluctuations: 0.0\n",
      "step: 17770 loss: 38.994069 time elapsed: 22.8796 learning rate: 0.000100, scenario: 0, slope: -0.02257097770412947, fluctuations: 0.0\n",
      "step: 17780 loss: 38.774195 time elapsed: 22.8921 learning rate: 0.000100, scenario: 0, slope: -0.0224208116224406, fluctuations: 0.0\n",
      "step: 17790 loss: 38.556826 time elapsed: 22.9047 learning rate: 0.000100, scenario: 0, slope: -0.022339158063019848, fluctuations: 0.0\n",
      "step: 17800 loss: 38.353091 time elapsed: 22.9169 learning rate: 0.000100, scenario: 0, slope: -0.022267136054051194, fluctuations: 0.0\n",
      "step: 17810 loss: 38.152364 time elapsed: 22.9301 learning rate: 0.000100, scenario: 0, slope: -0.0219592950974193, fluctuations: 0.0\n",
      "step: 17820 loss: 37.911936 time elapsed: 22.9428 learning rate: 0.000100, scenario: 0, slope: -0.02191115931414211, fluctuations: 0.0\n",
      "step: 17830 loss: 37.699733 time elapsed: 22.9554 learning rate: 0.000100, scenario: 0, slope: -0.021903843641299117, fluctuations: 0.0\n",
      "step: 17840 loss: 37.488889 time elapsed: 22.9678 learning rate: 0.000100, scenario: 0, slope: -0.021690685676739936, fluctuations: 0.0\n",
      "step: 17850 loss: 37.277578 time elapsed: 22.9803 learning rate: 0.000100, scenario: 0, slope: -0.02153875411699826, fluctuations: 0.0\n",
      "step: 17860 loss: 37.067913 time elapsed: 22.9933 learning rate: 0.000100, scenario: 0, slope: -0.021463344643812703, fluctuations: 0.0\n",
      "step: 17870 loss: 36.859706 time elapsed: 23.0072 learning rate: 0.000100, scenario: 0, slope: -0.02141391053651412, fluctuations: 0.0\n",
      "step: 17880 loss: 36.652849 time elapsed: 23.0199 learning rate: 0.000100, scenario: 0, slope: -0.0213727789784832, fluctuations: 0.0\n",
      "step: 17890 loss: 36.458714 time elapsed: 23.0327 learning rate: 0.000100, scenario: 0, slope: -0.021316677701401467, fluctuations: 0.0\n",
      "step: 17900 loss: 36.294565 time elapsed: 23.0451 learning rate: 0.000100, scenario: 0, slope: -0.020921935656434236, fluctuations: 0.01\n",
      "step: 17910 loss: 36.052316 time elapsed: 23.0582 learning rate: 0.000100, scenario: 0, slope: -0.020523971278846986, fluctuations: 0.02\n",
      "step: 17920 loss: 35.843477 time elapsed: 23.0708 learning rate: 0.000100, scenario: 0, slope: -0.02039659167346258, fluctuations: 0.02\n",
      "step: 17930 loss: 35.642316 time elapsed: 23.0836 learning rate: 0.000100, scenario: 0, slope: -0.02035919615333207, fluctuations: 0.02\n",
      "step: 17940 loss: 35.440108 time elapsed: 23.0962 learning rate: 0.000100, scenario: 0, slope: -0.02033950742989323, fluctuations: 0.02\n",
      "step: 17950 loss: 35.242438 time elapsed: 23.1088 learning rate: 0.000100, scenario: 0, slope: -0.020331424451764352, fluctuations: 0.02\n",
      "step: 17960 loss: 35.045200 time elapsed: 23.1215 learning rate: 0.000100, scenario: 0, slope: -0.02032856679374912, fluctuations: 0.02\n",
      "step: 17970 loss: 34.849182 time elapsed: 23.1340 learning rate: 0.000100, scenario: 0, slope: -0.020331376722762275, fluctuations: 0.02\n",
      "step: 17980 loss: 34.654296 time elapsed: 23.1466 learning rate: 0.000100, scenario: 0, slope: -0.020341233930470593, fluctuations: 0.02\n",
      "step: 17990 loss: 34.460733 time elapsed: 23.1590 learning rate: 0.000100, scenario: 0, slope: -0.020339258621542244, fluctuations: 0.02\n",
      "step: 18000 loss: 34.296140 time elapsed: 23.1714 learning rate: 0.000100, scenario: 0, slope: -0.019907326527696414, fluctuations: 0.01\n",
      "step: 18010 loss: 34.079511 time elapsed: 23.1849 learning rate: 0.000100, scenario: 0, slope: -0.019396334679301283, fluctuations: 0.01\n",
      "step: 18020 loss: 33.906029 time elapsed: 23.1970 learning rate: 0.000100, scenario: 0, slope: -0.019197934993563988, fluctuations: 0.02\n",
      "step: 18030 loss: 33.699136 time elapsed: 23.2114 learning rate: 0.000100, scenario: 0, slope: -0.019149616997376277, fluctuations: 0.02\n",
      "step: 18040 loss: 33.509311 time elapsed: 23.2248 learning rate: 0.000100, scenario: 0, slope: -0.01912496028278357, fluctuations: 0.02\n",
      "step: 18050 loss: 33.321724 time elapsed: 23.2376 learning rate: 0.000100, scenario: 0, slope: -0.01911810415626004, fluctuations: 0.02\n",
      "step: 18060 loss: 33.134779 time elapsed: 23.2504 learning rate: 0.000100, scenario: 0, slope: -0.019114319222131224, fluctuations: 0.02\n",
      "step: 18070 loss: 32.949327 time elapsed: 23.2632 learning rate: 0.000100, scenario: 0, slope: -0.019114358180692393, fluctuations: 0.02\n",
      "step: 18080 loss: 32.764863 time elapsed: 23.2759 learning rate: 0.000100, scenario: 0, slope: -0.019117866897418175, fluctuations: 0.02\n",
      "step: 18090 loss: 32.581273 time elapsed: 23.2884 learning rate: 0.000100, scenario: 0, slope: -0.019126085083296326, fluctuations: 0.02\n",
      "step: 18100 loss: 32.398626 time elapsed: 23.3010 learning rate: 0.000100, scenario: 0, slope: -0.01910644197038429, fluctuations: 0.02\n",
      "step: 18110 loss: 32.216903 time elapsed: 23.3142 learning rate: 0.000100, scenario: 0, slope: -0.01871526207836772, fluctuations: 0.01\n",
      "step: 18120 loss: 32.037450 time elapsed: 23.3271 learning rate: 0.000100, scenario: 0, slope: -0.018531514385984202, fluctuations: 0.0\n",
      "step: 18130 loss: 31.991890 time elapsed: 23.3393 learning rate: 0.000100, scenario: 0, slope: -0.018183874219964508, fluctuations: 0.0\n",
      "step: 18140 loss: 31.729855 time elapsed: 23.3518 learning rate: 0.000100, scenario: 0, slope: -0.017803177178379148, fluctuations: 0.02\n",
      "step: 18150 loss: 31.520670 time elapsed: 23.3642 learning rate: 0.000100, scenario: 0, slope: -0.017738347856926583, fluctuations: 0.03\n",
      "step: 18160 loss: 31.327444 time elapsed: 23.3767 learning rate: 0.000100, scenario: 0, slope: -0.01773549912292856, fluctuations: 0.03\n",
      "step: 18170 loss: 31.152711 time elapsed: 23.3895 learning rate: 0.000100, scenario: 0, slope: -0.017755339490407744, fluctuations: 0.03\n",
      "step: 18180 loss: 30.979369 time elapsed: 23.4019 learning rate: 0.000100, scenario: 0, slope: -0.01778399958986065, fluctuations: 0.03\n",
      "step: 18190 loss: 30.805561 time elapsed: 23.4167 learning rate: 0.000100, scenario: 0, slope: -0.01781542501138029, fluctuations: 0.03\n",
      "step: 18200 loss: 30.633630 time elapsed: 23.4295 learning rate: 0.000100, scenario: 0, slope: -0.017847009314216972, fluctuations: 0.03\n",
      "step: 18210 loss: 30.462373 time elapsed: 23.4428 learning rate: 0.000100, scenario: 0, slope: -0.017894388979812298, fluctuations: 0.03\n",
      "step: 18220 loss: 30.292035 time elapsed: 23.4556 learning rate: 0.000100, scenario: 0, slope: -0.01794805426882382, fluctuations: 0.03\n",
      "step: 18230 loss: 30.122479 time elapsed: 23.4683 learning rate: 0.000100, scenario: 0, slope: -0.017834701953354244, fluctuations: 0.02\n",
      "step: 18240 loss: 29.953741 time elapsed: 23.4810 learning rate: 0.000100, scenario: 0, slope: -0.01734756991474007, fluctuations: 0.01\n",
      "step: 18250 loss: 29.785823 time elapsed: 23.4935 learning rate: 0.000100, scenario: 0, slope: -0.017208165292361164, fluctuations: 0.0\n",
      "step: 18260 loss: 29.620655 time elapsed: 23.5057 learning rate: 0.000100, scenario: 0, slope: -0.017090113694241807, fluctuations: 0.0\n",
      "step: 18270 loss: 29.596701 time elapsed: 23.5182 learning rate: 0.000100, scenario: 0, slope: -0.01671101786331237, fluctuations: 0.0\n",
      "step: 18280 loss: 29.298367 time elapsed: 23.5309 learning rate: 0.000100, scenario: 0, slope: -0.01648763914981905, fluctuations: 0.02\n",
      "step: 18290 loss: 29.139944 time elapsed: 23.5435 learning rate: 0.000100, scenario: 0, slope: -0.01643288134480186, fluctuations: 0.03\n",
      "step: 18300 loss: 28.965414 time elapsed: 23.5560 learning rate: 0.000100, scenario: 0, slope: -0.016441752170855597, fluctuations: 0.03\n",
      "step: 18310 loss: 28.798612 time elapsed: 23.5687 learning rate: 0.000100, scenario: 0, slope: -0.016465978518675744, fluctuations: 0.03\n",
      "step: 18320 loss: 28.637343 time elapsed: 23.5810 learning rate: 0.000100, scenario: 0, slope: -0.016494333459018996, fluctuations: 0.03\n",
      "step: 18330 loss: 28.476555 time elapsed: 23.5934 learning rate: 0.000100, scenario: 0, slope: -0.016526079226667784, fluctuations: 0.03\n",
      "step: 18340 loss: 28.316316 time elapsed: 23.6057 learning rate: 0.000100, scenario: 0, slope: -0.016561098187462073, fluctuations: 0.03\n",
      "step: 18350 loss: 28.157112 time elapsed: 23.6199 learning rate: 0.000100, scenario: 0, slope: -0.016602007759537244, fluctuations: 0.03\n",
      "step: 18360 loss: 27.998559 time elapsed: 23.6337 learning rate: 0.000100, scenario: 0, slope: -0.016648697106698133, fluctuations: 0.03\n",
      "step: 18370 loss: 27.840769 time elapsed: 23.6466 learning rate: 0.000100, scenario: 0, slope: -0.016450531203789007, fluctuations: 0.02\n",
      "step: 18380 loss: 27.683700 time elapsed: 23.6593 learning rate: 0.000100, scenario: 0, slope: -0.016117814975464544, fluctuations: 0.01\n",
      "step: 18390 loss: 27.528226 time elapsed: 23.6718 learning rate: 0.000100, scenario: 0, slope: -0.015991938278724922, fluctuations: 0.0\n",
      "step: 18400 loss: 27.492796 time elapsed: 23.6843 learning rate: 0.000100, scenario: 0, slope: -0.01578873892264513, fluctuations: 0.0\n",
      "step: 18410 loss: 27.290636 time elapsed: 23.6979 learning rate: 0.000100, scenario: 0, slope: -0.01527987102909566, fluctuations: 0.02\n",
      "step: 18420 loss: 27.088461 time elapsed: 23.7104 learning rate: 0.000100, scenario: 0, slope: -0.015216081230773098, fluctuations: 0.03\n",
      "step: 18430 loss: 26.917313 time elapsed: 23.7232 learning rate: 0.000100, scenario: 0, slope: -0.015236645722465232, fluctuations: 0.03\n",
      "step: 18440 loss: 26.763601 time elapsed: 23.7366 learning rate: 0.000100, scenario: 0, slope: -0.015283277895367002, fluctuations: 0.03\n",
      "step: 18450 loss: 26.614108 time elapsed: 23.7495 learning rate: 0.000100, scenario: 0, slope: -0.015337963470271282, fluctuations: 0.03\n",
      "step: 18460 loss: 26.464637 time elapsed: 23.7619 learning rate: 0.000100, scenario: 0, slope: -0.015396133589324729, fluctuations: 0.03\n",
      "step: 18470 loss: 26.315551 time elapsed: 23.7744 learning rate: 0.000100, scenario: 0, slope: -0.015458405374268425, fluctuations: 0.03\n",
      "step: 18480 loss: 26.167564 time elapsed: 23.7873 learning rate: 0.000100, scenario: 0, slope: -0.01552887754931883, fluctuations: 0.03\n",
      "step: 18490 loss: 26.020183 time elapsed: 23.8000 learning rate: 0.000100, scenario: 0, slope: -0.015611593387766364, fluctuations: 0.03\n",
      "step: 18500 loss: 25.873489 time elapsed: 23.8126 learning rate: 0.000100, scenario: 0, slope: -0.015562237232997145, fluctuations: 0.03\n",
      "step: 18510 loss: 25.727440 time elapsed: 23.8273 learning rate: 0.000100, scenario: 0, slope: -0.0150386945701097, fluctuations: 0.01\n",
      "step: 18520 loss: 25.582047 time elapsed: 23.8405 learning rate: 0.000100, scenario: 0, slope: -0.014894718118457167, fluctuations: 0.0\n",
      "step: 18530 loss: 25.437298 time elapsed: 23.8532 learning rate: 0.000100, scenario: 0, slope: -0.014789485248010454, fluctuations: 0.0\n",
      "step: 18540 loss: 25.293192 time elapsed: 23.8660 learning rate: 0.000100, scenario: 0, slope: -0.014707789306893555, fluctuations: 0.0\n",
      "step: 18550 loss: 25.149748 time elapsed: 23.8782 learning rate: 0.000100, scenario: 0, slope: -0.014636737455681621, fluctuations: 0.0\n",
      "step: 18560 loss: 25.009123 time elapsed: 23.8907 learning rate: 0.000100, scenario: 0, slope: -0.014566917404604723, fluctuations: 0.0\n",
      "step: 18570 loss: 25.027821 time elapsed: 23.9031 learning rate: 0.000100, scenario: 0, slope: -0.014168361510137508, fluctuations: 0.0\n",
      "step: 18580 loss: 24.726083 time elapsed: 23.9162 learning rate: 0.000100, scenario: 0, slope: -0.013995883606221114, fluctuations: 0.02\n",
      "step: 18590 loss: 24.594282 time elapsed: 23.9291 learning rate: 0.000100, scenario: 0, slope: -0.01394843639266369, fluctuations: 0.03\n",
      "step: 18600 loss: 24.451393 time elapsed: 23.9415 learning rate: 0.000100, scenario: 0, slope: -0.013976684958423455, fluctuations: 0.03\n",
      "step: 18610 loss: 24.308605 time elapsed: 23.9544 learning rate: 0.000100, scenario: 0, slope: -0.01402618025825466, fluctuations: 0.03\n",
      "step: 18620 loss: 24.169024 time elapsed: 23.9669 learning rate: 0.000100, scenario: 0, slope: -0.014076499129952172, fluctuations: 0.03\n",
      "step: 18630 loss: 24.031822 time elapsed: 23.9798 learning rate: 0.000100, scenario: 0, slope: -0.014128660369900242, fluctuations: 0.03\n",
      "step: 18640 loss: 23.895263 time elapsed: 23.9923 learning rate: 0.000100, scenario: 0, slope: -0.0141845397760987, fluctuations: 0.03\n",
      "step: 18650 loss: 23.759214 time elapsed: 24.0049 learning rate: 0.000100, scenario: 0, slope: -0.014246701402420275, fluctuations: 0.03\n",
      "step: 18660 loss: 23.623988 time elapsed: 24.0173 learning rate: 0.000100, scenario: 0, slope: -0.014315100396567379, fluctuations: 0.03\n",
      "step: 18670 loss: 23.492561 time elapsed: 24.0325 learning rate: 0.000100, scenario: 0, slope: -0.014085351027008808, fluctuations: 0.02\n",
      "step: 18680 loss: 23.472528 time elapsed: 24.0456 learning rate: 0.000100, scenario: 0, slope: -0.013520406052508793, fluctuations: 0.01\n",
      "step: 18690 loss: 23.260735 time elapsed: 24.0581 learning rate: 0.000100, scenario: 0, slope: -0.013260090293285502, fluctuations: 0.02\n",
      "step: 18700 loss: 23.091866 time elapsed: 24.0706 learning rate: 0.000100, scenario: 0, slope: -0.013210196321588357, fluctuations: 0.02\n",
      "step: 18710 loss: 22.965794 time elapsed: 24.0842 learning rate: 0.000100, scenario: 0, slope: -0.013210176203684166, fluctuations: 0.02\n",
      "step: 18720 loss: 22.830566 time elapsed: 24.0970 learning rate: 0.000100, scenario: 0, slope: -0.013234012163341725, fluctuations: 0.02\n",
      "step: 18730 loss: 22.701247 time elapsed: 24.1094 learning rate: 0.000100, scenario: 0, slope: -0.013263478345125068, fluctuations: 0.02\n",
      "step: 18740 loss: 22.572195 time elapsed: 24.1221 learning rate: 0.000100, scenario: 0, slope: -0.013295344755352193, fluctuations: 0.02\n",
      "step: 18750 loss: 22.443742 time elapsed: 24.1348 learning rate: 0.000100, scenario: 0, slope: -0.013330665446883093, fluctuations: 0.02\n",
      "step: 18760 loss: 22.316047 time elapsed: 24.1475 learning rate: 0.000100, scenario: 0, slope: -0.01337116135646677, fluctuations: 0.02\n",
      "step: 18770 loss: 22.189004 time elapsed: 24.1601 learning rate: 0.000100, scenario: 0, slope: -0.01341328406659838, fluctuations: 0.02\n",
      "step: 18780 loss: 22.062585 time elapsed: 24.1725 learning rate: 0.000100, scenario: 0, slope: -0.01322962162514601, fluctuations: 0.01\n",
      "step: 18790 loss: 21.936790 time elapsed: 24.1850 learning rate: 0.000100, scenario: 0, slope: -0.01293844931358412, fluctuations: 0.0\n",
      "step: 18800 loss: 21.811615 time elapsed: 24.1973 learning rate: 0.000100, scenario: 0, slope: -0.01283569866981972, fluctuations: 0.0\n",
      "step: 18810 loss: 21.687063 time elapsed: 24.2103 learning rate: 0.000100, scenario: 0, slope: -0.012742992494760453, fluctuations: 0.0\n",
      "step: 18820 loss: 21.563137 time elapsed: 24.2225 learning rate: 0.000100, scenario: 0, slope: -0.012673699394931882, fluctuations: 0.0\n",
      "step: 18830 loss: 21.439906 time elapsed: 24.2368 learning rate: 0.000100, scenario: 0, slope: -0.012609004880882343, fluctuations: 0.0\n",
      "step: 18840 loss: 21.322690 time elapsed: 24.2498 learning rate: 0.000100, scenario: 0, slope: -0.01253700108439505, fluctuations: 0.0\n",
      "step: 18850 loss: 21.379405 time elapsed: 24.2624 learning rate: 0.000100, scenario: 0, slope: -0.01207715867527701, fluctuations: 0.01\n",
      "step: 18860 loss: 21.078697 time elapsed: 24.2748 learning rate: 0.000100, scenario: 0, slope: -0.011853325139499445, fluctuations: 0.02\n",
      "step: 18870 loss: 20.967943 time elapsed: 24.2872 learning rate: 0.000100, scenario: 0, slope: -0.011821637513525121, fluctuations: 0.03\n",
      "step: 18880 loss: 20.845631 time elapsed: 24.2998 learning rate: 0.000100, scenario: 0, slope: -0.011878461268709882, fluctuations: 0.03\n",
      "step: 18890 loss: 20.722033 time elapsed: 24.3124 learning rate: 0.000100, scenario: 0, slope: -0.011950407175342634, fluctuations: 0.03\n",
      "step: 18900 loss: 20.604115 time elapsed: 24.3246 learning rate: 0.000100, scenario: 0, slope: -0.012016357950614644, fluctuations: 0.03\n",
      "step: 18910 loss: 20.488000 time elapsed: 24.3375 learning rate: 0.000100, scenario: 0, slope: -0.012101047048641964, fluctuations: 0.03\n",
      "step: 18920 loss: 20.371958 time elapsed: 24.3503 learning rate: 0.000100, scenario: 0, slope: -0.012183429639576616, fluctuations: 0.03\n",
      "step: 18930 loss: 20.256867 time elapsed: 24.3626 learning rate: 0.000100, scenario: 0, slope: -0.012275496659133557, fluctuations: 0.03\n",
      "step: 18940 loss: 20.142303 time elapsed: 24.3757 learning rate: 0.000100, scenario: 0, slope: -0.012373454957322257, fluctuations: 0.03\n",
      "step: 18950 loss: 20.028395 time elapsed: 24.3885 learning rate: 0.000100, scenario: 0, slope: -0.01200186333167837, fluctuations: 0.02\n",
      "step: 18960 loss: 19.915328 time elapsed: 24.4012 learning rate: 0.000100, scenario: 0, slope: -0.011716331621840204, fluctuations: 0.01\n",
      "step: 18970 loss: 19.820861 time elapsed: 24.4139 learning rate: 0.000100, scenario: 0, slope: -0.01155042331091498, fluctuations: 0.0\n",
      "step: 18980 loss: 19.718197 time elapsed: 24.4260 learning rate: 0.000100, scenario: 0, slope: -0.01104319811194169, fluctuations: 0.01\n",
      "step: 18990 loss: 19.604798 time elapsed: 24.4406 learning rate: 0.000100, scenario: 0, slope: -0.01094915608271722, fluctuations: 0.02\n",
      "step: 19000 loss: 19.472248 time elapsed: 24.4536 learning rate: 0.000100, scenario: 0, slope: -0.010933633128639353, fluctuations: 0.03\n",
      "step: 19010 loss: 19.365956 time elapsed: 24.4668 learning rate: 0.000100, scenario: 0, slope: -0.010987899998875868, fluctuations: 0.03\n",
      "step: 19020 loss: 19.253230 time elapsed: 24.4799 learning rate: 0.000100, scenario: 0, slope: -0.01104810146519395, fluctuations: 0.03\n",
      "step: 19030 loss: 19.145977 time elapsed: 24.4924 learning rate: 0.000100, scenario: 0, slope: -0.011109871178254956, fluctuations: 0.03\n",
      "step: 19040 loss: 19.038161 time elapsed: 24.5050 learning rate: 0.000100, scenario: 0, slope: -0.01117463913463307, fluctuations: 0.03\n",
      "step: 19050 loss: 18.931504 time elapsed: 24.5175 learning rate: 0.000100, scenario: 0, slope: -0.011244322930648863, fluctuations: 0.03\n",
      "step: 19060 loss: 18.825386 time elapsed: 24.5303 learning rate: 0.000100, scenario: 0, slope: -0.011322095839882973, fluctuations: 0.03\n",
      "step: 19070 loss: 18.719819 time elapsed: 24.5428 learning rate: 0.000100, scenario: 0, slope: -0.01137910397263021, fluctuations: 0.03\n",
      "step: 19080 loss: 18.614825 time elapsed: 24.5552 learning rate: 0.000100, scenario: 0, slope: -0.010919160594258766, fluctuations: 0.02\n",
      "step: 19090 loss: 18.510392 time elapsed: 24.5678 learning rate: 0.000100, scenario: 0, slope: -0.010781421141505076, fluctuations: 0.0\n",
      "step: 19100 loss: 18.406511 time elapsed: 24.5802 learning rate: 0.000100, scenario: 0, slope: -0.01067089723095881, fluctuations: 0.0\n",
      "step: 19110 loss: 18.303180 time elapsed: 24.5931 learning rate: 0.000100, scenario: 0, slope: -0.010590452456465626, fluctuations: 0.0\n",
      "step: 19120 loss: 18.200395 time elapsed: 24.6055 learning rate: 0.000100, scenario: 0, slope: -0.01052862537120814, fluctuations: 0.0\n",
      "step: 19130 loss: 18.098168 time elapsed: 24.6182 learning rate: 0.000100, scenario: 0, slope: -0.01047018808166437, fluctuations: 0.0\n",
      "step: 19140 loss: 17.997871 time elapsed: 24.6305 learning rate: 0.000100, scenario: 0, slope: -0.01041189743756861, fluctuations: 0.0\n",
      "step: 19150 loss: 18.052627 time elapsed: 24.6449 learning rate: 0.000100, scenario: 0, slope: -0.010089691167323193, fluctuations: 0.0\n",
      "step: 19160 loss: 17.823217 time elapsed: 24.6582 learning rate: 0.000100, scenario: 0, slope: -0.009729320859679844, fluctuations: 0.02\n",
      "step: 19170 loss: 17.718026 time elapsed: 24.6709 learning rate: 0.000100, scenario: 0, slope: -0.009688783771521216, fluctuations: 0.04\n",
      "step: 19180 loss: 17.608106 time elapsed: 24.6836 learning rate: 0.000100, scenario: 0, slope: -0.009752794678802308, fluctuations: 0.04\n",
      "step: 19190 loss: 17.504857 time elapsed: 24.6961 learning rate: 0.000100, scenario: 0, slope: -0.009838664542073576, fluctuations: 0.04\n",
      "step: 19200 loss: 17.405313 time elapsed: 24.7084 learning rate: 0.000100, scenario: 0, slope: -0.009918523581382897, fluctuations: 0.04\n",
      "step: 19210 loss: 17.308416 time elapsed: 24.7214 learning rate: 0.000100, scenario: 0, slope: -0.010017261046911247, fluctuations: 0.04\n",
      "step: 19220 loss: 17.212436 time elapsed: 24.7338 learning rate: 0.000100, scenario: 0, slope: -0.010111622027838664, fluctuations: 0.04\n",
      "step: 19230 loss: 17.116999 time elapsed: 24.7464 learning rate: 0.000100, scenario: 0, slope: -0.010215357840423133, fluctuations: 0.04\n",
      "step: 19240 loss: 17.022968 time elapsed: 24.7590 learning rate: 0.000100, scenario: 0, slope: -0.010330284593211553, fluctuations: 0.04\n",
      "step: 19250 loss: 16.967220 time elapsed: 24.7717 learning rate: 0.000100, scenario: 0, slope: -0.010174045025562045, fluctuations: 0.03\n",
      "step: 19260 loss: 16.835078 time elapsed: 24.7843 learning rate: 0.000100, scenario: 0, slope: -0.009403078874562897, fluctuations: 0.03\n",
      "step: 19270 loss: 16.755999 time elapsed: 24.7967 learning rate: 0.000100, scenario: 0, slope: -0.009282953471382147, fluctuations: 0.02\n",
      "step: 19280 loss: 16.654663 time elapsed: 24.8091 learning rate: 0.000100, scenario: 0, slope: -0.009251320167038963, fluctuations: 0.02\n",
      "step: 19290 loss: 16.557821 time elapsed: 24.8216 learning rate: 0.000100, scenario: 0, slope: -0.009274101253737526, fluctuations: 0.02\n",
      "step: 19300 loss: 16.467023 time elapsed: 24.8350 learning rate: 0.000100, scenario: 0, slope: -0.009304322161332586, fluctuations: 0.02\n",
      "step: 19310 loss: 16.376405 time elapsed: 24.8505 learning rate: 0.000100, scenario: 0, slope: -0.00934647160640562, fluctuations: 0.02\n",
      "step: 19320 loss: 16.286124 time elapsed: 24.8636 learning rate: 0.000100, scenario: 0, slope: -0.009388636177264745, fluctuations: 0.02\n",
      "step: 19330 loss: 16.196369 time elapsed: 24.8766 learning rate: 0.000100, scenario: 0, slope: -0.009435350170187507, fluctuations: 0.02\n",
      "step: 19340 loss: 16.107113 time elapsed: 24.8896 learning rate: 0.000100, scenario: 0, slope: -0.009486076259688753, fluctuations: 0.02\n",
      "step: 19350 loss: 16.018341 time elapsed: 24.9025 learning rate: 0.000100, scenario: 0, slope: -0.009462620708553245, fluctuations: 0.02\n",
      "step: 19360 loss: 15.930017 time elapsed: 24.9149 learning rate: 0.000100, scenario: 0, slope: -0.009149340313214602, fluctuations: 0.01\n",
      "step: 19370 loss: 15.842138 time elapsed: 24.9272 learning rate: 0.000100, scenario: 0, slope: -0.009037799738393106, fluctuations: 0.0\n",
      "step: 19380 loss: 15.754708 time elapsed: 24.9397 learning rate: 0.000100, scenario: 0, slope: -0.008956966293477791, fluctuations: 0.0\n",
      "step: 19390 loss: 15.667872 time elapsed: 24.9522 learning rate: 0.000100, scenario: 0, slope: -0.008903665574776266, fluctuations: 0.0\n",
      "step: 19400 loss: 15.588601 time elapsed: 24.9645 learning rate: 0.000100, scenario: 0, slope: -0.008851603754264588, fluctuations: 0.0\n",
      "step: 19410 loss: 15.625682 time elapsed: 24.9775 learning rate: 0.000100, scenario: 0, slope: -0.008457619134944529, fluctuations: 0.01\n",
      "step: 19420 loss: 15.418122 time elapsed: 24.9901 learning rate: 0.000100, scenario: 0, slope: -0.008310086181560887, fluctuations: 0.02\n",
      "step: 19430 loss: 15.338661 time elapsed: 25.0026 learning rate: 0.000100, scenario: 0, slope: -0.008312813070061955, fluctuations: 0.03\n",
      "step: 19440 loss: 15.245106 time elapsed: 25.0152 learning rate: 0.000100, scenario: 0, slope: -0.008357969998230885, fluctuations: 0.03\n",
      "step: 19450 loss: 15.162020 time elapsed: 25.0279 learning rate: 0.000100, scenario: 0, slope: -0.008421413895883672, fluctuations: 0.03\n",
      "step: 19460 loss: 15.079165 time elapsed: 25.0401 learning rate: 0.000100, scenario: 0, slope: -0.008484841693761016, fluctuations: 0.03\n",
      "step: 19470 loss: 14.997882 time elapsed: 25.0548 learning rate: 0.000100, scenario: 0, slope: -0.008548638376301279, fluctuations: 0.03\n",
      "step: 19480 loss: 14.951152 time elapsed: 25.0677 learning rate: 0.000100, scenario: 0, slope: -0.008547598202241763, fluctuations: 0.03\n",
      "step: 19490 loss: 14.835671 time elapsed: 25.0812 learning rate: 0.000100, scenario: 0, slope: -0.00847066391179536, fluctuations: 0.04\n",
      "step: 19500 loss: 14.759796 time elapsed: 25.0939 learning rate: 0.000100, scenario: 0, slope: -0.00854218777465355, fluctuations: 0.05\n",
      "step: 19510 loss: 14.677906 time elapsed: 25.1073 learning rate: 0.000100, scenario: 0, slope: -0.008262953407234586, fluctuations: 0.04\n",
      "step: 19520 loss: 14.595450 time elapsed: 25.1198 learning rate: 0.000100, scenario: 0, slope: -0.00812544543356362, fluctuations: 0.03\n",
      "step: 19530 loss: 14.515273 time elapsed: 25.1322 learning rate: 0.000100, scenario: 0, slope: -0.008110081479536327, fluctuations: 0.02\n",
      "step: 19540 loss: 14.436254 time elapsed: 25.1446 learning rate: 0.000100, scenario: 0, slope: -0.008111626731935424, fluctuations: 0.02\n",
      "step: 19550 loss: 14.357799 time elapsed: 25.1571 learning rate: 0.000100, scenario: 0, slope: -0.008133791915005675, fluctuations: 0.02\n",
      "step: 19560 loss: 14.279747 time elapsed: 25.1696 learning rate: 0.000100, scenario: 0, slope: -0.008160330043345394, fluctuations: 0.02\n",
      "step: 19570 loss: 14.202111 time elapsed: 25.1824 learning rate: 0.000100, scenario: 0, slope: -0.008189969440989096, fluctuations: 0.02\n",
      "step: 19580 loss: 14.124894 time elapsed: 25.1950 learning rate: 0.000100, scenario: 0, slope: -0.008141775779519375, fluctuations: 0.02\n",
      "step: 19590 loss: 14.048092 time elapsed: 25.2077 learning rate: 0.000100, scenario: 0, slope: -0.00793096842131396, fluctuations: 0.01\n",
      "step: 19600 loss: 13.973395 time elapsed: 25.2199 learning rate: 0.000100, scenario: 0, slope: -0.00785386522461817, fluctuations: 0.0\n",
      "step: 19610 loss: 14.014358 time elapsed: 25.2331 learning rate: 0.000100, scenario: 0, slope: -0.007578437481950326, fluctuations: 0.0\n",
      "step: 19620 loss: 13.893931 time elapsed: 25.2453 learning rate: 0.000100, scenario: 0, slope: -0.007186625547589715, fluctuations: 0.01\n",
      "step: 19630 loss: 13.748133 time elapsed: 25.2589 learning rate: 0.000100, scenario: 0, slope: -0.007184168586735817, fluctuations: 0.03\n",
      "step: 19640 loss: 13.680887 time elapsed: 25.2726 learning rate: 0.000100, scenario: 0, slope: -0.007233774513190745, fluctuations: 0.04\n",
      "step: 19650 loss: 13.602794 time elapsed: 25.2854 learning rate: 0.000100, scenario: 0, slope: -0.007329428938234008, fluctuations: 0.04\n",
      "step: 19660 loss: 13.529034 time elapsed: 25.2981 learning rate: 0.000100, scenario: 0, slope: -0.00742388658005413, fluctuations: 0.04\n",
      "step: 19670 loss: 13.456764 time elapsed: 25.3114 learning rate: 0.000100, scenario: 0, slope: -0.007520259098676833, fluctuations: 0.04\n",
      "step: 19680 loss: 13.385243 time elapsed: 25.3242 learning rate: 0.000100, scenario: 0, slope: -0.007618679604446314, fluctuations: 0.04\n",
      "step: 19690 loss: 13.322629 time elapsed: 25.3367 learning rate: 0.000100, scenario: 0, slope: -0.007708603386785495, fluctuations: 0.04\n",
      "step: 19700 loss: 13.292491 time elapsed: 25.3491 learning rate: 0.000100, scenario: 0, slope: -0.007640618265422432, fluctuations: 0.05\n",
      "step: 19710 loss: 13.183265 time elapsed: 25.3621 learning rate: 0.000100, scenario: 0, slope: -0.0074849573597450675, fluctuations: 0.06\n",
      "step: 19720 loss: 13.102980 time elapsed: 25.3746 learning rate: 0.000100, scenario: 0, slope: -0.007118257517483612, fluctuations: 0.04\n",
      "step: 19730 loss: 13.032509 time elapsed: 25.3872 learning rate: 0.000100, scenario: 0, slope: -0.007054911610194957, fluctuations: 0.03\n",
      "step: 19740 loss: 12.963083 time elapsed: 25.4000 learning rate: 0.000100, scenario: 0, slope: -0.007051255550586809, fluctuations: 0.02\n",
      "step: 19750 loss: 12.893997 time elapsed: 25.4125 learning rate: 0.000100, scenario: 0, slope: -0.007073756777130037, fluctuations: 0.02\n",
      "step: 19760 loss: 12.825338 time elapsed: 25.4246 learning rate: 0.000100, scenario: 0, slope: -0.00710198434151801, fluctuations: 0.02\n",
      "step: 19770 loss: 12.757011 time elapsed: 25.4373 learning rate: 0.000100, scenario: 0, slope: -0.007136643408365551, fluctuations: 0.02\n",
      "step: 19780 loss: 12.688987 time elapsed: 25.4495 learning rate: 0.000100, scenario: 0, slope: -0.007173569887188019, fluctuations: 0.02\n",
      "step: 19790 loss: 12.621362 time elapsed: 25.4636 learning rate: 0.000100, scenario: 0, slope: -0.007194934719096469, fluctuations: 0.02\n",
      "step: 19800 loss: 12.554075 time elapsed: 25.4769 learning rate: 0.000100, scenario: 0, slope: -0.007009570892301505, fluctuations: 0.01\n",
      "step: 19810 loss: 12.487381 time elapsed: 25.4903 learning rate: 0.000100, scenario: 0, slope: -0.006880791653037178, fluctuations: 0.0\n",
      "step: 19820 loss: 12.429278 time elapsed: 25.5028 learning rate: 0.000100, scenario: 0, slope: -0.006810202697921478, fluctuations: 0.0\n",
      "step: 19830 loss: 12.461372 time elapsed: 25.5156 learning rate: 0.000100, scenario: 0, slope: -0.006471435262428127, fluctuations: 0.01\n",
      "step: 19840 loss: 12.311091 time elapsed: 25.5281 learning rate: 0.000100, scenario: 0, slope: -0.00634251518400398, fluctuations: 0.02\n",
      "step: 19850 loss: 12.224133 time elapsed: 25.5404 learning rate: 0.000100, scenario: 0, slope: -0.006362621113775427, fluctuations: 0.03\n",
      "step: 19860 loss: 12.162771 time elapsed: 25.5529 learning rate: 0.000100, scenario: 0, slope: -0.006418278113551738, fluctuations: 0.03\n",
      "step: 19870 loss: 12.096751 time elapsed: 25.5657 learning rate: 0.000100, scenario: 0, slope: -0.006485070739342831, fluctuations: 0.03\n",
      "step: 19880 loss: 12.031706 time elapsed: 25.5784 learning rate: 0.000100, scenario: 0, slope: -0.006555940022038083, fluctuations: 0.03\n",
      "step: 19890 loss: 11.972870 time elapsed: 25.5911 learning rate: 0.000100, scenario: 0, slope: -0.0066198955933320595, fluctuations: 0.03\n",
      "step: 19900 loss: 12.039292 time elapsed: 25.6035 learning rate: 0.000100, scenario: 0, slope: -0.006413759037379137, fluctuations: 0.03\n",
      "step: 19910 loss: 11.864410 time elapsed: 25.6171 learning rate: 0.000100, scenario: 0, slope: -0.0063480825595170305, fluctuations: 0.05\n",
      "step: 19920 loss: 11.790254 time elapsed: 25.6294 learning rate: 0.000100, scenario: 0, slope: -0.006459729706457471, fluctuations: 0.05\n",
      "step: 19930 loss: 11.720194 time elapsed: 25.6420 learning rate: 0.000100, scenario: 0, slope: -0.006267945847267466, fluctuations: 0.04\n",
      "step: 19940 loss: 11.660401 time elapsed: 25.6543 learning rate: 0.000100, scenario: 0, slope: -0.006199977766630849, fluctuations: 0.03\n",
      "step: 19950 loss: 11.598521 time elapsed: 25.6683 learning rate: 0.000100, scenario: 0, slope: -0.006247577617489207, fluctuations: 0.02\n",
      "step: 19960 loss: 11.537987 time elapsed: 25.6818 learning rate: 0.000100, scenario: 0, slope: -0.0063097268133146985, fluctuations: 0.02\n",
      "step: 19970 loss: 11.477897 time elapsed: 25.6950 learning rate: 0.000100, scenario: 0, slope: -0.006385830773731407, fluctuations: 0.02\n",
      "step: 19980 loss: 11.418098 time elapsed: 25.7082 learning rate: 0.000100, scenario: 0, slope: -0.006473537895225949, fluctuations: 0.02\n",
      "step: 19990 loss: 11.358639 time elapsed: 25.7214 learning rate: 0.000100, scenario: 0, slope: -0.0065620288736965805, fluctuations: 0.02\n",
      "step: 20000 loss: 11.299468 time elapsed: 25.7338 learning rate: 0.000100, scenario: 0, slope: -0.006401614238834458, fluctuations: 0.01\n",
      "step: 20010 loss: 11.240593 time elapsed: 25.7469 learning rate: 0.000100, scenario: 0, slope: -0.006104354012242354, fluctuations: 0.0\n",
      "step: 20020 loss: 11.182012 time elapsed: 25.7592 learning rate: 0.000100, scenario: 0, slope: -0.006022496157957727, fluctuations: 0.0\n",
      "step: 20030 loss: 11.123843 time elapsed: 25.7716 learning rate: 0.000100, scenario: 0, slope: -0.005971500072235598, fluctuations: 0.0\n",
      "step: 20040 loss: 11.076139 time elapsed: 25.7841 learning rate: 0.000100, scenario: 0, slope: -0.005918249615945785, fluctuations: 0.0\n",
      "step: 20050 loss: 11.126823 time elapsed: 25.7967 learning rate: 0.000100, scenario: 0, slope: -0.005404503927927797, fluctuations: 0.01\n",
      "step: 20060 loss: 10.967589 time elapsed: 25.8094 learning rate: 0.000100, scenario: 0, slope: -0.0053155752094110895, fluctuations: 0.02\n",
      "step: 20070 loss: 10.901023 time elapsed: 25.8220 learning rate: 0.000100, scenario: 0, slope: -0.005321472903295535, fluctuations: 0.04\n",
      "step: 20080 loss: 10.847132 time elapsed: 25.8345 learning rate: 0.000100, scenario: 0, slope: -0.005415787471761719, fluctuations: 0.05\n",
      "step: 20090 loss: 10.786606 time elapsed: 25.8474 learning rate: 0.000100, scenario: 0, slope: -0.005536209887284797, fluctuations: 0.05\n",
      "step: 20100 loss: 10.731933 time elapsed: 25.8600 learning rate: 0.000100, scenario: 0, slope: -0.005640915686015385, fluctuations: 0.05\n",
      "step: 20110 loss: 10.677024 time elapsed: 25.8759 learning rate: 0.000100, scenario: 0, slope: -0.005769427494993134, fluctuations: 0.05\n",
      "step: 20120 loss: 10.622776 time elapsed: 25.8890 learning rate: 0.000100, scenario: 0, slope: -0.005890899835321592, fluctuations: 0.05\n",
      "step: 20130 loss: 10.568763 time elapsed: 25.9018 learning rate: 0.000100, scenario: 0, slope: -0.0060240265182367435, fluctuations: 0.05\n",
      "step: 20140 loss: 10.515401 time elapsed: 25.9147 learning rate: 0.000100, scenario: 0, slope: -0.006157931910771592, fluctuations: 0.05\n",
      "step: 20150 loss: 10.478076 time elapsed: 25.9271 learning rate: 0.000100, scenario: 0, slope: -0.005669785580237739, fluctuations: 0.04\n",
      "step: 20160 loss: 10.426598 time elapsed: 25.9397 learning rate: 0.000100, scenario: 0, slope: -0.005227221896449579, fluctuations: 0.03\n",
      "step: 20170 loss: 10.364360 time elapsed: 25.9524 learning rate: 0.000100, scenario: 0, slope: -0.005109195209937782, fluctuations: 0.03\n",
      "step: 20180 loss: 10.310512 time elapsed: 25.9649 learning rate: 0.000100, scenario: 0, slope: -0.0051089438258141275, fluctuations: 0.03\n",
      "step: 20190 loss: 10.252403 time elapsed: 25.9776 learning rate: 0.000100, scenario: 0, slope: -0.005159950624049324, fluctuations: 0.03\n",
      "step: 20200 loss: 10.201639 time elapsed: 25.9898 learning rate: 0.000100, scenario: 0, slope: -0.005211895367514138, fluctuations: 0.03\n",
      "step: 20210 loss: 10.149330 time elapsed: 26.0027 learning rate: 0.000100, scenario: 0, slope: -0.005281088412848122, fluctuations: 0.03\n",
      "step: 20220 loss: 10.098044 time elapsed: 26.0154 learning rate: 0.000100, scenario: 0, slope: -0.005346645509501574, fluctuations: 0.03\n",
      "step: 20230 loss: 10.047179 time elapsed: 26.0282 learning rate: 0.000100, scenario: 0, slope: -0.005415442010433048, fluctuations: 0.03\n",
      "step: 20240 loss: 9.996540 time elapsed: 26.0409 learning rate: 0.000100, scenario: 0, slope: -0.005488900816165116, fluctuations: 0.03\n",
      "step: 20250 loss: 9.946143 time elapsed: 26.0536 learning rate: 0.000100, scenario: 0, slope: -0.005538399468512513, fluctuations: 0.03\n",
      "step: 20260 loss: 9.895994 time elapsed: 26.0661 learning rate: 0.000100, scenario: 0, slope: -0.0052339682432362145, fluctuations: 0.02\n",
      "step: 20270 loss: 9.846092 time elapsed: 26.0806 learning rate: 0.000100, scenario: 0, slope: -0.005159190038983674, fluctuations: 0.0\n",
      "step: 20280 loss: 9.796596 time elapsed: 26.0940 learning rate: 0.000100, scenario: 0, slope: -0.005090783546735003, fluctuations: 0.0\n",
      "step: 20290 loss: 9.769728 time elapsed: 26.1079 learning rate: 0.000100, scenario: 0, slope: -0.0050247623403046, fluctuations: 0.0\n",
      "step: 20300 loss: 9.711263 time elapsed: 26.1211 learning rate: 0.000100, scenario: 0, slope: -0.004454564804477815, fluctuations: 0.01\n",
      "step: 20310 loss: 9.690379 time elapsed: 26.1342 learning rate: 0.000100, scenario: 0, slope: -0.00434574801188775, fluctuations: 0.03\n",
      "step: 20320 loss: 9.617601 time elapsed: 26.1466 learning rate: 0.000100, scenario: 0, slope: -0.0044140731685764194, fluctuations: 0.04\n",
      "step: 20330 loss: 9.559116 time elapsed: 26.1591 learning rate: 0.000100, scenario: 0, slope: -0.004522801642369, fluctuations: 0.06\n",
      "step: 20340 loss: 9.510295 time elapsed: 26.1719 learning rate: 0.000100, scenario: 0, slope: -0.004673758976812069, fluctuations: 0.06\n",
      "step: 20350 loss: 9.464189 time elapsed: 26.1842 learning rate: 0.000100, scenario: 0, slope: -0.004819585562105929, fluctuations: 0.06\n",
      "step: 20360 loss: 9.417638 time elapsed: 26.1972 learning rate: 0.000100, scenario: 0, slope: -0.004964334132103251, fluctuations: 0.06\n",
      "step: 20370 loss: 9.371516 time elapsed: 26.2102 learning rate: 0.000100, scenario: 0, slope: -0.005114711428993248, fluctuations: 0.06\n",
      "step: 20380 loss: 9.325834 time elapsed: 26.2230 learning rate: 0.000100, scenario: 0, slope: -0.005280260003107759, fluctuations: 0.06\n",
      "step: 20390 loss: 9.281130 time elapsed: 26.2357 learning rate: 0.000100, scenario: 0, slope: -0.005432202574973221, fluctuations: 0.06\n",
      "step: 20400 loss: 9.266858 time elapsed: 26.2483 learning rate: 0.000100, scenario: 0, slope: -0.004841391388074223, fluctuations: 0.05\n",
      "step: 20410 loss: 9.190268 time elapsed: 26.2617 learning rate: 0.000100, scenario: 0, slope: -0.004405030183135204, fluctuations: 0.04\n",
      "step: 20420 loss: 9.155769 time elapsed: 26.2755 learning rate: 0.000100, scenario: 0, slope: -0.004336721722830729, fluctuations: 0.03\n",
      "step: 20430 loss: 9.106444 time elapsed: 26.2906 learning rate: 0.000100, scenario: 0, slope: -0.004341206888454945, fluctuations: 0.03\n",
      "step: 20440 loss: 9.057619 time elapsed: 26.3034 learning rate: 0.000100, scenario: 0, slope: -0.004385469325349777, fluctuations: 0.03\n",
      "step: 20450 loss: 9.013593 time elapsed: 26.3161 learning rate: 0.000100, scenario: 0, slope: -0.004440135006897237, fluctuations: 0.03\n",
      "step: 20460 loss: 8.969968 time elapsed: 26.3288 learning rate: 0.000100, scenario: 0, slope: -0.004498102634466238, fluctuations: 0.03\n",
      "step: 20470 loss: 8.926471 time elapsed: 26.3415 learning rate: 0.000100, scenario: 0, slope: -0.004557977718977656, fluctuations: 0.03\n",
      "step: 20480 loss: 8.883211 time elapsed: 26.3552 learning rate: 0.000100, scenario: 0, slope: -0.004621012133553845, fluctuations: 0.03\n",
      "step: 20490 loss: 8.840173 time elapsed: 26.3692 learning rate: 0.000100, scenario: 0, slope: -0.004687188437014203, fluctuations: 0.03\n",
      "step: 20500 loss: 8.797349 time elapsed: 26.3835 learning rate: 0.000100, scenario: 0, slope: -0.004706407035647381, fluctuations: 0.03\n",
      "step: 20510 loss: 8.754735 time elapsed: 26.3976 learning rate: 0.000100, scenario: 0, slope: -0.004443194466581859, fluctuations: 0.02\n",
      "step: 20520 loss: 8.712317 time elapsed: 26.4116 learning rate: 0.000100, scenario: 0, slope: -0.00437275797621097, fluctuations: 0.0\n",
      "step: 20530 loss: 8.670156 time elapsed: 26.4257 learning rate: 0.000100, scenario: 0, slope: -0.004322608174650996, fluctuations: 0.0\n",
      "step: 20540 loss: 8.630815 time elapsed: 26.4391 learning rate: 0.000100, scenario: 0, slope: -0.0042907064972385845, fluctuations: 0.0\n",
      "step: 20550 loss: 8.732147 time elapsed: 26.4527 learning rate: 0.000100, scenario: 0, slope: -0.0039945039111141814, fluctuations: 0.0\n",
      "step: 20560 loss: 8.615216 time elapsed: 26.4672 learning rate: 0.000100, scenario: 0, slope: -0.0036749380821328244, fluctuations: 0.02\n",
      "step: 20570 loss: 8.507104 time elapsed: 26.4828 learning rate: 0.000100, scenario: 0, slope: -0.003694197112069715, fluctuations: 0.03\n",
      "step: 20580 loss: 8.472328 time elapsed: 26.4974 learning rate: 0.000100, scenario: 0, slope: -0.00376271724945493, fluctuations: 0.05\n",
      "step: 20590 loss: 8.426436 time elapsed: 26.5115 learning rate: 0.000100, scenario: 0, slope: -0.0039017578760950624, fluctuations: 0.05\n",
      "step: 20600 loss: 8.386496 time elapsed: 26.5254 learning rate: 0.000100, scenario: 0, slope: -0.004020901606801903, fluctuations: 0.05\n",
      "step: 20610 loss: 8.346283 time elapsed: 26.5400 learning rate: 0.000100, scenario: 0, slope: -0.00416643238992324, fluctuations: 0.05\n",
      "step: 20620 loss: 8.307140 time elapsed: 26.5544 learning rate: 0.000100, scenario: 0, slope: -0.004300185773928361, fluctuations: 0.05\n",
      "step: 20630 loss: 8.268221 time elapsed: 26.5685 learning rate: 0.000100, scenario: 0, slope: -0.0044416582386937985, fluctuations: 0.05\n",
      "step: 20640 loss: 8.237371 time elapsed: 26.5812 learning rate: 0.000100, scenario: 0, slope: -0.004578625444413569, fluctuations: 0.05\n",
      "step: 20650 loss: 8.259797 time elapsed: 26.5955 learning rate: 0.000100, scenario: 0, slope: -0.00418492613427809, fluctuations: 0.06\n",
      "step: 20660 loss: 8.167780 time elapsed: 26.6091 learning rate: 0.000100, scenario: 0, slope: -0.003760217161188421, fluctuations: 0.05\n",
      "step: 20670 loss: 8.115601 time elapsed: 26.6225 learning rate: 0.000100, scenario: 0, slope: -0.0037068594912182517, fluctuations: 0.05\n",
      "step: 20680 loss: 8.077282 time elapsed: 26.6361 learning rate: 0.000100, scenario: 0, slope: -0.003733552949230756, fluctuations: 0.03\n",
      "step: 20690 loss: 8.039159 time elapsed: 26.6495 learning rate: 0.000100, scenario: 0, slope: -0.003784037711922898, fluctuations: 0.03\n",
      "step: 20700 loss: 8.001217 time elapsed: 26.6628 learning rate: 0.000100, scenario: 0, slope: -0.003834312196691689, fluctuations: 0.03\n",
      "step: 20710 loss: 7.963631 time elapsed: 26.6784 learning rate: 0.000100, scenario: 0, slope: -0.0039023953520857008, fluctuations: 0.03\n",
      "step: 20720 loss: 7.926286 time elapsed: 26.6942 learning rate: 0.000100, scenario: 0, slope: -0.003967087666997532, fluctuations: 0.03\n",
      "step: 20730 loss: 7.889246 time elapsed: 26.7084 learning rate: 0.000100, scenario: 0, slope: -0.004035335331929361, fluctuations: 0.03\n",
      "step: 20740 loss: 7.852382 time elapsed: 26.7222 learning rate: 0.000100, scenario: 0, slope: -0.004091829652530756, fluctuations: 0.03\n",
      "step: 20750 loss: 7.815716 time elapsed: 26.7361 learning rate: 0.000100, scenario: 0, slope: -0.0038739440020335333, fluctuations: 0.02\n",
      "step: 20760 loss: 7.780284 time elapsed: 26.7499 learning rate: 0.000100, scenario: 0, slope: -0.0037665975448581174, fluctuations: 0.01\n",
      "step: 20770 loss: 7.792488 time elapsed: 26.7628 learning rate: 0.000100, scenario: 0, slope: -0.0036389354890365763, fluctuations: 0.0\n",
      "step: 20780 loss: 7.711846 time elapsed: 26.7761 learning rate: 0.000100, scenario: 0, slope: -0.003299669535721463, fluctuations: 0.01\n",
      "step: 20790 loss: 7.692186 time elapsed: 26.7901 learning rate: 0.000100, scenario: 0, slope: -0.0032624464793883172, fluctuations: 0.02\n",
      "step: 20800 loss: 7.640214 time elapsed: 26.8039 learning rate: 0.000100, scenario: 0, slope: -0.0032968802095759597, fluctuations: 0.04\n",
      "step: 20810 loss: 7.602345 time elapsed: 26.8184 learning rate: 0.000100, scenario: 0, slope: -0.0034050784037868336, fluctuations: 0.04\n",
      "step: 20820 loss: 7.568199 time elapsed: 26.8315 learning rate: 0.000100, scenario: 0, slope: -0.003502478190546386, fluctuations: 0.04\n",
      "step: 20830 loss: 7.540163 time elapsed: 26.8458 learning rate: 0.000100, scenario: 0, slope: -0.0035874142084499554, fluctuations: 0.04\n",
      "step: 20840 loss: 7.584833 time elapsed: 26.8597 learning rate: 0.000100, scenario: 0, slope: -0.0034369562494444596, fluctuations: 0.05\n",
      "step: 20850 loss: 7.478791 time elapsed: 26.8735 learning rate: 0.000100, scenario: 0, slope: -0.0034777327566476585, fluctuations: 0.06\n",
      "step: 20860 loss: 7.431921 time elapsed: 26.8878 learning rate: 0.000100, scenario: 0, slope: -0.0036418482584323225, fluctuations: 0.07\n",
      "step: 20870 loss: 7.400305 time elapsed: 26.9039 learning rate: 0.000100, scenario: 0, slope: -0.003733153795051978, fluctuations: 0.07\n",
      "step: 20880 loss: 7.364538 time elapsed: 26.9183 learning rate: 0.000100, scenario: 0, slope: -0.003465032003027749, fluctuations: 0.06\n",
      "step: 20890 loss: 7.330621 time elapsed: 26.9317 learning rate: 0.000100, scenario: 0, slope: -0.003451700672850974, fluctuations: 0.04\n",
      "step: 20900 loss: 7.297533 time elapsed: 26.9452 learning rate: 0.000100, scenario: 0, slope: -0.003479240666585199, fluctuations: 0.03\n",
      "step: 20910 loss: 7.264594 time elapsed: 26.9598 learning rate: 0.000100, scenario: 0, slope: -0.003551492966515787, fluctuations: 0.03\n",
      "step: 20920 loss: 7.231808 time elapsed: 26.9742 learning rate: 0.000100, scenario: 0, slope: -0.0036255188407077303, fluctuations: 0.03\n",
      "step: 20930 loss: 7.199159 time elapsed: 26.9881 learning rate: 0.000100, scenario: 0, slope: -0.0036949395037185998, fluctuations: 0.03\n",
      "step: 20940 loss: 7.166643 time elapsed: 27.0022 learning rate: 0.000100, scenario: 0, slope: -0.0034700878606408886, fluctuations: 0.02\n",
      "step: 20950 loss: 7.134285 time elapsed: 27.0157 learning rate: 0.000100, scenario: 0, slope: -0.003345542890443618, fluctuations: 0.01\n",
      "step: 20960 loss: 7.103502 time elapsed: 27.0291 learning rate: 0.000100, scenario: 0, slope: -0.0033074724395249066, fluctuations: 0.0\n",
      "step: 20970 loss: 7.189611 time elapsed: 27.0432 learning rate: 0.000100, scenario: 0, slope: -0.003076154757122516, fluctuations: 0.0\n",
      "step: 20980 loss: 7.117165 time elapsed: 27.0561 learning rate: 0.000100, scenario: 0, slope: -0.0026589086934129843, fluctuations: 0.01\n",
      "step: 20990 loss: 7.014199 time elapsed: 27.0690 learning rate: 0.000100, scenario: 0, slope: -0.0026804976488411696, fluctuations: 0.03\n",
      "step: 21000 loss: 6.983779 time elapsed: 27.0818 learning rate: 0.000100, scenario: 0, slope: -0.0027416575430917883, fluctuations: 0.05\n",
      "step: 21010 loss: 6.948765 time elapsed: 27.0966 learning rate: 0.000100, scenario: 0, slope: -0.002905207192159035, fluctuations: 0.05\n",
      "step: 21020 loss: 6.916421 time elapsed: 27.1123 learning rate: 0.000100, scenario: 0, slope: -0.0030518472094956013, fluctuations: 0.05\n",
      "step: 21030 loss: 6.886440 time elapsed: 27.1269 learning rate: 0.000100, scenario: 0, slope: -0.0031966353913851116, fluctuations: 0.05\n",
      "step: 21040 loss: 6.856020 time elapsed: 27.1408 learning rate: 0.000100, scenario: 0, slope: -0.0033431403038931946, fluctuations: 0.05\n",
      "step: 21050 loss: 6.826209 time elapsed: 27.1549 learning rate: 0.000100, scenario: 0, slope: -0.003497047168971974, fluctuations: 0.05\n",
      "step: 21060 loss: 6.803907 time elapsed: 27.1688 learning rate: 0.000100, scenario: 0, slope: -0.003650113397586471, fluctuations: 0.05\n",
      "step: 21070 loss: 6.846724 time elapsed: 27.1823 learning rate: 0.000100, scenario: 0, slope: -0.00333824985788006, fluctuations: 0.06\n",
      "step: 21080 loss: 6.747359 time elapsed: 27.1961 learning rate: 0.000100, scenario: 0, slope: -0.0028485666712761743, fluctuations: 0.05\n",
      "step: 21090 loss: 6.709607 time elapsed: 27.2101 learning rate: 0.000100, scenario: 0, slope: -0.0027803287323674296, fluctuations: 0.05\n",
      "step: 21100 loss: 6.682338 time elapsed: 27.2244 learning rate: 0.000100, scenario: 0, slope: -0.0028024625032807074, fluctuations: 0.04\n",
      "step: 21110 loss: 6.650786 time elapsed: 27.2390 learning rate: 0.000100, scenario: 0, slope: -0.0028677675554019486, fluctuations: 0.04\n",
      "step: 21120 loss: 6.621339 time elapsed: 27.2540 learning rate: 0.000100, scenario: 0, slope: -0.0029383563925628226, fluctuations: 0.04\n",
      "step: 21130 loss: 6.592654 time elapsed: 27.2690 learning rate: 0.000100, scenario: 0, slope: -0.0030127888214532936, fluctuations: 0.04\n",
      "step: 21140 loss: 6.564096 time elapsed: 27.2823 learning rate: 0.000100, scenario: 0, slope: -0.0030905574676963255, fluctuations: 0.04\n",
      "step: 21150 loss: 6.535645 time elapsed: 27.2959 learning rate: 0.000100, scenario: 0, slope: -0.0031731970123715925, fluctuations: 0.04\n",
      "step: 21160 loss: 6.507307 time elapsed: 27.3108 learning rate: 0.000100, scenario: 0, slope: -0.0032479767908576043, fluctuations: 0.04\n",
      "step: 21170 loss: 6.479068 time elapsed: 27.3269 learning rate: 0.000100, scenario: 0, slope: -0.003015687313888916, fluctuations: 0.03\n",
      "step: 21180 loss: 6.450932 time elapsed: 27.3411 learning rate: 0.000100, scenario: 0, slope: -0.0029060119815132487, fluctuations: 0.02\n",
      "step: 21190 loss: 6.422904 time elapsed: 27.3549 learning rate: 0.000100, scenario: 0, slope: -0.0028746559411069197, fluctuations: 0.0\n",
      "step: 21200 loss: 6.394993 time elapsed: 27.3686 learning rate: 0.000100, scenario: 0, slope: -0.0028501400054810003, fluctuations: 0.0\n",
      "step: 21210 loss: 6.368668 time elapsed: 27.3823 learning rate: 0.000100, scenario: 0, slope: -0.002829018491376337, fluctuations: 0.0\n",
      "step: 21220 loss: 6.574131 time elapsed: 27.3957 learning rate: 0.000100, scenario: 0, slope: -0.0024411962384569223, fluctuations: 0.0\n",
      "step: 21230 loss: 6.344092 time elapsed: 27.4099 learning rate: 0.000100, scenario: 0, slope: -0.001998384939683554, fluctuations: 0.02\n",
      "step: 21240 loss: 6.303178 time elapsed: 27.4246 learning rate: 0.000100, scenario: 0, slope: -0.0020100917689071713, fluctuations: 0.04\n",
      "step: 21250 loss: 6.266210 time elapsed: 27.4390 learning rate: 0.000100, scenario: 0, slope: -0.0021406678813687867, fluctuations: 0.06\n",
      "step: 21260 loss: 6.236791 time elapsed: 27.4526 learning rate: 0.000100, scenario: 0, slope: -0.0023605228828934744, fluctuations: 0.06\n",
      "step: 21270 loss: 6.209641 time elapsed: 27.4656 learning rate: 0.000100, scenario: 0, slope: -0.0025737826134767804, fluctuations: 0.06\n",
      "step: 21280 loss: 6.183386 time elapsed: 27.4799 learning rate: 0.000100, scenario: 0, slope: -0.002781303522903146, fluctuations: 0.06\n",
      "step: 21290 loss: 6.157538 time elapsed: 27.4938 learning rate: 0.000100, scenario: 0, slope: -0.0029908587083583155, fluctuations: 0.06\n",
      "step: 21300 loss: 6.131992 time elapsed: 27.5070 learning rate: 0.000100, scenario: 0, slope: -0.0031899641150673324, fluctuations: 0.06\n",
      "step: 21310 loss: 6.106615 time elapsed: 27.5211 learning rate: 0.000100, scenario: 0, slope: -0.0034588108665801185, fluctuations: 0.06\n",
      "step: 21320 loss: 6.082866 time elapsed: 27.5364 learning rate: 0.000100, scenario: 0, slope: -0.0034341528476356854, fluctuations: 0.05\n",
      "step: 21330 loss: 6.138154 time elapsed: 27.5502 learning rate: 0.000100, scenario: 0, slope: -0.0025827988071079554, fluctuations: 0.04\n",
      "step: 21340 loss: 6.074454 time elapsed: 27.5634 learning rate: 0.000100, scenario: 0, slope: -0.0022550715442058136, fluctuations: 0.03\n",
      "step: 21350 loss: 6.009414 time elapsed: 27.5768 learning rate: 0.000100, scenario: 0, slope: -0.002234415769227487, fluctuations: 0.03\n",
      "step: 21360 loss: 5.986135 time elapsed: 27.5906 learning rate: 0.000100, scenario: 0, slope: -0.0022825283847249405, fluctuations: 0.04\n",
      "step: 21370 loss: 5.958155 time elapsed: 27.6045 learning rate: 0.000100, scenario: 0, slope: -0.0023622686529699774, fluctuations: 0.04\n",
      "step: 21380 loss: 5.932976 time elapsed: 27.6187 learning rate: 0.000100, scenario: 0, slope: -0.0024513842529847066, fluctuations: 0.04\n",
      "step: 21390 loss: 5.908746 time elapsed: 27.6319 learning rate: 0.000100, scenario: 0, slope: -0.002540822588566628, fluctuations: 0.04\n",
      "step: 21400 loss: 5.884271 time elapsed: 27.6460 learning rate: 0.000100, scenario: 0, slope: -0.002623235025323358, fluctuations: 0.04\n",
      "step: 21410 loss: 5.860018 time elapsed: 27.6606 learning rate: 0.000100, scenario: 0, slope: -0.002728845622094364, fluctuations: 0.04\n",
      "step: 21420 loss: 5.835899 time elapsed: 27.6742 learning rate: 0.000100, scenario: 0, slope: -0.0028297971357822806, fluctuations: 0.04\n",
      "step: 21430 loss: 5.811865 time elapsed: 27.6878 learning rate: 0.000100, scenario: 0, slope: -0.002758896377235623, fluctuations: 0.04\n",
      "step: 21440 loss: 5.787909 time elapsed: 27.7006 learning rate: 0.000100, scenario: 0, slope: -0.002528169777273935, fluctuations: 0.02\n",
      "step: 21450 loss: 5.764029 time elapsed: 27.7132 learning rate: 0.000100, scenario: 0, slope: -0.002451524972025169, fluctuations: 0.01\n",
      "step: 21460 loss: 5.740227 time elapsed: 27.7257 learning rate: 0.000100, scenario: 0, slope: -0.002429224549606591, fluctuations: 0.0\n",
      "step: 21470 loss: 5.716502 time elapsed: 27.7388 learning rate: 0.000100, scenario: 0, slope: -0.0024111395018972847, fluctuations: 0.0\n",
      "step: 21480 loss: 5.692854 time elapsed: 27.7541 learning rate: 0.000100, scenario: 0, slope: -0.0024011760231652233, fluctuations: 0.0\n",
      "step: 21490 loss: 5.669287 time elapsed: 27.7682 learning rate: 0.000100, scenario: 0, slope: -0.002392048228420765, fluctuations: 0.0\n",
      "step: 21500 loss: 5.645983 time elapsed: 27.7810 learning rate: 0.000100, scenario: 0, slope: -0.0023844594131793165, fluctuations: 0.0\n",
      "step: 21510 loss: 5.637872 time elapsed: 27.7955 learning rate: 0.000100, scenario: 0, slope: -0.002351604816127987, fluctuations: 0.0\n",
      "step: 21520 loss: 5.631510 time elapsed: 27.8091 learning rate: 0.000100, scenario: 0, slope: -0.0019404705015806743, fluctuations: 0.01\n",
      "step: 21530 loss: 5.605263 time elapsed: 27.8227 learning rate: 0.000100, scenario: 0, slope: -0.0018994348365300197, fluctuations: 0.02\n",
      "step: 21540 loss: 5.576987 time elapsed: 27.8368 learning rate: 0.000100, scenario: 0, slope: -0.0018917854539939707, fluctuations: 0.04\n",
      "step: 21550 loss: 5.547192 time elapsed: 27.8506 learning rate: 0.000100, scenario: 0, slope: -0.0018510737299231098, fluctuations: 0.05\n",
      "step: 21560 loss: 5.519627 time elapsed: 27.8636 learning rate: 0.000100, scenario: 0, slope: -0.001982901587130384, fluctuations: 0.06\n",
      "step: 21570 loss: 5.488668 time elapsed: 27.8778 learning rate: 0.000100, scenario: 0, slope: -0.0021504677183499517, fluctuations: 0.06\n",
      "step: 21580 loss: 5.466005 time elapsed: 27.8910 learning rate: 0.000100, scenario: 0, slope: -0.0023159669291101695, fluctuations: 0.06\n",
      "step: 21590 loss: 5.443705 time elapsed: 27.9055 learning rate: 0.000100, scenario: 0, slope: -0.002492944532747864, fluctuations: 0.06\n",
      "step: 21600 loss: 5.422460 time elapsed: 27.9212 learning rate: 0.000100, scenario: 0, slope: -0.002656754166803248, fluctuations: 0.06\n",
      "step: 21610 loss: 5.403908 time elapsed: 27.9386 learning rate: 0.000100, scenario: 0, slope: -0.0028416371778403992, fluctuations: 0.06\n",
      "step: 21620 loss: 5.414344 time elapsed: 27.9568 learning rate: 0.000100, scenario: 0, slope: -0.0024073365942895637, fluctuations: 0.05\n",
      "step: 21630 loss: 5.357932 time elapsed: 27.9735 learning rate: 0.000100, scenario: 0, slope: -0.0022356557421973795, fluctuations: 0.04\n",
      "step: 21640 loss: 5.339764 time elapsed: 27.9893 learning rate: 0.000100, scenario: 0, slope: -0.002177586899069784, fluctuations: 0.04\n",
      "step: 21650 loss: 5.317707 time elapsed: 28.0034 learning rate: 0.000100, scenario: 0, slope: -0.0020677619516101956, fluctuations: 0.03\n",
      "step: 21660 loss: 5.293574 time elapsed: 28.0173 learning rate: 0.000100, scenario: 0, slope: -0.0020984143687236743, fluctuations: 0.02\n",
      "step: 21670 loss: 5.272013 time elapsed: 28.0306 learning rate: 0.000100, scenario: 0, slope: -0.002138639095715799, fluctuations: 0.02\n",
      "step: 21680 loss: 5.250794 time elapsed: 28.0443 learning rate: 0.000100, scenario: 0, slope: -0.0021865585830850307, fluctuations: 0.02\n",
      "step: 21690 loss: 5.230321 time elapsed: 28.0579 learning rate: 0.000100, scenario: 0, slope: -0.002243559078994053, fluctuations: 0.02\n",
      "step: 21700 loss: 5.213806 time elapsed: 28.0708 learning rate: 0.000100, scenario: 0, slope: -0.0022860364191729277, fluctuations: 0.02\n",
      "step: 21710 loss: 5.252662 time elapsed: 28.0841 learning rate: 0.000100, scenario: 0, slope: -0.0021795639237636878, fluctuations: 0.02\n",
      "step: 21720 loss: 5.175194 time elapsed: 28.0970 learning rate: 0.000100, scenario: 0, slope: -0.00199997844616563, fluctuations: 0.03\n",
      "step: 21730 loss: 5.151173 time elapsed: 28.1097 learning rate: 0.000100, scenario: 0, slope: -0.001865669960939486, fluctuations: 0.03\n",
      "step: 21740 loss: 5.128728 time elapsed: 28.1225 learning rate: 0.000100, scenario: 0, slope: -0.001889422913789234, fluctuations: 0.03\n",
      "step: 21750 loss: 5.107586 time elapsed: 28.1350 learning rate: 0.000100, scenario: 0, slope: -0.0019530159213782725, fluctuations: 0.03\n",
      "step: 21760 loss: 5.087502 time elapsed: 28.1479 learning rate: 0.000100, scenario: 0, slope: -0.0020253349123797596, fluctuations: 0.03\n",
      "step: 21770 loss: 5.068955 time elapsed: 28.1608 learning rate: 0.000100, scenario: 0, slope: -0.0020957333344025857, fluctuations: 0.03\n",
      "step: 21780 loss: 5.075039 time elapsed: 28.1750 learning rate: 0.000100, scenario: 0, slope: -0.002120308387845891, fluctuations: 0.03\n",
      "step: 21790 loss: 5.032895 time elapsed: 28.1888 learning rate: 0.000100, scenario: 0, slope: -0.0020092511108462706, fluctuations: 0.04\n",
      "step: 21800 loss: 5.009111 time elapsed: 28.2019 learning rate: 0.000100, scenario: 0, slope: -0.002079596274602866, fluctuations: 0.05\n",
      "step: 21810 loss: 4.989424 time elapsed: 28.2156 learning rate: 0.000100, scenario: 0, slope: -0.002035205103164874, fluctuations: 0.06\n",
      "step: 21820 loss: 4.970091 time elapsed: 28.2282 learning rate: 0.000100, scenario: 0, slope: -0.001930284679669153, fluctuations: 0.05\n",
      "step: 21830 loss: 4.951190 time elapsed: 28.2408 learning rate: 0.000100, scenario: 0, slope: -0.0019467898318042763, fluctuations: 0.04\n",
      "step: 21840 loss: 4.943614 time elapsed: 28.2535 learning rate: 0.000100, scenario: 0, slope: -0.001968740722625872, fluctuations: 0.03\n",
      "step: 21850 loss: 4.990428 time elapsed: 28.2660 learning rate: 0.000100, scenario: 0, slope: -0.0017487818607041647, fluctuations: 0.04\n",
      "step: 21860 loss: 4.915256 time elapsed: 28.2786 learning rate: 0.000100, scenario: 0, slope: -0.0017661177794466734, fluctuations: 0.05\n",
      "step: 21870 loss: 4.878990 time elapsed: 28.2912 learning rate: 0.000100, scenario: 0, slope: -0.0018837415744787768, fluctuations: 0.06\n",
      "step: 21880 loss: 4.856888 time elapsed: 28.3035 learning rate: 0.000100, scenario: 0, slope: -0.001980887844227233, fluctuations: 0.06\n",
      "step: 21890 loss: 4.838265 time elapsed: 28.3160 learning rate: 0.000100, scenario: 0, slope: -0.0018649584703005258, fluctuations: 0.05\n",
      "step: 21900 loss: 4.819856 time elapsed: 28.3285 learning rate: 0.000100, scenario: 0, slope: -0.001903919273049408, fluctuations: 0.04\n",
      "step: 21910 loss: 4.801616 time elapsed: 28.3418 learning rate: 0.000100, scenario: 0, slope: -0.001982727740626608, fluctuations: 0.03\n",
      "step: 21920 loss: 4.783387 time elapsed: 28.3546 learning rate: 0.000100, scenario: 0, slope: -0.0020664725579895168, fluctuations: 0.03\n",
      "step: 21930 loss: 4.766492 time elapsed: 28.3671 learning rate: 0.000100, scenario: 0, slope: -0.002155313438859702, fluctuations: 0.03\n",
      "step: 21940 loss: 4.794865 time elapsed: 28.3816 learning rate: 0.000100, scenario: 0, slope: -0.0021366516300029455, fluctuations: 0.03\n",
      "step: 21950 loss: 4.738497 time elapsed: 28.3962 learning rate: 0.000100, scenario: 0, slope: -0.0016498604450635751, fluctuations: 0.03\n",
      "step: 21960 loss: 4.725989 time elapsed: 28.4098 learning rate: 0.000100, scenario: 0, slope: -0.0015417992401081956, fluctuations: 0.03\n",
      "step: 21970 loss: 4.698691 time elapsed: 28.4228 learning rate: 0.000100, scenario: 0, slope: -0.0015596198437049572, fluctuations: 0.04\n",
      "step: 21980 loss: 4.676641 time elapsed: 28.4368 learning rate: 0.000100, scenario: 0, slope: -0.001639030645321009, fluctuations: 0.04\n",
      "step: 21990 loss: 4.659173 time elapsed: 28.4499 learning rate: 0.000100, scenario: 0, slope: -0.0017271668083573648, fluctuations: 0.04\n",
      "step: 22000 loss: 4.643440 time elapsed: 28.4625 learning rate: 0.000100, scenario: 0, slope: -0.0018055095311684081, fluctuations: 0.04\n",
      "step: 22010 loss: 4.674341 time elapsed: 28.4767 learning rate: 0.000100, scenario: 0, slope: -0.0018089034604307779, fluctuations: 0.04\n",
      "step: 22020 loss: 4.609918 time elapsed: 28.4904 learning rate: 0.000100, scenario: 0, slope: -0.0016592925458004367, fluctuations: 0.05\n",
      "step: 22030 loss: 4.598236 time elapsed: 28.5040 learning rate: 0.000100, scenario: 0, slope: -0.0017628916435008272, fluctuations: 0.06\n",
      "step: 22040 loss: 4.579424 time elapsed: 28.5169 learning rate: 0.000100, scenario: 0, slope: -0.0018250733847913073, fluctuations: 0.08\n",
      "step: 22050 loss: 4.559006 time elapsed: 28.5299 learning rate: 0.000100, scenario: 0, slope: -0.0016541711993268945, fluctuations: 0.07\n",
      "step: 22060 loss: 4.540769 time elapsed: 28.5429 learning rate: 0.000100, scenario: 0, slope: -0.001688390931811999, fluctuations: 0.05\n",
      "step: 22070 loss: 4.523865 time elapsed: 28.5558 learning rate: 0.000100, scenario: 0, slope: -0.0017531823690168636, fluctuations: 0.04\n",
      "step: 22080 loss: 4.507180 time elapsed: 28.5686 learning rate: 0.000100, scenario: 0, slope: -0.0018435393307739765, fluctuations: 0.04\n",
      "step: 22090 loss: 4.490580 time elapsed: 28.5827 learning rate: 0.000100, scenario: 0, slope: -0.0019424659285013444, fluctuations: 0.04\n",
      "step: 22100 loss: 4.474102 time elapsed: 28.5960 learning rate: 0.000100, scenario: 0, slope: -0.0020359758498120196, fluctuations: 0.04\n",
      "step: 22110 loss: 4.457714 time elapsed: 28.6097 learning rate: 0.000100, scenario: 0, slope: -0.002046484634566711, fluctuations: 0.04\n",
      "step: 22120 loss: 4.441687 time elapsed: 28.6232 learning rate: 0.000100, scenario: 0, slope: -0.0017662577622749914, fluctuations: 0.03\n",
      "step: 22130 loss: 4.443498 time elapsed: 28.6368 learning rate: 0.000100, scenario: 0, slope: -0.0016677814823387726, fluctuations: 0.01\n",
      "step: 22140 loss: 4.450601 time elapsed: 28.6499 learning rate: 0.000100, scenario: 0, slope: -0.0012433171235313658, fluctuations: 0.01\n",
      "step: 22150 loss: 4.412363 time elapsed: 28.6627 learning rate: 0.000100, scenario: 0, slope: -0.0011558847357305115, fluctuations: 0.02\n",
      "step: 22160 loss: 4.384880 time elapsed: 28.6754 learning rate: 0.000100, scenario: 0, slope: -0.0011991129813925612, fluctuations: 0.04\n",
      "step: 22170 loss: 4.364710 time elapsed: 28.6880 learning rate: 0.000100, scenario: 0, slope: -0.0013115039506634354, fluctuations: 0.05\n",
      "step: 22180 loss: 4.347679 time elapsed: 28.7007 learning rate: 0.000100, scenario: 0, slope: -0.0014492287145255278, fluctuations: 0.05\n",
      "step: 22190 loss: 4.332525 time elapsed: 28.7135 learning rate: 0.000100, scenario: 0, slope: -0.0015806888835631409, fluctuations: 0.05\n",
      "step: 22200 loss: 4.323564 time elapsed: 28.7261 learning rate: 0.000100, scenario: 0, slope: -0.0016864765410069952, fluctuations: 0.05\n",
      "step: 22210 loss: 4.347780 time elapsed: 28.7392 learning rate: 0.000100, scenario: 0, slope: -0.0016858576240754701, fluctuations: 0.05\n",
      "step: 22220 loss: 4.288897 time elapsed: 28.7517 learning rate: 0.000100, scenario: 0, slope: -0.0017937643324904082, fluctuations: 0.06\n",
      "step: 22230 loss: 4.269955 time elapsed: 28.7644 learning rate: 0.000100, scenario: 0, slope: -0.0019425837067753705, fluctuations: 0.07\n",
      "step: 22240 loss: 4.256584 time elapsed: 28.7769 learning rate: 0.000100, scenario: 0, slope: -0.0016249767874989183, fluctuations: 0.06\n",
      "step: 22250 loss: 4.240877 time elapsed: 28.7895 learning rate: 0.000100, scenario: 0, slope: -0.001551223637629863, fluctuations: 0.04\n",
      "step: 22260 loss: 4.225052 time elapsed: 28.8037 learning rate: 0.000100, scenario: 0, slope: -0.0015491679513000294, fluctuations: 0.03\n",
      "step: 22270 loss: 4.209978 time elapsed: 28.8168 learning rate: 0.000100, scenario: 0, slope: -0.0015895056592108465, fluctuations: 0.02\n",
      "step: 22280 loss: 4.195473 time elapsed: 28.8295 learning rate: 0.000100, scenario: 0, slope: -0.0016427513077414664, fluctuations: 0.02\n",
      "step: 22290 loss: 4.186324 time elapsed: 28.8421 learning rate: 0.000100, scenario: 0, slope: -0.001686109169286474, fluctuations: 0.02\n",
      "step: 22300 loss: 4.256963 time elapsed: 28.8547 learning rate: 0.000100, scenario: 0, slope: -0.0015409801767423679, fluctuations: 0.02\n",
      "step: 22310 loss: 4.179265 time elapsed: 28.8678 learning rate: 0.000100, scenario: 0, slope: -0.0012881754471752828, fluctuations: 0.02\n",
      "step: 22320 loss: 4.149165 time elapsed: 28.8804 learning rate: 0.000100, scenario: 0, slope: -0.0012060705394740772, fluctuations: 0.04\n",
      "step: 22330 loss: 4.126538 time elapsed: 28.8932 learning rate: 0.000100, scenario: 0, slope: -0.0012490186227173098, fluctuations: 0.04\n",
      "step: 22340 loss: 4.109480 time elapsed: 28.9056 learning rate: 0.000100, scenario: 0, slope: -0.0013347011511310415, fluctuations: 0.04\n",
      "step: 22350 loss: 4.094514 time elapsed: 28.9180 learning rate: 0.000100, scenario: 0, slope: -0.0014305684220256496, fluctuations: 0.04\n",
      "step: 22360 loss: 4.080088 time elapsed: 28.9305 learning rate: 0.000100, scenario: 0, slope: -0.0015234182882692033, fluctuations: 0.04\n",
      "step: 22370 loss: 4.066345 time elapsed: 28.9434 learning rate: 0.000100, scenario: 0, slope: -0.0016200620828500801, fluctuations: 0.04\n",
      "step: 22380 loss: 4.080604 time elapsed: 28.9562 learning rate: 0.000100, scenario: 0, slope: -0.001670700213453762, fluctuations: 0.04\n",
      "step: 22390 loss: 4.042002 time elapsed: 28.9691 learning rate: 0.000100, scenario: 0, slope: -0.0014647374573987365, fluctuations: 0.05\n",
      "step: 22400 loss: 4.036738 time elapsed: 28.9820 learning rate: 0.000100, scenario: 0, slope: -0.001329979985175242, fluctuations: 0.06\n",
      "step: 22410 loss: 4.016672 time elapsed: 28.9954 learning rate: 0.000100, scenario: 0, slope: -0.0011872053852694292, fluctuations: 0.06\n",
      "step: 22420 loss: 3.996899 time elapsed: 29.0111 learning rate: 0.000100, scenario: 0, slope: -0.0012203040571405856, fluctuations: 0.06\n",
      "step: 22430 loss: 3.984067 time elapsed: 29.0251 learning rate: 0.000100, scenario: 0, slope: -0.0013073594568072597, fluctuations: 0.05\n",
      "step: 22440 loss: 3.970235 time elapsed: 29.0392 learning rate: 0.000100, scenario: 0, slope: -0.0014037510709794439, fluctuations: 0.05\n",
      "step: 22450 loss: 3.958548 time elapsed: 29.0537 learning rate: 0.000100, scenario: 0, slope: -0.0014998077261683613, fluctuations: 0.05\n",
      "step: 22460 loss: 4.002985 time elapsed: 29.0672 learning rate: 0.000100, scenario: 0, slope: -0.001485181059775374, fluctuations: 0.05\n",
      "step: 22470 loss: 3.936995 time elapsed: 29.0804 learning rate: 0.000100, scenario: 0, slope: -0.0013696722421919487, fluctuations: 0.06\n",
      "step: 22480 loss: 3.928378 time elapsed: 29.0944 learning rate: 0.000100, scenario: 0, slope: -0.0014361884390657, fluctuations: 0.07\n",
      "step: 22490 loss: 3.909519 time elapsed: 29.1085 learning rate: 0.000100, scenario: 0, slope: -0.0011799842121340593, fluctuations: 0.08\n",
      "step: 22500 loss: 3.892216 time elapsed: 29.1218 learning rate: 0.000100, scenario: 0, slope: -0.0012044897207686915, fluctuations: 0.07\n",
      "step: 22510 loss: 3.878059 time elapsed: 29.1361 learning rate: 0.000100, scenario: 0, slope: -0.0012801500205012828, fluctuations: 0.05\n",
      "step: 22520 loss: 3.865003 time elapsed: 29.1494 learning rate: 0.000100, scenario: 0, slope: -0.0013708604625612306, fluctuations: 0.04\n",
      "step: 22530 loss: 3.852117 time elapsed: 29.1624 learning rate: 0.000100, scenario: 0, slope: -0.0014693112033481864, fluctuations: 0.04\n",
      "step: 22540 loss: 3.839223 time elapsed: 29.1759 learning rate: 0.000100, scenario: 0, slope: -0.001573380938276219, fluctuations: 0.04\n",
      "step: 22550 loss: 3.826373 time elapsed: 29.1900 learning rate: 0.000100, scenario: 0, slope: -0.0016810984909209352, fluctuations: 0.04\n",
      "step: 22560 loss: 3.813621 time elapsed: 29.2059 learning rate: 0.000100, scenario: 0, slope: -0.0016587149540639248, fluctuations: 0.04\n",
      "step: 22570 loss: 3.801041 time elapsed: 29.2208 learning rate: 0.000100, scenario: 0, slope: -0.001394771497210616, fluctuations: 0.03\n",
      "step: 22580 loss: 3.796419 time elapsed: 29.2349 learning rate: 0.000100, scenario: 0, slope: -0.0013137968911519332, fluctuations: 0.01\n",
      "step: 22590 loss: 3.899935 time elapsed: 29.2481 learning rate: 0.000100, scenario: 0, slope: -0.0009223528083128388, fluctuations: 0.01\n",
      "step: 22600 loss: 3.770419 time elapsed: 29.2618 learning rate: 0.000100, scenario: 0, slope: -0.0007886913554454215, fluctuations: 0.02\n",
      "step: 22610 loss: 3.764638 time elapsed: 29.2762 learning rate: 0.000100, scenario: 0, slope: -0.0008458273130698652, fluctuations: 0.03\n",
      "step: 22620 loss: 3.741053 time elapsed: 29.2903 learning rate: 0.000100, scenario: 0, slope: -0.0009384459534055332, fluctuations: 0.05\n",
      "step: 22630 loss: 3.728783 time elapsed: 29.3042 learning rate: 0.000100, scenario: 0, slope: -0.0010719294048821344, fluctuations: 0.05\n",
      "step: 22640 loss: 3.716912 time elapsed: 29.3180 learning rate: 0.000100, scenario: 0, slope: -0.0012032052322033637, fluctuations: 0.05\n",
      "step: 22650 loss: 3.724294 time elapsed: 29.3313 learning rate: 0.000100, scenario: 0, slope: -0.0012893586029928282, fluctuations: 0.05\n",
      "step: 22660 loss: 3.718152 time elapsed: 29.3457 learning rate: 0.000100, scenario: 0, slope: -0.0012176001596400005, fluctuations: 0.06\n",
      "step: 22670 loss: 3.691991 time elapsed: 29.3595 learning rate: 0.000100, scenario: 0, slope: -0.0013727831081328725, fluctuations: 0.07\n",
      "step: 22680 loss: 3.672800 time elapsed: 29.3731 learning rate: 0.000100, scenario: 0, slope: -0.0015692614241953735, fluctuations: 0.08\n",
      "step: 22690 loss: 3.657399 time elapsed: 29.3863 learning rate: 0.000100, scenario: 0, slope: -0.001334481622697385, fluctuations: 0.07\n",
      "step: 22700 loss: 3.643820 time elapsed: 29.3990 learning rate: 0.000100, scenario: 0, slope: -0.001222610718271564, fluctuations: 0.06\n",
      "step: 22710 loss: 3.632282 time elapsed: 29.4143 learning rate: 0.000100, scenario: 0, slope: -0.0012497062970032236, fluctuations: 0.04\n",
      "step: 22720 loss: 3.620378 time elapsed: 29.4289 learning rate: 0.000100, scenario: 0, slope: -0.001296478391576441, fluctuations: 0.03\n",
      "step: 22730 loss: 3.608932 time elapsed: 29.4431 learning rate: 0.000100, scenario: 0, slope: -0.0013571571011842675, fluctuations: 0.03\n",
      "step: 22740 loss: 3.598528 time elapsed: 29.4571 learning rate: 0.000100, scenario: 0, slope: -0.0014216046338673895, fluctuations: 0.03\n",
      "step: 22750 loss: 3.610418 time elapsed: 29.4712 learning rate: 0.000100, scenario: 0, slope: -0.001390169348173076, fluctuations: 0.03\n",
      "step: 22760 loss: 3.606026 time elapsed: 29.4853 learning rate: 0.000100, scenario: 0, slope: -0.00088650817007589, fluctuations: 0.03\n",
      "step: 22770 loss: 3.570354 time elapsed: 29.4983 learning rate: 0.000100, scenario: 0, slope: -0.0008475909452509623, fluctuations: 0.03\n",
      "step: 22780 loss: 3.552694 time elapsed: 29.5121 learning rate: 0.000100, scenario: 0, slope: -0.0008942083590405051, fluctuations: 0.03\n",
      "step: 22790 loss: 3.540642 time elapsed: 29.5259 learning rate: 0.000100, scenario: 0, slope: -0.0009685253217534539, fluctuations: 0.04\n",
      "step: 22800 loss: 3.529484 time elapsed: 29.5402 learning rate: 0.000100, scenario: 0, slope: -0.0010509917491231984, fluctuations: 0.04\n",
      "step: 22810 loss: 3.518498 time elapsed: 29.5541 learning rate: 0.000100, scenario: 0, slope: -0.001153588711486295, fluctuations: 0.04\n",
      "step: 22820 loss: 3.507721 time elapsed: 29.5674 learning rate: 0.000100, scenario: 0, slope: -0.0012479345381963588, fluctuations: 0.04\n",
      "step: 22830 loss: 3.511557 time elapsed: 29.5806 learning rate: 0.000100, scenario: 0, slope: -0.0013175129232771295, fluctuations: 0.04\n",
      "step: 22840 loss: 3.532411 time elapsed: 29.5939 learning rate: 0.000100, scenario: 0, slope: -0.0010855896640030434, fluctuations: 0.05\n",
      "step: 22850 loss: 3.475560 time elapsed: 29.6086 learning rate: 0.000100, scenario: 0, slope: -0.0011250112312617877, fluctuations: 0.06\n",
      "step: 22860 loss: 3.472130 time elapsed: 29.6249 learning rate: 0.000100, scenario: 0, slope: -0.000885596535966595, fluctuations: 0.06\n",
      "step: 22870 loss: 3.454824 time elapsed: 29.6394 learning rate: 0.000100, scenario: 0, slope: -0.0009062405695760285, fluctuations: 0.07\n",
      "step: 22880 loss: 3.442510 time elapsed: 29.6540 learning rate: 0.000100, scenario: 0, slope: -0.0009976527710110414, fluctuations: 0.06\n",
      "step: 22890 loss: 3.432211 time elapsed: 29.6683 learning rate: 0.000100, scenario: 0, slope: -0.0010927811811561054, fluctuations: 0.05\n",
      "step: 22900 loss: 3.423452 time elapsed: 29.6818 learning rate: 0.000100, scenario: 0, slope: -0.0011799257552031167, fluctuations: 0.05\n",
      "step: 22910 loss: 3.468817 time elapsed: 29.6965 learning rate: 0.000100, scenario: 0, slope: -0.0011801065017819296, fluctuations: 0.05\n",
      "step: 22920 loss: 3.406239 time elapsed: 29.7103 learning rate: 0.000100, scenario: 0, slope: -0.0010573726911241924, fluctuations: 0.06\n",
      "step: 22930 loss: 3.400905 time elapsed: 29.7246 learning rate: 0.000100, scenario: 0, slope: -0.0011528461113807214, fluctuations: 0.07\n",
      "step: 22940 loss: 3.385463 time elapsed: 29.7385 learning rate: 0.000100, scenario: 0, slope: -0.0008921643214915451, fluctuations: 0.08\n",
      "step: 22950 loss: 3.371509 time elapsed: 29.7524 learning rate: 0.000100, scenario: 0, slope: -0.0009270386372845957, fluctuations: 0.07\n",
      "step: 22960 loss: 3.359949 time elapsed: 29.7657 learning rate: 0.000100, scenario: 0, slope: -0.0010010250666896532, fluctuations: 0.05\n",
      "step: 22970 loss: 3.349481 time elapsed: 29.7788 learning rate: 0.000100, scenario: 0, slope: -0.0010884760849655506, fluctuations: 0.04\n",
      "step: 22980 loss: 3.339292 time elapsed: 29.7925 learning rate: 0.000100, scenario: 0, slope: -0.0011908505511295457, fluctuations: 0.04\n",
      "step: 22990 loss: 3.329103 time elapsed: 29.8060 learning rate: 0.000100, scenario: 0, slope: -0.0012982150008079732, fluctuations: 0.04\n",
      "step: 23000 loss: 3.318977 time elapsed: 29.8207 learning rate: 0.000100, scenario: 0, slope: -0.0013983545835526373, fluctuations: 0.04\n",
      "step: 23010 loss: 3.308921 time elapsed: 29.8371 learning rate: 0.000100, scenario: 0, slope: -0.001395657003487809, fluctuations: 0.04\n",
      "step: 23020 loss: 3.299050 time elapsed: 29.8511 learning rate: 0.000100, scenario: 0, slope: -0.0011238554146345747, fluctuations: 0.03\n",
      "step: 23030 loss: 3.296880 time elapsed: 29.8642 learning rate: 0.000100, scenario: 0, slope: -0.0010430283909216803, fluctuations: 0.01\n",
      "step: 23040 loss: 3.401584 time elapsed: 29.8786 learning rate: 0.000100, scenario: 0, slope: -0.0006742402970929574, fluctuations: 0.01\n",
      "step: 23050 loss: 3.283182 time elapsed: 29.8929 learning rate: 0.000100, scenario: 0, slope: -0.0005335184758255159, fluctuations: 0.02\n",
      "step: 23060 loss: 3.269797 time elapsed: 29.9074 learning rate: 0.000100, scenario: 0, slope: -0.0005862272393647429, fluctuations: 0.03\n",
      "step: 23070 loss: 3.253269 time elapsed: 29.9213 learning rate: 0.000100, scenario: 0, slope: -0.0006730590723495895, fluctuations: 0.05\n",
      "step: 23080 loss: 3.242518 time elapsed: 29.9358 learning rate: 0.000100, scenario: 0, slope: -0.0008049165791397724, fluctuations: 0.05\n",
      "step: 23090 loss: 3.235358 time elapsed: 29.9496 learning rate: 0.000100, scenario: 0, slope: -0.0009293486690653813, fluctuations: 0.05\n",
      "step: 23100 loss: 3.253685 time elapsed: 29.9631 learning rate: 0.000100, scenario: 0, slope: -0.0009841365348071754, fluctuations: 0.05\n",
      "step: 23110 loss: 3.216060 time elapsed: 29.9775 learning rate: 0.000100, scenario: 0, slope: -0.0009878500765098004, fluctuations: 0.06\n",
      "step: 23120 loss: 3.209852 time elapsed: 29.9912 learning rate: 0.000100, scenario: 0, slope: -0.0011523541092886731, fluctuations: 0.07\n",
      "step: 23130 loss: 3.196800 time elapsed: 30.0048 learning rate: 0.000100, scenario: 0, slope: -0.0013485371071351908, fluctuations: 0.08\n",
      "step: 23140 loss: 3.183919 time elapsed: 30.0176 learning rate: 0.000100, scenario: 0, slope: -0.0011319402485206924, fluctuations: 0.07\n",
      "step: 23150 loss: 3.175398 time elapsed: 30.0321 learning rate: 0.000100, scenario: 0, slope: -0.0010054263418342378, fluctuations: 0.06\n",
      "step: 23160 loss: 3.165415 time elapsed: 30.0474 learning rate: 0.000100, scenario: 0, slope: -0.001021437854154991, fluctuations: 0.04\n",
      "step: 23170 loss: 3.156067 time elapsed: 30.0618 learning rate: 0.000100, scenario: 0, slope: -0.001057132933265915, fluctuations: 0.03\n",
      "step: 23180 loss: 3.147296 time elapsed: 30.0760 learning rate: 0.000100, scenario: 0, slope: -0.001107727458685374, fluctuations: 0.03\n",
      "step: 23190 loss: 3.154810 time elapsed: 30.0899 learning rate: 0.000100, scenario: 0, slope: -0.0011273135261430105, fluctuations: 0.03\n",
      "step: 23200 loss: 3.200602 time elapsed: 30.1035 learning rate: 0.000100, scenario: 0, slope: -0.0007489850556565035, fluctuations: 0.04\n",
      "step: 23210 loss: 3.124448 time elapsed: 30.1183 learning rate: 0.000100, scenario: 0, slope: -0.0005215298950733077, fluctuations: 0.04\n",
      "step: 23220 loss: 3.115534 time elapsed: 30.1323 learning rate: 0.000100, scenario: 0, slope: -0.0005665708391814248, fluctuations: 0.04\n",
      "step: 23230 loss: 3.105538 time elapsed: 30.1459 learning rate: 0.000100, scenario: 0, slope: -0.0006496805318272369, fluctuations: 0.05\n",
      "step: 23240 loss: 3.094067 time elapsed: 30.1600 learning rate: 0.000100, scenario: 0, slope: -0.0007747551660315394, fluctuations: 0.05\n",
      "step: 23250 loss: 3.084355 time elapsed: 30.1741 learning rate: 0.000100, scenario: 0, slope: -0.0008999548593193856, fluctuations: 0.05\n",
      "step: 23260 loss: 3.075501 time elapsed: 30.1881 learning rate: 0.000100, scenario: 0, slope: -0.0010241951563036047, fluctuations: 0.05\n",
      "step: 23270 loss: 3.066866 time elapsed: 30.2019 learning rate: 0.000100, scenario: 0, slope: -0.0011518349014930714, fluctuations: 0.05\n",
      "step: 23280 loss: 3.063251 time elapsed: 30.2149 learning rate: 0.000100, scenario: 0, slope: -0.0012767800988164725, fluctuations: 0.05\n",
      "step: 23290 loss: 3.183820 time elapsed: 30.2284 learning rate: 0.000100, scenario: 0, slope: -0.001031151731233861, fluctuations: 0.05\n",
      "step: 23300 loss: 3.060836 time elapsed: 30.2431 learning rate: 0.000100, scenario: 0, slope: -0.0006380261469275141, fluctuations: 0.06\n",
      "step: 23310 loss: 3.039337 time elapsed: 30.2598 learning rate: 0.000100, scenario: 0, slope: -0.000554872261851358, fluctuations: 0.06\n",
      "step: 23320 loss: 3.027102 time elapsed: 30.2738 learning rate: 0.000100, scenario: 0, slope: -0.0006063811348859146, fluctuations: 0.06\n",
      "step: 23330 loss: 3.016174 time elapsed: 30.2876 learning rate: 0.000100, scenario: 0, slope: -0.0007046785256240946, fluctuations: 0.06\n",
      "step: 23340 loss: 3.007986 time elapsed: 30.3018 learning rate: 0.000100, scenario: 0, slope: -0.0008231076317364653, fluctuations: 0.06\n",
      "step: 23350 loss: 2.998988 time elapsed: 30.3148 learning rate: 0.000100, scenario: 0, slope: -0.0009428979702543851, fluctuations: 0.06\n",
      "step: 23360 loss: 2.990611 time elapsed: 30.3286 learning rate: 0.000100, scenario: 0, slope: -0.0010638831008096107, fluctuations: 0.06\n",
      "step: 23370 loss: 2.982330 time elapsed: 30.3421 learning rate: 0.000100, scenario: 0, slope: -0.00119040886257875, fluctuations: 0.06\n",
      "step: 23380 loss: 2.975209 time elapsed: 30.3556 learning rate: 0.000100, scenario: 0, slope: -0.0013166889063946751, fluctuations: 0.06\n",
      "step: 23390 loss: 3.029211 time elapsed: 30.3687 learning rate: 0.000100, scenario: 0, slope: -0.0009912629918127306, fluctuations: 0.05\n",
      "step: 23400 loss: 2.975796 time elapsed: 30.3816 learning rate: 0.000100, scenario: 0, slope: -0.00045325135567842454, fluctuations: 0.05\n",
      "step: 23410 loss: 2.971015 time elapsed: 30.3948 learning rate: 0.000100, scenario: 0, slope: -0.0003882578523240551, fluctuations: 0.05\n",
      "step: 23420 loss: 2.944472 time elapsed: 30.4076 learning rate: 0.000100, scenario: 0, slope: -0.0004567877023880692, fluctuations: 0.05\n",
      "step: 23430 loss: 2.935277 time elapsed: 30.4198 learning rate: 0.000100, scenario: 0, slope: -0.0005881715130799597, fluctuations: 0.04\n",
      "step: 23440 loss: 2.926760 time elapsed: 30.4325 learning rate: 0.000100, scenario: 0, slope: -0.0007204690035482754, fluctuations: 0.04\n",
      "step: 23450 loss: 2.918094 time elapsed: 30.4451 learning rate: 0.000100, scenario: 0, slope: -0.000855137281598446, fluctuations: 0.04\n",
      "step: 23460 loss: 2.910047 time elapsed: 30.4576 learning rate: 0.000100, scenario: 0, slope: -0.000991726570068061, fluctuations: 0.04\n",
      "step: 23470 loss: 2.902195 time elapsed: 30.4719 learning rate: 0.000100, scenario: 0, slope: -0.0011328000946969484, fluctuations: 0.04\n",
      "step: 23480 loss: 2.894320 time elapsed: 30.4860 learning rate: 0.000100, scenario: 0, slope: -0.0012806210329906658, fluctuations: 0.04\n",
      "step: 23490 loss: 2.886463 time elapsed: 30.4990 learning rate: 0.000100, scenario: 0, slope: -0.0013112766735494347, fluctuations: 0.04\n",
      "step: 23500 loss: 2.878667 time elapsed: 30.5117 learning rate: 0.000100, scenario: 0, slope: -0.0009500476264522286, fluctuations: 0.03\n",
      "step: 23510 loss: 2.872904 time elapsed: 30.5248 learning rate: 0.000100, scenario: 0, slope: -0.0008348407843193712, fluctuations: 0.01\n",
      "step: 23520 loss: 2.971219 time elapsed: 30.5373 learning rate: 0.000100, scenario: 0, slope: -0.0006008836101466946, fluctuations: 0.0\n",
      "step: 23530 loss: 2.907872 time elapsed: 30.5498 learning rate: 0.000100, scenario: 0, slope: -0.00034398923420256105, fluctuations: 0.01\n",
      "step: 23540 loss: 2.851371 time elapsed: 30.5626 learning rate: 0.000100, scenario: 0, slope: -0.0003706278864273507, fluctuations: 0.03\n",
      "step: 23550 loss: 2.845738 time elapsed: 30.5751 learning rate: 0.000100, scenario: 0, slope: -0.00045999297853031016, fluctuations: 0.04\n",
      "step: 23560 loss: 2.833482 time elapsed: 30.5878 learning rate: 0.000100, scenario: 0, slope: -0.0005636986818987835, fluctuations: 0.06\n",
      "step: 23570 loss: 2.825891 time elapsed: 30.6002 learning rate: 0.000100, scenario: 0, slope: -0.000698502930719769, fluctuations: 0.06\n",
      "step: 23580 loss: 2.818106 time elapsed: 30.6127 learning rate: 0.000100, scenario: 0, slope: -0.0008268502716347796, fluctuations: 0.06\n",
      "step: 23590 loss: 2.810522 time elapsed: 30.6254 learning rate: 0.000100, scenario: 0, slope: -0.0009542631830238184, fluctuations: 0.06\n",
      "step: 23600 loss: 2.807135 time elapsed: 30.6378 learning rate: 0.000100, scenario: 0, slope: -0.0010677299576520355, fluctuations: 0.06\n",
      "step: 23610 loss: 2.948573 time elapsed: 30.6509 learning rate: 0.000100, scenario: 0, slope: -0.0008812989829225174, fluctuations: 0.06\n",
      "step: 23620 loss: 2.836137 time elapsed: 30.6634 learning rate: 0.000100, scenario: 0, slope: -0.0006100921858349883, fluctuations: 0.08\n",
      "step: 23630 loss: 2.784032 time elapsed: 30.6776 learning rate: 0.000100, scenario: 0, slope: -0.0003945437889126998, fluctuations: 0.07\n",
      "step: 23640 loss: 2.780288 time elapsed: 30.6908 learning rate: 0.000100, scenario: 0, slope: -0.0004115574877306488, fluctuations: 0.08\n",
      "step: 23650 loss: 2.767302 time elapsed: 30.7035 learning rate: 0.000100, scenario: 0, slope: -0.0005327491213984547, fluctuations: 0.07\n",
      "step: 23660 loss: 2.760563 time elapsed: 30.7164 learning rate: 0.000100, scenario: 0, slope: -0.0006660575002162441, fluctuations: 0.07\n",
      "step: 23670 loss: 2.753196 time elapsed: 30.7291 learning rate: 0.000100, scenario: 0, slope: -0.0008132244933165989, fluctuations: 0.07\n",
      "step: 23680 loss: 2.745921 time elapsed: 30.7416 learning rate: 0.000100, scenario: 0, slope: -0.000958970660841948, fluctuations: 0.07\n",
      "step: 23690 loss: 2.738821 time elapsed: 30.7541 learning rate: 0.000100, scenario: 0, slope: -0.0011103511529499768, fluctuations: 0.07\n",
      "step: 23700 loss: 2.731769 time elapsed: 30.7667 learning rate: 0.000100, scenario: 0, slope: -0.001252531213284275, fluctuations: 0.07\n",
      "step: 23710 loss: 2.724733 time elapsed: 30.7798 learning rate: 0.000100, scenario: 0, slope: -0.0011217595470492388, fluctuations: 0.06\n",
      "step: 23720 loss: 2.717702 time elapsed: 30.7926 learning rate: 0.000100, scenario: 0, slope: -0.0008023244124996906, fluctuations: 0.05\n",
      "step: 23730 loss: 2.710674 time elapsed: 30.8052 learning rate: 0.000100, scenario: 0, slope: -0.0007449112137331893, fluctuations: 0.04\n",
      "step: 23740 loss: 2.703659 time elapsed: 30.8179 learning rate: 0.000100, scenario: 0, slope: -0.0007186466394659721, fluctuations: 0.02\n",
      "step: 23750 loss: 2.696921 time elapsed: 30.8307 learning rate: 0.000100, scenario: 0, slope: -0.0007085207033479177, fluctuations: 0.01\n",
      "step: 23760 loss: 2.722635 time elapsed: 30.8434 learning rate: 0.000100, scenario: 0, slope: -0.0006563997031526229, fluctuations: 0.0\n",
      "step: 23770 loss: 2.694938 time elapsed: 30.8562 learning rate: 0.000096, scenario: -1, slope: -0.00019027847617793668, fluctuations: 0.01\n",
      "step: 23780 loss: 2.695096 time elapsed: 30.8707 learning rate: 0.000087, scenario: -1, slope: -0.0001882958377612571, fluctuations: 0.03\n",
      "step: 23790 loss: 2.676007 time elapsed: 30.8859 learning rate: 0.000079, scenario: -1, slope: -0.0002720930341101763, fluctuations: 0.06\n",
      "step: 23800 loss: 2.666114 time elapsed: 30.8993 learning rate: 0.000078, scenario: 0, slope: -0.0003467683493897086, fluctuations: 0.1\n",
      "step: 23810 loss: 2.661063 time elapsed: 30.9133 learning rate: 0.000078, scenario: 0, slope: -0.0005056759158893714, fluctuations: 0.12\n",
      "step: 23820 loss: 2.655473 time elapsed: 30.9267 learning rate: 0.000078, scenario: 0, slope: -0.0006517643894445554, fluctuations: 0.12\n",
      "step: 23830 loss: 2.650213 time elapsed: 30.9404 learning rate: 0.000078, scenario: 0, slope: -0.0007816206283087242, fluctuations: 0.12\n",
      "step: 23840 loss: 2.644993 time elapsed: 30.9535 learning rate: 0.000078, scenario: 0, slope: -0.0009126218420917521, fluctuations: 0.12\n",
      "step: 23850 loss: 2.639775 time elapsed: 30.9664 learning rate: 0.000078, scenario: 0, slope: -0.0010607388414497596, fluctuations: 0.12\n",
      "step: 23860 loss: 2.634564 time elapsed: 30.9792 learning rate: 0.000078, scenario: 0, slope: -0.0011848993883662574, fluctuations: 0.12\n",
      "step: 23870 loss: 2.629355 time elapsed: 30.9917 learning rate: 0.000078, scenario: 0, slope: -0.0006451743101983205, fluctuations: 0.11\n",
      "step: 23880 loss: 2.624142 time elapsed: 31.0045 learning rate: 0.000078, scenario: 0, slope: -0.0005499145879514986, fluctuations: 0.08\n",
      "step: 23890 loss: 2.618927 time elapsed: 31.0173 learning rate: 0.000078, scenario: 0, slope: -0.0005286673633352852, fluctuations: 0.05\n",
      "step: 23900 loss: 2.613710 time elapsed: 31.0300 learning rate: 0.000078, scenario: 0, slope: -0.0005242935855092673, fluctuations: 0.02\n",
      "step: 23910 loss: 2.608490 time elapsed: 31.0431 learning rate: 0.000078, scenario: 0, slope: -0.0005223103530748681, fluctuations: 0.0\n",
      "step: 23920 loss: 2.603268 time elapsed: 31.0557 learning rate: 0.000078, scenario: 0, slope: -0.0005216156195372696, fluctuations: 0.0\n",
      "step: 23930 loss: 2.598043 time elapsed: 31.0681 learning rate: 0.000078, scenario: 0, slope: -0.0005215374479304644, fluctuations: 0.0\n",
      "step: 23940 loss: 2.592816 time elapsed: 31.0813 learning rate: 0.000078, scenario: 0, slope: -0.0005216679734770396, fluctuations: 0.0\n",
      "step: 23950 loss: 2.587587 time elapsed: 31.0954 learning rate: 0.000078, scenario: 0, slope: -0.0005218776221344391, fluctuations: 0.0\n",
      "step: 23960 loss: 2.582356 time elapsed: 31.1087 learning rate: 0.000078, scenario: 0, slope: -0.0005221083998683451, fluctuations: 0.0\n",
      "step: 23970 loss: 2.577122 time elapsed: 31.1211 learning rate: 0.000078, scenario: 0, slope: -0.000522342042516952, fluctuations: 0.0\n",
      "step: 23980 loss: 2.571887 time elapsed: 31.1340 learning rate: 0.000078, scenario: 0, slope: -0.0005225729916295303, fluctuations: 0.0\n",
      "step: 23990 loss: 2.566649 time elapsed: 31.1472 learning rate: 0.000078, scenario: 0, slope: -0.0005227989151749222, fluctuations: 0.0\n",
      "step: 24000 loss: 2.561410 time elapsed: 31.1602 learning rate: 0.000078, scenario: 0, slope: -0.0005229971285384895, fluctuations: 0.0\n",
      "step: 24010 loss: 2.556169 time elapsed: 31.1744 learning rate: 0.000078, scenario: 0, slope: -0.0005232325262942698, fluctuations: 0.0\n",
      "step: 24020 loss: 2.550926 time elapsed: 31.1881 learning rate: 0.000078, scenario: 0, slope: -0.0005234398189542882, fluctuations: 0.0\n",
      "step: 24030 loss: 2.545681 time elapsed: 31.2011 learning rate: 0.000078, scenario: 0, slope: -0.0005236406778077852, fluctuations: 0.0\n",
      "step: 24040 loss: 2.540435 time elapsed: 31.2143 learning rate: 0.000078, scenario: 0, slope: -0.0005238350671828725, fluctuations: 0.0\n",
      "step: 24050 loss: 2.535187 time elapsed: 31.2279 learning rate: 0.000078, scenario: 0, slope: -0.0005240229612652308, fluctuations: 0.0\n",
      "step: 24060 loss: 2.529937 time elapsed: 31.2409 learning rate: 0.000078, scenario: 0, slope: -0.000524204338368889, fluctuations: 0.0\n",
      "step: 24070 loss: 2.524686 time elapsed: 31.2545 learning rate: 0.000078, scenario: 0, slope: -0.000524379178630945, fluctuations: 0.0\n",
      "step: 24080 loss: 2.519434 time elapsed: 31.2671 learning rate: 0.000078, scenario: 0, slope: -0.0005245474632299289, fluctuations: 0.0\n",
      "step: 24090 loss: 2.514180 time elapsed: 31.2818 learning rate: 0.000078, scenario: 0, slope: -0.0005247091741541782, fluctuations: 0.0\n",
      "step: 24100 loss: 2.508925 time elapsed: 31.2964 learning rate: 0.000078, scenario: 0, slope: -0.0005248490791754341, fluctuations: 0.0\n",
      "step: 24110 loss: 2.503669 time elapsed: 31.3109 learning rate: 0.000078, scenario: 0, slope: -0.0005250128064076713, fluctuations: 0.0\n",
      "step: 24120 loss: 2.498412 time elapsed: 31.3244 learning rate: 0.000078, scenario: 0, slope: -0.0005251546950771975, fluctuations: 0.0\n",
      "step: 24130 loss: 2.493154 time elapsed: 31.3370 learning rate: 0.000078, scenario: 0, slope: -0.0005252899447107556, fluctuations: 0.0\n",
      "step: 24140 loss: 2.487894 time elapsed: 31.3497 learning rate: 0.000078, scenario: 0, slope: -0.0005254185405227127, fluctuations: 0.0\n",
      "step: 24150 loss: 2.482634 time elapsed: 31.3625 learning rate: 0.000078, scenario: 0, slope: -0.0005255404683276036, fluctuations: 0.0\n",
      "step: 24160 loss: 2.477373 time elapsed: 31.3754 learning rate: 0.000078, scenario: 0, slope: -0.0005256557145338805, fluctuations: 0.0\n",
      "step: 24170 loss: 2.472111 time elapsed: 31.3878 learning rate: 0.000078, scenario: 0, slope: -0.0005257642661394233, fluctuations: 0.0\n",
      "step: 24180 loss: 2.466849 time elapsed: 31.4002 learning rate: 0.000078, scenario: 0, slope: -0.0005258661107281362, fluctuations: 0.0\n",
      "step: 24190 loss: 2.461586 time elapsed: 31.4129 learning rate: 0.000078, scenario: 0, slope: -0.0005259612364674287, fluctuations: 0.0\n",
      "step: 24200 loss: 2.456322 time elapsed: 31.4252 learning rate: 0.000078, scenario: 0, slope: -0.0005260410957054125, fluctuations: 0.0\n",
      "step: 24210 loss: 2.451058 time elapsed: 31.4382 learning rate: 0.000078, scenario: 0, slope: -0.0005261312869737466, fluctuations: 0.0\n",
      "step: 24220 loss: 2.445793 time elapsed: 31.4506 learning rate: 0.000078, scenario: 0, slope: -0.0005262061909798876, fluctuations: 0.0\n",
      "step: 24230 loss: 2.440528 time elapsed: 31.4627 learning rate: 0.000078, scenario: 0, slope: -0.0005262743346135893, fluctuations: 0.0\n",
      "step: 24240 loss: 2.435262 time elapsed: 31.4749 learning rate: 0.000078, scenario: 0, slope: -0.0005263357089436379, fluctuations: 0.0\n",
      "step: 24250 loss: 2.429997 time elapsed: 31.4873 learning rate: 0.000078, scenario: 0, slope: -0.0005263903056191412, fluctuations: 0.0\n",
      "step: 24260 loss: 2.424731 time elapsed: 31.4998 learning rate: 0.000078, scenario: 0, slope: -0.0005264381168701836, fluctuations: 0.0\n",
      "step: 24270 loss: 2.419465 time elapsed: 31.5136 learning rate: 0.000078, scenario: 0, slope: -0.0005264791355084817, fluctuations: 0.0\n",
      "step: 24280 loss: 2.414199 time elapsed: 31.5273 learning rate: 0.000078, scenario: 0, slope: -0.0005265133549286776, fluctuations: 0.0\n",
      "step: 24290 loss: 2.408933 time elapsed: 31.5401 learning rate: 0.000078, scenario: 0, slope: -0.0005265407691099539, fluctuations: 0.0\n",
      "step: 24300 loss: 2.403667 time elapsed: 31.5528 learning rate: 0.000078, scenario: 0, slope: -0.0005265596188889356, fluctuations: 0.0\n",
      "step: 24310 loss: 2.398401 time elapsed: 31.5662 learning rate: 0.000078, scenario: 0, slope: -0.0005265751605994776, fluctuations: 0.0\n",
      "step: 24320 loss: 2.393136 time elapsed: 31.5787 learning rate: 0.000078, scenario: 0, slope: -0.0005265821287994363, fluctuations: 0.0\n",
      "step: 24330 loss: 2.387871 time elapsed: 31.5911 learning rate: 0.000078, scenario: 0, slope: -0.0005265822735462882, fluctuations: 0.0\n",
      "step: 24340 loss: 2.382606 time elapsed: 31.6034 learning rate: 0.000078, scenario: 0, slope: -0.0005265755917621569, fluctuations: 0.0\n",
      "step: 24350 loss: 2.377341 time elapsed: 31.6160 learning rate: 0.000078, scenario: 0, slope: -0.0005265620809630616, fluctuations: 0.0\n",
      "step: 24360 loss: 2.372078 time elapsed: 31.6285 learning rate: 0.000078, scenario: 0, slope: -0.0005265417392554591, fluctuations: 0.0\n",
      "step: 24370 loss: 2.366814 time elapsed: 31.6413 learning rate: 0.000078, scenario: 0, slope: -0.0005265145602311271, fluctuations: 0.0\n",
      "step: 24380 loss: 2.361560 time elapsed: 31.6539 learning rate: 0.000078, scenario: 0, slope: -0.000526471375389357, fluctuations: 0.0\n",
      "step: 24390 loss: 2.381170 time elapsed: 31.6662 learning rate: 0.000078, scenario: 0, slope: -0.0005002256007553618, fluctuations: 0.0\n",
      "step: 24400 loss: 2.547393 time elapsed: 31.6787 learning rate: 0.000075, scenario: -1, slope: 0.00033497211684953255, fluctuations: 0.01\n",
      "step: 24410 loss: 2.382927 time elapsed: 31.6919 learning rate: 0.000068, scenario: -1, slope: 0.0003967057320374494, fluctuations: 0.05\n",
      "step: 24420 loss: 2.361132 time elapsed: 31.7042 learning rate: 0.000061, scenario: -1, slope: 0.00025811662972338874, fluctuations: 0.08\n",
      "step: 24430 loss: 2.343494 time elapsed: 31.7184 learning rate: 0.000056, scenario: -1, slope: 7.330903784597478e-05, fluctuations: 0.12\n",
      "step: 24440 loss: 2.337832 time elapsed: 31.7324 learning rate: 0.000050, scenario: -1, slope: -0.00011761166800567796, fluctuations: 0.17\n",
      "step: 24450 loss: 2.334080 time elapsed: 31.7452 learning rate: 0.000048, scenario: 0, slope: -0.0004178264827799647, fluctuations: 0.18\n",
      "step: 24460 loss: 2.331130 time elapsed: 31.7579 learning rate: 0.000048, scenario: 0, slope: -0.0006759550035571304, fluctuations: 0.18\n",
      "step: 24470 loss: 2.328306 time elapsed: 31.7703 learning rate: 0.000048, scenario: 0, slope: -0.000921309574316443, fluctuations: 0.18\n",
      "step: 24480 loss: 2.325496 time elapsed: 31.7831 learning rate: 0.000048, scenario: 0, slope: -0.0012011309103049056, fluctuations: 0.18\n",
      "step: 24490 loss: 2.322681 time elapsed: 31.7956 learning rate: 0.000048, scenario: 0, slope: -0.001541155848517673, fluctuations: 0.18\n",
      "step: 24500 loss: 2.319859 time elapsed: 31.8080 learning rate: 0.000048, scenario: 0, slope: -0.0006895589456230634, fluctuations: 0.16\n",
      "step: 24510 loss: 2.317030 time elapsed: 31.8208 learning rate: 0.000048, scenario: 0, slope: -0.0003403501521262865, fluctuations: 0.13\n",
      "step: 24520 loss: 2.314195 time elapsed: 31.8331 learning rate: 0.000048, scenario: 0, slope: -0.00030114016253884955, fluctuations: 0.09\n",
      "step: 24530 loss: 2.311356 time elapsed: 31.8455 learning rate: 0.000048, scenario: 0, slope: -0.000286862901355953, fluctuations: 0.05\n",
      "step: 24540 loss: 2.308511 time elapsed: 31.8578 learning rate: 0.000048, scenario: 0, slope: -0.00028553155915582315, fluctuations: 0.0\n",
      "step: 24550 loss: 2.305660 time elapsed: 31.8700 learning rate: 0.000048, scenario: 0, slope: -0.000283787853494859, fluctuations: 0.0\n",
      "step: 24560 loss: 2.302803 time elapsed: 31.8823 learning rate: 0.000048, scenario: 0, slope: -0.00028354424075384947, fluctuations: 0.0\n",
      "step: 24570 loss: 2.299941 time elapsed: 31.8950 learning rate: 0.000048, scenario: 0, slope: -0.00028382657518001363, fluctuations: 0.0\n",
      "step: 24580 loss: 2.297073 time elapsed: 31.9073 learning rate: 0.000048, scenario: 0, slope: -0.0002842996896528923, fluctuations: 0.0\n",
      "step: 24590 loss: 2.294199 time elapsed: 31.9215 learning rate: 0.000048, scenario: 0, slope: -0.00028484072676099927, fluctuations: 0.0\n",
      "step: 24600 loss: 2.291319 time elapsed: 31.9364 learning rate: 0.000048, scenario: 0, slope: -0.00028534869557565344, fluctuations: 0.0\n",
      "step: 24610 loss: 2.288434 time elapsed: 31.9520 learning rate: 0.000048, scenario: 0, slope: -0.0002859764396427419, fluctuations: 0.0\n",
      "step: 24620 loss: 2.285543 time elapsed: 31.9646 learning rate: 0.000048, scenario: 0, slope: -0.0002865490165447723, fluctuations: 0.0\n",
      "step: 24630 loss: 2.282647 time elapsed: 31.9770 learning rate: 0.000048, scenario: 0, slope: -0.00028712082033671645, fluctuations: 0.0\n",
      "step: 24640 loss: 2.279745 time elapsed: 31.9907 learning rate: 0.000048, scenario: 0, slope: -0.0002876912373154614, fluctuations: 0.0\n",
      "step: 24650 loss: 2.276837 time elapsed: 32.0037 learning rate: 0.000048, scenario: 0, slope: -0.0002882600468551218, fluctuations: 0.0\n",
      "step: 24660 loss: 2.273924 time elapsed: 32.0161 learning rate: 0.000048, scenario: 0, slope: -0.0002888271406237183, fluctuations: 0.0\n",
      "step: 24670 loss: 2.271005 time elapsed: 32.0288 learning rate: 0.000048, scenario: 0, slope: -0.000289392451685839, fluctuations: 0.0\n",
      "step: 24680 loss: 2.268080 time elapsed: 32.0414 learning rate: 0.000048, scenario: 0, slope: -0.0002899559375840899, fluctuations: 0.0\n",
      "step: 24690 loss: 2.265150 time elapsed: 32.0540 learning rate: 0.000048, scenario: 0, slope: -0.00029051756526595396, fluctuations: 0.0\n",
      "step: 24700 loss: 2.262215 time elapsed: 32.0665 learning rate: 0.000048, scenario: 0, slope: -0.00029102141608776616, fluctuations: 0.0\n",
      "step: 24710 loss: 2.259274 time elapsed: 32.0797 learning rate: 0.000048, scenario: 0, slope: -0.00029163512477811064, fluctuations: 0.0\n",
      "step: 24720 loss: 2.256327 time elapsed: 32.0924 learning rate: 0.000048, scenario: 0, slope: -0.0002921909989827083, fluctuations: 0.0\n",
      "step: 24730 loss: 2.253375 time elapsed: 32.1050 learning rate: 0.000048, scenario: 0, slope: -0.00029274489916696035, fluctuations: 0.0\n",
      "step: 24740 loss: 2.250418 time elapsed: 32.1179 learning rate: 0.000048, scenario: 0, slope: -0.0002932967982898891, fluctuations: 0.0\n",
      "step: 24750 loss: 2.247455 time elapsed: 32.1320 learning rate: 0.000048, scenario: 0, slope: -0.00029384666979109027, fluctuations: 0.0\n",
      "step: 24760 loss: 2.244487 time elapsed: 32.1455 learning rate: 0.000048, scenario: 0, slope: -0.00029439448752428495, fluctuations: 0.0\n",
      "step: 24770 loss: 2.241514 time elapsed: 32.1582 learning rate: 0.000048, scenario: 0, slope: -0.00029494022571662106, fluctuations: 0.0\n",
      "step: 24780 loss: 2.238535 time elapsed: 32.1708 learning rate: 0.000048, scenario: 0, slope: -0.0002954838589353575, fluctuations: 0.0\n",
      "step: 24790 loss: 2.235551 time elapsed: 32.1837 learning rate: 0.000048, scenario: 0, slope: -0.0002960253620601279, fluctuations: 0.0\n",
      "step: 24800 loss: 2.232561 time elapsed: 32.1965 learning rate: 0.000048, scenario: 0, slope: -0.0002965108731135389, fluctuations: 0.0\n",
      "step: 24810 loss: 2.229567 time elapsed: 32.2098 learning rate: 0.000048, scenario: 0, slope: -0.00029710187897348853, fluctuations: 0.0\n",
      "step: 24820 loss: 2.226567 time elapsed: 32.2224 learning rate: 0.000048, scenario: 0, slope: -0.0002976368438930676, fluctuations: 0.0\n",
      "step: 24830 loss: 2.223562 time elapsed: 32.2347 learning rate: 0.000048, scenario: 0, slope: -0.0002981695809504208, fluctuations: 0.0\n",
      "step: 24840 loss: 2.220551 time elapsed: 32.2472 learning rate: 0.000048, scenario: 0, slope: -0.00029870006630464536, fluctuations: 0.0\n",
      "step: 24850 loss: 2.217536 time elapsed: 32.2598 learning rate: 0.000048, scenario: 0, slope: -0.0002992282763321937, fluctuations: 0.0\n",
      "step: 24860 loss: 2.214515 time elapsed: 32.2722 learning rate: 0.000048, scenario: 0, slope: -0.0002997541876176397, fluctuations: 0.0\n",
      "step: 24870 loss: 2.211489 time elapsed: 32.2844 learning rate: 0.000048, scenario: 0, slope: -0.0003002777769469385, fluctuations: 0.0\n",
      "step: 24880 loss: 2.208458 time elapsed: 32.2966 learning rate: 0.000048, scenario: 0, slope: -0.0003007990212998397, fluctuations: 0.0\n",
      "step: 24890 loss: 2.205422 time elapsed: 32.3090 learning rate: 0.000048, scenario: 0, slope: -0.000301317897845025, fluctuations: 0.0\n",
      "step: 24900 loss: 2.202381 time elapsed: 32.3211 learning rate: 0.000048, scenario: 0, slope: -0.00030178284353820094, fluctuations: 0.0\n",
      "step: 24910 loss: 2.199335 time elapsed: 32.3351 learning rate: 0.000048, scenario: 0, slope: -0.0003023484571020278, fluctuations: 0.0\n",
      "step: 24920 loss: 2.196284 time elapsed: 32.3490 learning rate: 0.000048, scenario: 0, slope: -0.000302860095054412, fluctuations: 0.0\n",
      "step: 24930 loss: 2.193228 time elapsed: 32.3621 learning rate: 0.000048, scenario: 0, slope: -0.00030336927567395404, fluctuations: 0.0\n",
      "step: 24940 loss: 2.190166 time elapsed: 32.3749 learning rate: 0.000048, scenario: 0, slope: -0.0003038759770130422, fluctuations: 0.0\n",
      "step: 24950 loss: 2.187100 time elapsed: 32.3875 learning rate: 0.000048, scenario: 0, slope: -0.000304380177292719, fluctuations: 0.0\n",
      "step: 24960 loss: 2.184030 time elapsed: 32.3999 learning rate: 0.000048, scenario: 0, slope: -0.0003048818549005665, fluctuations: 0.0\n",
      "step: 24970 loss: 2.180954 time elapsed: 32.4132 learning rate: 0.000048, scenario: 0, slope: -0.00030538098839033074, fluctuations: 0.0\n",
      "step: 24980 loss: 2.177873 time elapsed: 32.4260 learning rate: 0.000048, scenario: 0, slope: -0.00030587755647956504, fluctuations: 0.0\n",
      "step: 24990 loss: 2.174788 time elapsed: 32.4386 learning rate: 0.000048, scenario: 0, slope: -0.0003063715380494125, fluctuations: 0.0\n",
      "step: 25000 loss: 2.171698 time elapsed: 32.4508 learning rate: 0.000048, scenario: 0, slope: -0.0003068138926650557, fluctuations: 0.0\n",
      "step: 25010 loss: 2.168603 time elapsed: 32.4638 learning rate: 0.000048, scenario: 0, slope: -0.00030735165797104964, fluctuations: 0.0\n",
      "step: 25020 loss: 2.165503 time elapsed: 32.4763 learning rate: 0.000048, scenario: 0, slope: -0.00030783775489878735, fluctuations: 0.0\n",
      "step: 25030 loss: 2.162398 time elapsed: 32.4887 learning rate: 0.000048, scenario: 0, slope: -0.00030832118245930474, fluctuations: 0.0\n",
      "step: 25040 loss: 2.159289 time elapsed: 32.5010 learning rate: 0.000048, scenario: 0, slope: -0.00030880192034694534, fluctuations: 0.0\n",
      "step: 25050 loss: 2.156175 time elapsed: 32.5137 learning rate: 0.000048, scenario: 0, slope: -0.00030927994841904586, fluctuations: 0.0\n",
      "step: 25060 loss: 2.153057 time elapsed: 32.5276 learning rate: 0.000048, scenario: 0, slope: -0.000309755246696323, fluctuations: 0.0\n",
      "step: 25070 loss: 2.149934 time elapsed: 32.5416 learning rate: 0.000048, scenario: 0, slope: -0.0003102277953637394, fluctuations: 0.0\n",
      "step: 25080 loss: 2.146806 time elapsed: 32.5544 learning rate: 0.000048, scenario: 0, slope: -0.0003106975747708591, fluctuations: 0.0\n",
      "step: 25090 loss: 2.143674 time elapsed: 32.5669 learning rate: 0.000048, scenario: 0, slope: -0.0003111645654328311, fluctuations: 0.0\n",
      "step: 25100 loss: 2.140537 time elapsed: 32.5793 learning rate: 0.000048, scenario: 0, slope: -0.0003115824566813543, fluctuations: 0.0\n",
      "step: 25110 loss: 2.137396 time elapsed: 32.5927 learning rate: 0.000048, scenario: 0, slope: -0.0003120901034153874, fluctuations: 0.0\n",
      "step: 25120 loss: 2.134251 time elapsed: 32.6052 learning rate: 0.000048, scenario: 0, slope: -0.000312548612603181, fluctuations: 0.0\n",
      "step: 25130 loss: 2.131101 time elapsed: 32.6179 learning rate: 0.000048, scenario: 0, slope: -0.00031300425678221515, fluctuations: 0.0\n",
      "step: 25140 loss: 2.127946 time elapsed: 32.6305 learning rate: 0.000048, scenario: 0, slope: -0.00031345701731130886, fluctuations: 0.0\n",
      "step: 25150 loss: 2.124787 time elapsed: 32.6430 learning rate: 0.000048, scenario: 0, slope: -0.0003139068757214681, fluctuations: 0.0\n",
      "step: 25160 loss: 2.121624 time elapsed: 32.6554 learning rate: 0.000048, scenario: 0, slope: -0.0003143538137177145, fluctuations: 0.0\n",
      "step: 25170 loss: 2.118457 time elapsed: 32.6677 learning rate: 0.000048, scenario: 0, slope: -0.000314797813180001, fluctuations: 0.0\n",
      "step: 25180 loss: 2.115285 time elapsed: 32.6798 learning rate: 0.000048, scenario: 0, slope: -0.00031523885616507755, fluctuations: 0.0\n",
      "step: 25190 loss: 2.112109 time elapsed: 32.6919 learning rate: 0.000048, scenario: 0, slope: -0.0003156769249082397, fluctuations: 0.0\n",
      "step: 25200 loss: 2.108929 time elapsed: 32.7039 learning rate: 0.000048, scenario: 0, slope: -0.00031606862926199426, fluctuations: 0.0\n",
      "step: 25210 loss: 2.105745 time elapsed: 32.7171 learning rate: 0.000048, scenario: 0, slope: -0.0003165440695083106, fluctuations: 0.0\n",
      "step: 25220 loss: 2.102556 time elapsed: 32.7297 learning rate: 0.000048, scenario: 0, slope: -0.0003169731107409977, fluctuations: 0.0\n",
      "step: 25230 loss: 2.099363 time elapsed: 32.7436 learning rate: 0.000048, scenario: 0, slope: -0.0003173991084859468, fluctuations: 0.0\n",
      "step: 25240 loss: 2.096167 time elapsed: 32.7576 learning rate: 0.000048, scenario: 0, slope: -0.0003178220458937731, fluctuations: 0.0\n",
      "step: 25250 loss: 2.092966 time elapsed: 32.7713 learning rate: 0.000048, scenario: 0, slope: -0.00031824190630346514, fluctuations: 0.0\n",
      "step: 25260 loss: 2.089761 time elapsed: 32.7838 learning rate: 0.000048, scenario: 0, slope: -0.00031865867324392067, fluctuations: 0.0\n",
      "step: 25270 loss: 2.086552 time elapsed: 32.7961 learning rate: 0.000048, scenario: 0, slope: -0.0003190723304359814, fluctuations: 0.0\n",
      "step: 25280 loss: 2.083339 time elapsed: 32.8084 learning rate: 0.000048, scenario: 0, slope: -0.0003194828617941588, fluctuations: 0.0\n",
      "step: 25290 loss: 2.080123 time elapsed: 32.8204 learning rate: 0.000048, scenario: 0, slope: -0.0003198902514286589, fluctuations: 0.0\n",
      "step: 25300 loss: 2.076902 time elapsed: 32.8324 learning rate: 0.000048, scenario: 0, slope: -0.00032025420295190746, fluctuations: 0.0\n",
      "step: 25310 loss: 2.073677 time elapsed: 32.8455 learning rate: 0.000048, scenario: 0, slope: -0.0003206955429555683, fluctuations: 0.0\n",
      "step: 25320 loss: 2.070449 time elapsed: 32.8580 learning rate: 0.000048, scenario: 0, slope: -0.00032109341406288397, fluctuations: 0.0\n",
      "step: 25330 loss: 2.067217 time elapsed: 32.8705 learning rate: 0.000048, scenario: 0, slope: -0.00032148808187996465, fluctuations: 0.0\n",
      "step: 25340 loss: 2.063981 time elapsed: 32.8829 learning rate: 0.000048, scenario: 0, slope: -0.00032187953152288757, fluctuations: 0.0\n",
      "step: 25350 loss: 2.060741 time elapsed: 32.8954 learning rate: 0.000048, scenario: 0, slope: -0.00032226774831464164, fluctuations: 0.0\n",
      "step: 25360 loss: 2.057498 time elapsed: 32.9076 learning rate: 0.000048, scenario: 0, slope: -0.0003226527177871783, fluctuations: 0.0\n",
      "step: 25370 loss: 2.054251 time elapsed: 32.9202 learning rate: 0.000048, scenario: 0, slope: -0.00032303442568329014, fluctuations: 0.0\n",
      "step: 25380 loss: 2.051000 time elapsed: 32.9321 learning rate: 0.000048, scenario: 0, slope: -0.000323412857958385, fluctuations: 0.0\n",
      "step: 25390 loss: 2.047746 time elapsed: 32.9453 learning rate: 0.000048, scenario: 0, slope: -0.000323788000782654, fluctuations: 0.0\n",
      "step: 25400 loss: 2.044488 time elapsed: 32.9589 learning rate: 0.000048, scenario: 0, slope: -0.00032412280558810646, fluctuations: 0.0\n",
      "step: 25410 loss: 2.041227 time elapsed: 32.9728 learning rate: 0.000048, scenario: 0, slope: -0.0003245283638436996, fluctuations: 0.0\n",
      "step: 25420 loss: 2.037962 time elapsed: 32.9855 learning rate: 0.000048, scenario: 0, slope: -0.00032489355751147956, fluctuations: 0.0\n",
      "step: 25430 loss: 2.034694 time elapsed: 32.9981 learning rate: 0.000048, scenario: 0, slope: -0.00032525540859404487, fluctuations: 0.0\n",
      "step: 25440 loss: 2.031422 time elapsed: 33.0120 learning rate: 0.000048, scenario: 0, slope: -0.0003256139043637603, fluctuations: 0.0\n",
      "step: 25450 loss: 2.028147 time elapsed: 33.0250 learning rate: 0.000048, scenario: 0, slope: -0.0003259690323188488, fluctuations: 0.0\n",
      "step: 25460 loss: 2.024868 time elapsed: 33.0385 learning rate: 0.000048, scenario: 0, slope: -0.000326320780185887, fluctuations: 0.0\n",
      "step: 25470 loss: 2.021587 time elapsed: 33.0520 learning rate: 0.000048, scenario: 0, slope: -0.00032666913592152124, fluctuations: 0.0\n",
      "step: 25480 loss: 2.018301 time elapsed: 33.0645 learning rate: 0.000048, scenario: 0, slope: -0.0003270140877137799, fluctuations: 0.0\n",
      "step: 25490 loss: 2.015013 time elapsed: 33.0781 learning rate: 0.000048, scenario: 0, slope: -0.00032735562398489765, fluctuations: 0.0\n",
      "step: 25500 loss: 2.011721 time elapsed: 33.0916 learning rate: 0.000048, scenario: 0, slope: -0.0003276600769786447, fluctuations: 0.0\n",
      "step: 25510 loss: 2.008426 time elapsed: 33.1053 learning rate: 0.000048, scenario: 0, slope: -0.0003280284048298664, fluctuations: 0.0\n",
      "step: 25520 loss: 2.005128 time elapsed: 33.1181 learning rate: 0.000048, scenario: 0, slope: -0.0003283596274323233, fluctuations: 0.0\n",
      "step: 25530 loss: 2.001827 time elapsed: 33.1311 learning rate: 0.000048, scenario: 0, slope: -0.00032868739057405035, fluctuations: 0.0\n",
      "step: 25540 loss: 1.998523 time elapsed: 33.1456 learning rate: 0.000048, scenario: 0, slope: -0.00032901168387227753, fluctuations: 0.0\n",
      "step: 25550 loss: 1.995216 time elapsed: 33.1599 learning rate: 0.000048, scenario: 0, slope: -0.0003293324971885988, fluctuations: 0.0\n",
      "step: 25560 loss: 1.991906 time elapsed: 33.1740 learning rate: 0.000048, scenario: 0, slope: -0.00032964982063082415, fluctuations: 0.0\n",
      "step: 25570 loss: 1.988592 time elapsed: 33.1870 learning rate: 0.000048, scenario: 0, slope: -0.00032996364455376085, fluctuations: 0.0\n",
      "step: 25580 loss: 1.985276 time elapsed: 33.1995 learning rate: 0.000048, scenario: 0, slope: -0.00033027395917786595, fluctuations: 0.0\n",
      "step: 25590 loss: 1.981957 time elapsed: 33.2119 learning rate: 0.000048, scenario: 0, slope: -0.00033058032735326084, fluctuations: 0.0\n",
      "step: 25600 loss: 1.979365 time elapsed: 33.2244 learning rate: 0.000048, scenario: 0, slope: -0.00033048844124693416, fluctuations: 0.0\n",
      "step: 25610 loss: 2.020936 time elapsed: 33.2376 learning rate: 0.000048, scenario: -1, slope: -9.49414135844463e-05, fluctuations: 0.01\n",
      "step: 25620 loss: 1.992444 time elapsed: 33.2502 learning rate: 0.000044, scenario: -1, slope: -5.668029425434674e-05, fluctuations: 0.03\n",
      "step: 25630 loss: 1.971983 time elapsed: 33.2629 learning rate: 0.000040, scenario: -1, slope: -8.189839163847586e-05, fluctuations: 0.07\n",
      "step: 25640 loss: 1.968458 time elapsed: 33.2754 learning rate: 0.000036, scenario: -1, slope: -0.00012420534793597255, fluctuations: 0.11\n",
      "step: 25650 loss: 1.965348 time elapsed: 33.2880 learning rate: 0.000032, scenario: -1, slope: -0.00019019196023152925, fluctuations: 0.14\n",
      "step: 25660 loss: 1.963303 time elapsed: 33.3007 learning rate: 0.000032, scenario: 0, slope: -0.0002664578170747575, fluctuations: 0.15\n",
      "step: 25670 loss: 1.961151 time elapsed: 33.3132 learning rate: 0.000032, scenario: 0, slope: -0.0003335948921664691, fluctuations: 0.15\n",
      "step: 25680 loss: 1.959122 time elapsed: 33.3260 learning rate: 0.000032, scenario: 0, slope: -0.00039632480411454636, fluctuations: 0.15\n",
      "step: 25690 loss: 1.957088 time elapsed: 33.3386 learning rate: 0.000032, scenario: 0, slope: -0.0004656966925635801, fluctuations: 0.15\n",
      "step: 25700 loss: 1.955053 time elapsed: 33.3512 learning rate: 0.000032, scenario: 0, slope: -0.000543931620934334, fluctuations: 0.15\n",
      "step: 25710 loss: 1.953020 time elapsed: 33.3659 learning rate: 0.000032, scenario: 0, slope: -0.00031854313890996845, fluctuations: 0.14\n",
      "step: 25720 loss: 1.950980 time elapsed: 33.3801 learning rate: 0.000032, scenario: 0, slope: -0.00023993339952991697, fluctuations: 0.11\n",
      "step: 25730 loss: 1.948936 time elapsed: 33.3937 learning rate: 0.000032, scenario: 0, slope: -0.00021266466288442923, fluctuations: 0.08\n",
      "step: 25740 loss: 1.946888 time elapsed: 33.4080 learning rate: 0.000032, scenario: 0, slope: -0.00020603051171802072, fluctuations: 0.04\n",
      "step: 25750 loss: 1.944834 time elapsed: 33.4221 learning rate: 0.000032, scenario: 0, slope: -0.00020463108559104465, fluctuations: 0.0\n",
      "step: 25760 loss: 1.942777 time elapsed: 33.4367 learning rate: 0.000032, scenario: 0, slope: -0.0002041474753785699, fluctuations: 0.0\n",
      "step: 25770 loss: 1.940714 time elapsed: 33.4502 learning rate: 0.000032, scenario: 0, slope: -0.00020432775373179765, fluctuations: 0.0\n",
      "step: 25780 loss: 1.938647 time elapsed: 33.4637 learning rate: 0.000032, scenario: 0, slope: -0.00020468573402967469, fluctuations: 0.0\n",
      "step: 25790 loss: 1.936575 time elapsed: 33.4777 learning rate: 0.000032, scenario: 0, slope: -0.00020512168690661818, fluctuations: 0.0\n",
      "step: 25800 loss: 1.934498 time elapsed: 33.4913 learning rate: 0.000032, scenario: 0, slope: -0.00020553479286463292, fluctuations: 0.0\n",
      "step: 25810 loss: 1.932416 time elapsed: 33.5065 learning rate: 0.000032, scenario: 0, slope: -0.00020604938359468193, fluctuations: 0.0\n",
      "step: 25820 loss: 1.930330 time elapsed: 33.5205 learning rate: 0.000032, scenario: 0, slope: -0.00020651969033000527, fluctuations: 0.0\n",
      "step: 25830 loss: 1.928240 time elapsed: 33.5337 learning rate: 0.000032, scenario: 0, slope: -0.00020699006470029213, fluctuations: 0.0\n",
      "step: 25840 loss: 1.926144 time elapsed: 33.5479 learning rate: 0.000032, scenario: 0, slope: -0.00020746003317704173, fluctuations: 0.0\n",
      "step: 25850 loss: 1.924044 time elapsed: 33.5625 learning rate: 0.000032, scenario: 0, slope: -0.0002079292552651411, fluctuations: 0.0\n",
      "step: 25860 loss: 1.921939 time elapsed: 33.5774 learning rate: 0.000032, scenario: 0, slope: -0.00020839763602450306, fluctuations: 0.0\n",
      "step: 25870 loss: 1.919830 time elapsed: 33.5911 learning rate: 0.000032, scenario: 0, slope: -0.00020886512447948824, fluctuations: 0.0\n",
      "step: 25880 loss: 1.917716 time elapsed: 33.6046 learning rate: 0.000032, scenario: 0, slope: -0.00020933168824076426, fluctuations: 0.0\n",
      "step: 25890 loss: 1.915597 time elapsed: 33.6183 learning rate: 0.000032, scenario: 0, slope: -0.0002097973053939733, fluctuations: 0.0\n",
      "step: 25900 loss: 1.913474 time elapsed: 33.6314 learning rate: 0.000032, scenario: 0, slope: -0.00021021553477090066, fluctuations: 0.0\n",
      "step: 25910 loss: 1.911346 time elapsed: 33.6450 learning rate: 0.000032, scenario: 0, slope: -0.00021072562079226074, fluctuations: 0.0\n",
      "step: 25920 loss: 1.909214 time elapsed: 33.6575 learning rate: 0.000032, scenario: 0, slope: -0.00021118828254882848, fluctuations: 0.0\n",
      "step: 25930 loss: 1.907077 time elapsed: 33.6708 learning rate: 0.000032, scenario: 0, slope: -0.00021164992357566433, fluctuations: 0.0\n",
      "step: 25940 loss: 1.904936 time elapsed: 33.6837 learning rate: 0.000032, scenario: 0, slope: -0.00021211052674420416, fluctuations: 0.0\n",
      "step: 25950 loss: 1.902789 time elapsed: 33.6961 learning rate: 0.000032, scenario: 0, slope: -0.00021257007519637572, fluctuations: 0.0\n",
      "step: 25960 loss: 1.900639 time elapsed: 33.7086 learning rate: 0.000032, scenario: 0, slope: -0.0002130285523052036, fluctuations: 0.0\n",
      "step: 25970 loss: 1.898484 time elapsed: 33.7211 learning rate: 0.000032, scenario: 0, slope: -0.0002134859416483316, fluctuations: 0.0\n",
      "step: 25980 loss: 1.896324 time elapsed: 33.7338 learning rate: 0.000032, scenario: 0, slope: -0.00021394222698639092, fluctuations: 0.0\n",
      "step: 25990 loss: 1.894160 time elapsed: 33.7463 learning rate: 0.000032, scenario: 0, slope: -0.00021439739224535234, fluctuations: 0.0\n",
      "step: 26000 loss: 1.891991 time elapsed: 33.7587 learning rate: 0.000032, scenario: 0, slope: -0.0002148060701473525, fluctuations: 0.0\n",
      "step: 26010 loss: 1.889818 time elapsed: 33.7729 learning rate: 0.000032, scenario: 0, slope: -0.0002153042989719873, fluctuations: 0.0\n",
      "step: 26020 loss: 1.887641 time elapsed: 33.7881 learning rate: 0.000032, scenario: 0, slope: -0.00021575600899933077, fluctuations: 0.0\n",
      "step: 26030 loss: 1.885459 time elapsed: 33.8030 learning rate: 0.000032, scenario: 0, slope: -0.00021620653604833255, fluctuations: 0.0\n",
      "step: 26040 loss: 1.883272 time elapsed: 33.8162 learning rate: 0.000032, scenario: 0, slope: -0.00021665586469616916, fluctuations: 0.0\n",
      "step: 26050 loss: 1.881082 time elapsed: 33.8296 learning rate: 0.000032, scenario: 0, slope: -0.00021710397962688253, fluctuations: 0.0\n",
      "step: 26060 loss: 1.878886 time elapsed: 33.8432 learning rate: 0.000032, scenario: 0, slope: -0.00021755086562651982, fluctuations: 0.0\n",
      "step: 26070 loss: 1.876687 time elapsed: 33.8559 learning rate: 0.000032, scenario: 0, slope: -0.00021799650757835798, fluctuations: 0.0\n",
      "step: 26080 loss: 1.874483 time elapsed: 33.8686 learning rate: 0.000032, scenario: 0, slope: -0.00021844089045959192, fluctuations: 0.0\n",
      "step: 26090 loss: 1.872274 time elapsed: 33.8809 learning rate: 0.000032, scenario: 0, slope: -0.00021888399933824696, fluctuations: 0.0\n",
      "step: 26100 loss: 1.870061 time elapsed: 33.8933 learning rate: 0.000032, scenario: 0, slope: -0.00021928169578675549, fluctuations: 0.0\n",
      "step: 26110 loss: 1.867844 time elapsed: 33.9065 learning rate: 0.000032, scenario: 0, slope: -0.00021976633579938026, fluctuations: 0.0\n",
      "step: 26120 loss: 1.865623 time elapsed: 33.9191 learning rate: 0.000032, scenario: 0, slope: -0.00022020553395123854, fluctuations: 0.0\n",
      "step: 26130 loss: 1.863397 time elapsed: 33.9322 learning rate: 0.000032, scenario: 0, slope: -0.0002206433992359124, fluctuations: 0.0\n",
      "step: 26140 loss: 1.861167 time elapsed: 33.9449 learning rate: 0.000032, scenario: 0, slope: -0.00022107991714485377, fluctuations: 0.0\n",
      "step: 26150 loss: 1.858932 time elapsed: 33.9583 learning rate: 0.000032, scenario: 0, slope: -0.0002215150732505714, fluctuations: 0.0\n",
      "step: 26160 loss: 1.856694 time elapsed: 33.9748 learning rate: 0.000032, scenario: 0, slope: -0.00022194885320538609, fluctuations: 0.0\n",
      "step: 26170 loss: 1.854451 time elapsed: 33.9909 learning rate: 0.000032, scenario: 0, slope: -0.00022238124274139297, fluctuations: 0.0\n",
      "step: 26180 loss: 1.852204 time elapsed: 34.0079 learning rate: 0.000032, scenario: 0, slope: -0.0002228122276696955, fluctuations: 0.0\n",
      "step: 26190 loss: 1.849952 time elapsed: 34.0211 learning rate: 0.000032, scenario: 0, slope: -0.0002232417938806079, fluctuations: 0.0\n",
      "step: 26200 loss: 1.847697 time elapsed: 34.0347 learning rate: 0.000032, scenario: 0, slope: -0.0002236271788689531, fluctuations: 0.0\n",
      "step: 26210 loss: 1.845437 time elapsed: 34.0484 learning rate: 0.000032, scenario: 0, slope: -0.0002240966141058374, fluctuations: 0.0\n",
      "step: 26220 loss: 1.843173 time elapsed: 34.0614 learning rate: 0.000032, scenario: 0, slope: -0.00022452184029553198, fluctuations: 0.0\n",
      "step: 26230 loss: 1.840905 time elapsed: 34.0740 learning rate: 0.000032, scenario: 0, slope: -0.0002249455921186416, fluctuations: 0.0\n",
      "step: 26240 loss: 1.838632 time elapsed: 34.0865 learning rate: 0.000032, scenario: 0, slope: -0.0002253678558613522, fluctuations: 0.0\n",
      "step: 26250 loss: 1.836356 time elapsed: 34.0990 learning rate: 0.000032, scenario: 0, slope: -0.00022578861788969542, fluctuations: 0.0\n",
      "step: 26260 loss: 1.834075 time elapsed: 34.1118 learning rate: 0.000032, scenario: 0, slope: -0.00022620786465006988, fluctuations: 0.0\n",
      "step: 26270 loss: 1.831791 time elapsed: 34.1243 learning rate: 0.000032, scenario: 0, slope: -0.0002266255826699027, fluctuations: 0.0\n",
      "step: 26280 loss: 1.829502 time elapsed: 34.1368 learning rate: 0.000032, scenario: 0, slope: -0.00022704175855800272, fluctuations: 0.0\n",
      "step: 26290 loss: 1.827209 time elapsed: 34.1493 learning rate: 0.000032, scenario: 0, slope: -0.00022745637900540287, fluctuations: 0.0\n",
      "step: 26300 loss: 1.824912 time elapsed: 34.1617 learning rate: 0.000032, scenario: 0, slope: -0.00022782819657298523, fluctuations: 0.0\n",
      "step: 26310 loss: 1.822611 time elapsed: 34.1748 learning rate: 0.000032, scenario: 0, slope: -0.000228280900756366, fluctuations: 0.0\n",
      "step: 26320 loss: 1.820306 time elapsed: 34.1874 learning rate: 0.000032, scenario: 0, slope: -0.00022869077585855701, fluctuations: 0.0\n",
      "step: 26330 loss: 1.817997 time elapsed: 34.2007 learning rate: 0.000032, scenario: 0, slope: -0.00022909904311849177, fluctuations: 0.0\n",
      "step: 26340 loss: 1.815684 time elapsed: 34.2145 learning rate: 0.000032, scenario: 0, slope: -0.00022950568964853768, fluctuations: 0.0\n",
      "step: 26350 loss: 1.813367 time elapsed: 34.2288 learning rate: 0.000032, scenario: 0, slope: -0.00022991070264719536, fluctuations: 0.0\n",
      "step: 26360 loss: 1.811046 time elapsed: 34.2428 learning rate: 0.000032, scenario: 0, slope: -0.0002303140694005081, fluctuations: 0.0\n",
      "step: 26370 loss: 1.808721 time elapsed: 34.2557 learning rate: 0.000032, scenario: 0, slope: -0.00023071577728274247, fluctuations: 0.0\n",
      "step: 26380 loss: 1.806393 time elapsed: 34.2689 learning rate: 0.000032, scenario: 0, slope: -0.00023111581375754128, fluctuations: 0.0\n",
      "step: 26390 loss: 1.804060 time elapsed: 34.2821 learning rate: 0.000032, scenario: 0, slope: -0.0002315141663783643, fluctuations: 0.0\n",
      "step: 26400 loss: 1.801723 time elapsed: 34.2952 learning rate: 0.000032, scenario: 0, slope: -0.0002318712338283868, fluctuations: 0.0\n",
      "step: 26410 loss: 1.799383 time elapsed: 34.3095 learning rate: 0.000032, scenario: 0, slope: -0.00023230577072847744, fluctuations: 0.0\n",
      "step: 26420 loss: 1.797038 time elapsed: 34.3226 learning rate: 0.000032, scenario: 0, slope: -0.00023269899802356547, fluctuations: 0.0\n",
      "step: 26430 loss: 1.794690 time elapsed: 34.3363 learning rate: 0.000032, scenario: 0, slope: -0.0002330904925981541, fluctuations: 0.0\n",
      "step: 26440 loss: 1.792338 time elapsed: 34.3493 learning rate: 0.000032, scenario: 0, slope: -0.00023348024247020908, fluctuations: 0.0\n",
      "step: 26450 loss: 1.789983 time elapsed: 34.3621 learning rate: 0.000032, scenario: 0, slope: -0.000233868235753298, fluctuations: 0.0\n",
      "step: 26460 loss: 1.787623 time elapsed: 34.3746 learning rate: 0.000032, scenario: 0, slope: -0.00023425446065766147, fluctuations: 0.0\n",
      "step: 26470 loss: 1.785260 time elapsed: 34.3870 learning rate: 0.000032, scenario: 0, slope: -0.00023463890549120267, fluctuations: 0.0\n",
      "step: 26480 loss: 1.782893 time elapsed: 34.4012 learning rate: 0.000032, scenario: 0, slope: -0.00023502155866058933, fluctuations: 0.0\n",
      "step: 26490 loss: 1.780522 time elapsed: 34.4159 learning rate: 0.000032, scenario: 0, slope: -0.00023540240867163078, fluctuations: 0.0\n",
      "step: 26500 loss: 1.778147 time elapsed: 34.4300 learning rate: 0.000032, scenario: 0, slope: -0.00023574362256274542, fluctuations: 0.0\n",
      "step: 26510 loss: 1.775769 time elapsed: 34.4476 learning rate: 0.000032, scenario: 0, slope: -0.00023615865374762018, fluctuations: 0.0\n",
      "step: 26520 loss: 1.773387 time elapsed: 34.4631 learning rate: 0.000032, scenario: 0, slope: -0.00023653402633133563, fluctuations: 0.0\n",
      "step: 26530 loss: 1.771002 time elapsed: 34.4771 learning rate: 0.000032, scenario: 0, slope: -0.00023690755079647336, fluctuations: 0.0\n",
      "step: 26540 loss: 1.768613 time elapsed: 34.4915 learning rate: 0.000032, scenario: 0, slope: -0.0002372792161613807, fluctuations: 0.0\n",
      "step: 26550 loss: 1.766220 time elapsed: 34.5058 learning rate: 0.000032, scenario: 0, slope: -0.00023764901155001462, fluctuations: 0.0\n",
      "step: 26560 loss: 1.763823 time elapsed: 34.5194 learning rate: 0.000032, scenario: 0, slope: -0.0002380169261922427, fluctuations: 0.0\n",
      "step: 26570 loss: 1.761424 time elapsed: 34.5330 learning rate: 0.000032, scenario: 0, slope: -0.00023838294942532645, fluctuations: 0.0\n",
      "step: 26580 loss: 1.759020 time elapsed: 34.5464 learning rate: 0.000032, scenario: 0, slope: -0.00023874707069458987, fluctuations: 0.0\n",
      "step: 26590 loss: 1.756613 time elapsed: 34.5605 learning rate: 0.000032, scenario: 0, slope: -0.00023910927955410726, fluctuations: 0.0\n",
      "step: 26600 loss: 1.754202 time elapsed: 34.5733 learning rate: 0.000032, scenario: 0, slope: -0.00023943362387259974, fluctuations: 0.0\n",
      "step: 26610 loss: 1.751788 time elapsed: 34.5868 learning rate: 0.000032, scenario: 0, slope: -0.00023982791881171542, fluctuations: 0.0\n",
      "step: 26620 loss: 1.749371 time elapsed: 34.5992 learning rate: 0.000032, scenario: 0, slope: -0.00024018432887119897, fluctuations: 0.0\n",
      "step: 26630 loss: 1.746950 time elapsed: 34.6128 learning rate: 0.000032, scenario: 0, slope: -0.00024053878584556684, fluctuations: 0.0\n",
      "step: 26640 loss: 1.744526 time elapsed: 34.6272 learning rate: 0.000032, scenario: 0, slope: -0.0002408912798469687, fluctuations: 0.0\n",
      "step: 26650 loss: 1.742098 time elapsed: 34.6413 learning rate: 0.000032, scenario: 0, slope: -0.0002412418011020506, fluctuations: 0.0\n",
      "step: 26660 loss: 1.739666 time elapsed: 34.6550 learning rate: 0.000032, scenario: 0, slope: -0.00024159033995199193, fluctuations: 0.0\n",
      "step: 26670 loss: 1.737232 time elapsed: 34.6683 learning rate: 0.000032, scenario: 0, slope: -0.0002419368868538033, fluctuations: 0.0\n",
      "step: 26680 loss: 1.734794 time elapsed: 34.6814 learning rate: 0.000032, scenario: 0, slope: -0.00024228143238090875, fluctuations: 0.0\n",
      "step: 26690 loss: 1.732353 time elapsed: 34.6939 learning rate: 0.000032, scenario: 0, slope: -0.0002426239672240552, fluctuations: 0.0\n",
      "step: 26700 loss: 1.729908 time elapsed: 34.7062 learning rate: 0.000032, scenario: 0, slope: -0.0002429305218487981, fluctuations: 0.0\n",
      "step: 26710 loss: 1.727460 time elapsed: 34.7193 learning rate: 0.000032, scenario: 0, slope: -0.0002433029682111337, fluctuations: 0.0\n",
      "step: 26720 loss: 1.725009 time elapsed: 34.7317 learning rate: 0.000032, scenario: 0, slope: -0.00024363941632883466, fluctuations: 0.0\n",
      "step: 26730 loss: 1.722555 time elapsed: 34.7442 learning rate: 0.000032, scenario: 0, slope: -0.00024397381771092296, fluctuations: 0.0\n",
      "step: 26740 loss: 1.720097 time elapsed: 34.7567 learning rate: 0.000032, scenario: 0, slope: -0.00024430616344794156, fluctuations: 0.0\n",
      "step: 26750 loss: 1.717636 time elapsed: 34.7693 learning rate: 0.000032, scenario: 0, slope: -0.0002446362003084196, fluctuations: 0.0\n",
      "step: 26760 loss: 1.715639 time elapsed: 34.7819 learning rate: 0.000032, scenario: 0, slope: -0.0002444626072258089, fluctuations: 0.0\n",
      "step: 26770 loss: 1.729759 time elapsed: 34.7944 learning rate: 0.000032, scenario: -1, slope: -0.00010941235287443296, fluctuations: 0.01\n",
      "step: 26780 loss: 1.723335 time elapsed: 34.8071 learning rate: 0.000029, scenario: -1, slope: -8.216312630718714e-05, fluctuations: 0.03\n",
      "step: 26790 loss: 1.709572 time elapsed: 34.8199 learning rate: 0.000026, scenario: -1, slope: -9.831582038553409e-05, fluctuations: 0.07\n",
      "step: 26800 loss: 1.707370 time elapsed: 34.8331 learning rate: 0.000024, scenario: -1, slope: -0.00012256343433344954, fluctuations: 0.1\n",
      "step: 26810 loss: 1.705865 time elapsed: 34.8489 learning rate: 0.000022, scenario: -1, slope: -0.00015776071545723678, fluctuations: 0.14\n",
      "step: 26820 loss: 1.704180 time elapsed: 34.8641 learning rate: 0.000021, scenario: 0, slope: -0.00019603937308866368, fluctuations: 0.16\n",
      "step: 26830 loss: 1.702759 time elapsed: 34.8778 learning rate: 0.000021, scenario: 0, slope: -0.00023240375463341555, fluctuations: 0.16\n",
      "step: 26840 loss: 1.701368 time elapsed: 34.8921 learning rate: 0.000021, scenario: 0, slope: -0.00026581876051606486, fluctuations: 0.16\n",
      "step: 26850 loss: 1.699958 time elapsed: 34.9060 learning rate: 0.000021, scenario: 0, slope: -0.00030340216669043476, fluctuations: 0.16\n",
      "step: 26860 loss: 1.698552 time elapsed: 34.9196 learning rate: 0.000021, scenario: 0, slope: -0.0003528030198708493, fluctuations: 0.16\n",
      "step: 26870 loss: 1.697146 time elapsed: 34.9327 learning rate: 0.000021, scenario: 0, slope: -0.00021444950958081574, fluctuations: 0.15\n",
      "step: 26880 loss: 1.695739 time elapsed: 34.9459 learning rate: 0.000020, scenario: -1, slope: -0.0001613314219455635, fluctuations: 0.12\n",
      "step: 26890 loss: 1.694427 time elapsed: 34.9597 learning rate: 0.000018, scenario: -1, slope: -0.00014779279564311367, fluctuations: 0.08\n",
      "step: 26900 loss: 1.693236 time elapsed: 34.9732 learning rate: 0.000017, scenario: -1, slope: -0.00014093159654496315, fluctuations: 0.05\n",
      "step: 26910 loss: 1.692147 time elapsed: 34.9872 learning rate: 0.000015, scenario: -1, slope: -0.00013724518734084274, fluctuations: 0.01\n",
      "step: 26920 loss: 1.691088 time elapsed: 35.0000 learning rate: 0.000017, scenario: 1, slope: -0.0001334494566534808, fluctuations: 0.0\n",
      "step: 26930 loss: 1.689918 time elapsed: 35.0127 learning rate: 0.000018, scenario: 1, slope: -0.00012919950938305097, fluctuations: 0.0\n",
      "step: 26940 loss: 1.688622 time elapsed: 35.0254 learning rate: 0.000020, scenario: 1, slope: -0.00012545873022181935, fluctuations: 0.0\n",
      "step: 26950 loss: 1.687186 time elapsed: 35.0385 learning rate: 0.000022, scenario: 1, slope: -0.00012309242672213155, fluctuations: 0.0\n",
      "step: 26960 loss: 1.685596 time elapsed: 35.0528 learning rate: 0.000025, scenario: 1, slope: -0.0001229191004790803, fluctuations: 0.0\n",
      "step: 26970 loss: 1.683835 time elapsed: 35.0678 learning rate: 0.000027, scenario: 1, slope: -0.00012569676972894365, fluctuations: 0.0\n",
      "step: 26980 loss: 1.681886 time elapsed: 35.0819 learning rate: 0.000030, scenario: 1, slope: -0.00013208673676317478, fluctuations: 0.0\n",
      "step: 26990 loss: 1.679728 time elapsed: 35.0958 learning rate: 0.000033, scenario: 1, slope: -0.00014233461714079635, fluctuations: 0.0\n",
      "step: 27000 loss: 1.677342 time elapsed: 35.1094 learning rate: 0.000036, scenario: 1, slope: -0.00015450031216685203, fluctuations: 0.0\n",
      "step: 27010 loss: 1.677368 time elapsed: 35.1231 learning rate: 0.000039, scenario: 0, slope: -0.00017048969279487048, fluctuations: 0.0\n",
      "step: 27020 loss: 1.754060 time elapsed: 35.1359 learning rate: 0.000037, scenario: -1, slope: 0.000637476876253437, fluctuations: 0.03\n",
      "step: 27030 loss: 1.741049 time elapsed: 35.1490 learning rate: 0.000034, scenario: -1, slope: 0.000820564349950365, fluctuations: 0.07\n",
      "step: 27040 loss: 1.683941 time elapsed: 35.1617 learning rate: 0.000030, scenario: -1, slope: 0.0007297650967531113, fluctuations: 0.12\n",
      "step: 27050 loss: 1.677014 time elapsed: 35.1742 learning rate: 0.000027, scenario: -1, slope: 0.00048208711416637765, fluctuations: 0.15\n",
      "step: 27060 loss: 1.670758 time elapsed: 35.1879 learning rate: 0.000025, scenario: -1, slope: 0.00023047069709872063, fluctuations: 0.19\n",
      "step: 27070 loss: 1.666920 time elapsed: 35.2014 learning rate: 0.000022, scenario: -1, slope: -4.254108980091875e-05, fluctuations: 0.23\n",
      "step: 27080 loss: 1.666325 time elapsed: 35.2154 learning rate: 0.000022, scenario: 0, slope: -0.0003420045516246972, fluctuations: 0.26\n",
      "step: 27090 loss: 1.665335 time elapsed: 35.2301 learning rate: 0.000022, scenario: 0, slope: -0.0006560076913426166, fluctuations: 0.27\n",
      "step: 27100 loss: 1.664472 time elapsed: 35.2437 learning rate: 0.000022, scenario: 0, slope: -0.0009656729991134566, fluctuations: 0.27\n",
      "step: 27110 loss: 1.663658 time elapsed: 35.2586 learning rate: 0.000022, scenario: 0, slope: -0.0014997929258795835, fluctuations: 0.27\n",
      "step: 27120 loss: 1.662832 time elapsed: 35.2732 learning rate: 0.000022, scenario: 0, slope: -0.0007229310451780295, fluctuations: 0.23\n",
      "step: 27130 loss: 1.662009 time elapsed: 35.2875 learning rate: 0.000022, scenario: 0, slope: -0.00022964952580102434, fluctuations: 0.19\n",
      "step: 27140 loss: 1.661187 time elapsed: 35.3007 learning rate: 0.000021, scenario: -1, slope: -0.00013576221845706794, fluctuations: 0.15\n",
      "step: 27150 loss: 1.660411 time elapsed: 35.3143 learning rate: 0.000019, scenario: -1, slope: -0.0001001306970390218, fluctuations: 0.11\n",
      "step: 27160 loss: 1.659708 time elapsed: 35.3269 learning rate: 0.000017, scenario: -1, slope: -8.651805201554592e-05, fluctuations: 0.07\n",
      "step: 27170 loss: 1.659070 time elapsed: 35.3397 learning rate: 0.000016, scenario: -1, slope: -8.233533340815104e-05, fluctuations: 0.03\n",
      "step: 27180 loss: 1.658490 time elapsed: 35.3531 learning rate: 0.000015, scenario: 1, slope: -7.881737869070388e-05, fluctuations: 0.0\n",
      "step: 27190 loss: 1.657905 time elapsed: 35.3664 learning rate: 0.000016, scenario: 1, slope: -7.539527456133354e-05, fluctuations: 0.0\n",
      "step: 27200 loss: 1.657257 time elapsed: 35.3792 learning rate: 0.000018, scenario: 1, slope: -7.243210101568403e-05, fluctuations: 0.0\n",
      "step: 27210 loss: 1.656545 time elapsed: 35.3928 learning rate: 0.000019, scenario: 1, slope: -6.933452936022343e-05, fluctuations: 0.0\n",
      "step: 27220 loss: 1.655755 time elapsed: 35.4065 learning rate: 0.000021, scenario: 1, slope: -6.748808710194304e-05, fluctuations: 0.0\n",
      "step: 27230 loss: 1.654881 time elapsed: 35.4192 learning rate: 0.000024, scenario: 1, slope: -6.70935284659051e-05, fluctuations: 0.0\n",
      "step: 27240 loss: 1.653911 time elapsed: 35.4325 learning rate: 0.000026, scenario: 1, slope: -6.862730920738568e-05, fluctuations: 0.0\n",
      "step: 27250 loss: 1.652836 time elapsed: 35.4447 learning rate: 0.000029, scenario: 1, slope: -7.237585819449288e-05, fluctuations: 0.0\n",
      "step: 27260 loss: 1.651645 time elapsed: 35.4589 learning rate: 0.000032, scenario: 1, slope: -7.82285033441476e-05, fluctuations: 0.0\n",
      "step: 27270 loss: 1.650325 time elapsed: 35.4745 learning rate: 0.000035, scenario: 1, slope: -8.591826333754104e-05, fluctuations: 0.0\n",
      "step: 27280 loss: 1.648863 time elapsed: 35.4890 learning rate: 0.000039, scenario: 1, slope: -9.506180943105544e-05, fluctuations: 0.0\n",
      "step: 27290 loss: 1.647244 time elapsed: 35.5024 learning rate: 0.000043, scenario: 1, slope: -0.00010529926888772264, fluctuations: 0.0\n",
      "step: 27300 loss: 1.645451 time elapsed: 35.5156 learning rate: 0.000047, scenario: 1, slope: -0.00011547061579709161, fluctuations: 0.0\n",
      "step: 27310 loss: 1.643484 time elapsed: 35.5312 learning rate: 0.000052, scenario: 1, slope: -0.00012920076481692073, fluctuations: 0.0\n",
      "step: 27320 loss: 2.322728 time elapsed: 35.5461 learning rate: 0.000054, scenario: -1, slope: 0.00087214775332122, fluctuations: 0.01\n",
      "step: 27330 loss: 1.936301 time elapsed: 35.5605 learning rate: 0.000049, scenario: -1, slope: 0.0016976553097030897, fluctuations: 0.05\n",
      "step: 27340 loss: 1.763784 time elapsed: 35.5756 learning rate: 0.000044, scenario: -1, slope: 0.0016929483472928786, fluctuations: 0.09\n",
      "step: 27350 loss: 1.674048 time elapsed: 35.5908 learning rate: 0.000040, scenario: -1, slope: 0.0013290978490390424, fluctuations: 0.13\n",
      "step: 27360 loss: 1.652422 time elapsed: 35.6056 learning rate: 0.000036, scenario: -1, slope: 0.0008705430217685587, fluctuations: 0.17\n",
      "step: 27370 loss: 1.636452 time elapsed: 35.6195 learning rate: 0.000033, scenario: -1, slope: 0.0003626731739030635, fluctuations: 0.21\n",
      "step: 27380 loss: 1.637180 time elapsed: 35.6344 learning rate: 0.000030, scenario: 0, slope: -0.00020452748291809508, fluctuations: 0.24\n",
      "step: 27390 loss: 1.635204 time elapsed: 35.6490 learning rate: 0.000030, scenario: 0, slope: -0.0007653222439131877, fluctuations: 0.28\n",
      "step: 27400 loss: 1.634206 time elapsed: 35.6631 learning rate: 0.000030, scenario: 0, slope: -0.0013517115990096583, fluctuations: 0.31\n",
      "step: 27410 loss: 1.633563 time elapsed: 35.6796 learning rate: 0.000030, scenario: 0, slope: -0.0022579590122180186, fluctuations: 0.34\n",
      "step: 27420 loss: 1.632824 time elapsed: 35.6935 learning rate: 0.000030, scenario: 0, slope: -0.0015688696214182043, fluctuations: 0.33\n",
      "step: 27430 loss: 1.632109 time elapsed: 35.7069 learning rate: 0.000030, scenario: 0, slope: -0.0006176826846130355, fluctuations: 0.28\n",
      "step: 27440 loss: 1.631410 time elapsed: 35.7208 learning rate: 0.000030, scenario: 0, slope: -0.00020642121319337708, fluctuations: 0.24\n",
      "step: 27450 loss: 1.630709 time elapsed: 35.7341 learning rate: 0.000029, scenario: -1, slope: -0.00011508009938029332, fluctuations: 0.2\n",
      "step: 27460 loss: 1.630059 time elapsed: 35.7472 learning rate: 0.000026, scenario: -1, slope: -8.829983888412928e-05, fluctuations: 0.16\n",
      "step: 27470 loss: 1.629470 time elapsed: 35.7606 learning rate: 0.000023, scenario: -1, slope: -7.755275961399434e-05, fluctuations: 0.12\n",
      "step: 27480 loss: 1.628935 time elapsed: 35.7735 learning rate: 0.000021, scenario: -1, slope: -6.985626067392995e-05, fluctuations: 0.09\n",
      "step: 27490 loss: 1.628450 time elapsed: 35.7867 learning rate: 0.000019, scenario: -1, slope: -6.646152085107988e-05, fluctuations: 0.06\n",
      "step: 27500 loss: 1.628009 time elapsed: 35.7993 learning rate: 0.000017, scenario: -1, slope: -6.375034234071699e-05, fluctuations: 0.03\n",
      "step: 27510 loss: 1.627600 time elapsed: 35.8125 learning rate: 0.000017, scenario: 1, slope: -6.016641363272061e-05, fluctuations: 0.0\n",
      "step: 27520 loss: 1.627167 time elapsed: 35.8249 learning rate: 0.000019, scenario: 1, slope: -5.628828036408812e-05, fluctuations: 0.0\n",
      "step: 27530 loss: 1.626686 time elapsed: 35.8379 learning rate: 0.000021, scenario: 1, slope: -5.266421896277981e-05, fluctuations: 0.0\n",
      "step: 27540 loss: 1.626153 time elapsed: 35.8504 learning rate: 0.000023, scenario: 1, slope: -4.9813035807456296e-05, fluctuations: 0.0\n",
      "step: 27550 loss: 1.625562 time elapsed: 35.8648 learning rate: 0.000026, scenario: 1, slope: -4.8267548608041364e-05, fluctuations: 0.0\n",
      "step: 27560 loss: 1.624907 time elapsed: 35.8786 learning rate: 0.000029, scenario: 1, slope: -4.83482209937008e-05, fluctuations: 0.0\n",
      "step: 27570 loss: 1.624180 time elapsed: 35.8926 learning rate: 0.000031, scenario: 1, slope: -5.0087634153964214e-05, fluctuations: 0.0\n",
      "step: 27580 loss: 1.623374 time elapsed: 35.9058 learning rate: 0.000035, scenario: 1, slope: -5.342445603620756e-05, fluctuations: 0.0\n",
      "step: 27590 loss: 1.622480 time elapsed: 35.9186 learning rate: 0.000038, scenario: 1, slope: -5.821769947000534e-05, fluctuations: 0.0\n",
      "step: 27600 loss: 1.621490 time elapsed: 35.9312 learning rate: 0.000042, scenario: 1, slope: -6.359437265764033e-05, fluctuations: 0.0\n",
      "step: 27610 loss: 1.620402 time elapsed: 35.9445 learning rate: 0.000046, scenario: 1, slope: -7.118551063638328e-05, fluctuations: 0.0\n",
      "step: 27620 loss: 1.619198 time elapsed: 35.9569 learning rate: 0.000051, scenario: 1, slope: -7.883990570333721e-05, fluctuations: 0.0\n",
      "step: 27630 loss: 1.617864 time elapsed: 35.9694 learning rate: 0.000057, scenario: 1, slope: -8.727496930241125e-05, fluctuations: 0.0\n",
      "step: 27640 loss: 1.616386 time elapsed: 35.9817 learning rate: 0.000063, scenario: 1, slope: -9.658519129578311e-05, fluctuations: 0.0\n",
      "step: 27650 loss: 1.614750 time elapsed: 35.9938 learning rate: 0.000069, scenario: 1, slope: -0.00010687442537063617, fluctuations: 0.0\n",
      "step: 27660 loss: 1.612938 time elapsed: 36.0065 learning rate: 0.000076, scenario: 1, slope: -0.00011825668091835649, fluctuations: 0.0\n",
      "step: 27670 loss: 1.610932 time elapsed: 36.0191 learning rate: 0.000084, scenario: 1, slope: -0.00013085696113671104, fluctuations: 0.0\n",
      "step: 27680 loss: 1.608713 time elapsed: 36.0316 learning rate: 0.000093, scenario: 1, slope: -0.00014481213857689518, fluctuations: 0.0\n",
      "step: 27690 loss: 1.606437 time elapsed: 36.0441 learning rate: 0.000103, scenario: 1, slope: -0.00016014920247990158, fluctuations: 0.0\n",
      "step: 27700 loss: 5.140220 time elapsed: 36.0559 learning rate: 0.000100, scenario: -1, slope: 0.004616830193740195, fluctuations: 0.02\n",
      "step: 27710 loss: 2.257675 time elapsed: 36.0706 learning rate: 0.000091, scenario: -1, slope: 0.006361900464177457, fluctuations: 0.07\n",
      "step: 27720 loss: 1.881602 time elapsed: 36.0845 learning rate: 0.000082, scenario: -1, slope: 0.006065181087441284, fluctuations: 0.11\n",
      "step: 27730 loss: 1.612879 time elapsed: 36.0985 learning rate: 0.000074, scenario: -1, slope: 0.004797817516259129, fluctuations: 0.15\n",
      "step: 27740 loss: 1.639570 time elapsed: 36.1117 learning rate: 0.000067, scenario: -1, slope: 0.0029185227579623177, fluctuations: 0.18\n",
      "step: 27750 loss: 1.599165 time elapsed: 36.1276 learning rate: 0.000061, scenario: -1, slope: 0.0010892126570469108, fluctuations: 0.22\n",
      "step: 27760 loss: 1.600449 time elapsed: 36.1406 learning rate: 0.000057, scenario: 0, slope: -0.0009619562494212543, fluctuations: 0.25\n",
      "step: 27770 loss: 1.599114 time elapsed: 36.1547 learning rate: 0.000057, scenario: 0, slope: -0.003115785729681893, fluctuations: 0.28\n",
      "step: 27780 loss: 1.597382 time elapsed: 36.1685 learning rate: 0.000057, scenario: 0, slope: -0.005636817398236486, fluctuations: 0.31\n",
      "step: 27790 loss: 1.596146 time elapsed: 36.1818 learning rate: 0.000057, scenario: 0, slope: -0.009337020781622716, fluctuations: 0.35\n",
      "step: 27800 loss: 1.595294 time elapsed: 36.1946 learning rate: 0.000057, scenario: 0, slope: -0.0069600718290935126, fluctuations: 0.34\n",
      "step: 27810 loss: 1.594548 time elapsed: 36.2080 learning rate: 0.000057, scenario: 0, slope: -0.0020682500255116976, fluctuations: 0.29\n",
      "step: 27820 loss: 1.593801 time elapsed: 36.2208 learning rate: 0.000057, scenario: 0, slope: -0.0007096701332421276, fluctuations: 0.25\n",
      "step: 27830 loss: 1.593045 time elapsed: 36.2336 learning rate: 0.000057, scenario: 0, slope: -0.0003383158322767275, fluctuations: 0.21\n",
      "step: 27840 loss: 1.592288 time elapsed: 36.2465 learning rate: 0.000056, scenario: -1, slope: -0.0001354931577675536, fluctuations: 0.18\n",
      "step: 27850 loss: 1.591571 time elapsed: 36.2604 learning rate: 0.000051, scenario: -1, slope: -0.00010486288331886033, fluctuations: 0.14\n",
      "step: 27860 loss: 1.590921 time elapsed: 36.2746 learning rate: 0.000046, scenario: -1, slope: -8.354601523208422e-05, fluctuations: 0.11\n",
      "step: 27870 loss: 1.590330 time elapsed: 36.2892 learning rate: 0.000041, scenario: -1, slope: -7.609417720376804e-05, fluctuations: 0.08\n",
      "step: 27880 loss: 1.589794 time elapsed: 36.3028 learning rate: 0.000037, scenario: -1, slope: -7.217555218692172e-05, fluctuations: 0.05\n",
      "step: 27890 loss: 1.589307 time elapsed: 36.3161 learning rate: 0.000034, scenario: -1, slope: -6.915039778759823e-05, fluctuations: 0.02\n",
      "step: 27900 loss: 1.588853 time elapsed: 36.3294 learning rate: 0.000035, scenario: 1, slope: -6.610783881154148e-05, fluctuations: 0.0\n",
      "step: 27910 loss: 1.588364 time elapsed: 36.3441 learning rate: 0.000038, scenario: 1, slope: -6.179636204198831e-05, fluctuations: 0.0\n",
      "step: 27920 loss: 1.587823 time elapsed: 36.3581 learning rate: 0.000042, scenario: 1, slope: -5.811078670876029e-05, fluctuations: 0.0\n",
      "step: 27930 loss: 1.587223 time elapsed: 36.3719 learning rate: 0.000047, scenario: 1, slope: -5.5239268388406745e-05, fluctuations: 0.0\n",
      "step: 27940 loss: 1.586557 time elapsed: 36.3855 learning rate: 0.000052, scenario: 1, slope: -5.372945159158084e-05, fluctuations: 0.0\n",
      "step: 27950 loss: 1.585819 time elapsed: 36.3989 learning rate: 0.000057, scenario: 1, slope: -5.399995539969164e-05, fluctuations: 0.0\n",
      "step: 27960 loss: 1.585001 time elapsed: 36.4126 learning rate: 0.000063, scenario: 1, slope: -5.61163641864356e-05, fluctuations: 0.0\n",
      "step: 27970 loss: 1.584094 time elapsed: 36.4269 learning rate: 0.000070, scenario: 1, slope: -6.0000664039261384e-05, fluctuations: 0.0\n",
      "step: 27980 loss: 1.583089 time elapsed: 36.4411 learning rate: 0.000077, scenario: 1, slope: -6.54860114776592e-05, fluctuations: 0.0\n",
      "step: 27990 loss: 1.581974 time elapsed: 36.4584 learning rate: 0.000085, scenario: 1, slope: -7.231403557646869e-05, fluctuations: 0.0\n",
      "step: 28000 loss: 1.580740 time elapsed: 36.4737 learning rate: 0.000093, scenario: 1, slope: -7.932677988315688e-05, fluctuations: 0.0\n",
      "step: 28010 loss: 1.579384 time elapsed: 36.4893 learning rate: 0.000103, scenario: 1, slope: -8.881196123719184e-05, fluctuations: 0.0\n",
      "step: 28020 loss: 1.577884 time elapsed: 36.5042 learning rate: 0.000113, scenario: 1, slope: -9.833574228000161e-05, fluctuations: 0.0\n",
      "step: 28030 loss: 1.576223 time elapsed: 36.5179 learning rate: 0.000125, scenario: 1, slope: -0.00010882554673151983, fluctuations: 0.0\n",
      "step: 28040 loss: 1.574384 time elapsed: 36.5307 learning rate: 0.000138, scenario: 1, slope: -0.00012039751854652777, fluctuations: 0.0\n",
      "step: 28050 loss: 1.572347 time elapsed: 36.5443 learning rate: 0.000153, scenario: 1, slope: -0.0001331791483190419, fluctuations: 0.0\n",
      "step: 28060 loss: 6.146473 time elapsed: 36.5577 learning rate: 0.000163, scenario: -1, slope: 0.0029279673012794403, fluctuations: 0.0\n",
      "step: 28070 loss: 7.255582 time elapsed: 36.5721 learning rate: 0.000148, scenario: -1, slope: 0.01698248349699636, fluctuations: 0.04\n",
      "step: 28080 loss: 3.432061 time elapsed: 36.5861 learning rate: 0.000133, scenario: -1, slope: 0.017719676359392117, fluctuations: 0.08\n",
      "step: 28090 loss: 2.256960 time elapsed: 36.5997 learning rate: 0.000121, scenario: -1, slope: 0.01481691639826015, fluctuations: 0.12\n",
      "step: 28100 loss: 1.652383 time elapsed: 36.6128 learning rate: 0.000110, scenario: -1, slope: 0.011392781414261553, fluctuations: 0.16\n",
      "step: 28110 loss: 1.614879 time elapsed: 36.6274 learning rate: 0.000100, scenario: -1, slope: 0.006180709274577832, fluctuations: 0.2\n",
      "step: 28120 loss: 1.564732 time elapsed: 36.6411 learning rate: 0.000090, scenario: -1, slope: 0.0007908257637881132, fluctuations: 0.23\n",
      "step: 28130 loss: 1.566310 time elapsed: 36.6544 learning rate: 0.000088, scenario: 0, slope: -0.004620278008071744, fluctuations: 0.26\n",
      "step: 28140 loss: 1.565456 time elapsed: 36.6679 learning rate: 0.000088, scenario: 0, slope: -0.010501734448305444, fluctuations: 0.29\n",
      "step: 28150 loss: 1.563517 time elapsed: 36.6839 learning rate: 0.000088, scenario: 0, slope: -0.01778634490474619, fluctuations: 0.32\n",
      "step: 28160 loss: 1.562069 time elapsed: 36.6991 learning rate: 0.000088, scenario: 0, slope: -0.04136284408045595, fluctuations: 0.35\n",
      "step: 28170 loss: 1.561176 time elapsed: 36.7136 learning rate: 0.000088, scenario: 0, slope: -0.008368758173703313, fluctuations: 0.34\n",
      "step: 28180 loss: 1.560488 time elapsed: 36.7274 learning rate: 0.000088, scenario: 0, slope: -0.002062602621704183, fluctuations: 0.3\n",
      "step: 28190 loss: 1.559803 time elapsed: 36.7412 learning rate: 0.000088, scenario: 0, slope: -0.0008479971673756375, fluctuations: 0.26\n",
      "step: 28200 loss: 1.559106 time elapsed: 36.7545 learning rate: 0.000088, scenario: 0, slope: -0.0003837846184645025, fluctuations: 0.23\n",
      "step: 28210 loss: 1.558410 time elapsed: 36.7689 learning rate: 0.000087, scenario: -1, slope: -0.00015682390044828828, fluctuations: 0.19\n",
      "step: 28220 loss: 1.557739 time elapsed: 36.7817 learning rate: 0.000080, scenario: -1, slope: -0.00011346789582220118, fluctuations: 0.15\n",
      "step: 28230 loss: 1.557130 time elapsed: 36.7954 learning rate: 0.000072, scenario: -1, slope: -8.325029541729229e-05, fluctuations: 0.12\n",
      "step: 28240 loss: 1.556577 time elapsed: 36.8095 learning rate: 0.000065, scenario: -1, slope: -7.202892348764838e-05, fluctuations: 0.09\n",
      "step: 28250 loss: 1.556076 time elapsed: 36.8232 learning rate: 0.000059, scenario: -1, slope: -6.717975593160384e-05, fluctuations: 0.06\n",
      "step: 28260 loss: 1.555620 time elapsed: 36.8369 learning rate: 0.000053, scenario: -1, slope: -6.421392172879753e-05, fluctuations: 0.03\n",
      "step: 28270 loss: 1.555204 time elapsed: 36.8496 learning rate: 0.000052, scenario: 1, slope: -6.115752348167927e-05, fluctuations: 0.0\n",
      "step: 28280 loss: 1.554769 time elapsed: 36.8629 learning rate: 0.000058, scenario: 1, slope: -5.746273126167419e-05, fluctuations: 0.0\n",
      "step: 28290 loss: 1.554287 time elapsed: 36.8769 learning rate: 0.000064, scenario: 1, slope: -5.392021887369176e-05, fluctuations: 0.0\n",
      "step: 28300 loss: 1.553753 time elapsed: 36.8918 learning rate: 0.000070, scenario: 1, slope: -5.1269871930630474e-05, fluctuations: 0.0\n",
      "step: 28310 loss: 1.553165 time elapsed: 36.9076 learning rate: 0.000077, scenario: 1, slope: -4.9253598226943484e-05, fluctuations: 0.0\n",
      "step: 28320 loss: 1.552514 time elapsed: 36.9227 learning rate: 0.000085, scenario: 1, slope: -4.903201883304114e-05, fluctuations: 0.0\n",
      "step: 28330 loss: 1.551793 time elapsed: 36.9364 learning rate: 0.000094, scenario: 1, slope: -5.047981804091236e-05, fluctuations: 0.0\n",
      "step: 28340 loss: 1.550992 time elapsed: 36.9498 learning rate: 0.000104, scenario: 1, slope: -5.355048156699641e-05, fluctuations: 0.0\n",
      "step: 28350 loss: 1.550105 time elapsed: 36.9632 learning rate: 0.000114, scenario: 1, slope: -5.811718818222105e-05, fluctuations: 0.0\n",
      "step: 28360 loss: 1.549122 time elapsed: 36.9767 learning rate: 0.000126, scenario: 1, slope: -6.397092426388255e-05, fluctuations: 0.0\n",
      "step: 28370 loss: 1.548032 time elapsed: 36.9904 learning rate: 0.000140, scenario: 1, slope: -7.081909123151244e-05, fluctuations: 0.0\n",
      "step: 28380 loss: 1.546823 time elapsed: 37.0040 learning rate: 0.000154, scenario: 1, slope: -7.84517236122262e-05, fluctuations: 0.0\n",
      "step: 28390 loss: 1.545485 time elapsed: 37.0180 learning rate: 0.000170, scenario: 1, slope: -8.692654569578389e-05, fluctuations: 0.0\n",
      "step: 28400 loss: 1.544001 time elapsed: 37.0305 learning rate: 0.000186, scenario: 1, slope: -9.53530780839175e-05, fluctuations: 0.0\n",
      "step: 28410 loss: 1.542373 time elapsed: 37.0457 learning rate: 0.000206, scenario: 1, slope: -0.00010673896185577436, fluctuations: 0.0\n",
      "step: 28420 loss: 1.540571 time elapsed: 37.0594 learning rate: 0.000227, scenario: 1, slope: -0.00011817046192645182, fluctuations: 0.0\n",
      "step: 28430 loss: 1.538576 time elapsed: 37.0736 learning rate: 0.000251, scenario: 1, slope: -0.00013076121310820216, fluctuations: 0.0\n",
      "step: 28440 loss: 1.536366 time elapsed: 37.0886 learning rate: 0.000277, scenario: 1, slope: -0.00014465105437008415, fluctuations: 0.0\n",
      "step: 28450 loss: 3.139002 time elapsed: 37.1042 learning rate: 0.000302, scenario: -1, slope: 0.0009297724728729169, fluctuations: 0.0\n",
      "step: 28460 loss: 6.824523 time elapsed: 37.1196 learning rate: 0.000273, scenario: -1, slope: 0.031873516125371835, fluctuations: 0.03\n",
      "step: 28470 loss: 4.067430 time elapsed: 37.1339 learning rate: 0.000247, scenario: -1, slope: 0.036340228169846674, fluctuations: 0.06\n",
      "step: 28480 loss: 2.104088 time elapsed: 37.1469 learning rate: 0.000223, scenario: -1, slope: 0.030647087938726458, fluctuations: 0.1\n",
      "step: 28490 loss: 1.727931 time elapsed: 37.1611 learning rate: 0.000202, scenario: -1, slope: 0.021579951648870402, fluctuations: 0.13\n",
      "step: 28500 loss: 1.609022 time elapsed: 37.1749 learning rate: 0.000184, scenario: -1, slope: 0.014085915630247395, fluctuations: 0.17\n",
      "step: 28510 loss: 1.529690 time elapsed: 37.1898 learning rate: 0.000167, scenario: -1, slope: 0.0027521943387102893, fluctuations: 0.21\n",
      "step: 28520 loss: 1.534087 time elapsed: 37.2041 learning rate: 0.000164, scenario: 0, slope: -0.008488488610252838, fluctuations: 0.24\n",
      "step: 28530 loss: 1.526617 time elapsed: 37.2177 learning rate: 0.000164, scenario: 0, slope: -0.020365243496608797, fluctuations: 0.28\n",
      "step: 28540 loss: 1.525782 time elapsed: 37.2312 learning rate: 0.000164, scenario: 0, slope: -0.03451645873691893, fluctuations: 0.31\n",
      "step: 28550 loss: 1.524470 time elapsed: 37.2451 learning rate: 0.000164, scenario: 0, slope: -0.0531861273844629, fluctuations: 0.35\n",
      "step: 28560 loss: 1.523439 time elapsed: 37.2589 learning rate: 0.000164, scenario: 0, slope: -0.012775530213732587, fluctuations: 0.33\n",
      "step: 28570 loss: 1.522557 time elapsed: 37.2729 learning rate: 0.000164, scenario: 0, slope: -0.0032889003374348418, fluctuations: 0.29\n",
      "step: 28580 loss: 1.521692 time elapsed: 37.2867 learning rate: 0.000164, scenario: 0, slope: -0.0013570100079960715, fluctuations: 0.25\n",
      "step: 28590 loss: 1.520850 time elapsed: 37.3020 learning rate: 0.000164, scenario: 0, slope: -0.00044318606953672776, fluctuations: 0.22\n",
      "step: 28600 loss: 1.520003 time elapsed: 37.3174 learning rate: 0.000164, scenario: 0, slope: -0.0002752841812686679, fluctuations: 0.18\n",
      "step: 28610 loss: 1.519161 time elapsed: 37.3337 learning rate: 0.000155, scenario: -1, slope: -0.00014464202657945137, fluctuations: 0.14\n",
      "step: 28620 loss: 1.518381 time elapsed: 37.3476 learning rate: 0.000141, scenario: -1, slope: -0.00010161720071597875, fluctuations: 0.11\n",
      "step: 28630 loss: 1.517673 time elapsed: 37.3615 learning rate: 0.000127, scenario: -1, slope: -8.871313820974408e-05, fluctuations: 0.08\n",
      "step: 28640 loss: 1.517030 time elapsed: 37.3755 learning rate: 0.000115, scenario: -1, slope: -8.392265525664316e-05, fluctuations: 0.04\n",
      "step: 28650 loss: 1.516447 time elapsed: 37.3892 learning rate: 0.000104, scenario: -1, slope: -8.03046463501521e-05, fluctuations: 0.01\n",
      "step: 28660 loss: 1.515915 time elapsed: 37.4034 learning rate: 0.000101, scenario: 1, slope: -7.658151019805461e-05, fluctuations: 0.0\n",
      "step: 28670 loss: 1.515359 time elapsed: 37.4169 learning rate: 0.000112, scenario: 1, slope: -7.245111012171798e-05, fluctuations: 0.0\n",
      "step: 28680 loss: 1.514742 time elapsed: 37.4302 learning rate: 0.000124, scenario: 1, slope: -6.844194698141206e-05, fluctuations: 0.0\n",
      "step: 28690 loss: 1.514058 time elapsed: 37.4428 learning rate: 0.000137, scenario: 1, slope: -6.510534212235853e-05, fluctuations: 0.0\n",
      "step: 28700 loss: 1.513300 time elapsed: 37.4552 learning rate: 0.000149, scenario: 1, slope: -6.317560932045458e-05, fluctuations: 0.0\n",
      "step: 28710 loss: 1.512467 time elapsed: 37.4683 learning rate: 0.000165, scenario: 1, slope: -6.280211640314086e-05, fluctuations: 0.0\n",
      "step: 28720 loss: 1.511544 time elapsed: 37.4812 learning rate: 0.000182, scenario: 1, slope: -6.467151171434053e-05, fluctuations: 0.0\n",
      "step: 28730 loss: 1.510521 time elapsed: 37.4935 learning rate: 0.000201, scenario: 1, slope: -6.860689707028027e-05, fluctuations: 0.0\n",
      "step: 28740 loss: 1.509387 time elapsed: 37.5061 learning rate: 0.000223, scenario: 1, slope: -7.444416641132723e-05, fluctuations: 0.0\n",
      "step: 28750 loss: 1.508131 time elapsed: 37.5182 learning rate: 0.000246, scenario: 1, slope: -8.191386699898305e-05, fluctuations: 0.0\n",
      "step: 28760 loss: 1.506739 time elapsed: 37.5325 learning rate: 0.000272, scenario: 1, slope: -9.063934077702552e-05, fluctuations: 0.0\n",
      "step: 28770 loss: 1.505198 time elapsed: 37.5467 learning rate: 0.000300, scenario: 1, slope: -0.00010034925220445805, fluctuations: 0.0\n",
      "step: 28780 loss: 1.503491 time elapsed: 37.5600 learning rate: 0.000331, scenario: 1, slope: -0.00011111416244753923, fluctuations: 0.0\n",
      "step: 28790 loss: 1.501601 time elapsed: 37.5729 learning rate: 0.000366, scenario: 1, slope: -0.00012305254933701754, fluctuations: 0.0\n",
      "step: 28800 loss: 1.499510 time elapsed: 37.5856 learning rate: 0.000400, scenario: 1, slope: -0.00013490721959564142, fluctuations: 0.0\n",
      "step: 28810 loss: 111.470387 time elapsed: 37.5989 learning rate: 0.000410, scenario: -1, slope: 0.080343954076926, fluctuations: 0.0\n",
      "step: 28820 loss: 2.040615 time elapsed: 37.6117 learning rate: 0.000371, scenario: -1, slope: 0.09781383738540335, fluctuations: 0.05\n",
      "step: 28830 loss: 9.030702 time elapsed: 37.6240 learning rate: 0.000336, scenario: -1, slope: 0.11051415204682268, fluctuations: 0.09\n",
      "step: 28840 loss: 2.595385 time elapsed: 37.6365 learning rate: 0.000304, scenario: -1, slope: 0.09461280982685501, fluctuations: 0.13\n",
      "step: 28850 loss: 1.924592 time elapsed: 37.6491 learning rate: 0.000274, scenario: -1, slope: 0.06902682428522805, fluctuations: 0.17\n",
      "step: 28860 loss: 1.800223 time elapsed: 37.6616 learning rate: 0.000248, scenario: -1, slope: 0.034444225879131235, fluctuations: 0.2\n",
      "step: 28870 loss: 1.652187 time elapsed: 37.6742 learning rate: 0.000227, scenario: 0, slope: -0.00028865302346067447, fluctuations: 0.23\n",
      "step: 28880 loss: 1.523337 time elapsed: 37.6866 learning rate: 0.000227, scenario: 0, slope: -0.035036345376322595, fluctuations: 0.27\n",
      "step: 28890 loss: 1.495755 time elapsed: 37.6990 learning rate: 0.000227, scenario: 0, slope: -0.07556694168444897, fluctuations: 0.3\n",
      "step: 28900 loss: 1.496267 time elapsed: 37.7115 learning rate: 0.000227, scenario: 0, slope: -0.12146070188128595, fluctuations: 0.33\n",
      "step: 28910 loss: 1.495798 time elapsed: 37.7241 learning rate: 0.000227, scenario: 0, slope: -0.19410022264920498, fluctuations: 0.35\n",
      "step: 28920 loss: 1.493625 time elapsed: 37.7381 learning rate: 0.000227, scenario: 0, slope: -0.074628976648985, fluctuations: 0.33\n",
      "step: 28930 loss: 1.492174 time elapsed: 37.7521 learning rate: 0.000227, scenario: 0, slope: -0.020217354327920146, fluctuations: 0.33\n",
      "step: 28940 loss: 1.491367 time elapsed: 37.7659 learning rate: 0.000227, scenario: 0, slope: -0.008166389235495605, fluctuations: 0.32\n",
      "step: 28950 loss: 1.490643 time elapsed: 37.7788 learning rate: 0.000227, scenario: 0, slope: -0.0018051982928173108, fluctuations: 0.3\n",
      "step: 28960 loss: 1.489890 time elapsed: 37.7912 learning rate: 0.000227, scenario: 0, slope: -0.000866339582757723, fluctuations: 0.26\n",
      "step: 28970 loss: 1.489148 time elapsed: 37.8034 learning rate: 0.000227, scenario: 0, slope: -0.00026224840009628057, fluctuations: 0.23\n",
      "step: 28980 loss: 1.488422 time elapsed: 37.8159 learning rate: 0.000225, scenario: -1, slope: -0.00014754287076812922, fluctuations: 0.2\n",
      "step: 28990 loss: 1.487724 time elapsed: 37.8284 learning rate: 0.000205, scenario: -1, slope: -0.00010143010289724496, fluctuations: 0.17\n",
      "step: 29000 loss: 1.487093 time elapsed: 37.8406 learning rate: 0.000187, scenario: -1, slope: -8.31276924056273e-05, fluctuations: 0.14\n",
      "step: 29010 loss: 1.486518 time elapsed: 37.8536 learning rate: 0.000169, scenario: -1, slope: -7.579319240538284e-05, fluctuations: 0.1\n",
      "step: 29020 loss: 1.485997 time elapsed: 37.8661 learning rate: 0.000153, scenario: -1, slope: -7.07037207905238e-05, fluctuations: 0.07\n",
      "step: 29030 loss: 1.485525 time elapsed: 37.8786 learning rate: 0.000139, scenario: -1, slope: -6.72263268867195e-05, fluctuations: 0.04\n",
      "step: 29040 loss: 1.485099 time elapsed: 37.8911 learning rate: 0.000127, scenario: 1, slope: -6.393593083085172e-05, fluctuations: 0.0\n",
      "step: 29050 loss: 1.484677 time elapsed: 37.9036 learning rate: 0.000141, scenario: 1, slope: -5.974685265635858e-05, fluctuations: 0.0\n",
      "step: 29060 loss: 1.484211 time elapsed: 37.9165 learning rate: 0.000155, scenario: 1, slope: -5.5664754062105286e-05, fluctuations: 0.0\n",
      "step: 29070 loss: 1.483695 time elapsed: 37.9300 learning rate: 0.000172, scenario: 1, slope: -5.218410711056683e-05, fluctuations: 0.0\n",
      "step: 29080 loss: 1.483124 time elapsed: 37.9454 learning rate: 0.000189, scenario: 1, slope: -4.981899210556101e-05, fluctuations: 0.0\n",
      "step: 29090 loss: 1.482492 time elapsed: 37.9597 learning rate: 0.000209, scenario: 1, slope: -4.90180230431344e-05, fluctuations: 0.0\n",
      "step: 29100 loss: 1.481794 time elapsed: 37.9736 learning rate: 0.000229, scenario: 1, slope: -4.973831779882096e-05, fluctuations: 0.0\n",
      "step: 29110 loss: 1.481028 time elapsed: 37.9872 learning rate: 0.000253, scenario: 1, slope: -5.24174343593632e-05, fluctuations: 0.0\n",
      "step: 29120 loss: 1.480181 time elapsed: 38.0000 learning rate: 0.000279, scenario: 1, slope: -5.644135501182343e-05, fluctuations: 0.0\n",
      "step: 29130 loss: 1.479246 time elapsed: 38.0128 learning rate: 0.000308, scenario: 1, slope: -6.177619508503753e-05, fluctuations: 0.0\n",
      "step: 29140 loss: 1.478211 time elapsed: 38.0269 learning rate: 0.000341, scenario: 1, slope: -6.814261195469514e-05, fluctuations: 0.0\n",
      "step: 29150 loss: 1.477068 time elapsed: 38.0401 learning rate: 0.000376, scenario: 1, slope: -7.525362130982648e-05, fluctuations: 0.0\n",
      "step: 29160 loss: 1.475805 time elapsed: 38.0545 learning rate: 0.000416, scenario: 1, slope: -8.31034459303989e-05, fluctuations: 0.0\n",
      "step: 29170 loss: 1.474409 time elapsed: 38.0685 learning rate: 0.000459, scenario: 1, slope: -9.177599838871908e-05, fluctuations: 0.0\n",
      "step: 29180 loss: 1.472868 time elapsed: 38.0822 learning rate: 0.000507, scenario: 1, slope: -0.00010136347522301129, fluctuations: 0.0\n",
      "step: 29190 loss: 1.471165 time elapsed: 38.0969 learning rate: 0.000560, scenario: 1, slope: -0.00011196701560654024, fluctuations: 0.0\n",
      "step: 29200 loss: 1.469286 time elapsed: 38.1117 learning rate: 0.000613, scenario: 1, slope: -0.00012247029251277682, fluctuations: 0.0\n",
      "step: 29210 loss: 45.198214 time elapsed: 38.1278 learning rate: 0.000641, scenario: -1, slope: 0.02971437645754507, fluctuations: 0.0\n",
      "step: 29220 loss: 61.983123 time elapsed: 38.1436 learning rate: 0.000580, scenario: -1, slope: 0.21505286383255942, fluctuations: 0.04\n",
      "step: 29230 loss: 7.229171 time elapsed: 38.1584 learning rate: 0.000524, scenario: -1, slope: 0.2070932465430527, fluctuations: 0.09\n",
      "step: 29240 loss: 3.663492 time elapsed: 38.1729 learning rate: 0.000474, scenario: -1, slope: 0.18749387654088506, fluctuations: 0.13\n",
      "step: 29250 loss: 3.603531 time elapsed: 38.1868 learning rate: 0.000429, scenario: -1, slope: 0.1328344303984422, fluctuations: 0.16\n",
      "step: 29260 loss: 1.614934 time elapsed: 38.2017 learning rate: 0.000388, scenario: -1, slope: 0.07662261461570474, fluctuations: 0.2\n",
      "step: 29270 loss: 1.627703 time elapsed: 38.2151 learning rate: 0.000351, scenario: -1, slope: 0.01191829498970781, fluctuations: 0.23\n",
      "step: 29280 loss: 1.553843 time elapsed: 38.2294 learning rate: 0.000344, scenario: 0, slope: -0.053455840158465905, fluctuations: 0.26\n",
      "step: 29290 loss: 1.493581 time elapsed: 38.2440 learning rate: 0.000344, scenario: 0, slope: -0.1254148948323493, fluctuations: 0.3\n",
      "step: 29300 loss: 1.475530 time elapsed: 38.2582 learning rate: 0.000344, scenario: 0, slope: -0.2045985977400325, fluctuations: 0.32\n",
      "step: 29310 loss: 1.469946 time elapsed: 38.2725 learning rate: 0.000344, scenario: 0, slope: -0.4708885790759567, fluctuations: 0.35\n",
      "step: 29320 loss: 1.468820 time elapsed: 38.2866 learning rate: 0.000344, scenario: 0, slope: -0.08336088461541959, fluctuations: 0.34\n",
      "step: 29330 loss: 1.467774 time elapsed: 38.3000 learning rate: 0.000344, scenario: 0, slope: -0.04168178428292095, fluctuations: 0.32\n",
      "step: 29340 loss: 1.466689 time elapsed: 38.3135 learning rate: 0.000344, scenario: 0, slope: -0.009245217428470733, fluctuations: 0.3\n",
      "step: 29350 loss: 1.465681 time elapsed: 38.3272 learning rate: 0.000344, scenario: 0, slope: -0.0032507640274021536, fluctuations: 0.26\n",
      "step: 29360 loss: 1.464795 time elapsed: 38.3425 learning rate: 0.000344, scenario: 0, slope: -0.0015650212878903186, fluctuations: 0.22\n",
      "step: 29370 loss: 1.463967 time elapsed: 38.3582 learning rate: 0.000344, scenario: 0, slope: -0.0005214346963081094, fluctuations: 0.19\n",
      "step: 29380 loss: 1.463168 time elapsed: 38.3737 learning rate: 0.000344, scenario: 0, slope: -0.0002299084970111438, fluctuations: 0.16\n",
      "step: 29390 loss: 1.462388 time elapsed: 38.3881 learning rate: 0.000340, scenario: -1, slope: -0.00013929475751049027, fluctuations: 0.13\n",
      "step: 29400 loss: 1.461657 time elapsed: 38.4017 learning rate: 0.000311, scenario: -1, slope: -0.00010788186535109759, fluctuations: 0.1\n",
      "step: 29410 loss: 1.461001 time elapsed: 38.4155 learning rate: 0.000281, scenario: -1, slope: -8.962668304703743e-05, fluctuations: 0.07\n",
      "step: 29420 loss: 1.460414 time elapsed: 38.4297 learning rate: 0.000254, scenario: -1, slope: -8.323845813776667e-05, fluctuations: 0.03\n",
      "step: 29430 loss: 1.459888 time elapsed: 38.4437 learning rate: 0.000238, scenario: 1, slope: -7.770414664627572e-05, fluctuations: 0.0\n",
      "step: 29440 loss: 1.459362 time elapsed: 38.4582 learning rate: 0.000263, scenario: 1, slope: -7.25414069336736e-05, fluctuations: 0.0\n",
      "step: 29450 loss: 1.458784 time elapsed: 38.4724 learning rate: 0.000291, scenario: 1, slope: -6.80862850939249e-05, fluctuations: 0.0\n",
      "step: 29460 loss: 1.458149 time elapsed: 38.4865 learning rate: 0.000321, scenario: 1, slope: -6.444922539641858e-05, fluctuations: 0.0\n",
      "step: 29470 loss: 1.457452 time elapsed: 38.5021 learning rate: 0.000355, scenario: 1, slope: -6.193570694837379e-05, fluctuations: 0.0\n",
      "step: 29480 loss: 1.456685 time elapsed: 38.5169 learning rate: 0.000392, scenario: 1, slope: -6.0891794805451045e-05, fluctuations: 0.0\n",
      "step: 29490 loss: 1.455841 time elapsed: 38.5307 learning rate: 0.000433, scenario: 1, slope: -6.164297196586118e-05, fluctuations: 0.0\n",
      "step: 29500 loss: 1.454915 time elapsed: 38.5448 learning rate: 0.000473, scenario: 1, slope: -6.403552842289545e-05, fluctuations: 0.0\n",
      "step: 29510 loss: 1.453905 time elapsed: 38.5612 learning rate: 0.000523, scenario: 1, slope: -6.899282982651517e-05, fluctuations: 0.0\n",
      "step: 29520 loss: 1.452796 time elapsed: 38.5770 learning rate: 0.000577, scenario: 1, slope: -7.511174568650825e-05, fluctuations: 0.0\n",
      "step: 29530 loss: 1.451577 time elapsed: 38.5923 learning rate: 0.000638, scenario: 1, slope: -8.236724419911085e-05, fluctuations: 0.0\n",
      "step: 29540 loss: 1.450235 time elapsed: 38.6062 learning rate: 0.000704, scenario: 1, slope: -9.040906500376772e-05, fluctuations: 0.0\n",
      "step: 29550 loss: 1.448760 time elapsed: 38.6209 learning rate: 0.000778, scenario: 1, slope: -9.924745526531674e-05, fluctuations: 0.0\n",
      "step: 29560 loss: 1.447136 time elapsed: 38.6347 learning rate: 0.000860, scenario: 1, slope: -0.00010897844321997708, fluctuations: 0.0\n",
      "step: 29570 loss: 1.445349 time elapsed: 38.6480 learning rate: 0.000949, scenario: 1, slope: -0.00011970872273442852, fluctuations: 0.0\n",
      "step: 29580 loss: 1.443382 time elapsed: 38.6623 learning rate: 0.001049, scenario: 1, slope: -0.00013155637778681354, fluctuations: 0.0\n",
      "step: 29590 loss: 1.454288 time elapsed: 38.6766 learning rate: 0.001158, scenario: 1, slope: -0.00013557344006219726, fluctuations: 0.0\n",
      "step: 29600 loss: 35.375929 time elapsed: 38.6907 learning rate: 0.001085, scenario: -1, slope: 1.0080999685594905, fluctuations: 0.01\n",
      "step: 29610 loss: 154.843867 time elapsed: 38.7058 learning rate: 0.000981, scenario: -1, slope: 0.8967043994669076, fluctuations: 0.06\n",
      "step: 29620 loss: 35.227898 time elapsed: 38.7198 learning rate: 0.000888, scenario: -1, slope: 0.799480086598717, fluctuations: 0.11\n",
      "step: 29630 loss: 19.504329 time elapsed: 38.7331 learning rate: 0.000803, scenario: -1, slope: 0.6196171616499933, fluctuations: 0.14\n",
      "step: 29640 loss: 2.712497 time elapsed: 38.7462 learning rate: 0.000726, scenario: -1, slope: 0.40535729731552494, fluctuations: 0.18\n",
      "step: 29650 loss: 3.509772 time elapsed: 38.7618 learning rate: 0.000657, scenario: -1, slope: 0.15390411377865898, fluctuations: 0.21\n",
      "step: 29660 loss: 1.876573 time elapsed: 38.7772 learning rate: 0.000618, scenario: 0, slope: -0.08704700061455825, fluctuations: 0.25\n",
      "step: 29670 loss: 1.576380 time elapsed: 38.7931 learning rate: 0.000618, scenario: 0, slope: -0.36018344927813944, fluctuations: 0.28\n",
      "step: 29680 loss: 1.574965 time elapsed: 38.8071 learning rate: 0.000618, scenario: 0, slope: -0.6750636946671799, fluctuations: 0.31\n",
      "step: 29690 loss: 1.565154 time elapsed: 38.8217 learning rate: 0.000618, scenario: 0, slope: -1.100415905402671, fluctuations: 0.34\n",
      "step: 29700 loss: 1.540134 time elapsed: 38.8355 learning rate: 0.000618, scenario: 0, slope: -0.4032633753694522, fluctuations: 0.35\n",
      "step: 29710 loss: 1.524700 time elapsed: 38.8498 learning rate: 0.000618, scenario: 0, slope: -0.16103944921387053, fluctuations: 0.34\n",
      "step: 29720 loss: 1.517525 time elapsed: 38.8631 learning rate: 0.000618, scenario: 0, slope: -0.05745413381534877, fluctuations: 0.33\n",
      "step: 29730 loss: 1.512262 time elapsed: 38.8763 learning rate: 0.000618, scenario: 0, slope: -0.021844652614071067, fluctuations: 0.31\n",
      "step: 29740 loss: 1.507334 time elapsed: 38.8895 learning rate: 0.000618, scenario: 0, slope: -0.01087711928159276, fluctuations: 0.27\n",
      "step: 29750 loss: 1.503022 time elapsed: 38.9023 learning rate: 0.000618, scenario: 0, slope: -0.0031549167857388098, fluctuations: 0.24\n",
      "step: 29760 loss: 1.499270 time elapsed: 38.9150 learning rate: 0.000618, scenario: 0, slope: -0.0016010349550686614, fluctuations: 0.21\n",
      "step: 29770 loss: 1.495882 time elapsed: 38.9276 learning rate: 0.000618, scenario: 0, slope: -0.0009561540184438249, fluctuations: 0.18\n",
      "step: 29780 loss: 1.492779 time elapsed: 38.9400 learning rate: 0.000618, scenario: 0, slope: -0.0006954798868657725, fluctuations: 0.14\n",
      "step: 29790 loss: 1.489921 time elapsed: 38.9524 learning rate: 0.000618, scenario: 0, slope: -0.0005155563643307583, fluctuations: 0.11\n",
      "step: 29800 loss: 1.487269 time elapsed: 38.9649 learning rate: 0.000618, scenario: 0, slope: -0.00043561015915773973, fluctuations: 0.08\n",
      "step: 29810 loss: 1.484791 time elapsed: 38.9777 learning rate: 0.000618, scenario: 0, slope: -0.00037102190703925075, fluctuations: 0.05\n",
      "step: 29820 loss: 1.482464 time elapsed: 38.9917 learning rate: 0.000618, scenario: 0, slope: -0.0003380951038770972, fluctuations: 0.01\n",
      "step: 29830 loss: 1.480270 time elapsed: 39.0055 learning rate: 0.000618, scenario: 0, slope: -0.0003081151198474768, fluctuations: 0.0\n",
      "step: 29840 loss: 1.478191 time elapsed: 39.0198 learning rate: 0.000618, scenario: 0, slope: -0.00028293682857554115, fluctuations: 0.0\n",
      "step: 29850 loss: 1.476216 time elapsed: 39.0350 learning rate: 0.000618, scenario: 0, slope: -0.00026219179720459075, fluctuations: 0.0\n",
      "step: 29860 loss: 1.474332 time elapsed: 39.0479 learning rate: 0.000618, scenario: 0, slope: -0.0002446550277942026, fluctuations: 0.0\n",
      "step: 29870 loss: 1.472531 time elapsed: 39.0608 learning rate: 0.000618, scenario: 0, slope: -0.00022960362805961724, fluctuations: 0.0\n",
      "step: 29880 loss: 1.470805 time elapsed: 39.0735 learning rate: 0.000618, scenario: 0, slope: -0.00021653161887068978, fluctuations: 0.0\n",
      "step: 29890 loss: 1.469147 time elapsed: 39.0861 learning rate: 0.000618, scenario: 0, slope: -0.00020506132921864164, fluctuations: 0.0\n",
      "step: 29900 loss: 1.467549 time elapsed: 39.0985 learning rate: 0.000618, scenario: 0, slope: -0.0001958736996261617, fluctuations: 0.0\n",
      "step: 29910 loss: 1.466008 time elapsed: 39.1114 learning rate: 0.000618, scenario: 0, slope: -0.00018586737098144157, fluctuations: 0.0\n",
      "step: 29920 loss: 1.464518 time elapsed: 39.1241 learning rate: 0.000618, scenario: 0, slope: -0.00017776006682319198, fluctuations: 0.0\n",
      "step: 29930 loss: 1.463075 time elapsed: 39.1367 learning rate: 0.000618, scenario: 0, slope: -0.00017045564431147716, fluctuations: 0.0\n",
      "step: 29940 loss: 1.461675 time elapsed: 39.1494 learning rate: 0.000618, scenario: 0, slope: -0.00016384608781030933, fluctuations: 0.0\n",
      "step: 29950 loss: 1.460315 time elapsed: 39.1619 learning rate: 0.000618, scenario: 0, slope: -0.0001578430724610675, fluctuations: 0.0\n",
      "step: 29960 loss: 1.458991 time elapsed: 39.1746 learning rate: 0.000618, scenario: 0, slope: -0.0001523734949981399, fluctuations: 0.0\n",
      "step: 29970 loss: 1.457700 time elapsed: 39.1885 learning rate: 0.000618, scenario: 0, slope: -0.00014737615783486942, fluctuations: 0.0\n",
      "step: 29980 loss: 1.456395 time elapsed: 39.2034 learning rate: 0.000683, scenario: 1, slope: -0.00014288687017887435, fluctuations: 0.0\n",
      "step: 29990 loss: 1.454990 time elapsed: 39.2176 learning rate: 0.000754, scenario: 1, slope: -0.00013939375972525991, fluctuations: 0.0\n",
      "step: 30000 loss: 1.453478 time elapsed: 39.2305 learning rate: 0.000825, scenario: 1, slope: -0.0001375480385429303, fluctuations: 0.0\n",
      "step: 30010 loss: 1.451866 time elapsed: 39.2446 learning rate: 0.000911, scenario: 1, slope: -0.00013737081778281642, fluctuations: 0.0\n",
      "step: 30020 loss: 1.450138 time elapsed: 39.2582 learning rate: 0.001007, scenario: 1, slope: -0.0001394340029654021, fluctuations: 0.0\n",
      "step: 30030 loss: 1.448284 time elapsed: 39.2717 learning rate: 0.001112, scenario: 1, slope: -0.00014376762080311486, fluctuations: 0.0\n",
      "step: 30040 loss: 1.446318 time elapsed: 39.2850 learning rate: 0.001157, scenario: 0, slope: -0.0001503513871559065, fluctuations: 0.0\n",
      "step: 30050 loss: 1.444382 time elapsed: 39.2979 learning rate: 0.001157, scenario: 0, slope: -0.00015849384334386642, fluctuations: 0.0\n",
      "step: 30060 loss: 1.442499 time elapsed: 39.3124 learning rate: 0.001157, scenario: 0, slope: -0.0001669640606713195, fluctuations: 0.0\n",
      "step: 30070 loss: 1.440664 time elapsed: 39.3264 learning rate: 0.001157, scenario: 0, slope: -0.00017458080877176413, fluctuations: 0.0\n",
      "step: 30080 loss: 1.438871 time elapsed: 39.3401 learning rate: 0.001157, scenario: 0, slope: -0.00018032593861580566, fluctuations: 0.0\n",
      "step: 30090 loss: 1.437115 time elapsed: 39.3542 learning rate: 0.001157, scenario: 0, slope: -0.00018381830323445527, fluctuations: 0.0\n",
      "step: 30100 loss: 1.435392 time elapsed: 39.3679 learning rate: 0.001157, scenario: 0, slope: -0.00018503575069875954, fluctuations: 0.0\n",
      "step: 30110 loss: 1.433699 time elapsed: 39.3821 learning rate: 0.001157, scenario: 0, slope: -0.00018425245933096924, fluctuations: 0.0\n",
      "step: 30120 loss: 1.432031 time elapsed: 39.3972 learning rate: 0.001157, scenario: 0, slope: -0.00018177556961694247, fluctuations: 0.0\n",
      "step: 30130 loss: 1.430385 time elapsed: 39.4123 learning rate: 0.001157, scenario: 0, slope: -0.00017830390482133417, fluctuations: 0.0\n",
      "step: 30140 loss: 1.428760 time elapsed: 39.4271 learning rate: 0.001157, scenario: 0, slope: -0.00017471947628371732, fluctuations: 0.0\n",
      "step: 30150 loss: 1.427153 time elapsed: 39.4412 learning rate: 0.001157, scenario: 0, slope: -0.00017152540550776057, fluctuations: 0.0\n",
      "step: 30160 loss: 1.425561 time elapsed: 39.4552 learning rate: 0.001157, scenario: 0, slope: -0.00016870170170600919, fluctuations: 0.0\n",
      "step: 30170 loss: 1.423982 time elapsed: 39.4691 learning rate: 0.001157, scenario: 0, slope: -0.00016621026890645239, fluctuations: 0.0\n",
      "step: 30180 loss: 1.422412 time elapsed: 39.4826 learning rate: 0.001157, scenario: 0, slope: -0.00016402333777156832, fluctuations: 0.0\n",
      "step: 30190 loss: 1.420846 time elapsed: 39.4965 learning rate: 0.001157, scenario: 0, slope: -0.00016213117403935263, fluctuations: 0.0\n",
      "step: 30200 loss: 1.419272 time elapsed: 39.5102 learning rate: 0.001157, scenario: 0, slope: -0.0001607013332324133, fluctuations: 0.0\n",
      "step: 30210 loss: 1.417680 time elapsed: 39.5248 learning rate: 0.001157, scenario: 0, slope: -0.00015939412709906617, fluctuations: 0.0\n",
      "step: 30220 loss: 1.416108 time elapsed: 39.5387 learning rate: 0.001157, scenario: 0, slope: -0.00015859168599691512, fluctuations: 0.0\n",
      "step: 30230 loss: 1.414549 time elapsed: 39.5525 learning rate: 0.001157, scenario: 0, slope: -0.00015800465201335203, fluctuations: 0.0\n",
      "step: 30240 loss: 1.413000 time elapsed: 39.5656 learning rate: 0.001157, scenario: 0, slope: -0.00015753391735276616, fluctuations: 0.0\n",
      "step: 30250 loss: 1.411458 time elapsed: 39.5793 learning rate: 0.001157, scenario: 0, slope: -0.00015711177484934168, fluctuations: 0.0\n",
      "step: 30260 loss: 1.409920 time elapsed: 39.5941 learning rate: 0.001157, scenario: 0, slope: -0.00015668707030915097, fluctuations: 0.0\n",
      "step: 30270 loss: 1.408385 time elapsed: 39.6098 learning rate: 0.001157, scenario: 0, slope: -0.00015622154906191966, fluctuations: 0.0\n",
      "step: 30280 loss: 1.406851 time elapsed: 39.6247 learning rate: 0.001157, scenario: 0, slope: -0.00015569145322517328, fluctuations: 0.0\n",
      "step: 30290 loss: 1.405316 time elapsed: 39.6391 learning rate: 0.001157, scenario: 0, slope: -0.00015509754853154916, fluctuations: 0.0\n",
      "step: 30300 loss: 1.403781 time elapsed: 39.6526 learning rate: 0.001157, scenario: 0, slope: -0.00015455142453575275, fluctuations: 0.0\n",
      "step: 30310 loss: 1.402244 time elapsed: 39.6664 learning rate: 0.001157, scenario: 0, slope: -0.0001540307835604524, fluctuations: 0.0\n",
      "step: 30320 loss: 1.400703 time elapsed: 39.6797 learning rate: 0.001157, scenario: 0, slope: -0.00015378134637965082, fluctuations: 0.0\n",
      "step: 30330 loss: 1.399160 time elapsed: 39.6943 learning rate: 0.001157, scenario: 0, slope: -0.00015368758387710715, fluctuations: 0.0\n",
      "step: 30340 loss: 1.397612 time elapsed: 39.7090 learning rate: 0.001157, scenario: 0, slope: -0.00015371597525328865, fluctuations: 0.0\n",
      "step: 30350 loss: 1.396060 time elapsed: 39.7228 learning rate: 0.001157, scenario: 0, slope: -0.00015384669172471065, fluctuations: 0.0\n",
      "step: 30360 loss: 1.394504 time elapsed: 39.7364 learning rate: 0.001157, scenario: 0, slope: -0.00015406310158030758, fluctuations: 0.0\n",
      "step: 30370 loss: 1.397513 time elapsed: 39.7499 learning rate: 0.001157, scenario: 0, slope: -0.0001494977182895473, fluctuations: 0.0\n",
      "step: 30380 loss: 32.953131 time elapsed: 39.7637 learning rate: 0.001128, scenario: -1, slope: 0.03243450968216074, fluctuations: 0.0\n",
      "step: 30390 loss: 18.166283 time elapsed: 39.7773 learning rate: 0.001020, scenario: -1, slope: 0.11086629620067472, fluctuations: 0.02\n",
      "step: 30400 loss: 1.740694 time elapsed: 39.7907 learning rate: 0.000932, scenario: -1, slope: 0.10328953908555119, fluctuations: 0.06\n",
      "step: 30410 loss: 1.440670 time elapsed: 39.8062 learning rate: 0.000843, scenario: -1, slope: 0.08361479605302247, fluctuations: 0.1\n",
      "step: 30420 loss: 1.873558 time elapsed: 39.8222 learning rate: 0.000762, scenario: -1, slope: 0.05932034197981547, fluctuations: 0.14\n",
      "step: 30430 loss: 1.433765 time elapsed: 39.8374 learning rate: 0.000689, scenario: -1, slope: 0.03325520793793862, fluctuations: 0.19\n",
      "step: 30440 loss: 1.410877 time elapsed: 39.8522 learning rate: 0.000623, scenario: -1, slope: 0.0025264992073337606, fluctuations: 0.24\n",
      "step: 30450 loss: 1.404988 time elapsed: 39.8653 learning rate: 0.000623, scenario: 0, slope: -0.03336145093644246, fluctuations: 0.28\n",
      "step: 30460 loss: 1.391177 time elapsed: 39.8788 learning rate: 0.000623, scenario: 0, slope: -0.07145426665087605, fluctuations: 0.33\n",
      "step: 30470 loss: 1.386281 time elapsed: 39.8927 learning rate: 0.000623, scenario: 0, slope: -0.11835774957587583, fluctuations: 0.38\n",
      "step: 30480 loss: 1.385220 time elapsed: 39.9066 learning rate: 0.000623, scenario: 0, slope: -0.13144486115169285, fluctuations: 0.43\n",
      "step: 30490 loss: 1.384441 time elapsed: 39.9207 learning rate: 0.000623, scenario: 0, slope: -0.019888303659217704, fluctuations: 0.44\n",
      "step: 30500 loss: 1.383537 time elapsed: 39.9343 learning rate: 0.000623, scenario: 0, slope: -0.004940888190472491, fluctuations: 0.41\n",
      "step: 30510 loss: 1.382642 time elapsed: 39.9486 learning rate: 0.000623, scenario: 0, slope: -0.002619547046816887, fluctuations: 0.36\n",
      "step: 30520 loss: 1.381785 time elapsed: 39.9621 learning rate: 0.000623, scenario: 0, slope: -0.0006444355218275536, fluctuations: 0.32\n",
      "step: 30530 loss: 1.380950 time elapsed: 39.9763 learning rate: 0.000623, scenario: 0, slope: -0.00032584655542733723, fluctuations: 0.27\n",
      "step: 30540 loss: 1.380123 time elapsed: 39.9905 learning rate: 0.000617, scenario: -1, slope: -0.00013613946516534046, fluctuations: 0.23\n",
      "step: 30550 loss: 1.379334 time elapsed: 40.0045 learning rate: 0.000558, scenario: -1, slope: -9.422834207340669e-05, fluctuations: 0.18\n",
      "step: 30560 loss: 1.378621 time elapsed: 40.0200 learning rate: 0.000505, scenario: -1, slope: -9.055022131602262e-05, fluctuations: 0.13\n",
      "step: 30570 loss: 1.377976 time elapsed: 40.0360 learning rate: 0.000457, scenario: -1, slope: -8.379990229061596e-05, fluctuations: 0.08\n",
      "step: 30580 loss: 1.377392 time elapsed: 40.0514 learning rate: 0.000413, scenario: -1, slope: -8.03780220954704e-05, fluctuations: 0.03\n",
      "step: 30590 loss: 1.376857 time elapsed: 40.0683 learning rate: 0.000411, scenario: 1, slope: -7.656501254731205e-05, fluctuations: 0.0\n",
      "step: 30600 loss: 1.376292 time elapsed: 40.0815 learning rate: 0.000449, scenario: 1, slope: -7.282277235251478e-05, fluctuations: 0.0\n",
      "step: 30610 loss: 1.375671 time elapsed: 40.0982 learning rate: 0.000496, scenario: 1, slope: -6.85237128362053e-05, fluctuations: 0.0\n",
      "step: 30620 loss: 1.374985 time elapsed: 40.1123 learning rate: 0.000548, scenario: 1, slope: -6.53633199919101e-05, fluctuations: 0.0\n",
      "step: 30630 loss: 1.374225 time elapsed: 40.1262 learning rate: 0.000605, scenario: 1, slope: -6.341636304863889e-05, fluctuations: 0.0\n",
      "step: 30640 loss: 1.373385 time elapsed: 40.1407 learning rate: 0.000669, scenario: 1, slope: -6.318918009338134e-05, fluctuations: 0.0\n",
      "step: 30650 loss: 1.372454 time elapsed: 40.1559 learning rate: 0.000739, scenario: 1, slope: -6.507062900336334e-05, fluctuations: 0.0\n",
      "step: 30660 loss: 1.371426 time elapsed: 40.1706 learning rate: 0.000816, scenario: 1, slope: -6.904682072954055e-05, fluctuations: 0.0\n",
      "step: 30670 loss: 1.370287 time elapsed: 40.1845 learning rate: 0.000901, scenario: 1, slope: -7.49252659334104e-05, fluctuations: 0.0\n",
      "step: 30680 loss: 1.369029 time elapsed: 40.1976 learning rate: 0.000996, scenario: 1, slope: -8.241077233419435e-05, fluctuations: 0.0\n",
      "step: 30690 loss: 1.367637 time elapsed: 40.2121 learning rate: 0.001100, scenario: 1, slope: -9.110623475519772e-05, fluctuations: 0.0\n",
      "step: 30700 loss: 1.366098 time elapsed: 40.2295 learning rate: 0.001203, scenario: 1, slope: -9.975766655511639e-05, fluctuations: 0.0\n",
      "step: 30710 loss: 1.364412 time elapsed: 40.2498 learning rate: 0.001329, scenario: 1, slope: -0.00011141999802824032, fluctuations: 0.0\n",
      "step: 30720 loss: 1.364165 time elapsed: 40.2648 learning rate: 0.001468, scenario: 1, slope: -0.00012193300193327042, fluctuations: 0.0\n",
      "step: 30730 loss: 278.011580 time elapsed: 40.2818 learning rate: 0.001389, scenario: -1, slope: 0.7505538492935024, fluctuations: 0.01\n",
      "step: 30740 loss: 21.723695 time elapsed: 40.3009 learning rate: 0.001256, scenario: -1, slope: 0.7673420255463415, fluctuations: 0.06\n",
      "step: 30750 loss: 52.165459 time elapsed: 40.3145 learning rate: 0.001136, scenario: -1, slope: 0.8071483575871972, fluctuations: 0.1\n",
      "step: 30760 loss: 20.479768 time elapsed: 40.3285 learning rate: 0.001027, scenario: -1, slope: 0.6555808807595517, fluctuations: 0.14\n",
      "step: 30770 loss: 5.402335 time elapsed: 40.3427 learning rate: 0.000929, scenario: -1, slope: 0.4399946063539038, fluctuations: 0.18\n",
      "step: 30780 loss: 2.441615 time elapsed: 40.3566 learning rate: 0.000840, scenario: -1, slope: 0.2033671827940117, fluctuations: 0.22\n",
      "step: 30790 loss: 2.051133 time elapsed: 40.3706 learning rate: 0.000783, scenario: 0, slope: -0.06652006474079993, fluctuations: 0.25\n",
      "step: 30800 loss: 1.670753 time elapsed: 40.3841 learning rate: 0.000783, scenario: 0, slope: -0.3109593976683593, fluctuations: 0.28\n",
      "step: 30810 loss: 1.591601 time elapsed: 40.3982 learning rate: 0.000783, scenario: 0, slope: -0.6530828983728456, fluctuations: 0.32\n",
      "step: 30820 loss: 1.518545 time elapsed: 40.4122 learning rate: 0.000783, scenario: 0, slope: -1.0889146215027226, fluctuations: 0.36\n",
      "step: 30830 loss: 1.501060 time elapsed: 40.4259 learning rate: 0.000783, scenario: 0, slope: -0.7928790919445632, fluctuations: 0.37\n",
      "step: 30840 loss: 1.482898 time elapsed: 40.4409 learning rate: 0.000783, scenario: 0, slope: -0.3820667498835691, fluctuations: 0.36\n",
      "step: 30850 loss: 1.471301 time elapsed: 40.4560 learning rate: 0.000783, scenario: 0, slope: -0.09611544573565088, fluctuations: 0.35\n",
      "step: 30860 loss: 1.462886 time elapsed: 40.4712 learning rate: 0.000783, scenario: 0, slope: -0.033778008306277085, fluctuations: 0.31\n",
      "step: 30870 loss: 1.455389 time elapsed: 40.4863 learning rate: 0.000783, scenario: 0, slope: -0.014898480275721673, fluctuations: 0.27\n",
      "step: 30880 loss: 1.449261 time elapsed: 40.5008 learning rate: 0.000783, scenario: 0, slope: -0.005688579435735461, fluctuations: 0.24\n",
      "step: 30890 loss: 1.443756 time elapsed: 40.5164 learning rate: 0.000783, scenario: 0, slope: -0.0027805159990507036, fluctuations: 0.2\n",
      "step: 30900 loss: 1.438870 time elapsed: 40.5304 learning rate: 0.000783, scenario: 0, slope: -0.0015930536343560454, fluctuations: 0.17\n",
      "step: 30910 loss: 1.434430 time elapsed: 40.5454 learning rate: 0.000783, scenario: 0, slope: -0.0010066291234910975, fluctuations: 0.13\n",
      "step: 30920 loss: 1.430372 time elapsed: 40.5593 learning rate: 0.000783, scenario: 0, slope: -0.0007561224210346778, fluctuations: 0.1\n",
      "step: 30930 loss: 1.426631 time elapsed: 40.5734 learning rate: 0.000783, scenario: 0, slope: -0.0006292162917487522, fluctuations: 0.06\n",
      "step: 30940 loss: 1.423164 time elapsed: 40.5877 learning rate: 0.000783, scenario: 0, slope: -0.0005439624734654105, fluctuations: 0.03\n",
      "step: 30950 loss: 1.419935 time elapsed: 40.6014 learning rate: 0.000783, scenario: 0, slope: -0.0004906417759832433, fluctuations: 0.0\n",
      "step: 30960 loss: 1.416915 time elapsed: 40.6144 learning rate: 0.000783, scenario: 0, slope: -0.00044083557917001413, fluctuations: 0.0\n",
      "step: 30970 loss: 1.414080 time elapsed: 40.6291 learning rate: 0.000783, scenario: 0, slope: -0.00040149989794275345, fluctuations: 0.0\n",
      "step: 30980 loss: 1.411410 time elapsed: 40.6433 learning rate: 0.000783, scenario: 0, slope: -0.0003689453836065833, fluctuations: 0.0\n",
      "step: 30990 loss: 1.408888 time elapsed: 40.6589 learning rate: 0.000783, scenario: 0, slope: -0.00034145335081879396, fluctuations: 0.0\n",
      "step: 31000 loss: 1.406499 time elapsed: 40.6741 learning rate: 0.000783, scenario: 0, slope: -0.00031998765383404147, fluctuations: 0.0\n",
      "step: 31010 loss: 1.404231 time elapsed: 40.6902 learning rate: 0.000783, scenario: 0, slope: -0.00029708691274196107, fluctuations: 0.0\n",
      "step: 31020 loss: 1.402071 time elapsed: 40.7043 learning rate: 0.000783, scenario: 0, slope: -0.0002788502048702743, fluctuations: 0.0\n",
      "step: 31030 loss: 1.400010 time elapsed: 40.7182 learning rate: 0.000783, scenario: 0, slope: -0.0002626420419601506, fluctuations: 0.0\n",
      "step: 31040 loss: 1.398038 time elapsed: 40.7316 learning rate: 0.000783, scenario: 0, slope: -0.0002481513579241575, fluctuations: 0.0\n",
      "step: 31050 loss: 1.396148 time elapsed: 40.7450 learning rate: 0.000783, scenario: 0, slope: -0.00023513695714250696, fluctuations: 0.0\n",
      "step: 31060 loss: 1.394332 time elapsed: 40.7579 learning rate: 0.000783, scenario: 0, slope: -0.00022340679767530197, fluctuations: 0.0\n",
      "step: 31070 loss: 1.392584 time elapsed: 40.7706 learning rate: 0.000783, scenario: 0, slope: -0.0002128042936704804, fluctuations: 0.0\n",
      "step: 31080 loss: 1.390897 time elapsed: 40.7833 learning rate: 0.000783, scenario: 0, slope: -0.00020319928266992237, fluctuations: 0.0\n",
      "step: 31090 loss: 1.389267 time elapsed: 40.7961 learning rate: 0.000783, scenario: 0, slope: -0.00019448169887371905, fluctuations: 0.0\n",
      "step: 31100 loss: 1.387688 time elapsed: 40.8083 learning rate: 0.000783, scenario: 0, slope: -0.00018731635400872002, fluctuations: 0.0\n",
      "step: 31110 loss: 1.386156 time elapsed: 40.8215 learning rate: 0.000783, scenario: 0, slope: -0.00017934379535982167, fluctuations: 0.0\n",
      "step: 31120 loss: 1.384667 time elapsed: 40.8340 learning rate: 0.000783, scenario: 0, slope: -0.00017276977126082018, fluctuations: 0.0\n",
      "step: 31130 loss: 1.383218 time elapsed: 40.8467 learning rate: 0.000783, scenario: 0, slope: -0.00016677153822704878, fluctuations: 0.0\n",
      "step: 31140 loss: 1.381805 time elapsed: 40.8599 learning rate: 0.000783, scenario: 0, slope: -0.00016129234408325378, fluctuations: 0.0\n",
      "step: 31150 loss: 1.380426 time elapsed: 40.8741 learning rate: 0.000783, scenario: 0, slope: -0.00015628108950187374, fluctuations: 0.0\n",
      "step: 31160 loss: 1.379078 time elapsed: 40.8881 learning rate: 0.000783, scenario: 0, slope: -0.0001516914160856883, fluctuations: 0.0\n",
      "step: 31170 loss: 1.377759 time elapsed: 40.9025 learning rate: 0.000783, scenario: 0, slope: -0.0001474810209905932, fluctuations: 0.0\n",
      "step: 31180 loss: 1.376467 time elapsed: 40.9155 learning rate: 0.000783, scenario: 0, slope: -0.00014361120765917003, fluctuations: 0.0\n",
      "step: 31190 loss: 1.375199 time elapsed: 40.9282 learning rate: 0.000783, scenario: 0, slope: -0.000140046686754884, fluctuations: 0.0\n",
      "step: 31200 loss: 1.373937 time elapsed: 40.9407 learning rate: 0.000831, scenario: 1, slope: -0.00013708783906527091, fluctuations: 0.0\n",
      "step: 31210 loss: 1.372593 time elapsed: 40.9539 learning rate: 0.000918, scenario: 1, slope: -0.00013415071952079197, fluctuations: 0.0\n",
      "step: 31220 loss: 1.371138 time elapsed: 40.9665 learning rate: 0.001014, scenario: 1, slope: -0.0001327048513217935, fluctuations: 0.0\n",
      "step: 31230 loss: 1.369564 time elapsed: 40.9788 learning rate: 0.001120, scenario: 1, slope: -0.00013292708950167427, fluctuations: 0.0\n",
      "step: 31240 loss: 1.367860 time elapsed: 40.9914 learning rate: 0.001237, scenario: 1, slope: -0.00013517830676335963, fluctuations: 0.0\n",
      "step: 31250 loss: 1.366024 time elapsed: 41.0039 learning rate: 0.001314, scenario: 0, slope: -0.00013968819314077777, fluctuations: 0.0\n",
      "step: 31260 loss: 1.364175 time elapsed: 41.0164 learning rate: 0.001314, scenario: 0, slope: -0.00014618676009618535, fluctuations: 0.0\n",
      "step: 31270 loss: 1.362363 time elapsed: 41.0291 learning rate: 0.001314, scenario: 0, slope: -0.0001537322341259549, fluctuations: 0.0\n",
      "step: 31280 loss: 1.360583 time elapsed: 41.0417 learning rate: 0.001314, scenario: 0, slope: -0.0001613644615775056, fluctuations: 0.0\n",
      "step: 31290 loss: 1.358831 time elapsed: 41.0544 learning rate: 0.001314, scenario: 0, slope: -0.0001681648628352245, fluctuations: 0.0\n",
      "step: 31300 loss: 1.357103 time elapsed: 41.0664 learning rate: 0.001314, scenario: 0, slope: -0.0001728589400812768, fluctuations: 0.0\n",
      "step: 31310 loss: 1.355397 time elapsed: 41.0811 learning rate: 0.001314, scenario: 0, slope: -0.00017628705248837784, fluctuations: 0.0\n",
      "step: 31320 loss: 1.353710 time elapsed: 41.0950 learning rate: 0.001314, scenario: 0, slope: -0.00017725160825755358, fluctuations: 0.0\n",
      "step: 31330 loss: 1.352039 time elapsed: 41.1090 learning rate: 0.001314, scenario: 0, slope: -0.00017649418779224266, fluctuations: 0.0\n",
      "step: 31340 loss: 1.350383 time elapsed: 41.1220 learning rate: 0.001314, scenario: 0, slope: -0.00017456438550166948, fluctuations: 0.0\n",
      "step: 31350 loss: 1.348741 time elapsed: 41.1348 learning rate: 0.001314, scenario: 0, slope: -0.0001722448798966493, fluctuations: 0.0\n",
      "step: 31360 loss: 1.347110 time elapsed: 41.1477 learning rate: 0.001314, scenario: 0, slope: -0.00017014801960845254, fluctuations: 0.0\n",
      "step: 31370 loss: 1.345489 time elapsed: 41.1602 learning rate: 0.001314, scenario: 0, slope: -0.0001683000208857481, fluctuations: 0.0\n",
      "step: 31380 loss: 1.343878 time elapsed: 41.1730 learning rate: 0.001314, scenario: 0, slope: -0.00016666864402902173, fluctuations: 0.0\n",
      "step: 31390 loss: 1.342275 time elapsed: 41.1858 learning rate: 0.001314, scenario: 0, slope: -0.00016522547755056955, fluctuations: 0.0\n",
      "step: 31400 loss: 1.340679 time elapsed: 41.1982 learning rate: 0.001314, scenario: 0, slope: -0.0001640673093450438, fluctuations: 0.0\n",
      "step: 31410 loss: 1.339090 time elapsed: 41.2112 learning rate: 0.001314, scenario: 0, slope: -0.00016280982996314414, fluctuations: 0.0\n",
      "step: 31420 loss: 1.337507 time elapsed: 41.2238 learning rate: 0.001314, scenario: 0, slope: -0.0001617989310190463, fluctuations: 0.0\n",
      "step: 31430 loss: 1.335929 time elapsed: 41.2366 learning rate: 0.001314, scenario: 0, slope: -0.00016089840647594487, fluctuations: 0.0\n",
      "step: 31440 loss: 1.334356 time elapsed: 41.2491 learning rate: 0.001314, scenario: 0, slope: -0.00016009543469793275, fluctuations: 0.0\n",
      "step: 31450 loss: 1.332787 time elapsed: 41.2620 learning rate: 0.001314, scenario: 0, slope: -0.00015937904093019862, fluctuations: 0.0\n",
      "step: 31460 loss: 1.331221 time elapsed: 41.2759 learning rate: 0.001314, scenario: 0, slope: -0.00015873980040361204, fluctuations: 0.0\n",
      "step: 31470 loss: 1.329659 time elapsed: 41.2914 learning rate: 0.001314, scenario: 0, slope: -0.00015816959185484012, fluctuations: 0.0\n",
      "step: 31480 loss: 1.328100 time elapsed: 41.3067 learning rate: 0.001314, scenario: 0, slope: -0.00015766139296029163, fluctuations: 0.0\n",
      "step: 31490 loss: 1.326543 time elapsed: 41.3215 learning rate: 0.001314, scenario: 0, slope: -0.00015720911035052546, fluctuations: 0.0\n",
      "step: 31500 loss: 1.324989 time elapsed: 41.3351 learning rate: 0.001314, scenario: 0, slope: -0.00015684546476475304, fluctuations: 0.0\n",
      "step: 31510 loss: 1.323436 time elapsed: 41.3482 learning rate: 0.001314, scenario: 0, slope: -0.00015645173961788256, fluctuations: 0.0\n",
      "step: 31520 loss: 1.321885 time elapsed: 41.3606 learning rate: 0.001314, scenario: 0, slope: -0.0001561379489881419, fluctuations: 0.0\n",
      "step: 31530 loss: 1.320335 time elapsed: 41.3730 learning rate: 0.001314, scenario: 0, slope: -0.00015586248815406046, fluctuations: 0.0\n",
      "step: 31540 loss: 1.318787 time elapsed: 41.3860 learning rate: 0.001314, scenario: 0, slope: -0.00015562219745112244, fluctuations: 0.0\n",
      "step: 31550 loss: 1.317239 time elapsed: 41.3987 learning rate: 0.001314, scenario: 0, slope: -0.00015541427708423318, fluctuations: 0.0\n",
      "step: 31560 loss: 1.315692 time elapsed: 41.4122 learning rate: 0.001314, scenario: 0, slope: -0.00015523623774643837, fluctuations: 0.0\n",
      "step: 31570 loss: 1.314146 time elapsed: 41.4266 learning rate: 0.001314, scenario: 0, slope: -0.00015508585884403607, fluctuations: 0.0\n",
      "step: 31580 loss: 1.312599 time elapsed: 41.4407 learning rate: 0.001314, scenario: 0, slope: -0.00015496115306518193, fluctuations: 0.0\n",
      "step: 31590 loss: 1.311053 time elapsed: 41.4544 learning rate: 0.001314, scenario: 0, slope: -0.00015486033624428677, fluctuations: 0.0\n",
      "step: 31600 loss: 1.309507 time elapsed: 41.4691 learning rate: 0.001314, scenario: 0, slope: -0.00015478869490035126, fluctuations: 0.0\n",
      "step: 31610 loss: 1.307960 time elapsed: 41.4852 learning rate: 0.001314, scenario: 0, slope: -0.00015472409795899993, fluctuations: 0.0\n",
      "step: 31620 loss: 1.306413 time elapsed: 41.5001 learning rate: 0.001314, scenario: 0, slope: -0.00015468589389038126, fluctuations: 0.0\n",
      "step: 31630 loss: 1.304889 time elapsed: 41.5157 learning rate: 0.001314, scenario: 0, slope: -0.00015464052217683313, fluctuations: 0.0\n",
      "step: 31640 loss: 1.363855 time elapsed: 41.5293 learning rate: 0.001340, scenario: 1, slope: -9.061848850278288e-05, fluctuations: 0.0\n",
      "step: 31650 loss: 68.252662 time elapsed: 41.5431 learning rate: 0.001243, scenario: -1, slope: 0.12088602578402642, fluctuations: 0.01\n",
      "step: 31660 loss: 19.387696 time elapsed: 41.5571 learning rate: 0.001124, scenario: -1, slope: 0.16022263780752016, fluctuations: 0.03\n",
      "step: 31670 loss: 1.473723 time elapsed: 41.5714 learning rate: 0.001016, scenario: -1, slope: 0.14085564082819607, fluctuations: 0.07\n",
      "step: 31680 loss: 1.658214 time elapsed: 41.5856 learning rate: 0.000919, scenario: -1, slope: 0.11345805809568854, fluctuations: 0.11\n",
      "step: 31690 loss: 1.723351 time elapsed: 41.5988 learning rate: 0.000831, scenario: -1, slope: 0.07543387201721012, fluctuations: 0.15\n",
      "step: 31700 loss: 1.464995 time elapsed: 41.6127 learning rate: 0.000759, scenario: -1, slope: 0.042306196333085196, fluctuations: 0.2\n",
      "step: 31710 loss: 1.352793 time elapsed: 41.6272 learning rate: 0.000708, scenario: 0, slope: -0.011067689408791522, fluctuations: 0.25\n",
      "step: 31720 loss: 1.309113 time elapsed: 41.6413 learning rate: 0.000708, scenario: 0, slope: -0.06280347221638988, fluctuations: 0.3\n",
      "step: 31730 loss: 1.300437 time elapsed: 41.6552 learning rate: 0.000708, scenario: 0, slope: -0.12229396004763525, fluctuations: 0.35\n",
      "step: 31740 loss: 1.299595 time elapsed: 41.6689 learning rate: 0.000708, scenario: 0, slope: -0.19721520009982205, fluctuations: 0.4\n",
      "step: 31750 loss: 1.298642 time elapsed: 41.6841 learning rate: 0.000708, scenario: 0, slope: -0.08164395955597266, fluctuations: 0.42\n",
      "step: 31760 loss: 1.297397 time elapsed: 41.6996 learning rate: 0.000708, scenario: 0, slope: -0.021350179091368292, fluctuations: 0.43\n",
      "step: 31770 loss: 1.296244 time elapsed: 41.7148 learning rate: 0.000708, scenario: 0, slope: -0.009673421949534052, fluctuations: 0.44\n",
      "step: 31780 loss: 1.295216 time elapsed: 41.7298 learning rate: 0.000708, scenario: 0, slope: -0.0022980783514465492, fluctuations: 0.4\n",
      "step: 31790 loss: 1.294257 time elapsed: 41.7439 learning rate: 0.000708, scenario: 0, slope: -0.00021681734992303083, fluctuations: 0.36\n",
      "step: 31800 loss: 1.293332 time elapsed: 41.7579 learning rate: 0.000708, scenario: 0, slope: -0.00030099684006027627, fluctuations: 0.31\n",
      "step: 31810 loss: 1.292424 time elapsed: 41.7723 learning rate: 0.000708, scenario: 0, slope: -0.0001533664224746967, fluctuations: 0.26\n",
      "step: 31820 loss: 1.291530 time elapsed: 41.7861 learning rate: 0.000687, scenario: -1, slope: -0.0001180079961051574, fluctuations: 0.21\n",
      "step: 31830 loss: 1.290700 time elapsed: 41.8003 learning rate: 0.000621, scenario: -1, slope: -0.00010353037499682948, fluctuations: 0.16\n",
      "step: 31840 loss: 1.289955 time elapsed: 41.8138 learning rate: 0.000562, scenario: -1, slope: -9.67857716244308e-05, fluctuations: 0.11\n",
      "step: 31850 loss: 1.289285 time elapsed: 41.8277 learning rate: 0.000508, scenario: -1, slope: -8.968975421267246e-05, fluctuations: 0.09\n",
      "step: 31860 loss: 1.288681 time elapsed: 41.8417 learning rate: 0.000459, scenario: -1, slope: -8.571957970245527e-05, fluctuations: 0.04\n",
      "step: 31870 loss: 1.288134 time elapsed: 41.8555 learning rate: 0.000448, scenario: 1, slope: -8.173652278306463e-05, fluctuations: 0.0\n",
      "step: 31880 loss: 1.287565 time elapsed: 41.8699 learning rate: 0.000495, scenario: 1, slope: -7.669086238516423e-05, fluctuations: 0.0\n",
      "step: 31890 loss: 1.286937 time elapsed: 41.8839 learning rate: 0.000547, scenario: 1, slope: -7.187821455085879e-05, fluctuations: 0.0\n",
      "step: 31900 loss: 1.286245 time elapsed: 41.8987 learning rate: 0.000598, scenario: 1, slope: -6.82180523578979e-05, fluctuations: 0.0\n",
      "step: 31910 loss: 1.285487 time elapsed: 41.9143 learning rate: 0.000660, scenario: 1, slope: -6.52159410745184e-05, fluctuations: 0.0\n",
      "step: 31920 loss: 1.284653 time elapsed: 41.9293 learning rate: 0.000729, scenario: 1, slope: -6.442981488657556e-05, fluctuations: 0.0\n",
      "step: 31930 loss: 1.283733 time elapsed: 41.9440 learning rate: 0.000806, scenario: 1, slope: -6.585244228072887e-05, fluctuations: 0.0\n",
      "step: 31940 loss: 1.282718 time elapsed: 41.9579 learning rate: 0.000890, scenario: 1, slope: -6.940915958735482e-05, fluctuations: 0.0\n",
      "step: 31950 loss: 1.281599 time elapsed: 41.9717 learning rate: 0.000983, scenario: 1, slope: -7.488751946050126e-05, fluctuations: 0.0\n",
      "step: 31960 loss: 1.280365 time elapsed: 41.9860 learning rate: 0.001086, scenario: 1, slope: -8.197714024192379e-05, fluctuations: 0.0\n",
      "step: 31970 loss: 1.279005 time elapsed: 41.9991 learning rate: 0.001200, scenario: 1, slope: -9.026689149868867e-05, fluctuations: 0.0\n",
      "step: 31980 loss: 1.277505 time elapsed: 42.0127 learning rate: 0.001325, scenario: 1, slope: -9.946195841333398e-05, fluctuations: 0.0\n",
      "step: 31990 loss: 1.275851 time elapsed: 42.0269 learning rate: 0.001464, scenario: 1, slope: -0.00010962181915479226, fluctuations: 0.0\n",
      "step: 32000 loss: 1.274100 time elapsed: 42.0402 learning rate: 0.001601, scenario: 1, slope: -0.00011967008147005619, fluctuations: 0.0\n",
      "step: 32010 loss: 1630.205690 time elapsed: 42.0551 learning rate: 0.001576, scenario: -1, slope: 1.5163830039690533, fluctuations: 0.0\n",
      "step: 32020 loss: 48.440691 time elapsed: 42.0696 learning rate: 0.001426, scenario: -1, slope: 1.2850052586344305, fluctuations: 0.05\n",
      "step: 32030 loss: 97.215468 time elapsed: 42.0832 learning rate: 0.001289, scenario: -1, slope: 1.3912959097461788, fluctuations: 0.09\n",
      "step: 32040 loss: 38.831107 time elapsed: 42.0964 learning rate: 0.001166, scenario: -1, slope: 1.1517834640114701, fluctuations: 0.13\n",
      "step: 32050 loss: 8.899112 time elapsed: 42.1123 learning rate: 0.001055, scenario: -1, slope: 0.7999379652495486, fluctuations: 0.17\n",
      "step: 32060 loss: 2.442849 time elapsed: 42.1282 learning rate: 0.000954, scenario: -1, slope: 0.4135814943663694, fluctuations: 0.21\n",
      "step: 32070 loss: 2.678170 time elapsed: 42.1434 learning rate: 0.000871, scenario: 0, slope: -0.027965604129160163, fluctuations: 0.24\n",
      "step: 32080 loss: 1.994111 time elapsed: 42.1580 learning rate: 0.000871, scenario: 0, slope: -0.46376470004128995, fluctuations: 0.28\n",
      "step: 32090 loss: 1.586254 time elapsed: 42.1722 learning rate: 0.000871, scenario: 0, slope: -0.9677530956457996, fluctuations: 0.31\n",
      "step: 32100 loss: 1.594340 time elapsed: 42.1862 learning rate: 0.000871, scenario: 0, slope: -1.5368317713159623, fluctuations: 0.34\n",
      "step: 32110 loss: 1.528862 time elapsed: 42.2004 learning rate: 0.000871, scenario: 0, slope: -1.5088518643189366, fluctuations: 0.37\n",
      "step: 32120 loss: 1.499068 time elapsed: 42.2138 learning rate: 0.000871, scenario: 0, slope: -0.6631022438130882, fluctuations: 0.35\n",
      "step: 32130 loss: 1.482914 time elapsed: 42.2282 learning rate: 0.000871, scenario: 0, slope: -0.15175782650082745, fluctuations: 0.34\n",
      "step: 32140 loss: 1.466076 time elapsed: 42.2422 learning rate: 0.000871, scenario: 0, slope: -0.04994543020322577, fluctuations: 0.3\n",
      "step: 32150 loss: 1.453863 time elapsed: 42.2564 learning rate: 0.000871, scenario: 0, slope: -0.022442272465928906, fluctuations: 0.26\n",
      "step: 32160 loss: 1.442868 time elapsed: 42.2703 learning rate: 0.000871, scenario: 0, slope: -0.008557464678457498, fluctuations: 0.23\n",
      "step: 32170 loss: 1.433302 time elapsed: 42.2842 learning rate: 0.000871, scenario: 0, slope: -0.004596773009876786, fluctuations: 0.19\n",
      "step: 32180 loss: 1.424778 time elapsed: 42.2973 learning rate: 0.000871, scenario: 0, slope: -0.0023955899105610515, fluctuations: 0.16\n",
      "step: 32190 loss: 1.417087 time elapsed: 42.3108 learning rate: 0.000871, scenario: 0, slope: -0.0017711382131183325, fluctuations: 0.12\n",
      "step: 32200 loss: 1.410104 time elapsed: 42.3258 learning rate: 0.000871, scenario: 0, slope: -0.001365411229039163, fluctuations: 0.09\n",
      "step: 32210 loss: 1.403712 time elapsed: 42.3417 learning rate: 0.000871, scenario: 0, slope: -0.0010792426038427473, fluctuations: 0.06\n",
      "step: 32220 loss: 1.397829 time elapsed: 42.3568 learning rate: 0.000871, scenario: 0, slope: -0.0009551322091630314, fluctuations: 0.02\n",
      "step: 32230 loss: 1.392386 time elapsed: 42.3712 learning rate: 0.000871, scenario: 0, slope: -0.0008504469062423582, fluctuations: 0.0\n",
      "step: 32240 loss: 1.387329 time elapsed: 42.3845 learning rate: 0.000871, scenario: 0, slope: -0.0007617448216577586, fluctuations: 0.0\n",
      "step: 32250 loss: 1.382614 time elapsed: 42.3973 learning rate: 0.000871, scenario: 0, slope: -0.0006896429099434141, fluctuations: 0.0\n",
      "step: 32260 loss: 1.378201 time elapsed: 42.4100 learning rate: 0.000871, scenario: 0, slope: -0.0006297182291733448, fluctuations: 0.0\n",
      "step: 32270 loss: 1.374061 time elapsed: 42.4225 learning rate: 0.000871, scenario: 0, slope: -0.0005788516851124595, fluctuations: 0.0\n",
      "step: 32280 loss: 1.370165 time elapsed: 42.4347 learning rate: 0.000871, scenario: 0, slope: -0.0005350045293359914, fluctuations: 0.0\n",
      "step: 32290 loss: 1.366492 time elapsed: 42.4473 learning rate: 0.000871, scenario: 0, slope: -0.0004967787800327309, fluctuations: 0.0\n",
      "step: 32300 loss: 1.363019 time elapsed: 42.4597 learning rate: 0.000871, scenario: 0, slope: -0.0004663023860916097, fluctuations: 0.0\n",
      "step: 32310 loss: 1.359731 time elapsed: 42.4726 learning rate: 0.000871, scenario: 0, slope: -0.0004332344480857587, fluctuations: 0.0\n",
      "step: 32320 loss: 1.356610 time elapsed: 42.4848 learning rate: 0.000871, scenario: 0, slope: -0.0004065186116749863, fluctuations: 0.0\n",
      "step: 32330 loss: 1.353644 time elapsed: 42.4971 learning rate: 0.000871, scenario: 0, slope: -0.0003824923146100248, fluctuations: 0.0\n",
      "step: 32340 loss: 1.350819 time elapsed: 42.5093 learning rate: 0.000871, scenario: 0, slope: -0.00036077487695835247, fluctuations: 0.0\n",
      "step: 32350 loss: 1.348125 time elapsed: 42.5216 learning rate: 0.000871, scenario: 0, slope: -0.00034105939377456423, fluctuations: 0.0\n",
      "step: 32360 loss: 1.345551 time elapsed: 42.5345 learning rate: 0.000871, scenario: 0, slope: -0.00032309486833416597, fluctuations: 0.0\n",
      "step: 32370 loss: 1.343088 time elapsed: 42.5481 learning rate: 0.000871, scenario: 0, slope: -0.00030667352077380167, fluctuations: 0.0\n",
      "step: 32380 loss: 1.340729 time elapsed: 42.5627 learning rate: 0.000871, scenario: 0, slope: -0.0002916212594567876, fluctuations: 0.0\n",
      "step: 32390 loss: 1.338465 time elapsed: 42.5771 learning rate: 0.000871, scenario: 0, slope: -0.00027779062434209764, fluctuations: 0.0\n",
      "step: 32400 loss: 1.336289 time elapsed: 42.5905 learning rate: 0.000871, scenario: 0, slope: -0.00026628290370580974, fluctuations: 0.0\n",
      "step: 32410 loss: 1.334196 time elapsed: 42.6037 learning rate: 0.000871, scenario: 0, slope: -0.0002533068736532838, fluctuations: 0.0\n",
      "step: 32420 loss: 1.332179 time elapsed: 42.6166 learning rate: 0.000871, scenario: 0, slope: -0.00024245012751749354, fluctuations: 0.0\n",
      "step: 32430 loss: 1.330233 time elapsed: 42.6293 learning rate: 0.000871, scenario: 0, slope: -0.00023240222625488623, fluctuations: 0.0\n",
      "step: 32440 loss: 1.328354 time elapsed: 42.6425 learning rate: 0.000871, scenario: 0, slope: -0.00022309001807768492, fluctuations: 0.0\n",
      "step: 32450 loss: 1.326536 time elapsed: 42.6551 learning rate: 0.000871, scenario: 0, slope: -0.00021444868349638078, fluctuations: 0.0\n",
      "step: 32460 loss: 1.324777 time elapsed: 42.6677 learning rate: 0.000871, scenario: 0, slope: -0.00020642051767906936, fluctuations: 0.0\n",
      "step: 32470 loss: 1.323071 time elapsed: 42.6801 learning rate: 0.000871, scenario: 0, slope: -0.00019895393819918287, fluctuations: 0.0\n",
      "step: 32480 loss: 1.321416 time elapsed: 42.6925 learning rate: 0.000871, scenario: 0, slope: -0.00019200266781100206, fluctuations: 0.0\n",
      "step: 32490 loss: 1.319808 time elapsed: 42.7048 learning rate: 0.000871, scenario: 0, slope: -0.00018552505471191382, fluctuations: 0.0\n",
      "step: 32500 loss: 1.318244 time elapsed: 42.7170 learning rate: 0.000871, scenario: 0, slope: -0.00018006903033250478, fluctuations: 0.0\n",
      "step: 32510 loss: 1.316722 time elapsed: 42.7300 learning rate: 0.000871, scenario: 0, slope: -0.00017384398526863675, fluctuations: 0.0\n",
      "step: 32520 loss: 1.315239 time elapsed: 42.7424 learning rate: 0.000871, scenario: 0, slope: -0.0001685756405415329, fluctuations: 0.0\n",
      "step: 32530 loss: 1.313791 time elapsed: 42.7548 learning rate: 0.000871, scenario: 0, slope: -0.00016365041180444232, fluctuations: 0.0\n",
      "step: 32540 loss: 1.312378 time elapsed: 42.7691 learning rate: 0.000871, scenario: 0, slope: -0.00015904274589905316, fluctuations: 0.0\n",
      "step: 32550 loss: 1.310997 time elapsed: 42.7830 learning rate: 0.000871, scenario: 0, slope: -0.00015472932801004145, fluctuations: 0.0\n",
      "step: 32560 loss: 1.309646 time elapsed: 42.7976 learning rate: 0.000871, scenario: 0, slope: -0.00015068885086684918, fluctuations: 0.0\n",
      "step: 32570 loss: 1.308324 time elapsed: 42.8108 learning rate: 0.000871, scenario: 0, slope: -0.00014690181249475288, fluctuations: 0.0\n",
      "step: 32580 loss: 1.307027 time elapsed: 42.8234 learning rate: 0.000871, scenario: 0, slope: -0.00014335033824767953, fluctuations: 0.0\n",
      "step: 32590 loss: 1.305756 time elapsed: 42.8362 learning rate: 0.000871, scenario: 0, slope: -0.00014001802361031173, fluctuations: 0.0\n",
      "step: 32600 loss: 1.304509 time elapsed: 42.8485 learning rate: 0.000871, scenario: 0, slope: -0.00013719383801222342, fluctuations: 0.0\n",
      "step: 32610 loss: 1.303283 time elapsed: 42.8616 learning rate: 0.000871, scenario: 0, slope: -0.00013395178516358315, fluctuations: 0.0\n",
      "step: 32620 loss: 1.302078 time elapsed: 42.8744 learning rate: 0.000889, scenario: 1, slope: -0.00013119122406495746, fluctuations: 0.0\n",
      "step: 32630 loss: 1.300827 time elapsed: 42.8874 learning rate: 0.000982, scenario: 1, slope: -0.00012874703018094996, fluctuations: 0.0\n",
      "step: 32640 loss: 1.299468 time elapsed: 42.9003 learning rate: 0.001084, scenario: 1, slope: -0.00012716898256886233, fluctuations: 0.0\n",
      "step: 32650 loss: 1.297994 time elapsed: 42.9127 learning rate: 0.001198, scenario: 1, slope: -0.00012698299309279112, fluctuations: 0.0\n",
      "step: 32660 loss: 1.296398 time elapsed: 42.9250 learning rate: 0.001323, scenario: 1, slope: -0.00012859536740570274, fluctuations: 0.0\n",
      "step: 32670 loss: 1.294677 time elapsed: 42.9376 learning rate: 0.001405, scenario: 0, slope: -0.00013228226637080598, fluctuations: 0.0\n",
      "step: 32680 loss: 1.292947 time elapsed: 42.9502 learning rate: 0.001405, scenario: 0, slope: -0.0001378423532420616, fluctuations: 0.0\n",
      "step: 32690 loss: 1.291253 time elapsed: 42.9631 learning rate: 0.001405, scenario: 0, slope: -0.00014443630816575768, fluctuations: 0.0\n",
      "step: 32700 loss: 1.289592 time elapsed: 42.9772 learning rate: 0.001405, scenario: 0, slope: -0.00015054198774196593, fluctuations: 0.0\n",
      "step: 32710 loss: 1.287963 time elapsed: 42.9917 learning rate: 0.001405, scenario: 0, slope: -0.00015731723334676514, fluctuations: 0.0\n",
      "step: 32720 loss: 1.286361 time elapsed: 43.0056 learning rate: 0.001405, scenario: 0, slope: -0.00016199085537149942, fluctuations: 0.0\n",
      "step: 32730 loss: 1.284786 time elapsed: 43.0186 learning rate: 0.001405, scenario: 0, slope: -0.00016463371691399743, fluctuations: 0.0\n",
      "step: 32740 loss: 1.283234 time elapsed: 43.0314 learning rate: 0.001405, scenario: 0, slope: -0.0001652604639489459, fluctuations: 0.0\n",
      "step: 32750 loss: 1.281704 time elapsed: 43.0441 learning rate: 0.001405, scenario: 0, slope: -0.00016418119446373017, fluctuations: 0.0\n",
      "step: 32760 loss: 1.280193 time elapsed: 43.0567 learning rate: 0.001405, scenario: 0, slope: -0.0001619260716917738, fluctuations: 0.0\n",
      "step: 32770 loss: 1.278701 time elapsed: 43.0693 learning rate: 0.001405, scenario: 0, slope: -0.00015924805089204933, fluctuations: 0.0\n",
      "step: 32780 loss: 1.277225 time elapsed: 43.0820 learning rate: 0.001405, scenario: 0, slope: -0.0001567403327757955, fluctuations: 0.0\n",
      "step: 32790 loss: 1.275765 time elapsed: 43.0950 learning rate: 0.001405, scenario: 0, slope: -0.00015444374406525083, fluctuations: 0.0\n",
      "step: 32800 loss: 1.274319 time elapsed: 43.1076 learning rate: 0.001405, scenario: 0, slope: -0.00015254295344773136, fluctuations: 0.0\n",
      "step: 32810 loss: 1.272885 time elapsed: 43.1207 learning rate: 0.001405, scenario: 0, slope: -0.00015041504040049406, fluctuations: 0.0\n",
      "step: 32820 loss: 1.271463 time elapsed: 43.1331 learning rate: 0.001405, scenario: 0, slope: -0.0001486507427565131, fluctuations: 0.0\n",
      "step: 32830 loss: 1.270052 time elapsed: 43.1454 learning rate: 0.001405, scenario: 0, slope: -0.00014703404042467063, fluctuations: 0.0\n",
      "step: 32840 loss: 1.268651 time elapsed: 43.1580 learning rate: 0.001405, scenario: 0, slope: -0.00014555234315661298, fluctuations: 0.0\n",
      "step: 32850 loss: 1.267259 time elapsed: 43.1708 learning rate: 0.001405, scenario: 0, slope: -0.00014419430840500604, fluctuations: 0.0\n",
      "step: 32860 loss: 1.265875 time elapsed: 43.1851 learning rate: 0.001405, scenario: 0, slope: -0.00014294970788591874, fluctuations: 0.0\n",
      "step: 32870 loss: 1.264499 time elapsed: 43.1994 learning rate: 0.001405, scenario: 0, slope: -0.0001418093042184183, fluctuations: 0.0\n",
      "step: 32880 loss: 1.263129 time elapsed: 43.2137 learning rate: 0.001405, scenario: 0, slope: -0.0001407647410159658, fluctuations: 0.0\n",
      "step: 32890 loss: 1.261766 time elapsed: 43.2272 learning rate: 0.001405, scenario: 0, slope: -0.00013980844618772164, fluctuations: 0.0\n",
      "step: 32900 loss: 1.260409 time elapsed: 43.2404 learning rate: 0.001405, scenario: 0, slope: -0.00013901755689423197, fluctuations: 0.0\n",
      "step: 32910 loss: 1.259056 time elapsed: 43.2548 learning rate: 0.001405, scenario: 0, slope: -0.00013813379657293362, fluctuations: 0.0\n",
      "step: 32920 loss: 1.257709 time elapsed: 43.2685 learning rate: 0.001405, scenario: 0, slope: -0.00013740350702457757, fluctuations: 0.0\n",
      "step: 32930 loss: 1.256365 time elapsed: 43.2818 learning rate: 0.001405, scenario: 0, slope: -0.00013673749375215188, fluctuations: 0.0\n",
      "step: 32940 loss: 1.255026 time elapsed: 43.2954 learning rate: 0.001405, scenario: 0, slope: -0.00013613102411923583, fluctuations: 0.0\n",
      "step: 32950 loss: 1.253690 time elapsed: 43.3095 learning rate: 0.001405, scenario: 0, slope: -0.00013557977308285667, fluctuations: 0.0\n",
      "step: 32960 loss: 1.252357 time elapsed: 43.3245 learning rate: 0.001405, scenario: 0, slope: -0.00013507978387307073, fluctuations: 0.0\n",
      "step: 32970 loss: 1.251026 time elapsed: 43.3387 learning rate: 0.001405, scenario: 0, slope: -0.00013462743325494698, fluctuations: 0.0\n",
      "step: 32980 loss: 1.249699 time elapsed: 43.3524 learning rate: 0.001405, scenario: 0, slope: -0.00013421940084225515, fluctuations: 0.0\n",
      "step: 32990 loss: 1.248373 time elapsed: 43.3671 learning rate: 0.001405, scenario: 0, slope: -0.0001338526420228965, fluctuations: 0.0\n",
      "step: 33000 loss: 1.247049 time elapsed: 43.3820 learning rate: 0.001405, scenario: 0, slope: -0.0001335555350134317, fluctuations: 0.0\n",
      "step: 33010 loss: 1.245726 time elapsed: 43.3971 learning rate: 0.001405, scenario: 0, slope: -0.00013323200563483868, fluctuations: 0.0\n",
      "step: 33020 loss: 1.244405 time elapsed: 43.4120 learning rate: 0.001405, scenario: 0, slope: -0.00013297321799717155, fluctuations: 0.0\n",
      "step: 33030 loss: 1.243085 time elapsed: 43.4255 learning rate: 0.001405, scenario: 0, slope: -0.00013274585033141417, fluctuations: 0.0\n",
      "step: 33040 loss: 1.241766 time elapsed: 43.4389 learning rate: 0.001405, scenario: 0, slope: -0.00013254793540593883, fluctuations: 0.0\n",
      "step: 33050 loss: 1.240448 time elapsed: 43.4516 learning rate: 0.001405, scenario: 0, slope: -0.00013237633067792525, fluctuations: 0.0\n",
      "step: 33060 loss: 1.241465 time elapsed: 43.4644 learning rate: 0.001405, scenario: 0, slope: -0.000129704127615786, fluctuations: 0.0\n",
      "step: 33070 loss: 16.799694 time elapsed: 43.4770 learning rate: 0.001369, scenario: -1, slope: 0.015557646743738263, fluctuations: 0.0\n",
      "step: 33080 loss: 2.468391 time elapsed: 43.4897 learning rate: 0.001238, scenario: -1, slope: 0.11107131119188729, fluctuations: 0.02\n",
      "step: 33090 loss: 4.093710 time elapsed: 43.5021 learning rate: 0.001120, scenario: -1, slope: 0.11150044977380995, fluctuations: 0.05\n",
      "step: 33100 loss: 1.243553 time elapsed: 43.5149 learning rate: 0.001023, scenario: -1, slope: 0.09458154213449753, fluctuations: 0.08\n",
      "step: 33110 loss: 1.349587 time elapsed: 43.5282 learning rate: 0.000925, scenario: -1, slope: 0.06844341271323681, fluctuations: 0.13\n",
      "step: 33120 loss: 1.355858 time elapsed: 43.5408 learning rate: 0.000837, scenario: -1, slope: 0.036509338696182815, fluctuations: 0.17\n",
      "step: 33130 loss: 1.287902 time elapsed: 43.5533 learning rate: 0.000757, scenario: -1, slope: 0.003924918385569708, fluctuations: 0.22\n",
      "step: 33140 loss: 1.251126 time elapsed: 43.5656 learning rate: 0.000749, scenario: 0, slope: -0.033292535500968175, fluctuations: 0.27\n",
      "step: 33150 loss: 1.239078 time elapsed: 43.5781 learning rate: 0.000749, scenario: 0, slope: -0.07521750435575254, fluctuations: 0.32\n",
      "step: 33160 loss: 1.235166 time elapsed: 43.5917 learning rate: 0.000749, scenario: 0, slope: -0.12248735343947993, fluctuations: 0.36\n",
      "step: 33170 loss: 1.233581 time elapsed: 43.6059 learning rate: 0.000749, scenario: 0, slope: -0.1636420260169278, fluctuations: 0.4\n",
      "step: 33180 loss: 1.232604 time elapsed: 43.6196 learning rate: 0.000749, scenario: 0, slope: -0.041988625295761334, fluctuations: 0.42\n",
      "step: 33190 loss: 1.231785 time elapsed: 43.6333 learning rate: 0.000749, scenario: 0, slope: -0.009313968868399655, fluctuations: 0.42\n",
      "step: 33200 loss: 1.231012 time elapsed: 43.6460 learning rate: 0.000749, scenario: 0, slope: -0.0016055465499081832, fluctuations: 0.39\n",
      "step: 33210 loss: 1.230257 time elapsed: 43.6593 learning rate: 0.000749, scenario: 0, slope: -0.0007654786028750379, fluctuations: 0.34\n",
      "step: 33220 loss: 1.229509 time elapsed: 43.6720 learning rate: 0.000749, scenario: 0, slope: -0.0001496879596033464, fluctuations: 0.3\n",
      "step: 33230 loss: 1.228778 time elapsed: 43.6846 learning rate: 0.000713, scenario: -1, slope: -0.00010384701847963934, fluctuations: 0.25\n",
      "step: 33240 loss: 1.228090 time elapsed: 43.6977 learning rate: 0.000658, scenario: -1, slope: -9.714291229735664e-05, fluctuations: 0.2\n",
      "step: 33250 loss: 1.227465 time elapsed: 43.7105 learning rate: 0.000595, scenario: -1, slope: -8.610892305319736e-05, fluctuations: 0.15\n",
      "step: 33260 loss: 1.226901 time elapsed: 43.7230 learning rate: 0.000538, scenario: -1, slope: -7.626899748508434e-05, fluctuations: 0.12\n",
      "step: 33270 loss: 1.226391 time elapsed: 43.7356 learning rate: 0.000486, scenario: -1, slope: -7.122567650466575e-05, fluctuations: 0.07\n",
      "step: 33280 loss: 1.225929 time elapsed: 43.7483 learning rate: 0.000440, scenario: -1, slope: -6.758673527883571e-05, fluctuations: 0.02\n",
      "step: 33290 loss: 1.225499 time elapsed: 43.7606 learning rate: 0.000455, scenario: 1, slope: -6.367077647075273e-05, fluctuations: 0.0\n",
      "step: 33300 loss: 1.225034 time elapsed: 43.7731 learning rate: 0.000498, scenario: 1, slope: -5.9829019611916636e-05, fluctuations: 0.0\n",
      "step: 33310 loss: 1.224524 time elapsed: 43.7862 learning rate: 0.000550, scenario: 1, slope: -5.556989723651032e-05, fluctuations: 0.0\n",
      "step: 33320 loss: 1.223961 time elapsed: 43.7992 learning rate: 0.000608, scenario: 1, slope: -5.260335865671542e-05, fluctuations: 0.0\n",
      "step: 33330 loss: 1.223338 time elapsed: 43.8131 learning rate: 0.000671, scenario: 1, slope: -5.100629326124394e-05, fluctuations: 0.0\n",
      "step: 33340 loss: 1.222649 time elapsed: 43.8270 learning rate: 0.000741, scenario: 1, slope: -5.108098257297091e-05, fluctuations: 0.0\n",
      "step: 33350 loss: 1.221887 time elapsed: 43.8408 learning rate: 0.000819, scenario: 1, slope: -5.2904553593918936e-05, fluctuations: 0.0\n",
      "step: 33360 loss: 1.221045 time elapsed: 43.8538 learning rate: 0.000905, scenario: 1, slope: -5.6385771014295096e-05, fluctuations: 0.0\n",
      "step: 33370 loss: 1.220114 time elapsed: 43.8663 learning rate: 0.000999, scenario: 1, slope: -6.135002353561321e-05, fluctuations: 0.0\n",
      "step: 33380 loss: 1.219085 time elapsed: 43.8789 learning rate: 0.001104, scenario: 1, slope: -6.753940538365039e-05, fluctuations: 0.0\n",
      "step: 33390 loss: 1.217949 time elapsed: 43.8916 learning rate: 0.001219, scenario: 1, slope: -7.462515691342325e-05, fluctuations: 0.0\n",
      "step: 33400 loss: 1.216693 time elapsed: 43.9042 learning rate: 0.001334, scenario: 1, slope: -8.16543804827517e-05, fluctuations: 0.0\n",
      "step: 33410 loss: 1.215319 time elapsed: 43.9172 learning rate: 0.001473, scenario: 1, slope: -9.111946716852021e-05, fluctuations: 0.0\n",
      "step: 33420 loss: 1.213806 time elapsed: 43.9297 learning rate: 0.001627, scenario: 1, slope: -0.0001005848403737878, fluctuations: 0.0\n",
      "step: 33430 loss: 259.003099 time elapsed: 43.9444 learning rate: 0.001668, scenario: -1, slope: 0.17946604533294566, fluctuations: 0.0\n",
      "step: 33440 loss: 399.770223 time elapsed: 43.9585 learning rate: 0.001508, scenario: -1, slope: 1.603299628892543, fluctuations: 0.04\n",
      "step: 33450 loss: 24.503884 time elapsed: 43.9718 learning rate: 0.001364, scenario: -1, slope: 1.4664597293254789, fluctuations: 0.09\n",
      "step: 33460 loss: 20.521939 time elapsed: 43.9845 learning rate: 0.001234, scenario: -1, slope: 1.327278802956451, fluctuations: 0.13\n",
      "step: 33470 loss: 6.224228 time elapsed: 43.9977 learning rate: 0.001116, scenario: -1, slope: 1.0016060815350445, fluctuations: 0.17\n",
      "step: 33480 loss: 6.884066 time elapsed: 44.0121 learning rate: 0.001009, scenario: -1, slope: 0.5390805619325509, fluctuations: 0.2\n",
      "step: 33490 loss: 2.082392 time elapsed: 44.0264 learning rate: 0.000913, scenario: -1, slope: 0.10643813303069155, fluctuations: 0.24\n",
      "step: 33500 loss: 1.820806 time elapsed: 44.0411 learning rate: 0.000903, scenario: 0, slope: -0.3237849240945302, fluctuations: 0.27\n",
      "step: 33510 loss: 1.744379 time elapsed: 44.0554 learning rate: 0.000903, scenario: 0, slope: -0.9029163372278984, fluctuations: 0.3\n",
      "step: 33520 loss: 1.583800 time elapsed: 44.0682 learning rate: 0.000903, scenario: 0, slope: -1.5802897825404854, fluctuations: 0.34\n",
      "step: 33530 loss: 1.517540 time elapsed: 44.0812 learning rate: 0.000903, scenario: 0, slope: -2.287951608770446, fluctuations: 0.37\n",
      "step: 33540 loss: 1.487953 time elapsed: 44.0938 learning rate: 0.000903, scenario: 0, slope: -0.4891444318097253, fluctuations: 0.35\n",
      "step: 33550 loss: 1.459214 time elapsed: 44.1065 learning rate: 0.000903, scenario: 0, slope: -0.3350061563329315, fluctuations: 0.33\n",
      "step: 33560 loss: 1.437758 time elapsed: 44.1191 learning rate: 0.000903, scenario: 0, slope: -0.10022253393469512, fluctuations: 0.29\n",
      "step: 33570 loss: 1.421526 time elapsed: 44.1320 learning rate: 0.000903, scenario: 0, slope: -0.03323791794487688, fluctuations: 0.25\n",
      "step: 33580 loss: 1.407531 time elapsed: 44.1446 learning rate: 0.000903, scenario: 0, slope: -0.010536781769719004, fluctuations: 0.22\n",
      "step: 33590 loss: 1.395521 time elapsed: 44.1569 learning rate: 0.000903, scenario: 0, slope: -0.005566236456117871, fluctuations: 0.19\n",
      "step: 33600 loss: 1.385046 time elapsed: 44.1693 learning rate: 0.000903, scenario: 0, slope: -0.003560265882560169, fluctuations: 0.16\n",
      "step: 33610 loss: 1.375736 time elapsed: 44.1824 learning rate: 0.000903, scenario: 0, slope: -0.0023489066519065877, fluctuations: 0.12\n",
      "step: 33620 loss: 1.367398 time elapsed: 44.1950 learning rate: 0.000903, scenario: 0, slope: -0.001727977246945625, fluctuations: 0.09\n",
      "step: 33630 loss: 1.359859 time elapsed: 44.2076 learning rate: 0.000903, scenario: 0, slope: -0.0014242215226992372, fluctuations: 0.05\n",
      "step: 33640 loss: 1.352992 time elapsed: 44.2215 learning rate: 0.000903, scenario: 0, slope: -0.0012036565630712426, fluctuations: 0.02\n",
      "step: 33650 loss: 1.346700 time elapsed: 44.2367 learning rate: 0.000903, scenario: 0, slope: -0.0010535971942432045, fluctuations: 0.0\n",
      "step: 33660 loss: 1.340906 time elapsed: 44.2518 learning rate: 0.000903, scenario: 0, slope: -0.0009266237317034118, fluctuations: 0.0\n",
      "step: 33670 loss: 1.335548 time elapsed: 44.2661 learning rate: 0.000903, scenario: 0, slope: -0.0008260713189897407, fluctuations: 0.0\n",
      "step: 33680 loss: 1.330574 time elapsed: 44.2798 learning rate: 0.000903, scenario: 0, slope: -0.0007443436581643297, fluctuations: 0.0\n",
      "step: 33690 loss: 1.325943 time elapsed: 44.2935 learning rate: 0.000903, scenario: 0, slope: -0.0006763291161823518, fluctuations: 0.0\n",
      "step: 33700 loss: 1.321617 time elapsed: 44.3068 learning rate: 0.000903, scenario: 0, slope: -0.0006240357891845388, fluctuations: 0.0\n",
      "step: 33710 loss: 1.317565 time elapsed: 44.3212 learning rate: 0.000903, scenario: 0, slope: -0.000569066605334128, fluctuations: 0.0\n",
      "step: 33720 loss: 1.313760 time elapsed: 44.3354 learning rate: 0.000903, scenario: 0, slope: -0.0005259044629241027, fluctuations: 0.0\n",
      "step: 33730 loss: 1.310178 time elapsed: 44.3488 learning rate: 0.000903, scenario: 0, slope: -0.0004879748605745136, fluctuations: 0.0\n",
      "step: 33740 loss: 1.306800 time elapsed: 44.3621 learning rate: 0.000903, scenario: 0, slope: -0.0004543796978025772, fluctuations: 0.0\n",
      "step: 33750 loss: 1.303606 time elapsed: 44.3756 learning rate: 0.000903, scenario: 0, slope: -0.000424430068650096, fluctuations: 0.0\n",
      "step: 33760 loss: 1.300579 time elapsed: 44.3889 learning rate: 0.000903, scenario: 0, slope: -0.00039758796371736207, fluctuations: 0.0\n",
      "step: 33770 loss: 1.297706 time elapsed: 44.4026 learning rate: 0.000903, scenario: 0, slope: -0.0003734245913252931, fluctuations: 0.0\n",
      "step: 33780 loss: 1.294974 time elapsed: 44.4170 learning rate: 0.000903, scenario: 0, slope: -0.000351592237154457, fluctuations: 0.0\n",
      "step: 33790 loss: 1.292370 time elapsed: 44.4320 learning rate: 0.000903, scenario: 0, slope: -0.0003318042672789022, fluctuations: 0.0\n",
      "step: 33800 loss: 1.289884 time elapsed: 44.4467 learning rate: 0.000903, scenario: 0, slope: -0.0003155441280440031, fluctuations: 0.0\n",
      "step: 33810 loss: 1.287506 time elapsed: 44.4621 learning rate: 0.000903, scenario: 0, slope: -0.0002974393754056605, fluctuations: 0.0\n",
      "step: 33820 loss: 1.285228 time elapsed: 44.4759 learning rate: 0.000903, scenario: 0, slope: -0.00028248575124514185, fluctuations: 0.0\n",
      "step: 33830 loss: 1.283042 time elapsed: 44.4897 learning rate: 0.000903, scenario: 0, slope: -0.000268810083305898, fluctuations: 0.0\n",
      "step: 33840 loss: 1.280940 time elapsed: 44.5034 learning rate: 0.000903, scenario: 0, slope: -0.00025628184538785875, fluctuations: 0.0\n",
      "step: 33850 loss: 1.278917 time elapsed: 44.5175 learning rate: 0.000903, scenario: 0, slope: -0.0002447867552634726, fluctuations: 0.0\n",
      "step: 33860 loss: 1.276966 time elapsed: 44.5310 learning rate: 0.000903, scenario: 0, slope: -0.00023422422442831724, fluctuations: 0.0\n",
      "step: 33870 loss: 1.275082 time elapsed: 44.5462 learning rate: 0.000903, scenario: 0, slope: -0.00022450532216903738, fluctuations: 0.0\n",
      "step: 33880 loss: 1.273260 time elapsed: 44.5601 learning rate: 0.000903, scenario: 0, slope: -0.00021555112529630027, fluctuations: 0.0\n",
      "step: 33890 loss: 1.271496 time elapsed: 44.5734 learning rate: 0.000903, scenario: 0, slope: -0.00020729136216005058, fluctuations: 0.0\n",
      "step: 33900 loss: 1.269785 time elapsed: 44.5864 learning rate: 0.000903, scenario: 0, slope: -0.00020039931553536612, fluctuations: 0.0\n",
      "step: 33910 loss: 1.268123 time elapsed: 44.5999 learning rate: 0.000903, scenario: 0, slope: -0.00019261072352166087, fluctuations: 0.0\n",
      "step: 33920 loss: 1.266508 time elapsed: 44.6126 learning rate: 0.000903, scenario: 0, slope: -0.00018608328066084236, fluctuations: 0.0\n",
      "step: 33930 loss: 1.264936 time elapsed: 44.6256 learning rate: 0.000903, scenario: 0, slope: -0.00018003564857632345, fluctuations: 0.0\n",
      "step: 33940 loss: 1.263403 time elapsed: 44.6399 learning rate: 0.000903, scenario: 0, slope: -0.00017442701865641422, fluctuations: 0.0\n",
      "step: 33950 loss: 1.261908 time elapsed: 44.6541 learning rate: 0.000903, scenario: 0, slope: -0.00016922057227563626, fluctuations: 0.0\n",
      "step: 33960 loss: 1.260448 time elapsed: 44.6688 learning rate: 0.000903, scenario: 0, slope: -0.000164383038225202, fluctuations: 0.0\n",
      "step: 33970 loss: 1.259020 time elapsed: 44.6820 learning rate: 0.000903, scenario: 0, slope: -0.0001598843065034776, fluctuations: 0.0\n",
      "step: 33980 loss: 1.257623 time elapsed: 44.6951 learning rate: 0.000903, scenario: 0, slope: -0.0001556970899695864, fluctuations: 0.0\n",
      "step: 33990 loss: 1.256253 time elapsed: 44.7079 learning rate: 0.000903, scenario: 0, slope: -0.00015179662689887165, fluctuations: 0.0\n",
      "step: 34000 loss: 1.254911 time elapsed: 44.7204 learning rate: 0.000903, scenario: 0, slope: -0.00014851274630887663, fluctuations: 0.0\n",
      "step: 34010 loss: 1.253593 time elapsed: 44.7334 learning rate: 0.000903, scenario: 0, slope: -0.00014476799793250694, fluctuations: 0.0\n",
      "step: 34020 loss: 1.252298 time elapsed: 44.7465 learning rate: 0.000903, scenario: 0, slope: -0.00014160072283809372, fluctuations: 0.0\n",
      "step: 34030 loss: 1.251025 time elapsed: 44.7591 learning rate: 0.000903, scenario: 0, slope: -0.00013864159463054457, fluctuations: 0.0\n",
      "step: 34040 loss: 1.249773 time elapsed: 44.7715 learning rate: 0.000903, scenario: 0, slope: -0.0001358750950472848, fluctuations: 0.0\n",
      "step: 34050 loss: 1.248540 time elapsed: 44.7839 learning rate: 0.000903, scenario: 0, slope: -0.00013328704148860744, fluctuations: 0.0\n",
      "step: 34060 loss: 1.247324 time elapsed: 44.7963 learning rate: 0.000903, scenario: 0, slope: -0.0001308644577150147, fluctuations: 0.0\n",
      "step: 34070 loss: 1.246126 time elapsed: 44.8089 learning rate: 0.000903, scenario: 0, slope: -0.0001285954582765222, fluctuations: 0.0\n",
      "step: 34080 loss: 1.244943 time elapsed: 44.8214 learning rate: 0.000903, scenario: 0, slope: -0.00012646914509688209, fluctuations: 0.0\n",
      "step: 34090 loss: 1.243758 time elapsed: 44.8343 learning rate: 0.000969, scenario: 1, slope: -0.0001244994337694076, fluctuations: 0.0\n",
      "step: 34100 loss: 1.242478 time elapsed: 44.8483 learning rate: 0.001059, scenario: 1, slope: -0.0001231574698546058, fluctuations: 0.0\n",
      "step: 34110 loss: 1.241095 time elapsed: 44.8627 learning rate: 0.001170, scenario: 1, slope: -0.00012266894905236487, fluctuations: 0.0\n",
      "step: 34120 loss: 1.239591 time elapsed: 44.8768 learning rate: 0.001293, scenario: 1, slope: -0.00012378340177369174, fluctuations: 0.0\n",
      "step: 34130 loss: 1.237964 time elapsed: 44.8908 learning rate: 0.001359, scenario: 0, slope: -0.00012675771085787096, fluctuations: 0.0\n",
      "step: 34140 loss: 1.236330 time elapsed: 44.9035 learning rate: 0.001359, scenario: 0, slope: -0.00013144518587951515, fluctuations: 0.0\n",
      "step: 34150 loss: 1.234721 time elapsed: 44.9162 learning rate: 0.001359, scenario: 0, slope: -0.00013715117684449468, fluctuations: 0.0\n",
      "step: 34160 loss: 1.233136 time elapsed: 44.9290 learning rate: 0.001359, scenario: 0, slope: -0.00014316966144989294, fluctuations: 0.0\n",
      "step: 34170 loss: 1.231572 time elapsed: 44.9414 learning rate: 0.001359, scenario: 0, slope: -0.00014881836288197276, fluctuations: 0.0\n",
      "step: 34180 loss: 1.230027 time elapsed: 44.9538 learning rate: 0.001359, scenario: 0, slope: -0.00015343714841447727, fluctuations: 0.0\n",
      "step: 34190 loss: 1.228501 time elapsed: 44.9663 learning rate: 0.001359, scenario: 0, slope: -0.00015641151415744187, fluctuations: 0.0\n",
      "step: 34200 loss: 1.226992 time elapsed: 44.9788 learning rate: 0.001359, scenario: 0, slope: -0.00015752859675910373, fluctuations: 0.0\n",
      "step: 34210 loss: 1.225498 time elapsed: 44.9916 learning rate: 0.001359, scenario: 0, slope: -0.00015716779188259042, fluctuations: 0.0\n",
      "step: 34220 loss: 1.224019 time elapsed: 45.0040 learning rate: 0.001359, scenario: 0, slope: -0.00015566418729445292, fluctuations: 0.0\n",
      "step: 34230 loss: 1.222554 time elapsed: 45.0162 learning rate: 0.001359, scenario: 0, slope: -0.00015377118640659494, fluctuations: 0.0\n",
      "step: 34240 loss: 1.221100 time elapsed: 45.0285 learning rate: 0.001359, scenario: 0, slope: -0.00015198910326545912, fluctuations: 0.0\n",
      "step: 34250 loss: 1.219659 time elapsed: 45.0410 learning rate: 0.001359, scenario: 0, slope: -0.0001503417383208987, fluctuations: 0.0\n",
      "step: 34260 loss: 1.218228 time elapsed: 45.0532 learning rate: 0.001359, scenario: 0, slope: -0.00014881834691599137, fluctuations: 0.0\n",
      "step: 34270 loss: 1.216807 time elapsed: 45.0672 learning rate: 0.001359, scenario: 0, slope: -0.0001474085222379901, fluctuations: 0.0\n",
      "step: 34280 loss: 1.215395 time elapsed: 45.0811 learning rate: 0.001359, scenario: 0, slope: -0.00014610266278010766, fluctuations: 0.0\n",
      "step: 34290 loss: 1.213992 time elapsed: 45.0953 learning rate: 0.001359, scenario: 0, slope: -0.00014489205581853662, fluctuations: 0.0\n",
      "step: 34300 loss: 1.212596 time elapsed: 45.1079 learning rate: 0.001359, scenario: 0, slope: -0.00014387743391696038, fluctuations: 0.0\n",
      "step: 34310 loss: 1.211209 time elapsed: 45.1212 learning rate: 0.001359, scenario: 0, slope: -0.00014272591311315205, fluctuations: 0.0\n",
      "step: 34320 loss: 1.209828 time elapsed: 45.1338 learning rate: 0.001359, scenario: 0, slope: -0.00014175688586901374, fluctuations: 0.0\n",
      "step: 34330 loss: 1.208453 time elapsed: 45.1463 learning rate: 0.001359, scenario: 0, slope: -0.00014085597347874502, fluctuations: 0.0\n",
      "step: 34340 loss: 1.207085 time elapsed: 45.1590 learning rate: 0.001359, scenario: 0, slope: -0.00014001794520710175, fluctuations: 0.0\n",
      "step: 34350 loss: 1.205722 time elapsed: 45.1716 learning rate: 0.001359, scenario: 0, slope: -0.00013923806134912586, fluctuations: 0.0\n",
      "step: 34360 loss: 1.204365 time elapsed: 45.1846 learning rate: 0.001359, scenario: 0, slope: -0.00013851202055421084, fluctuations: 0.0\n",
      "step: 34370 loss: 1.203012 time elapsed: 45.1974 learning rate: 0.001359, scenario: 0, slope: -0.00013783591335495143, fluctuations: 0.0\n",
      "step: 34380 loss: 1.201664 time elapsed: 45.2107 learning rate: 0.001359, scenario: 0, slope: -0.00013720618114262945, fluctuations: 0.0\n",
      "step: 34390 loss: 1.200320 time elapsed: 45.2240 learning rate: 0.001359, scenario: 0, slope: -0.00013661957990968574, fluctuations: 0.0\n",
      "step: 34400 loss: 1.198980 time elapsed: 45.2366 learning rate: 0.001359, scenario: 0, slope: -0.00013612606274193733, fluctuations: 0.0\n",
      "step: 34410 loss: 1.197644 time elapsed: 45.2516 learning rate: 0.001359, scenario: 0, slope: -0.00013556417846972022, fluctuations: 0.0\n",
      "step: 34420 loss: 1.196311 time elapsed: 45.2672 learning rate: 0.001359, scenario: 0, slope: -0.00013509019223956512, fluctuations: 0.0\n",
      "step: 34430 loss: 1.194981 time elapsed: 45.2819 learning rate: 0.001359, scenario: 0, slope: -0.00013464891725833403, fluctuations: 0.0\n",
      "step: 34440 loss: 1.193654 time elapsed: 45.2973 learning rate: 0.001359, scenario: 0, slope: -0.00013423826772606241, fluctuations: 0.0\n",
      "step: 34450 loss: 1.192330 time elapsed: 45.3111 learning rate: 0.001359, scenario: 0, slope: -0.0001338563264476951, fluctuations: 0.0\n",
      "step: 34460 loss: 1.191009 time elapsed: 45.3250 learning rate: 0.001359, scenario: 0, slope: -0.00013350132893619844, fluctuations: 0.0\n",
      "step: 34470 loss: 1.189689 time elapsed: 45.3392 learning rate: 0.001359, scenario: 0, slope: -0.00013317164920280378, fluctuations: 0.0\n",
      "step: 34480 loss: 1.188372 time elapsed: 45.3529 learning rate: 0.001359, scenario: 0, slope: -0.00013286578703524283, fluctuations: 0.0\n",
      "step: 34490 loss: 1.187057 time elapsed: 45.3660 learning rate: 0.001359, scenario: 0, slope: -0.00013258235659522054, fluctuations: 0.0\n",
      "step: 34500 loss: 1.185744 time elapsed: 45.3791 learning rate: 0.001359, scenario: 0, slope: -0.00013234538700035426, fluctuations: 0.0\n",
      "step: 34510 loss: 1.184432 time elapsed: 45.3928 learning rate: 0.001359, scenario: 0, slope: -0.00013207775903546014, fluctuations: 0.0\n",
      "step: 34520 loss: 1.183121 time elapsed: 45.4055 learning rate: 0.001359, scenario: 0, slope: -0.00013185430505153103, fluctuations: 0.0\n",
      "step: 34530 loss: 1.181812 time elapsed: 45.4182 learning rate: 0.001359, scenario: 0, slope: -0.0001316486933263594, fluctuations: 0.0\n",
      "step: 34540 loss: 1.180505 time elapsed: 45.4304 learning rate: 0.001359, scenario: 0, slope: -0.0001314599754209132, fluctuations: 0.0\n",
      "step: 34550 loss: 1.179198 time elapsed: 45.4428 learning rate: 0.001359, scenario: 0, slope: -0.00013128726895306265, fluctuations: 0.0\n",
      "step: 34560 loss: 1.177893 time elapsed: 45.4553 learning rate: 0.001359, scenario: 0, slope: -0.00013112937226371382, fluctuations: 0.0\n",
      "step: 34570 loss: 1.177263 time elapsed: 45.4683 learning rate: 0.001359, scenario: 0, slope: -0.00013025730865502506, fluctuations: 0.0\n",
      "step: 34580 loss: 5.313973 time elapsed: 45.4824 learning rate: 0.001351, scenario: -1, slope: 0.0037403298962466747, fluctuations: 0.0\n",
      "step: 34590 loss: 24.021839 time elapsed: 45.4966 learning rate: 0.001222, scenario: -1, slope: 0.13343511644382844, fluctuations: 0.02\n",
      "step: 34600 loss: 3.262895 time elapsed: 45.5108 learning rate: 0.001116, scenario: -1, slope: 0.13669507981929038, fluctuations: 0.04\n",
      "step: 34610 loss: 3.857337 time elapsed: 45.5250 learning rate: 0.001010, scenario: -1, slope: 0.11243361603755643, fluctuations: 0.08\n",
      "step: 34620 loss: 1.673853 time elapsed: 45.5379 learning rate: 0.000913, scenario: -1, slope: 0.08789782450686336, fluctuations: 0.13\n",
      "step: 34630 loss: 1.324047 time elapsed: 45.5506 learning rate: 0.000826, scenario: -1, slope: 0.050615323042141225, fluctuations: 0.17\n",
      "step: 34640 loss: 1.192423 time elapsed: 45.5631 learning rate: 0.000747, scenario: -1, slope: 0.012775658755428214, fluctuations: 0.22\n",
      "step: 34650 loss: 1.189871 time elapsed: 45.5759 learning rate: 0.000732, scenario: 0, slope: -0.03257252110453342, fluctuations: 0.26\n",
      "step: 34660 loss: 1.182104 time elapsed: 45.5885 learning rate: 0.000732, scenario: 0, slope: -0.08007644403231304, fluctuations: 0.3\n",
      "step: 34670 loss: 1.175978 time elapsed: 45.6011 learning rate: 0.000732, scenario: 0, slope: -0.13574169169182942, fluctuations: 0.35\n",
      "step: 34680 loss: 1.172750 time elapsed: 45.6137 learning rate: 0.000732, scenario: 0, slope: -0.2049797880095363, fluctuations: 0.4\n",
      "step: 34690 loss: 1.171065 time elapsed: 45.6263 learning rate: 0.000732, scenario: 0, slope: -0.03366969671801317, fluctuations: 0.43\n",
      "step: 34700 loss: 1.169974 time elapsed: 45.6389 learning rate: 0.000732, scenario: 0, slope: -0.011554768336813237, fluctuations: 0.44\n",
      "step: 34710 loss: 1.169083 time elapsed: 45.6520 learning rate: 0.000732, scenario: 0, slope: -0.0023795119433495282, fluctuations: 0.4\n",
      "step: 34720 loss: 1.168262 time elapsed: 45.6645 learning rate: 0.000732, scenario: 0, slope: -0.0007237794799993326, fluctuations: 0.36\n",
      "step: 34730 loss: 1.167470 time elapsed: 45.6775 learning rate: 0.000732, scenario: 0, slope: -0.00045541579711803366, fluctuations: 0.31\n",
      "step: 34740 loss: 1.166694 time elapsed: 45.6916 learning rate: 0.000732, scenario: 0, slope: -0.00023663172476210816, fluctuations: 0.26\n",
      "step: 34750 loss: 1.165931 time elapsed: 45.7059 learning rate: 0.000725, scenario: -1, slope: -0.0001115073103980627, fluctuations: 0.23\n",
      "step: 34760 loss: 1.165208 time elapsed: 45.7202 learning rate: 0.000655, scenario: -1, slope: -8.387927417388814e-05, fluctuations: 0.18\n",
      "step: 34770 loss: 1.164561 time elapsed: 45.7330 learning rate: 0.000593, scenario: -1, slope: -7.988414233142419e-05, fluctuations: 0.13\n",
      "step: 34780 loss: 1.163979 time elapsed: 45.7461 learning rate: 0.000536, scenario: -1, slope: -7.726526024541734e-05, fluctuations: 0.08\n",
      "step: 34790 loss: 1.163455 time elapsed: 45.7590 learning rate: 0.000485, scenario: -1, slope: -7.415278836450724e-05, fluctuations: 0.03\n",
      "step: 34800 loss: 1.162974 time elapsed: 45.7716 learning rate: 0.000487, scenario: 1, slope: -7.095195467882447e-05, fluctuations: 0.0\n",
      "step: 34810 loss: 1.162465 time elapsed: 45.7848 learning rate: 0.000538, scenario: 1, slope: -6.629719543539276e-05, fluctuations: 0.0\n",
      "step: 34820 loss: 1.161905 time elapsed: 45.7970 learning rate: 0.000594, scenario: 1, slope: -6.239819604058458e-05, fluctuations: 0.0\n",
      "step: 34830 loss: 1.161286 time elapsed: 45.8094 learning rate: 0.000656, scenario: 1, slope: -5.92415895014091e-05, fluctuations: 0.0\n",
      "step: 34840 loss: 1.160605 time elapsed: 45.8219 learning rate: 0.000725, scenario: 1, slope: -5.727449962904758e-05, fluctuations: 0.0\n",
      "step: 34850 loss: 1.159854 time elapsed: 45.8340 learning rate: 0.000801, scenario: 1, slope: -5.692437228767742e-05, fluctuations: 0.0\n",
      "step: 34860 loss: 1.159026 time elapsed: 45.8464 learning rate: 0.000885, scenario: 1, slope: -5.8505080534090386e-05, fluctuations: 0.0\n",
      "step: 34870 loss: 1.158114 time elapsed: 45.8586 learning rate: 0.000977, scenario: 1, slope: -6.196254168159728e-05, fluctuations: 0.0\n",
      "step: 34880 loss: 1.157109 time elapsed: 45.8712 learning rate: 0.001080, scenario: 1, slope: -6.708658086082343e-05, fluctuations: 0.0\n",
      "step: 34890 loss: 1.156001 time elapsed: 45.8845 learning rate: 0.001193, scenario: 1, slope: -7.35803515611869e-05, fluctuations: 0.0\n",
      "step: 34900 loss: 1.154781 time elapsed: 45.8995 learning rate: 0.001304, scenario: 1, slope: -8.028201696252461e-05, fluctuations: 0.0\n",
      "step: 34910 loss: 1.153449 time elapsed: 45.9147 learning rate: 0.001441, scenario: 1, slope: -8.930329424287245e-05, fluctuations: 0.0\n",
      "step: 34920 loss: 1.151984 time elapsed: 45.9296 learning rate: 0.001592, scenario: 1, slope: -9.830602357047827e-05, fluctuations: 0.0\n",
      "step: 34930 loss: 24.280705 time elapsed: 45.9431 learning rate: 0.001664, scenario: -1, slope: 0.015884491780661827, fluctuations: 0.0\n",
      "step: 34940 loss: 544.389465 time elapsed: 45.9567 learning rate: 0.001505, scenario: -1, slope: 1.5757600396086864, fluctuations: 0.03\n",
      "step: 34950 loss: 7.668855 time elapsed: 45.9702 learning rate: 0.001361, scenario: -1, slope: 1.4177476078675457, fluctuations: 0.08\n",
      "step: 34960 loss: 20.914691 time elapsed: 45.9838 learning rate: 0.001231, scenario: -1, slope: 1.258949628810963, fluctuations: 0.12\n",
      "step: 34970 loss: 3.668421 time elapsed: 45.9967 learning rate: 0.001113, scenario: -1, slope: 0.9398357036642615, fluctuations: 0.16\n",
      "step: 34980 loss: 4.232193 time elapsed: 46.0105 learning rate: 0.001007, scenario: -1, slope: 0.5589914793572629, fluctuations: 0.2\n",
      "step: 34990 loss: 2.580944 time elapsed: 46.0242 learning rate: 0.000911, scenario: -1, slope: 0.10705212457354936, fluctuations: 0.23\n",
      "step: 35000 loss: 2.113379 time elapsed: 46.0376 learning rate: 0.000892, scenario: 0, slope: -0.29043364327562693, fluctuations: 0.26\n",
      "step: 35010 loss: 1.492881 time elapsed: 46.0518 learning rate: 0.000892, scenario: 0, slope: -0.8201087326887686, fluctuations: 0.3\n",
      "step: 35020 loss: 1.491659 time elapsed: 46.0652 learning rate: 0.000892, scenario: 0, slope: -1.4136657427497654, fluctuations: 0.33\n",
      "step: 35030 loss: 1.432313 time elapsed: 46.0793 learning rate: 0.000892, scenario: 0, slope: -2.302980109547306, fluctuations: 0.37\n",
      "step: 35040 loss: 1.389297 time elapsed: 46.0945 learning rate: 0.000892, scenario: 0, slope: -0.507471939382615, fluctuations: 0.36\n",
      "step: 35050 loss: 1.373302 time elapsed: 46.1094 learning rate: 0.000892, scenario: 0, slope: -0.29519083787843026, fluctuations: 0.34\n",
      "step: 35060 loss: 1.354807 time elapsed: 46.1250 learning rate: 0.000892, scenario: 0, slope: -0.08813661918103437, fluctuations: 0.32\n",
      "step: 35070 loss: 1.341334 time elapsed: 46.1409 learning rate: 0.000892, scenario: 0, slope: -0.0340607476084886, fluctuations: 0.28\n",
      "step: 35080 loss: 1.329719 time elapsed: 46.1549 learning rate: 0.000892, scenario: 0, slope: -0.00912587597072892, fluctuations: 0.25\n",
      "step: 35090 loss: 1.319500 time elapsed: 46.1689 learning rate: 0.000892, scenario: 0, slope: -0.005661612426848403, fluctuations: 0.21\n",
      "step: 35100 loss: 1.310536 time elapsed: 46.1825 learning rate: 0.000892, scenario: 0, slope: -0.003245823391121234, fluctuations: 0.18\n",
      "step: 35110 loss: 1.302473 time elapsed: 46.1962 learning rate: 0.000892, scenario: 0, slope: -0.0019075999521118837, fluctuations: 0.15\n",
      "step: 35120 loss: 1.295199 time elapsed: 46.2091 learning rate: 0.000892, scenario: 0, slope: -0.001460805534653277, fluctuations: 0.11\n",
      "step: 35130 loss: 1.288572 time elapsed: 46.2218 learning rate: 0.000892, scenario: 0, slope: -0.0011610538302371692, fluctuations: 0.08\n",
      "step: 35140 loss: 1.282501 time elapsed: 46.2345 learning rate: 0.000892, scenario: 0, slope: -0.001010702518445461, fluctuations: 0.04\n",
      "step: 35150 loss: 1.276908 time elapsed: 46.2474 learning rate: 0.000892, scenario: 0, slope: -0.0008924126929121522, fluctuations: 0.01\n",
      "step: 35160 loss: 1.271732 time elapsed: 46.2605 learning rate: 0.000892, scenario: 0, slope: -0.0007990232249559102, fluctuations: 0.0\n",
      "step: 35170 loss: 1.266923 time elapsed: 46.2732 learning rate: 0.000892, scenario: 0, slope: -0.0007191519635710799, fluctuations: 0.0\n",
      "step: 35180 loss: 1.262437 time elapsed: 46.2859 learning rate: 0.000892, scenario: 0, slope: -0.0006531098677907103, fluctuations: 0.0\n",
      "step: 35190 loss: 1.258241 time elapsed: 46.3003 learning rate: 0.000892, scenario: 0, slope: -0.0005974901397605766, fluctuations: 0.0\n",
      "step: 35200 loss: 1.254304 time elapsed: 46.3140 learning rate: 0.000892, scenario: 0, slope: -0.0005542824169549733, fluctuations: 0.0\n",
      "step: 35210 loss: 1.250601 time elapsed: 46.3287 learning rate: 0.000892, scenario: 0, slope: -0.000508504564253687, fluctuations: 0.0\n",
      "step: 35220 loss: 1.247108 time elapsed: 46.3433 learning rate: 0.000892, scenario: 0, slope: -0.0004723224221826779, fluctuations: 0.0\n",
      "step: 35230 loss: 1.243808 time elapsed: 46.3563 learning rate: 0.000892, scenario: 0, slope: -0.00044037293692346055, fluctuations: 0.0\n",
      "step: 35240 loss: 1.240682 time elapsed: 46.3693 learning rate: 0.000892, scenario: 0, slope: -0.0004119573247971246, fluctuations: 0.0\n",
      "step: 35250 loss: 1.237716 time elapsed: 46.3822 learning rate: 0.000892, scenario: 0, slope: -0.0003865298679413089, fluctuations: 0.0\n",
      "step: 35260 loss: 1.234896 time elapsed: 46.3947 learning rate: 0.000892, scenario: 0, slope: -0.0003636559712451808, fluctuations: 0.0\n",
      "step: 35270 loss: 1.232210 time elapsed: 46.4070 learning rate: 0.000892, scenario: 0, slope: -0.0003429851177206974, fluctuations: 0.0\n",
      "step: 35280 loss: 1.229648 time elapsed: 46.4191 learning rate: 0.000892, scenario: 0, slope: -0.00032423076455207823, fluctuations: 0.0\n",
      "step: 35290 loss: 1.227199 time elapsed: 46.4314 learning rate: 0.000892, scenario: 0, slope: -0.00030715589770951294, fluctuations: 0.0\n",
      "step: 35300 loss: 1.224856 time elapsed: 46.4434 learning rate: 0.000892, scenario: 0, slope: -0.0002930599801535742, fluctuations: 0.0\n",
      "step: 35310 loss: 1.222610 time elapsed: 46.4559 learning rate: 0.000892, scenario: 0, slope: -0.00027728251968798514, fluctuations: 0.0\n",
      "step: 35320 loss: 1.220454 time elapsed: 46.4681 learning rate: 0.000892, scenario: 0, slope: -0.00026417387478123875, fluctuations: 0.0\n",
      "step: 35330 loss: 1.218382 time elapsed: 46.4804 learning rate: 0.000892, scenario: 0, slope: -0.00025211375449868926, fluctuations: 0.0\n",
      "step: 35340 loss: 1.216388 time elapsed: 46.4928 learning rate: 0.000892, scenario: 0, slope: -0.0002409960640082851, fluctuations: 0.0\n",
      "step: 35350 loss: 1.214466 time elapsed: 46.5058 learning rate: 0.000892, scenario: 0, slope: -0.00023072840853631218, fluctuations: 0.0\n",
      "step: 35360 loss: 1.212611 time elapsed: 46.5193 learning rate: 0.000892, scenario: 0, slope: -0.00022122986737857321, fluctuations: 0.0\n",
      "step: 35370 loss: 1.210819 time elapsed: 46.5338 learning rate: 0.000892, scenario: 0, slope: -0.00021242921071248715, fluctuations: 0.0\n",
      "step: 35380 loss: 1.209086 time elapsed: 46.5492 learning rate: 0.000892, scenario: 0, slope: -0.00020426345574042843, fluctuations: 0.0\n",
      "step: 35390 loss: 1.207407 time elapsed: 46.5625 learning rate: 0.000892, scenario: 0, slope: -0.0001966766859424052, fluctuations: 0.0\n",
      "step: 35400 loss: 1.205779 time elapsed: 46.5751 learning rate: 0.000892, scenario: 0, slope: -0.00019030233286732612, fluctuations: 0.0\n",
      "step: 35410 loss: 1.204200 time elapsed: 46.5882 learning rate: 0.000892, scenario: 0, slope: -0.0001830460832046648, fluctuations: 0.0\n",
      "step: 35420 loss: 1.202665 time elapsed: 46.6009 learning rate: 0.000892, scenario: 0, slope: -0.00017691776178922796, fluctuations: 0.0\n",
      "step: 35430 loss: 1.201172 time elapsed: 46.6134 learning rate: 0.000892, scenario: 0, slope: -0.000171198193328457, fluctuations: 0.0\n",
      "step: 35440 loss: 1.199719 time elapsed: 46.6259 learning rate: 0.000892, scenario: 0, slope: -0.00016585499546429306, fluctuations: 0.0\n",
      "step: 35450 loss: 1.198302 time elapsed: 46.6384 learning rate: 0.000892, scenario: 0, slope: -0.0001608589045739996, fluctuations: 0.0\n",
      "step: 35460 loss: 1.196921 time elapsed: 46.6507 learning rate: 0.000892, scenario: 0, slope: -0.00015618341658701123, fluctuations: 0.0\n",
      "step: 35470 loss: 1.195572 time elapsed: 46.6632 learning rate: 0.000892, scenario: 0, slope: -0.0001518044767465619, fluctuations: 0.0\n",
      "step: 35480 loss: 1.194253 time elapsed: 46.6753 learning rate: 0.000892, scenario: 0, slope: -0.00014770021047087105, fluctuations: 0.0\n",
      "step: 35490 loss: 1.192964 time elapsed: 46.6876 learning rate: 0.000892, scenario: 0, slope: -0.000143850688931293, fluctuations: 0.0\n",
      "step: 35500 loss: 1.191702 time elapsed: 46.7000 learning rate: 0.000892, scenario: 0, slope: -0.0001405888608974872, fluctuations: 0.0\n",
      "step: 35510 loss: 1.190465 time elapsed: 46.7136 learning rate: 0.000892, scenario: 0, slope: -0.00013684468906389628, fluctuations: 0.0\n",
      "step: 35520 loss: 1.189252 time elapsed: 46.7275 learning rate: 0.000892, scenario: 0, slope: -0.00013365635968466932, fluctuations: 0.0\n",
      "step: 35530 loss: 1.188062 time elapsed: 46.7418 learning rate: 0.000892, scenario: 0, slope: -0.00013065877516907733, fluctuations: 0.0\n",
      "step: 35540 loss: 1.186894 time elapsed: 46.7561 learning rate: 0.000892, scenario: 0, slope: -0.00012783911447712104, fluctuations: 0.0\n",
      "step: 35550 loss: 1.185745 time elapsed: 46.7700 learning rate: 0.000892, scenario: 0, slope: -0.00012518558677737513, fluctuations: 0.0\n",
      "step: 35560 loss: 1.184616 time elapsed: 46.7830 learning rate: 0.000892, scenario: 0, slope: -0.00012268733403690983, fluctuations: 0.0\n",
      "step: 35570 loss: 1.183504 time elapsed: 46.7960 learning rate: 0.000892, scenario: 0, slope: -0.00012033434422512713, fluctuations: 0.0\n",
      "step: 35580 loss: 1.182393 time elapsed: 46.8098 learning rate: 0.000957, scenario: 1, slope: -0.00011813977496699897, fluctuations: 0.0\n",
      "step: 35590 loss: 1.181196 time elapsed: 46.8242 learning rate: 0.001057, scenario: 1, slope: -0.00011644673363482887, fluctuations: 0.0\n",
      "step: 35600 loss: 1.179896 time elapsed: 46.8380 learning rate: 0.001156, scenario: 1, slope: -0.00011579313333644348, fluctuations: 0.0\n",
      "step: 35610 loss: 1.178497 time elapsed: 46.8525 learning rate: 0.001277, scenario: 1, slope: -0.00011655894364034816, fluctuations: 0.0\n",
      "step: 35620 loss: 1.176982 time elapsed: 46.8654 learning rate: 0.001396, scenario: 0, slope: -0.00011905047662584936, fluctuations: 0.0\n",
      "step: 35630 loss: 1.175414 time elapsed: 46.8790 learning rate: 0.001396, scenario: 0, slope: -0.00012332994200883615, fluctuations: 0.0\n",
      "step: 35640 loss: 1.173876 time elapsed: 46.8923 learning rate: 0.001396, scenario: 0, slope: -0.00012878682196302283, fluctuations: 0.0\n",
      "step: 35650 loss: 1.172367 time elapsed: 46.9069 learning rate: 0.001396, scenario: 0, slope: -0.00013467789728823187, fluctuations: 0.0\n",
      "step: 35660 loss: 1.170884 time elapsed: 46.9227 learning rate: 0.001396, scenario: 0, slope: -0.00014028981799238337, fluctuations: 0.0\n",
      "step: 35670 loss: 1.169425 time elapsed: 46.9382 learning rate: 0.001396, scenario: 0, slope: -0.00014493740651553744, fluctuations: 0.0\n",
      "step: 35680 loss: 1.167988 time elapsed: 46.9531 learning rate: 0.001396, scenario: 0, slope: -0.00014798506550286697, fluctuations: 0.0\n",
      "step: 35690 loss: 1.166572 time elapsed: 46.9675 learning rate: 0.001396, scenario: 0, slope: -0.00014921677619728936, fluctuations: 0.0\n",
      "step: 35700 loss: 1.165174 time elapsed: 46.9803 learning rate: 0.001396, scenario: 0, slope: -0.00014895330553544694, fluctuations: 0.0\n",
      "step: 35710 loss: 1.163794 time elapsed: 46.9967 learning rate: 0.001396, scenario: 0, slope: -0.00014729367457628188, fluctuations: 0.0\n",
      "step: 35720 loss: 1.162430 time elapsed: 47.0105 learning rate: 0.001396, scenario: 0, slope: -0.00014510294980929595, fluctuations: 0.0\n",
      "step: 35730 loss: 1.161080 time elapsed: 47.0238 learning rate: 0.001396, scenario: 0, slope: -0.0001429471223534044, fluctuations: 0.0\n",
      "step: 35740 loss: 1.159744 time elapsed: 47.0375 learning rate: 0.001396, scenario: 0, slope: -0.00014096278620955147, fluctuations: 0.0\n",
      "step: 35750 loss: 1.158420 time elapsed: 47.0524 learning rate: 0.001396, scenario: 0, slope: -0.00013913724124687144, fluctuations: 0.0\n",
      "step: 35760 loss: 1.157108 time elapsed: 47.0653 learning rate: 0.001396, scenario: 0, slope: -0.00013745765763659978, fluctuations: 0.0\n",
      "step: 35770 loss: 1.155806 time elapsed: 47.0789 learning rate: 0.001396, scenario: 0, slope: -0.0001359119811639888, fluctuations: 0.0\n",
      "step: 35780 loss: 1.154514 time elapsed: 47.0927 learning rate: 0.001396, scenario: 0, slope: -0.0001344891582849285, fluctuations: 0.0\n",
      "step: 35790 loss: 1.153230 time elapsed: 47.1054 learning rate: 0.001396, scenario: 0, slope: -0.00013317913456069322, fluctuations: 0.0\n",
      "step: 35800 loss: 1.151955 time elapsed: 47.1193 learning rate: 0.001396, scenario: 0, slope: -0.0001320889959209328, fluctuations: 0.0\n",
      "step: 35810 loss: 1.150688 time elapsed: 47.1340 learning rate: 0.001396, scenario: 0, slope: -0.0001308618217874036, fluctuations: 0.0\n",
      "step: 35820 loss: 1.149427 time elapsed: 47.1481 learning rate: 0.001396, scenario: 0, slope: -0.00012983872167187367, fluctuations: 0.0\n",
      "step: 35830 loss: 1.148173 time elapsed: 47.1621 learning rate: 0.001396, scenario: 0, slope: -0.00012889663701098016, fluctuations: 0.0\n",
      "step: 35840 loss: 1.146925 time elapsed: 47.1752 learning rate: 0.001396, scenario: 0, slope: -0.00012802933252765837, fluctuations: 0.0\n",
      "step: 35850 loss: 1.145682 time elapsed: 47.1881 learning rate: 0.001396, scenario: 0, slope: -0.00012723112268114224, fluctuations: 0.0\n",
      "step: 35860 loss: 1.144445 time elapsed: 47.2009 learning rate: 0.001396, scenario: 0, slope: -0.00012649681710191372, fluctuations: 0.0\n",
      "step: 35870 loss: 1.143211 time elapsed: 47.2134 learning rate: 0.001396, scenario: 0, slope: -0.000125821672105623, fluctuations: 0.0\n",
      "step: 35880 loss: 1.141982 time elapsed: 47.2262 learning rate: 0.001396, scenario: 0, slope: -0.00012520134754264434, fluctuations: 0.0\n",
      "step: 35890 loss: 1.140757 time elapsed: 47.2387 learning rate: 0.001396, scenario: 0, slope: -0.00012463186832888936, fluctuations: 0.0\n",
      "step: 35900 loss: 1.139535 time elapsed: 47.2512 learning rate: 0.001396, scenario: 0, slope: -0.00012415979140429102, fluctuations: 0.0\n",
      "step: 35910 loss: 1.138316 time elapsed: 47.2644 learning rate: 0.001396, scenario: 0, slope: -0.00012363116840665713, fluctuations: 0.0\n",
      "step: 35920 loss: 1.137100 time elapsed: 47.2769 learning rate: 0.001396, scenario: 0, slope: -0.00012319353129430717, fluctuations: 0.0\n",
      "step: 35930 loss: 1.135886 time elapsed: 47.2895 learning rate: 0.001396, scenario: 0, slope: -0.00012279385444470675, fluctuations: 0.0\n",
      "step: 35940 loss: 1.134675 time elapsed: 47.3021 learning rate: 0.001396, scenario: 0, slope: -0.00012242953900918336, fluctuations: 0.0\n",
      "step: 35950 loss: 1.133466 time elapsed: 47.3143 learning rate: 0.001396, scenario: 0, slope: -0.00012209819158029032, fluctuations: 0.0\n",
      "step: 35960 loss: 1.132259 time elapsed: 47.3269 learning rate: 0.001396, scenario: 0, slope: -0.00012179760614744194, fluctuations: 0.0\n",
      "step: 35970 loss: 1.131053 time elapsed: 47.3407 learning rate: 0.001396, scenario: 0, slope: -0.00012152574780692428, fluctuations: 0.0\n",
      "step: 35980 loss: 1.129849 time elapsed: 47.3554 learning rate: 0.001396, scenario: 0, slope: -0.00012128073803781026, fluctuations: 0.0\n",
      "step: 35990 loss: 1.128646 time elapsed: 47.3701 learning rate: 0.001396, scenario: 0, slope: -0.00012106084137675634, fluctuations: 0.0\n",
      "step: 36000 loss: 1.127444 time elapsed: 47.3851 learning rate: 0.001396, scenario: 0, slope: -0.00012088307745988886, fluctuations: 0.0\n",
      "step: 36010 loss: 1.126242 time elapsed: 47.3991 learning rate: 0.001396, scenario: 0, slope: -0.00012069008949245309, fluctuations: 0.0\n",
      "step: 36020 loss: 1.125042 time elapsed: 47.4127 learning rate: 0.001396, scenario: 0, slope: -0.00012053637546118016, fluctuations: 0.0\n",
      "step: 36030 loss: 1.123842 time elapsed: 47.4262 learning rate: 0.001396, scenario: 0, slope: -0.0001204020379356335, fluctuations: 0.0\n",
      "step: 36040 loss: 1.122643 time elapsed: 47.4393 learning rate: 0.001396, scenario: 0, slope: -0.00012028589642047549, fluctuations: 0.0\n",
      "step: 36050 loss: 1.121444 time elapsed: 47.4524 learning rate: 0.001396, scenario: 0, slope: -0.00012018685574544864, fluctuations: 0.0\n",
      "step: 36060 loss: 1.120245 time elapsed: 47.4655 learning rate: 0.001396, scenario: 0, slope: -0.00012010389923265729, fluctuations: 0.0\n",
      "step: 36070 loss: 1.119047 time elapsed: 47.4789 learning rate: 0.001396, scenario: 0, slope: -0.00012003608246014394, fluctuations: 0.0\n",
      "step: 36080 loss: 1.117848 time elapsed: 47.4922 learning rate: 0.001396, scenario: 0, slope: -0.00011998252741359766, fluctuations: 0.0\n",
      "step: 36090 loss: 1.116649 time elapsed: 47.5052 learning rate: 0.001396, scenario: 0, slope: -0.00011994225828771126, fluctuations: 0.0\n",
      "step: 36100 loss: 1.115705 time elapsed: 47.5188 learning rate: 0.001396, scenario: 0, slope: -0.00011978845883625938, fluctuations: 0.0\n",
      "step: 36110 loss: 2.337981 time elapsed: 47.5340 learning rate: 0.001403, scenario: -1, slope: 0.0010251551620452405, fluctuations: 0.0\n",
      "step: 36120 loss: 26.563896 time elapsed: 47.5494 learning rate: 0.001269, scenario: -1, slope: 0.11493227885693463, fluctuations: 0.01\n",
      "step: 36130 loss: 5.555477 time elapsed: 47.5646 learning rate: 0.001148, scenario: -1, slope: 0.12135869552248446, fluctuations: 0.04\n",
      "step: 36140 loss: 1.684960 time elapsed: 47.5797 learning rate: 0.001038, scenario: -1, slope: 0.10792332355101183, fluctuations: 0.08\n",
      "step: 36150 loss: 1.139279 time elapsed: 47.5935 learning rate: 0.000939, scenario: -1, slope: 0.0830784075465869, fluctuations: 0.12\n",
      "step: 36160 loss: 1.330563 time elapsed: 47.6065 learning rate: 0.000849, scenario: -1, slope: 0.050775978773212244, fluctuations: 0.16\n",
      "step: 36170 loss: 1.148726 time elapsed: 47.6194 learning rate: 0.000768, scenario: -1, slope: 0.013715945704566078, fluctuations: 0.2\n",
      "step: 36180 loss: 1.125289 time elapsed: 47.6320 learning rate: 0.000745, scenario: 0, slope: -0.023231955281511656, fluctuations: 0.25\n",
      "step: 36190 loss: 1.117698 time elapsed: 47.6447 learning rate: 0.000745, scenario: 0, slope: -0.06660098213783278, fluctuations: 0.27\n",
      "step: 36200 loss: 1.113974 time elapsed: 47.6570 learning rate: 0.000745, scenario: 0, slope: -0.11022049365445453, fluctuations: 0.32\n",
      "step: 36210 loss: 1.111631 time elapsed: 47.6699 learning rate: 0.000745, scenario: 0, slope: -0.17744816089473384, fluctuations: 0.37\n",
      "step: 36220 loss: 1.110208 time elapsed: 47.6824 learning rate: 0.000745, scenario: 0, slope: -0.05893263048967537, fluctuations: 0.4\n",
      "step: 36230 loss: 1.109274 time elapsed: 47.6952 learning rate: 0.000745, scenario: 0, slope: -0.015402515389444935, fluctuations: 0.42\n",
      "step: 36240 loss: 1.108527 time elapsed: 47.7082 learning rate: 0.000745, scenario: 0, slope: -0.0027918472635176904, fluctuations: 0.41\n",
      "step: 36250 loss: 1.107838 time elapsed: 47.7207 learning rate: 0.000745, scenario: 0, slope: -0.0013964672644135273, fluctuations: 0.36\n",
      "step: 36260 loss: 1.107169 time elapsed: 47.7331 learning rate: 0.000745, scenario: 0, slope: -0.00041584976331912095, fluctuations: 0.32\n",
      "step: 36270 loss: 1.106509 time elapsed: 47.7456 learning rate: 0.000745, scenario: 0, slope: -0.00022054097842465474, fluctuations: 0.28\n",
      "step: 36280 loss: 1.105853 time elapsed: 47.7591 learning rate: 0.000745, scenario: 0, slope: -0.0001225061551944957, fluctuations: 0.24\n",
      "step: 36290 loss: 1.105218 time elapsed: 47.7734 learning rate: 0.000681, scenario: -1, slope: -7.530187555430499e-05, fluctuations: 0.21\n",
      "step: 36300 loss: 1.104643 time elapsed: 47.7872 learning rate: 0.000622, scenario: -1, slope: -6.958512620825381e-05, fluctuations: 0.16\n",
      "step: 36310 loss: 1.104119 time elapsed: 47.8020 learning rate: 0.000562, scenario: -1, slope: -6.50053393829835e-05, fluctuations: 0.11\n",
      "step: 36320 loss: 1.103644 time elapsed: 47.8151 learning rate: 0.000508, scenario: -1, slope: -6.316753583760577e-05, fluctuations: 0.06\n",
      "step: 36330 loss: 1.103214 time elapsed: 47.8281 learning rate: 0.000460, scenario: -1, slope: -6.091161176973291e-05, fluctuations: 0.01\n",
      "step: 36340 loss: 1.102798 time elapsed: 47.8411 learning rate: 0.000505, scenario: 1, slope: -5.78759866940407e-05, fluctuations: 0.0\n",
      "step: 36350 loss: 1.102338 time elapsed: 47.8545 learning rate: 0.000558, scenario: 1, slope: -5.464894721539881e-05, fluctuations: 0.0\n",
      "step: 36360 loss: 1.101829 time elapsed: 47.8676 learning rate: 0.000617, scenario: 1, slope: -5.177563301987972e-05, fluctuations: 0.0\n",
      "step: 36370 loss: 1.101267 time elapsed: 47.8807 learning rate: 0.000681, scenario: 1, slope: -4.969359973957137e-05, fluctuations: 0.0\n",
      "step: 36380 loss: 1.100644 time elapsed: 47.8945 learning rate: 0.000753, scenario: 1, slope: -4.8827708760719736e-05, fluctuations: 0.0\n",
      "step: 36390 loss: 1.099956 time elapsed: 47.9080 learning rate: 0.000831, scenario: 1, slope: -4.9540701340749816e-05, fluctuations: 0.0\n",
      "step: 36400 loss: 1.099196 time elapsed: 47.9215 learning rate: 0.000909, scenario: 1, slope: -5.15898531815714e-05, fluctuations: 0.0\n",
      "step: 36410 loss: 1.098362 time elapsed: 47.9358 learning rate: 0.001004, scenario: 1, slope: -5.576960915171621e-05, fluctuations: 0.0\n",
      "step: 36420 loss: 1.097442 time elapsed: 47.9497 learning rate: 0.001109, scenario: 1, slope: -6.095930582143316e-05, fluctuations: 0.0\n",
      "step: 36430 loss: 1.096426 time elapsed: 47.9641 learning rate: 0.001225, scenario: 1, slope: -6.71936557314787e-05, fluctuations: 0.0\n",
      "step: 36440 loss: 1.095303 time elapsed: 47.9787 learning rate: 0.001354, scenario: 1, slope: -7.417179670366039e-05, fluctuations: 0.0\n",
      "step: 36450 loss: 1.094062 time elapsed: 47.9936 learning rate: 0.001495, scenario: 1, slope: -8.18601909027816e-05, fluctuations: 0.0\n",
      "step: 36460 loss: 1.092848 time elapsed: 48.0084 learning rate: 0.001652, scenario: 1, slope: -9.02267969429642e-05, fluctuations: 0.0\n",
      "step: 36470 loss: 1039.307752 time elapsed: 48.0219 learning rate: 0.001594, scenario: -1, slope: 0.9075886572232713, fluctuations: 0.01\n",
      "step: 36480 loss: 330.469324 time elapsed: 48.0354 learning rate: 0.001442, scenario: -1, slope: 1.3499363123493988, fluctuations: 0.05\n",
      "step: 36490 loss: 25.136693 time elapsed: 48.0487 learning rate: 0.001304, scenario: -1, slope: 1.1987028022430235, fluctuations: 0.1\n",
      "step: 36500 loss: 10.894644 time elapsed: 48.0610 learning rate: 0.001191, scenario: -1, slope: 1.0461437512250913, fluctuations: 0.13\n",
      "step: 36510 loss: 3.646954 time elapsed: 48.0739 learning rate: 0.001077, scenario: -1, slope: 0.7464429796294122, fluctuations: 0.18\n",
      "step: 36520 loss: 5.134713 time elapsed: 48.0862 learning rate: 0.000974, scenario: -1, slope: 0.3575533271260395, fluctuations: 0.21\n",
      "step: 36530 loss: 1.866563 time elapsed: 48.0987 learning rate: 0.000890, scenario: 0, slope: -0.005690827540148452, fluctuations: 0.25\n",
      "step: 36540 loss: 1.638865 time elapsed: 48.1111 learning rate: 0.000890, scenario: 0, slope: -0.41116074498413946, fluctuations: 0.28\n",
      "step: 36550 loss: 1.483158 time elapsed: 48.1235 learning rate: 0.000890, scenario: 0, slope: -0.8659941650824291, fluctuations: 0.32\n",
      "step: 36560 loss: 1.351538 time elapsed: 48.1360 learning rate: 0.000890, scenario: 0, slope: -1.457306448769059, fluctuations: 0.35\n",
      "step: 36570 loss: 1.333047 time elapsed: 48.1485 learning rate: 0.000890, scenario: 0, slope: -0.6916995163069164, fluctuations: 0.37\n",
      "step: 36580 loss: 1.303823 time elapsed: 48.1612 learning rate: 0.000890, scenario: 0, slope: -0.35601586446038586, fluctuations: 0.36\n",
      "step: 36590 loss: 1.284050 time elapsed: 48.1750 learning rate: 0.000890, scenario: 0, slope: -0.2088023681697139, fluctuations: 0.33\n",
      "step: 36600 loss: 1.270261 time elapsed: 48.1891 learning rate: 0.000890, scenario: 0, slope: -0.05250196754821865, fluctuations: 0.3\n",
      "step: 36610 loss: 1.258014 time elapsed: 48.2036 learning rate: 0.000890, scenario: 0, slope: -0.01766238172327006, fluctuations: 0.26\n",
      "step: 36620 loss: 1.247988 time elapsed: 48.2176 learning rate: 0.000890, scenario: 0, slope: -0.007648450455357199, fluctuations: 0.22\n",
      "step: 36630 loss: 1.239113 time elapsed: 48.2307 learning rate: 0.000890, scenario: 0, slope: -0.00403812505272551, fluctuations: 0.19\n",
      "step: 36640 loss: 1.231286 time elapsed: 48.2435 learning rate: 0.000890, scenario: 0, slope: -0.002533927550597876, fluctuations: 0.15\n",
      "step: 36650 loss: 1.224277 time elapsed: 48.2566 learning rate: 0.000890, scenario: 0, slope: -0.0016093386500862354, fluctuations: 0.12\n",
      "step: 36660 loss: 1.217942 time elapsed: 48.2697 learning rate: 0.000890, scenario: 0, slope: -0.0012653692706123213, fluctuations: 0.08\n",
      "step: 36670 loss: 1.212176 time elapsed: 48.2822 learning rate: 0.000890, scenario: 0, slope: -0.0010228817154672334, fluctuations: 0.05\n",
      "step: 36680 loss: 1.206894 time elapsed: 48.2951 learning rate: 0.000890, scenario: 0, slope: -0.0008806990826338657, fluctuations: 0.02\n",
      "step: 36690 loss: 1.202031 time elapsed: 48.3077 learning rate: 0.000890, scenario: 0, slope: -0.0007824526256015968, fluctuations: 0.0\n",
      "step: 36700 loss: 1.197535 time elapsed: 48.3198 learning rate: 0.000890, scenario: 0, slope: -0.0007032566609420796, fluctuations: 0.0\n",
      "step: 36710 loss: 1.193361 time elapsed: 48.3329 learning rate: 0.000890, scenario: 0, slope: -0.0006257644930287353, fluctuations: 0.0\n",
      "step: 36720 loss: 1.189473 time elapsed: 48.3454 learning rate: 0.000890, scenario: 0, slope: -0.0005681407247302581, fluctuations: 0.0\n",
      "step: 36730 loss: 1.185840 time elapsed: 48.3579 learning rate: 0.000890, scenario: 0, slope: -0.0005194953265028139, fluctuations: 0.0\n",
      "step: 36740 loss: 1.182436 time elapsed: 48.3704 learning rate: 0.000890, scenario: 0, slope: -0.00047775552459439625, fluctuations: 0.0\n",
      "step: 36750 loss: 1.179238 time elapsed: 48.3834 learning rate: 0.000890, scenario: 0, slope: -0.00044149520905262376, fluctuations: 0.0\n",
      "step: 36760 loss: 1.176226 time elapsed: 48.3975 learning rate: 0.000890, scenario: 0, slope: -0.00040968690770923474, fluctuations: 0.0\n",
      "step: 36770 loss: 1.173382 time elapsed: 48.4123 learning rate: 0.000890, scenario: 0, slope: -0.00038155935748611055, fluctuations: 0.0\n",
      "step: 36780 loss: 1.170692 time elapsed: 48.4274 learning rate: 0.000890, scenario: 0, slope: -0.00035652407692818115, fluctuations: 0.0\n",
      "step: 36790 loss: 1.168141 time elapsed: 48.4415 learning rate: 0.000890, scenario: 0, slope: -0.00033411973131736297, fluctuations: 0.0\n",
      "step: 36800 loss: 1.165717 time elapsed: 48.4555 learning rate: 0.000890, scenario: 0, slope: -0.0003159000288932724, fluctuations: 0.0\n",
      "step: 36810 loss: 1.163410 time elapsed: 48.4698 learning rate: 0.000890, scenario: 0, slope: -0.0002958030577586203, fluctuations: 0.0\n",
      "step: 36820 loss: 1.161209 time elapsed: 48.4827 learning rate: 0.000890, scenario: 0, slope: -0.0002793465911049012, fluctuations: 0.0\n",
      "step: 36830 loss: 1.159106 time elapsed: 48.4960 learning rate: 0.000890, scenario: 0, slope: -0.00026440409931506473, fluctuations: 0.0\n",
      "step: 36840 loss: 1.157093 time elapsed: 48.5093 learning rate: 0.000890, scenario: 0, slope: -0.0002508020556772738, fluctuations: 0.0\n",
      "step: 36850 loss: 1.155163 time elapsed: 48.5231 learning rate: 0.000890, scenario: 0, slope: -0.00023839232917821594, fluctuations: 0.0\n",
      "step: 36860 loss: 1.153309 time elapsed: 48.5366 learning rate: 0.000890, scenario: 0, slope: -0.00022704738135632003, fluctuations: 0.0\n",
      "step: 36870 loss: 1.151525 time elapsed: 48.5497 learning rate: 0.000890, scenario: 0, slope: -0.00021665659896281174, fluctuations: 0.0\n",
      "step: 36880 loss: 1.149806 time elapsed: 48.5635 learning rate: 0.000890, scenario: 0, slope: -0.00020712344906421686, fluctuations: 0.0\n",
      "step: 36890 loss: 1.148147 time elapsed: 48.5776 learning rate: 0.000890, scenario: 0, slope: -0.00019836323701438055, fluctuations: 0.0\n",
      "step: 36900 loss: 1.146544 time elapsed: 48.5919 learning rate: 0.000890, scenario: 0, slope: -0.000191078027013587, fluctuations: 0.0\n",
      "step: 36910 loss: 1.144992 time elapsed: 48.6067 learning rate: 0.000890, scenario: 0, slope: -0.00018287162142439301, fluctuations: 0.0\n",
      "step: 36920 loss: 1.143488 time elapsed: 48.6206 learning rate: 0.000890, scenario: 0, slope: -0.0001760155048859717, fluctuations: 0.0\n",
      "step: 36930 loss: 1.142029 time elapsed: 48.6346 learning rate: 0.000890, scenario: 0, slope: -0.0001696807208663467, fluctuations: 0.0\n",
      "step: 36940 loss: 1.140611 time elapsed: 48.6473 learning rate: 0.000890, scenario: 0, slope: -0.00016382061271892108, fluctuations: 0.0\n",
      "step: 36950 loss: 1.139232 time elapsed: 48.6602 learning rate: 0.000890, scenario: 0, slope: -0.00015839341217167139, fluctuations: 0.0\n",
      "step: 36960 loss: 1.137888 time elapsed: 48.6730 learning rate: 0.000890, scenario: 0, slope: -0.0001533616456247118, fluctuations: 0.0\n",
      "step: 36970 loss: 1.136578 time elapsed: 48.6857 learning rate: 0.000890, scenario: 0, slope: -0.00014869162569103601, fluctuations: 0.0\n",
      "step: 36980 loss: 1.135299 time elapsed: 48.6985 learning rate: 0.000890, scenario: 0, slope: -0.00014435301311559233, fluctuations: 0.0\n",
      "step: 36990 loss: 1.134049 time elapsed: 48.7111 learning rate: 0.000890, scenario: 0, slope: -0.00014031843731558127, fluctuations: 0.0\n",
      "step: 37000 loss: 1.132826 time elapsed: 48.7234 learning rate: 0.000890, scenario: 0, slope: -0.00013692677966801155, fluctuations: 0.0\n",
      "step: 37010 loss: 1.131629 time elapsed: 48.7364 learning rate: 0.000890, scenario: 0, slope: -0.00013306481734733598, fluctuations: 0.0\n",
      "step: 37020 loss: 1.130456 time elapsed: 48.7489 learning rate: 0.000890, scenario: 0, slope: -0.00012980310542082079, fluctuations: 0.0\n",
      "step: 37030 loss: 1.129305 time elapsed: 48.7611 learning rate: 0.000890, scenario: 0, slope: -0.0001267596190219234, fluctuations: 0.0\n",
      "step: 37040 loss: 1.128174 time elapsed: 48.7734 learning rate: 0.000890, scenario: 0, slope: -0.00012391762453452837, fluctuations: 0.0\n",
      "step: 37050 loss: 1.127064 time elapsed: 48.7870 learning rate: 0.000890, scenario: 0, slope: -0.00012126189241972762, fluctuations: 0.0\n",
      "step: 37060 loss: 1.125971 time elapsed: 48.8015 learning rate: 0.000890, scenario: 0, slope: -0.00011877854337000948, fluctuations: 0.0\n",
      "step: 37070 loss: 1.124896 time elapsed: 48.8156 learning rate: 0.000890, scenario: 0, slope: -0.00011645491178185991, fluctuations: 0.0\n",
      "step: 37080 loss: 1.123837 time elapsed: 48.8296 learning rate: 0.000890, scenario: 0, slope: -0.00011427942441989815, fluctuations: 0.0\n",
      "step: 37090 loss: 1.122778 time elapsed: 48.8431 learning rate: 0.000954, scenario: 1, slope: -0.00011226286084981009, fluctuations: 0.0\n",
      "step: 37100 loss: 1.121636 time elapsed: 48.8553 learning rate: 0.001044, scenario: 1, slope: -0.00011084747399779497, fluctuations: 0.0\n",
      "step: 37110 loss: 1.120404 time elapsed: 48.8683 learning rate: 0.001153, scenario: 1, slope: -0.00011016109898166864, fluctuations: 0.0\n",
      "step: 37120 loss: 1.119067 time elapsed: 48.8807 learning rate: 0.001273, scenario: 1, slope: -0.00011093709081772366, fluctuations: 0.0\n",
      "step: 37130 loss: 1.117615 time elapsed: 48.8934 learning rate: 0.001379, scenario: 0, slope: -0.00011338198534447854, fluctuations: 0.0\n",
      "step: 37140 loss: 1.116124 time elapsed: 48.9061 learning rate: 0.001379, scenario: 0, slope: -0.00011752027277381611, fluctuations: 0.0\n",
      "step: 37150 loss: 1.114660 time elapsed: 48.9189 learning rate: 0.001379, scenario: 0, slope: -0.0001227483540577121, fluctuations: 0.0\n",
      "step: 37160 loss: 1.113221 time elapsed: 48.9315 learning rate: 0.001379, scenario: 0, slope: -0.00012837684442374418, fluctuations: 0.0\n",
      "step: 37170 loss: 1.111804 time elapsed: 48.9442 learning rate: 0.001379, scenario: 0, slope: -0.00013374231924337596, fluctuations: 0.0\n",
      "step: 37180 loss: 1.110408 time elapsed: 48.9576 learning rate: 0.001379, scenario: 0, slope: -0.00013820565389744218, fluctuations: 0.0\n",
      "step: 37190 loss: 1.109031 time elapsed: 48.9709 learning rate: 0.001379, scenario: 0, slope: -0.00014117255742483227, fluctuations: 0.0\n",
      "step: 37200 loss: 1.107671 time elapsed: 48.9837 learning rate: 0.001379, scenario: 0, slope: -0.00014239092607075502, fluctuations: 0.0\n",
      "step: 37210 loss: 1.106328 time elapsed: 48.9985 learning rate: 0.001379, scenario: 0, slope: -0.00014221700649679606, fluctuations: 0.0\n",
      "step: 37220 loss: 1.104999 time elapsed: 49.0137 learning rate: 0.001379, scenario: 0, slope: -0.0001408478379862811, fluctuations: 0.0\n",
      "step: 37230 loss: 1.103684 time elapsed: 49.0287 learning rate: 0.001379, scenario: 0, slope: -0.0001389388557747806, fluctuations: 0.0\n",
      "step: 37240 loss: 1.102382 time elapsed: 49.0438 learning rate: 0.001379, scenario: 0, slope: -0.0001370857329662366, fluctuations: 0.0\n",
      "step: 37250 loss: 1.101092 time elapsed: 49.0576 learning rate: 0.001379, scenario: 0, slope: -0.00013538032126913217, fluctuations: 0.0\n",
      "step: 37260 loss: 1.099812 time elapsed: 49.0711 learning rate: 0.001379, scenario: 0, slope: -0.0001338105891430399, fluctuations: 0.0\n",
      "step: 37270 loss: 1.098542 time elapsed: 49.0842 learning rate: 0.001379, scenario: 0, slope: -0.00013236469879791403, fluctuations: 0.0\n",
      "step: 37280 loss: 1.097281 time elapsed: 49.0967 learning rate: 0.001379, scenario: 0, slope: -0.0001310316947818847, fluctuations: 0.0\n",
      "step: 37290 loss: 1.096029 time elapsed: 49.1095 learning rate: 0.001379, scenario: 0, slope: -0.000129801647303018, fluctuations: 0.0\n",
      "step: 37300 loss: 1.094785 time elapsed: 49.1218 learning rate: 0.001379, scenario: 0, slope: -0.0001287752253045465, fluctuations: 0.0\n",
      "step: 37310 loss: 1.093548 time elapsed: 49.1349 learning rate: 0.001379, scenario: 0, slope: -0.0001276155714336875, fluctuations: 0.0\n",
      "step: 37320 loss: 1.092318 time elapsed: 49.1476 learning rate: 0.001379, scenario: 0, slope: -0.00012664428579375187, fluctuations: 0.0\n",
      "step: 37330 loss: 1.091095 time elapsed: 49.1598 learning rate: 0.001379, scenario: 0, slope: -0.00012574525868570214, fluctuations: 0.0\n",
      "step: 37340 loss: 1.089877 time elapsed: 49.1721 learning rate: 0.001379, scenario: 0, slope: -0.0001249126294020813, fluctuations: 0.0\n",
      "step: 37350 loss: 1.088664 time elapsed: 49.1844 learning rate: 0.001379, scenario: 0, slope: -0.000124141107313987, fluctuations: 0.0\n",
      "step: 37360 loss: 1.087457 time elapsed: 49.1967 learning rate: 0.001379, scenario: 0, slope: -0.0001234259092210271, fluctuations: 0.0\n",
      "step: 37370 loss: 1.086254 time elapsed: 49.2091 learning rate: 0.001379, scenario: 0, slope: -0.00012276270420257608, fluctuations: 0.0\n",
      "step: 37380 loss: 1.085056 time elapsed: 49.2223 learning rate: 0.001379, scenario: 0, slope: -0.00012214756504247607, fluctuations: 0.0\n",
      "step: 37390 loss: 1.083862 time elapsed: 49.2363 learning rate: 0.001379, scenario: 0, slope: -0.00012157692539176631, fluctuations: 0.0\n",
      "step: 37400 loss: 1.082671 time elapsed: 49.2501 learning rate: 0.001379, scenario: 0, slope: -0.00012109871006446797, fluctuations: 0.0\n",
      "step: 37410 loss: 1.081484 time elapsed: 49.2646 learning rate: 0.001379, scenario: 0, slope: -0.0001205564609454852, fluctuations: 0.0\n",
      "step: 37420 loss: 1.080300 time elapsed: 49.2783 learning rate: 0.001379, scenario: 0, slope: -0.00012010098863211647, fluctuations: 0.0\n",
      "step: 37430 loss: 1.079120 time elapsed: 49.2912 learning rate: 0.001379, scenario: 0, slope: -0.00011967866488344243, fluctuations: 0.0\n",
      "step: 37440 loss: 1.077941 time elapsed: 49.3039 learning rate: 0.001379, scenario: 0, slope: -0.00011928723992024418, fluctuations: 0.0\n",
      "step: 37450 loss: 1.076766 time elapsed: 49.3163 learning rate: 0.001379, scenario: 0, slope: -0.0001189246535468736, fluctuations: 0.0\n",
      "step: 37460 loss: 1.075592 time elapsed: 49.3287 learning rate: 0.001379, scenario: 0, slope: -0.00011858901667558009, fluctuations: 0.0\n",
      "step: 37470 loss: 1.074421 time elapsed: 49.3413 learning rate: 0.001379, scenario: 0, slope: -0.0001182785948562949, fluctuations: 0.0\n",
      "step: 37480 loss: 1.073252 time elapsed: 49.3537 learning rate: 0.001379, scenario: 0, slope: -0.00011799179357532847, fluctuations: 0.0\n",
      "step: 37490 loss: 1.072084 time elapsed: 49.3660 learning rate: 0.001379, scenario: 0, slope: -0.00011772714511792777, fluctuations: 0.0\n",
      "step: 37500 loss: 1.070919 time elapsed: 49.3783 learning rate: 0.001379, scenario: 0, slope: -0.00011750678189710045, fluctuations: 0.0\n",
      "step: 37510 loss: 1.069754 time elapsed: 49.3913 learning rate: 0.001379, scenario: 0, slope: -0.0001172588310617855, fluctuations: 0.0\n",
      "step: 37520 loss: 1.068848 time elapsed: 49.4034 learning rate: 0.001379, scenario: 0, slope: -0.00011677208247920227, fluctuations: 0.0\n",
      "step: 37530 loss: 2.250111 time elapsed: 49.4159 learning rate: 0.001386, scenario: -1, slope: 0.000991870206987994, fluctuations: 0.0\n",
      "step: 37540 loss: 27.076678 time elapsed: 49.4281 learning rate: 0.001253, scenario: -1, slope: 0.11925282338518586, fluctuations: 0.01\n",
      "step: 37550 loss: 7.037113 time elapsed: 49.4421 learning rate: 0.001133, scenario: -1, slope: 0.1257450759164648, fluctuations: 0.04\n",
      "step: 37560 loss: 1.225679 time elapsed: 49.4561 learning rate: 0.001025, scenario: -1, slope: 0.11050023277296026, fluctuations: 0.08\n",
      "step: 37570 loss: 1.075161 time elapsed: 49.4702 learning rate: 0.000927, scenario: -1, slope: 0.08472903835890182, fluctuations: 0.12\n",
      "step: 37580 loss: 1.324148 time elapsed: 49.4846 learning rate: 0.000838, scenario: -1, slope: 0.05138659438095615, fluctuations: 0.16\n",
      "step: 37590 loss: 1.124877 time elapsed: 49.4976 learning rate: 0.000758, scenario: -1, slope: 0.01663930620578557, fluctuations: 0.21\n",
      "step: 37600 loss: 1.091686 time elapsed: 49.5104 learning rate: 0.000728, scenario: 0, slope: -0.01782892842639716, fluctuations: 0.26\n",
      "step: 37610 loss: 1.075182 time elapsed: 49.5240 learning rate: 0.000728, scenario: 0, slope: -0.06757771162644378, fluctuations: 0.31\n",
      "step: 37620 loss: 1.067731 time elapsed: 49.5368 learning rate: 0.000728, scenario: 0, slope: -0.11898038356147536, fluctuations: 0.36\n",
      "step: 37630 loss: 1.064939 time elapsed: 49.5496 learning rate: 0.000728, scenario: 0, slope: -0.18381309871194496, fluctuations: 0.41\n",
      "step: 37640 loss: 1.063702 time elapsed: 49.5633 learning rate: 0.000728, scenario: 0, slope: -0.060966109920469615, fluctuations: 0.42\n",
      "step: 37650 loss: 1.062873 time elapsed: 49.5770 learning rate: 0.000728, scenario: 0, slope: -0.014629405176603221, fluctuations: 0.44\n",
      "step: 37660 loss: 1.062142 time elapsed: 49.5903 learning rate: 0.000728, scenario: 0, slope: -0.0043386446455167624, fluctuations: 0.42\n",
      "step: 37670 loss: 1.061440 time elapsed: 49.6041 learning rate: 0.000728, scenario: 0, slope: -0.001332096431710527, fluctuations: 0.38\n",
      "step: 37680 loss: 1.060753 time elapsed: 49.6179 learning rate: 0.000728, scenario: 0, slope: -0.00021087789780727966, fluctuations: 0.34\n",
      "step: 37690 loss: 1.060078 time elapsed: 49.6326 learning rate: 0.000728, scenario: 0, slope: -0.00013284939242910932, fluctuations: 0.29\n",
      "step: 37700 loss: 1.059417 time elapsed: 49.6473 learning rate: 0.000700, scenario: -1, slope: -0.00010309696792134463, fluctuations: 0.24\n",
      "step: 37710 loss: 1.058806 time elapsed: 49.6629 learning rate: 0.000633, scenario: -1, slope: -7.733626714803101e-05, fluctuations: 0.19\n",
      "step: 37720 loss: 1.058257 time elapsed: 49.6777 learning rate: 0.000572, scenario: -1, slope: -7.36303257066289e-05, fluctuations: 0.14\n",
      "step: 37730 loss: 1.057763 time elapsed: 49.6914 learning rate: 0.000517, scenario: -1, slope: -6.830149402099559e-05, fluctuations: 0.1\n",
      "step: 37740 loss: 1.057318 time elapsed: 49.7048 learning rate: 0.000468, scenario: -1, slope: -6.384886202678382e-05, fluctuations: 0.06\n",
      "step: 37750 loss: 1.056917 time elapsed: 49.7179 learning rate: 0.000423, scenario: -1, slope: -6.054659635101886e-05, fluctuations: 0.01\n",
      "step: 37760 loss: 1.056535 time elapsed: 49.7305 learning rate: 0.000456, scenario: 1, slope: -5.6642610478484805e-05, fluctuations: 0.0\n",
      "step: 37770 loss: 1.056116 time elapsed: 49.7434 learning rate: 0.000504, scenario: 1, slope: -5.263959220343252e-05, fluctuations: 0.0\n",
      "step: 37780 loss: 1.055654 time elapsed: 49.7560 learning rate: 0.000556, scenario: 1, slope: -4.911361728169646e-05, fluctuations: 0.0\n",
      "step: 37790 loss: 1.055144 time elapsed: 49.7686 learning rate: 0.000615, scenario: 1, slope: -4.6500714925021566e-05, fluctuations: 0.0\n",
      "step: 37800 loss: 1.054582 time elapsed: 49.7812 learning rate: 0.000672, scenario: 1, slope: -4.527804800514429e-05, fluctuations: 0.0\n",
      "step: 37810 loss: 1.053967 time elapsed: 49.7944 learning rate: 0.000743, scenario: 1, slope: -4.549538355891525e-05, fluctuations: 0.0\n",
      "step: 37820 loss: 1.053289 time elapsed: 49.8070 learning rate: 0.000820, scenario: 1, slope: -4.730028561000579e-05, fluctuations: 0.0\n",
      "step: 37830 loss: 1.052542 time elapsed: 49.8198 learning rate: 0.000906, scenario: 1, slope: -5.0520343280683756e-05, fluctuations: 0.0\n",
      "step: 37840 loss: 1.051718 time elapsed: 49.8332 learning rate: 0.001001, scenario: 1, slope: -5.4974006615467444e-05, fluctuations: 0.0\n",
      "step: 37850 loss: 1.050810 time elapsed: 49.8469 learning rate: 0.001106, scenario: 1, slope: -6.040839519487618e-05, fluctuations: 0.0\n",
      "step: 37860 loss: 1.049809 time elapsed: 49.8609 learning rate: 0.001221, scenario: 1, slope: -6.653143446037436e-05, fluctuations: 0.0\n",
      "step: 37870 loss: 1.048706 time elapsed: 49.8747 learning rate: 0.001349, scenario: 1, slope: -7.328075792828679e-05, fluctuations: 0.0\n",
      "step: 37880 loss: 1.047491 time elapsed: 49.8878 learning rate: 0.001490, scenario: 1, slope: -8.072566890724568e-05, fluctuations: 0.0\n",
      "step: 37890 loss: 1.046154 time elapsed: 49.9007 learning rate: 0.001646, scenario: 1, slope: -8.894165115294244e-05, fluctuations: 0.0\n",
      "step: 37900 loss: 563.173083 time elapsed: 49.9131 learning rate: 0.001670, scenario: -1, slope: 0.06125411152322993, fluctuations: 0.0\n",
      "step: 37910 loss: 544.082544 time elapsed: 49.9263 learning rate: 0.001511, scenario: -1, slope: 1.497363578445886, fluctuations: 0.04\n",
      "step: 37920 loss: 41.751668 time elapsed: 49.9390 learning rate: 0.001366, scenario: -1, slope: 1.34795783789259, fluctuations: 0.09\n",
      "step: 37930 loss: 14.957617 time elapsed: 49.9515 learning rate: 0.001236, scenario: -1, slope: 1.2132957217223308, fluctuations: 0.13\n",
      "step: 37940 loss: 17.828080 time elapsed: 49.9641 learning rate: 0.001117, scenario: -1, slope: 0.8565399768306647, fluctuations: 0.16\n",
      "step: 37950 loss: 3.093983 time elapsed: 49.9764 learning rate: 0.001011, scenario: -1, slope: 0.48031911395401683, fluctuations: 0.2\n",
      "step: 37960 loss: 3.472384 time elapsed: 49.9889 learning rate: 0.000914, scenario: -1, slope: 0.05143992167287006, fluctuations: 0.23\n",
      "step: 37970 loss: 1.848297 time elapsed: 50.0014 learning rate: 0.000905, scenario: 0, slope: -0.3709214389813849, fluctuations: 0.27\n",
      "step: 37980 loss: 1.466951 time elapsed: 50.0140 learning rate: 0.000905, scenario: 0, slope: -0.8598426030774268, fluctuations: 0.3\n",
      "step: 37990 loss: 1.452220 time elapsed: 50.0263 learning rate: 0.000905, scenario: 0, slope: -1.4668089500906607, fluctuations: 0.33\n",
      "step: 38000 loss: 1.400518 time elapsed: 50.0387 learning rate: 0.000905, scenario: 0, slope: -2.1835754185290304, fluctuations: 0.36\n",
      "step: 38010 loss: 1.347697 time elapsed: 50.0535 learning rate: 0.000905, scenario: 0, slope: -0.5684094164505269, fluctuations: 0.35\n",
      "step: 38020 loss: 1.320754 time elapsed: 50.0676 learning rate: 0.000905, scenario: 0, slope: -0.213846084965774, fluctuations: 0.34\n",
      "step: 38030 loss: 1.301466 time elapsed: 50.0820 learning rate: 0.000905, scenario: 0, slope: -0.06822982098233173, fluctuations: 0.31\n",
      "step: 38040 loss: 1.283751 time elapsed: 50.0951 learning rate: 0.000905, scenario: 0, slope: -0.024373015178974007, fluctuations: 0.27\n",
      "step: 38050 loss: 1.268898 time elapsed: 50.1079 learning rate: 0.000905, scenario: 0, slope: -0.013293603287741183, fluctuations: 0.23\n",
      "step: 38060 loss: 1.256225 time elapsed: 50.1211 learning rate: 0.000905, scenario: 0, slope: -0.005304635336338088, fluctuations: 0.2\n",
      "step: 38070 loss: 1.244999 time elapsed: 50.1339 learning rate: 0.000905, scenario: 0, slope: -0.0032546230553753453, fluctuations: 0.17\n",
      "step: 38080 loss: 1.235023 time elapsed: 50.1465 learning rate: 0.000905, scenario: 0, slope: -0.002442392125516899, fluctuations: 0.13\n",
      "step: 38090 loss: 1.226081 time elapsed: 50.1600 learning rate: 0.000905, scenario: 0, slope: -0.0018018846036765902, fluctuations: 0.1\n",
      "step: 38100 loss: 1.217992 time elapsed: 50.1733 learning rate: 0.000905, scenario: 0, slope: -0.0015032993674267976, fluctuations: 0.07\n",
      "step: 38110 loss: 1.210627 time elapsed: 50.1882 learning rate: 0.000905, scenario: 0, slope: -0.0012520359999672746, fluctuations: 0.04\n",
      "step: 38120 loss: 1.203883 time elapsed: 50.2020 learning rate: 0.000905, scenario: 0, slope: -0.0011195999145400888, fluctuations: 0.0\n",
      "step: 38130 loss: 1.197674 time elapsed: 50.2148 learning rate: 0.000905, scenario: 0, slope: -0.0009888327675855505, fluctuations: 0.0\n",
      "step: 38140 loss: 1.191931 time elapsed: 50.2285 learning rate: 0.000905, scenario: 0, slope: -0.0008840848244217475, fluctuations: 0.0\n",
      "step: 38150 loss: 1.186598 time elapsed: 50.2438 learning rate: 0.000905, scenario: 0, slope: -0.0007977216151456323, fluctuations: 0.0\n",
      "step: 38160 loss: 1.181627 time elapsed: 50.2590 learning rate: 0.000905, scenario: 0, slope: -0.0007251376122336625, fluctuations: 0.0\n",
      "step: 38170 loss: 1.176979 time elapsed: 50.2737 learning rate: 0.000905, scenario: 0, slope: -0.000663398363131152, fluctuations: 0.0\n",
      "step: 38180 loss: 1.172621 time elapsed: 50.2889 learning rate: 0.000905, scenario: 0, slope: -0.0006102846073600804, fluctuations: 0.0\n",
      "step: 38190 loss: 1.168524 time elapsed: 50.3030 learning rate: 0.000905, scenario: 0, slope: -0.0005641370189421268, fluctuations: 0.0\n",
      "step: 38200 loss: 1.164662 time elapsed: 50.3159 learning rate: 0.000905, scenario: 0, slope: -0.000527515734456482, fluctuations: 0.0\n",
      "step: 38210 loss: 1.161014 time elapsed: 50.3292 learning rate: 0.000905, scenario: 0, slope: -0.000487989917199028, fluctuations: 0.0\n",
      "step: 38220 loss: 1.157561 time elapsed: 50.3421 learning rate: 0.000905, scenario: 0, slope: -0.0004562394055964957, fluctuations: 0.0\n",
      "step: 38230 loss: 1.154287 time elapsed: 50.3548 learning rate: 0.000905, scenario: 0, slope: -0.00042783607425567, fluctuations: 0.0\n",
      "step: 38240 loss: 1.151176 time elapsed: 50.3671 learning rate: 0.000905, scenario: 0, slope: -0.00040228893552620305, fluctuations: 0.0\n",
      "step: 38250 loss: 1.148216 time elapsed: 50.3795 learning rate: 0.000905, scenario: 0, slope: -0.00037920010284829363, fluctuations: 0.0\n",
      "step: 38260 loss: 1.145395 time elapsed: 50.3921 learning rate: 0.000905, scenario: 0, slope: -0.0003582438665948684, fluctuations: 0.0\n",
      "step: 38270 loss: 1.142702 time elapsed: 50.4046 learning rate: 0.000905, scenario: 0, slope: -0.0003391510549517879, fluctuations: 0.0\n",
      "step: 38280 loss: 1.140128 time elapsed: 50.4169 learning rate: 0.000905, scenario: 0, slope: -0.00032169715085561475, fluctuations: 0.0\n",
      "step: 38290 loss: 1.137664 time elapsed: 50.4293 learning rate: 0.000905, scenario: 0, slope: -0.000305693221038446, fluctuations: 0.0\n",
      "step: 38300 loss: 1.135302 time elapsed: 50.4418 learning rate: 0.000905, scenario: 0, slope: -0.0002923964125477148, fluctuations: 0.0\n",
      "step: 38310 loss: 1.133035 time elapsed: 50.4567 learning rate: 0.000905, scenario: 0, slope: -0.0002774172608940268, fluctuations: 0.0\n",
      "step: 38320 loss: 1.130857 time elapsed: 50.4706 learning rate: 0.000905, scenario: 0, slope: -0.00026489009112787504, fluctuations: 0.0\n",
      "step: 38330 loss: 1.128762 time elapsed: 50.4844 learning rate: 0.000905, scenario: 0, slope: -0.0002532951080545463, fluctuations: 0.0\n",
      "step: 38340 loss: 1.126743 time elapsed: 50.4977 learning rate: 0.000905, scenario: 0, slope: -0.0002425430751098664, fluctuations: 0.0\n",
      "step: 38350 loss: 1.124797 time elapsed: 50.5112 learning rate: 0.000905, scenario: 0, slope: -0.00023255575433379634, fluctuations: 0.0\n",
      "step: 38360 loss: 1.122919 time elapsed: 50.5242 learning rate: 0.000905, scenario: 0, slope: -0.00022326421119729198, fluctuations: 0.0\n",
      "step: 38370 loss: 1.121103 time elapsed: 50.5369 learning rate: 0.000905, scenario: 0, slope: -0.00021460743443065062, fluctuations: 0.0\n",
      "step: 38380 loss: 1.119348 time elapsed: 50.5498 learning rate: 0.000905, scenario: 0, slope: -0.00020653120326307256, fluctuations: 0.0\n",
      "step: 38390 loss: 1.117648 time elapsed: 50.5622 learning rate: 0.000905, scenario: 0, slope: -0.0001989871505529971, fluctuations: 0.0\n",
      "step: 38400 loss: 1.116000 time elapsed: 50.5744 learning rate: 0.000905, scenario: 0, slope: -0.00019261663922543598, fluctuations: 0.0\n",
      "step: 38410 loss: 1.114402 time elapsed: 50.5874 learning rate: 0.000905, scenario: 0, slope: -0.00018532682272460668, fluctuations: 0.0\n",
      "step: 38420 loss: 1.112850 time elapsed: 50.6000 learning rate: 0.000905, scenario: 0, slope: -0.00017913666176485403, fluctuations: 0.0\n",
      "step: 38430 loss: 1.111341 time elapsed: 50.6129 learning rate: 0.000905, scenario: 0, slope: -0.00017332988513508248, fluctuations: 0.0\n",
      "step: 38440 loss: 1.109874 time elapsed: 50.6252 learning rate: 0.000905, scenario: 0, slope: -0.00016787787329571561, fluctuations: 0.0\n",
      "step: 38450 loss: 1.108446 time elapsed: 50.6374 learning rate: 0.000905, scenario: 0, slope: -0.000162754656865547, fluctuations: 0.0\n",
      "step: 38460 loss: 1.107054 time elapsed: 50.6499 learning rate: 0.000905, scenario: 0, slope: -0.00015793661926092407, fluctuations: 0.0\n",
      "step: 38470 loss: 1.105697 time elapsed: 50.6621 learning rate: 0.000905, scenario: 0, slope: -0.00015340223890361954, fluctuations: 0.0\n",
      "step: 38480 loss: 1.104373 time elapsed: 50.6761 learning rate: 0.000905, scenario: 0, slope: -0.00014913186480060005, fluctuations: 0.0\n",
      "step: 38490 loss: 1.103079 time elapsed: 50.6902 learning rate: 0.000905, scenario: 0, slope: -0.00014510752042869694, fluctuations: 0.0\n",
      "step: 38500 loss: 1.101814 time elapsed: 50.7038 learning rate: 0.000905, scenario: 0, slope: -0.00014168232157134146, fluctuations: 0.0\n",
      "step: 38510 loss: 1.100578 time elapsed: 50.7182 learning rate: 0.000905, scenario: 0, slope: -0.00013773237591375978, fluctuations: 0.0\n",
      "step: 38520 loss: 1.099367 time elapsed: 50.7310 learning rate: 0.000905, scenario: 0, slope: -0.00013435254773258148, fluctuations: 0.0\n",
      "step: 38530 loss: 1.098181 time elapsed: 50.7435 learning rate: 0.000905, scenario: 0, slope: -0.00013116044157362251, fluctuations: 0.0\n",
      "step: 38540 loss: 1.097018 time elapsed: 50.7562 learning rate: 0.000905, scenario: 0, slope: -0.00012814424657638988, fluctuations: 0.0\n",
      "step: 38550 loss: 1.095877 time elapsed: 50.7686 learning rate: 0.000905, scenario: 0, slope: -0.00012529305350560376, fluctuations: 0.0\n",
      "step: 38560 loss: 1.094757 time elapsed: 50.7808 learning rate: 0.000905, scenario: 0, slope: -0.0001225967717604391, fluctuations: 0.0\n",
      "step: 38570 loss: 1.093657 time elapsed: 50.7958 learning rate: 0.000905, scenario: 0, slope: -0.0001200460552850852, fluctuations: 0.0\n",
      "step: 38580 loss: 1.092576 time elapsed: 50.8090 learning rate: 0.000905, scenario: 0, slope: -0.00011763223630205361, fluctuations: 0.0\n",
      "step: 38590 loss: 1.091513 time elapsed: 50.8222 learning rate: 0.000905, scenario: 0, slope: -0.0001153472659351599, fluctuations: 0.0\n",
      "step: 38600 loss: 1.090467 time elapsed: 50.8355 learning rate: 0.000905, scenario: 0, slope: -0.00011339476234996074, fluctuations: 0.0\n",
      "step: 38610 loss: 1.089436 time elapsed: 50.8487 learning rate: 0.000905, scenario: 0, slope: -0.00011113445567002939, fluctuations: 0.0\n",
      "step: 38620 loss: 1.088415 time elapsed: 50.8636 learning rate: 0.000951, scenario: 1, slope: -0.00010919912240754891, fluctuations: 0.0\n",
      "step: 38630 loss: 1.087326 time elapsed: 50.8789 learning rate: 0.001050, scenario: 1, slope: -0.00010761508266163472, fluctuations: 0.0\n",
      "step: 38640 loss: 1.086142 time elapsed: 50.8939 learning rate: 0.001160, scenario: 1, slope: -0.00010687947453479384, fluctuations: 0.0\n",
      "step: 38650 loss: 1.084856 time elapsed: 50.9094 learning rate: 0.001282, scenario: 1, slope: -0.0001074160959529368, fluctuations: 0.0\n",
      "step: 38660 loss: 1.083461 time elapsed: 50.9234 learning rate: 0.001402, scenario: 0, slope: -0.000109547323983888, fluctuations: 0.0\n",
      "step: 38670 loss: 1.082017 time elapsed: 50.9375 learning rate: 0.001402, scenario: 0, slope: -0.00011335014587719877, fluctuations: 0.0\n",
      "step: 38680 loss: 1.080601 time elapsed: 50.9510 learning rate: 0.001402, scenario: 0, slope: -0.00011827508214605679, fluctuations: 0.0\n",
      "step: 38690 loss: 1.079210 time elapsed: 50.9639 learning rate: 0.001402, scenario: 0, slope: -0.00012364909450623566, fluctuations: 0.0\n",
      "step: 38700 loss: 1.077844 time elapsed: 50.9765 learning rate: 0.001402, scenario: 0, slope: -0.00012833471090315483, fluctuations: 0.0\n",
      "step: 38710 loss: 1.076501 time elapsed: 50.9899 learning rate: 0.001402, scenario: 0, slope: -0.00013318305767559264, fluctuations: 0.0\n",
      "step: 38720 loss: 1.075178 time elapsed: 51.0027 learning rate: 0.001402, scenario: 0, slope: -0.00013612955899355354, fluctuations: 0.0\n",
      "step: 38730 loss: 1.073875 time elapsed: 51.0153 learning rate: 0.001402, scenario: 0, slope: -0.00013737001512270582, fluctuations: 0.0\n",
      "step: 38740 loss: 1.072589 time elapsed: 51.0278 learning rate: 0.001402, scenario: 0, slope: -0.0001370761705208868, fluctuations: 0.0\n",
      "step: 38750 loss: 1.071320 time elapsed: 51.0403 learning rate: 0.001402, scenario: 0, slope: -0.00013562429950035812, fluctuations: 0.0\n",
      "step: 38760 loss: 1.070066 time elapsed: 51.0526 learning rate: 0.001402, scenario: 0, slope: -0.0001335811902155513, fluctuations: 0.0\n",
      "step: 38770 loss: 1.068826 time elapsed: 51.0651 learning rate: 0.001402, scenario: 0, slope: -0.00013155908239331448, fluctuations: 0.0\n",
      "step: 38780 loss: 1.067600 time elapsed: 51.0778 learning rate: 0.001402, scenario: 0, slope: -0.00012968566471581976, fluctuations: 0.0\n",
      "step: 38790 loss: 1.066385 time elapsed: 51.0919 learning rate: 0.001402, scenario: 0, slope: -0.0001279509381191998, fluctuations: 0.0\n",
      "step: 38800 loss: 1.065182 time elapsed: 51.1057 learning rate: 0.001402, scenario: 0, slope: -0.00012649968556310476, fluctuations: 0.0\n",
      "step: 38810 loss: 1.063990 time elapsed: 51.1206 learning rate: 0.001402, scenario: 0, slope: -0.00012485656488916334, fluctuations: 0.0\n",
      "step: 38820 loss: 1.062807 time elapsed: 51.1348 learning rate: 0.001402, scenario: 0, slope: -0.00012347787922154129, fluctuations: 0.0\n",
      "step: 38830 loss: 1.061633 time elapsed: 51.1474 learning rate: 0.001402, scenario: 0, slope: -0.00012220007338635164, fluctuations: 0.0\n",
      "step: 38840 loss: 1.060467 time elapsed: 51.1602 learning rate: 0.001402, scenario: 0, slope: -0.00012101546067557692, fluctuations: 0.0\n",
      "step: 38850 loss: 1.059310 time elapsed: 51.1727 learning rate: 0.001402, scenario: 0, slope: -0.00011991701230175945, fluctuations: 0.0\n",
      "step: 38860 loss: 1.058159 time elapsed: 51.1852 learning rate: 0.001402, scenario: 0, slope: -0.00011889829841552837, fluctuations: 0.0\n",
      "step: 38870 loss: 1.057015 time elapsed: 51.1977 learning rate: 0.001402, scenario: 0, slope: -0.00011795343271478248, fluctuations: 0.0\n",
      "step: 38880 loss: 1.055877 time elapsed: 51.2099 learning rate: 0.001402, scenario: 0, slope: -0.00011707702196624186, fluctuations: 0.0\n",
      "step: 38890 loss: 1.054745 time elapsed: 51.2222 learning rate: 0.001402, scenario: 0, slope: -0.00011626412046608216, fluctuations: 0.0\n",
      "step: 38900 loss: 1.053619 time elapsed: 51.2346 learning rate: 0.001402, scenario: 0, slope: -0.00011558305012678282, fluctuations: 0.0\n",
      "step: 38910 loss: 1.052497 time elapsed: 51.2473 learning rate: 0.001402, scenario: 0, slope: -0.0001148110583537293, fluctuations: 0.0\n",
      "step: 38920 loss: 1.051380 time elapsed: 51.2596 learning rate: 0.001402, scenario: 0, slope: -0.00011416289541726344, fluctuations: 0.0\n",
      "step: 38930 loss: 1.050267 time elapsed: 51.2723 learning rate: 0.001402, scenario: 0, slope: -0.00011356217400475233, fluctuations: 0.0\n",
      "step: 38940 loss: 1.049157 time elapsed: 51.2850 learning rate: 0.001402, scenario: 0, slope: -0.00011300564750909556, fluctuations: 0.0\n",
      "step: 38950 loss: 1.048052 time elapsed: 51.2993 learning rate: 0.001402, scenario: 0, slope: -0.00011249032459214135, fluctuations: 0.0\n",
      "step: 38960 loss: 1.046949 time elapsed: 51.3134 learning rate: 0.001402, scenario: 0, slope: -0.00011201344712795153, fluctuations: 0.0\n",
      "step: 38970 loss: 1.045850 time elapsed: 51.3276 learning rate: 0.001402, scenario: 0, slope: -0.00011157247022692083, fluctuations: 0.0\n",
      "step: 38980 loss: 1.044754 time elapsed: 51.3410 learning rate: 0.001402, scenario: 0, slope: -0.00011116504412601291, fluctuations: 0.0\n",
      "step: 38990 loss: 1.043660 time elapsed: 51.3539 learning rate: 0.001402, scenario: 0, slope: -0.00011078899775328892, fluctuations: 0.0\n",
      "step: 39000 loss: 1.042568 time elapsed: 51.3665 learning rate: 0.001402, scenario: 0, slope: -0.00011047572348164131, fluctuations: 0.0\n",
      "step: 39010 loss: 1.041479 time elapsed: 51.3795 learning rate: 0.001402, scenario: 0, slope: -0.00011012316513411125, fluctuations: 0.0\n",
      "step: 39020 loss: 1.040391 time elapsed: 51.3924 learning rate: 0.001402, scenario: 0, slope: -0.0001098298024681957, fluctuations: 0.0\n",
      "step: 39030 loss: 1.039305 time elapsed: 51.4058 learning rate: 0.001402, scenario: 0, slope: -0.00010956064308387409, fluctuations: 0.0\n",
      "step: 39040 loss: 1.038221 time elapsed: 51.4196 learning rate: 0.001402, scenario: 0, slope: -0.00010931421058403954, fluctuations: 0.0\n",
      "step: 39050 loss: 1.037138 time elapsed: 51.4328 learning rate: 0.001402, scenario: 0, slope: -0.00010908913553122947, fluctuations: 0.0\n",
      "step: 39060 loss: 1.036057 time elapsed: 51.4465 learning rate: 0.001402, scenario: 0, slope: -0.00010888414690167859, fluctuations: 0.0\n",
      "step: 39070 loss: 1.034976 time elapsed: 51.4600 learning rate: 0.001402, scenario: 0, slope: -0.00010869806427605497, fluctuations: 0.0\n",
      "step: 39080 loss: 1.033897 time elapsed: 51.4735 learning rate: 0.001402, scenario: 0, slope: -0.00010852979069857866, fluctuations: 0.0\n",
      "step: 39090 loss: 1.032819 time elapsed: 51.4894 learning rate: 0.001402, scenario: 0, slope: -0.00010837830613330042, fluctuations: 0.0\n",
      "step: 39100 loss: 1.031741 time elapsed: 51.5042 learning rate: 0.001402, scenario: 0, slope: -0.00010825553461001785, fluctuations: 0.0\n",
      "step: 39110 loss: 1.030676 time elapsed: 51.5195 learning rate: 0.001402, scenario: 0, slope: -0.00010810858658292723, fluctuations: 0.0\n",
      "step: 39120 loss: 1.062625 time elapsed: 51.5351 learning rate: 0.001444, scenario: 1, slope: -7.358011345408248e-05, fluctuations: 0.0\n",
      "step: 39130 loss: 85.416109 time elapsed: 51.5487 learning rate: 0.001339, scenario: -1, slope: 0.12815680878468835, fluctuations: 0.01\n",
      "step: 39140 loss: 21.308520 time elapsed: 51.5624 learning rate: 0.001211, scenario: -1, slope: 0.1823751371792114, fluctuations: 0.03\n",
      "step: 39150 loss: 2.981527 time elapsed: 51.5760 learning rate: 0.001095, scenario: -1, slope: 0.16374344559806328, fluctuations: 0.07\n",
      "step: 39160 loss: 2.547412 time elapsed: 51.5901 learning rate: 0.000991, scenario: -1, slope: 0.1326267633378178, fluctuations: 0.11\n",
      "step: 39170 loss: 1.072726 time elapsed: 51.6045 learning rate: 0.000896, scenario: -1, slope: 0.09546940490287462, fluctuations: 0.16\n",
      "step: 39180 loss: 1.077583 time elapsed: 51.6184 learning rate: 0.000810, scenario: -1, slope: 0.04833629529601364, fluctuations: 0.21\n",
      "step: 39190 loss: 1.036899 time elapsed: 51.6316 learning rate: 0.000748, scenario: 0, slope: -0.006597522649820593, fluctuations: 0.26\n",
      "step: 39200 loss: 1.048662 time elapsed: 51.6450 learning rate: 0.000748, scenario: 0, slope: -0.06198091606135694, fluctuations: 0.3\n",
      "step: 39210 loss: 1.034924 time elapsed: 51.6590 learning rate: 0.000748, scenario: 0, slope: -0.1356183678286171, fluctuations: 0.35\n",
      "step: 39220 loss: 1.029479 time elapsed: 51.6724 learning rate: 0.000748, scenario: 0, slope: -0.22069243973211422, fluctuations: 0.4\n",
      "step: 39230 loss: 1.028583 time elapsed: 51.6859 learning rate: 0.000748, scenario: 0, slope: -0.11423434728230766, fluctuations: 0.44\n",
      "step: 39240 loss: 1.027819 time elapsed: 51.7005 learning rate: 0.000748, scenario: 0, slope: -0.029199250877454664, fluctuations: 0.45\n",
      "step: 39250 loss: 1.026905 time elapsed: 51.7155 learning rate: 0.000748, scenario: 0, slope: -0.010911026084035928, fluctuations: 0.46\n",
      "step: 39260 loss: 1.026169 time elapsed: 51.7309 learning rate: 0.000748, scenario: 0, slope: -0.002050338730291673, fluctuations: 0.42\n",
      "step: 39270 loss: 1.025523 time elapsed: 51.7457 learning rate: 0.000748, scenario: 0, slope: -0.00092612890286689, fluctuations: 0.37\n",
      "step: 39280 loss: 1.024885 time elapsed: 51.7600 learning rate: 0.000748, scenario: 0, slope: -0.000312673907270653, fluctuations: 0.32\n",
      "step: 39290 loss: 1.024252 time elapsed: 51.7736 learning rate: 0.000748, scenario: 0, slope: -0.00018516326428092713, fluctuations: 0.27\n",
      "step: 39300 loss: 1.023629 time elapsed: 51.7866 learning rate: 0.000733, scenario: -1, slope: -0.00010446177571989079, fluctuations: 0.23\n",
      "step: 39310 loss: 1.023046 time elapsed: 51.8001 learning rate: 0.000663, scenario: -1, slope: -7.733234457709071e-05, fluctuations: 0.18\n",
      "step: 39320 loss: 1.022522 time elapsed: 51.8131 learning rate: 0.000599, scenario: -1, slope: -6.809037113850047e-05, fluctuations: 0.13\n",
      "step: 39330 loss: 1.022050 time elapsed: 51.8256 learning rate: 0.000542, scenario: -1, slope: -6.345602548848105e-05, fluctuations: 0.08\n",
      "step: 39340 loss: 1.021625 time elapsed: 51.8383 learning rate: 0.000490, scenario: -1, slope: -5.99208459830972e-05, fluctuations: 0.04\n",
      "step: 39350 loss: 1.021240 time elapsed: 51.8507 learning rate: 0.000478, scenario: 1, slope: -5.718932862891858e-05, fluctuations: 0.0\n",
      "step: 39360 loss: 1.020840 time elapsed: 51.8631 learning rate: 0.000528, scenario: 1, slope: -5.371620192400901e-05, fluctuations: 0.0\n",
      "step: 39370 loss: 1.020398 time elapsed: 51.8758 learning rate: 0.000583, scenario: 1, slope: -5.041945761566161e-05, fluctuations: 0.0\n",
      "step: 39380 loss: 1.019911 time elapsed: 51.8883 learning rate: 0.000644, scenario: 1, slope: -4.768047901487023e-05, fluctuations: 0.0\n",
      "step: 39390 loss: 1.019374 time elapsed: 51.9008 learning rate: 0.000712, scenario: 1, slope: -4.588007377237645e-05, fluctuations: 0.0\n",
      "step: 39400 loss: 1.018782 time elapsed: 51.9137 learning rate: 0.000778, scenario: 1, slope: -4.5365416489409506e-05, fluctuations: 0.0\n",
      "step: 39410 loss: 1.018135 time elapsed: 51.9284 learning rate: 0.000860, scenario: 1, slope: -4.6423477134445356e-05, fluctuations: 0.0\n",
      "step: 39420 loss: 1.017423 time elapsed: 51.9425 learning rate: 0.000950, scenario: 1, slope: -4.894353071192478e-05, fluctuations: 0.0\n",
      "step: 39430 loss: 1.016639 time elapsed: 51.9565 learning rate: 0.001049, scenario: 1, slope: -5.279354397418332e-05, fluctuations: 0.0\n",
      "step: 39440 loss: 1.015774 time elapsed: 51.9697 learning rate: 0.001159, scenario: 1, slope: -5.775282899072959e-05, fluctuations: 0.0\n",
      "step: 39450 loss: 1.014823 time elapsed: 51.9826 learning rate: 0.001280, scenario: 1, slope: -6.352902326184584e-05, fluctuations: 0.0\n",
      "step: 39460 loss: 1.013775 time elapsed: 51.9956 learning rate: 0.001414, scenario: 1, slope: -6.99109824099827e-05, fluctuations: 0.0\n",
      "step: 39470 loss: 1.012621 time elapsed: 52.0082 learning rate: 0.001562, scenario: 1, slope: -7.693629179242892e-05, fluctuations: 0.0\n",
      "step: 39480 loss: 1.011351 time elapsed: 52.0209 learning rate: 0.001725, scenario: 1, slope: -8.467627813363105e-05, fluctuations: 0.0\n",
      "step: 39490 loss: 24.828234 time elapsed: 52.0336 learning rate: 0.001804, scenario: -1, slope: 0.01635227521792352, fluctuations: 0.0\n",
      "step: 39500 loss: 543.983176 time elapsed: 52.0462 learning rate: 0.001648, scenario: -1, slope: 1.4277552916898022, fluctuations: 0.03\n",
      "step: 39510 loss: 32.049483 time elapsed: 52.0600 learning rate: 0.001490, scenario: -1, slope: 1.7095679788405767, fluctuations: 0.08\n",
      "step: 39520 loss: 23.461009 time elapsed: 52.0738 learning rate: 0.001348, scenario: -1, slope: 1.5541246219849978, fluctuations: 0.12\n",
      "step: 39530 loss: 2.759841 time elapsed: 52.0876 learning rate: 0.001219, scenario: -1, slope: 1.1755904723656385, fluctuations: 0.16\n",
      "step: 39540 loss: 6.073682 time elapsed: 52.1013 learning rate: 0.001102, scenario: -1, slope: 0.7186062826545233, fluctuations: 0.2\n",
      "step: 39550 loss: 2.593079 time elapsed: 52.1143 learning rate: 0.000997, scenario: -1, slope: 0.1717991174211943, fluctuations: 0.23\n",
      "step: 39560 loss: 2.108646 time elapsed: 52.1294 learning rate: 0.000967, scenario: 0, slope: -0.36953844571406513, fluctuations: 0.26\n",
      "step: 39570 loss: 1.462845 time elapsed: 52.1447 learning rate: 0.000967, scenario: 0, slope: -0.9463605915300423, fluctuations: 0.3\n",
      "step: 39580 loss: 1.391057 time elapsed: 52.1599 learning rate: 0.000967, scenario: 0, slope: -1.6593730334217647, fluctuations: 0.33\n",
      "step: 39590 loss: 1.348107 time elapsed: 52.1750 learning rate: 0.000967, scenario: 0, slope: -2.6633440782472624, fluctuations: 0.36\n",
      "step: 39600 loss: 1.298440 time elapsed: 52.1886 learning rate: 0.000967, scenario: 0, slope: -1.2307997247855977, fluctuations: 0.36\n",
      "step: 39610 loss: 1.277116 time elapsed: 52.2030 learning rate: 0.000967, scenario: 0, slope: -0.3579198161874367, fluctuations: 0.34\n",
      "step: 39620 loss: 1.257350 time elapsed: 52.2163 learning rate: 0.000967, scenario: 0, slope: -0.10294105652125515, fluctuations: 0.3\n",
      "step: 39630 loss: 1.240798 time elapsed: 52.2290 learning rate: 0.000967, scenario: 0, slope: -0.04063731412069358, fluctuations: 0.26\n",
      "step: 39640 loss: 1.227300 time elapsed: 52.2428 learning rate: 0.000967, scenario: 0, slope: -0.012467269453120109, fluctuations: 0.23\n",
      "step: 39650 loss: 1.215340 time elapsed: 52.2563 learning rate: 0.000967, scenario: 0, slope: -0.0065971796633235175, fluctuations: 0.19\n",
      "step: 39660 loss: 1.204879 time elapsed: 52.2701 learning rate: 0.000967, scenario: 0, slope: -0.0031888586680260365, fluctuations: 0.16\n",
      "step: 39670 loss: 1.195517 time elapsed: 52.2836 learning rate: 0.000967, scenario: 0, slope: -0.0022066662363063078, fluctuations: 0.13\n",
      "step: 39680 loss: 1.187072 time elapsed: 52.2967 learning rate: 0.000967, scenario: 0, slope: -0.001741126805029109, fluctuations: 0.09\n",
      "step: 39690 loss: 1.179394 time elapsed: 52.3106 learning rate: 0.000967, scenario: 0, slope: -0.0013803350966237104, fluctuations: 0.06\n",
      "step: 39700 loss: 1.172366 time elapsed: 52.3240 learning rate: 0.000967, scenario: 0, slope: -0.0011983772215753102, fluctuations: 0.03\n",
      "step: 39710 loss: 1.165899 time elapsed: 52.3395 learning rate: 0.000967, scenario: 0, slope: -0.001049677066335491, fluctuations: 0.0\n",
      "step: 39720 loss: 1.159918 time elapsed: 52.3545 learning rate: 0.000967, scenario: 0, slope: -0.0009299019517290046, fluctuations: 0.0\n",
      "step: 39730 loss: 1.154367 time elapsed: 52.3691 learning rate: 0.000967, scenario: 0, slope: -0.000835185419553781, fluctuations: 0.0\n",
      "step: 39740 loss: 1.149195 time elapsed: 52.3834 learning rate: 0.000967, scenario: 0, slope: -0.0007571865920595753, fluctuations: 0.0\n",
      "step: 39750 loss: 1.144362 time elapsed: 52.3972 learning rate: 0.000967, scenario: 0, slope: -0.0006917314479906141, fluctuations: 0.0\n",
      "step: 39760 loss: 1.139834 time elapsed: 52.4108 learning rate: 0.000967, scenario: 0, slope: -0.0006357926830385119, fluctuations: 0.0\n",
      "step: 39770 loss: 1.135579 time elapsed: 52.4237 learning rate: 0.000967, scenario: 0, slope: -0.0005873628817736269, fluctuations: 0.0\n",
      "step: 39780 loss: 1.131572 time elapsed: 52.4364 learning rate: 0.000967, scenario: 0, slope: -0.0005449781618566857, fluctuations: 0.0\n",
      "step: 39790 loss: 1.127791 time elapsed: 52.4490 learning rate: 0.000967, scenario: 0, slope: -0.0005075454292197209, fluctuations: 0.0\n",
      "step: 39800 loss: 1.124216 time elapsed: 52.4614 learning rate: 0.000967, scenario: 0, slope: -0.00047740140961568644, fluctuations: 0.0\n",
      "step: 39810 loss: 1.120829 time elapsed: 52.4745 learning rate: 0.000967, scenario: 0, slope: -0.0004444079725404484, fluctuations: 0.0\n",
      "step: 39820 loss: 1.117613 time elapsed: 52.4870 learning rate: 0.000967, scenario: 0, slope: -0.0004175529280597811, fluctuations: 0.0\n",
      "step: 39830 loss: 1.114556 time elapsed: 52.4995 learning rate: 0.000967, scenario: 0, slope: -0.00039326535209138866, fluctuations: 0.0\n",
      "step: 39840 loss: 1.111644 time elapsed: 52.5118 learning rate: 0.000967, scenario: 0, slope: -0.000371215136185958, fluctuations: 0.0\n",
      "step: 39850 loss: 1.108866 time elapsed: 52.5240 learning rate: 0.000967, scenario: 0, slope: -0.00035113000782469224, fluctuations: 0.0\n",
      "step: 39860 loss: 1.106212 time elapsed: 52.5361 learning rate: 0.000967, scenario: 0, slope: -0.0003327824919576673, fluctuations: 0.0\n",
      "step: 39870 loss: 1.103672 time elapsed: 52.5502 learning rate: 0.000967, scenario: 0, slope: -0.00031598026262604845, fluctuations: 0.0\n",
      "step: 39880 loss: 1.101237 time elapsed: 52.5654 learning rate: 0.000967, scenario: 0, slope: -0.00030055898768511976, fluctuations: 0.0\n",
      "step: 39890 loss: 1.098901 time elapsed: 52.5793 learning rate: 0.000967, scenario: 0, slope: -0.0002863769385915985, fluctuations: 0.0\n",
      "step: 39900 loss: 1.096655 time elapsed: 52.5931 learning rate: 0.000967, scenario: 0, slope: -0.00027457045669092826, fluctuations: 0.0\n",
      "step: 39910 loss: 1.094494 time elapsed: 52.6071 learning rate: 0.000967, scenario: 0, slope: -0.0002612529496239961, fluctuations: 0.0\n",
      "step: 39920 loss: 1.092411 time elapsed: 52.6197 learning rate: 0.000967, scenario: 0, slope: -0.0002501080971462582, fluctuations: 0.0\n",
      "step: 39930 loss: 1.090402 time elapsed: 52.6324 learning rate: 0.000967, scenario: 0, slope: -0.00023979221320853875, fluctuations: 0.0\n",
      "step: 39940 loss: 1.088461 time elapsed: 52.6448 learning rate: 0.000967, scenario: 0, slope: -0.00023023051262954802, fluctuations: 0.0\n",
      "step: 39950 loss: 1.086583 time elapsed: 52.6578 learning rate: 0.000967, scenario: 0, slope: -0.00022135626400624295, fluctuations: 0.0\n",
      "step: 39960 loss: 1.084766 time elapsed: 52.6714 learning rate: 0.000967, scenario: 0, slope: -0.00021310973626642425, fluctuations: 0.0\n",
      "step: 39970 loss: 1.083003 time elapsed: 52.6848 learning rate: 0.000967, scenario: 0, slope: -0.00020543732099765905, fluctuations: 0.0\n",
      "step: 39980 loss: 1.081293 time elapsed: 52.6980 learning rate: 0.000967, scenario: 0, slope: -0.00019829079332687772, fluctuations: 0.0\n",
      "step: 39990 loss: 1.079632 time elapsed: 52.7112 learning rate: 0.000967, scenario: 0, slope: -0.0001916266833962364, fluctuations: 0.0\n",
      "step: 40000 loss: 1.078016 time elapsed: 52.7252 learning rate: 0.000967, scenario: 0, slope: -0.00018600892879828477, fluctuations: 0.0\n",
      "step: 40010 loss: 1.076444 time elapsed: 52.7396 learning rate: 0.000967, scenario: 0, slope: -0.0001795924506686702, fluctuations: 0.0\n",
      "step: 40020 loss: 1.074912 time elapsed: 52.7537 learning rate: 0.000967, scenario: 0, slope: -0.00017415466390041832, fluctuations: 0.0\n",
      "step: 40030 loss: 1.073418 time elapsed: 52.7694 learning rate: 0.000967, scenario: 0, slope: -0.00016906320644583976, fluctuations: 0.0\n",
      "step: 40040 loss: 1.071959 time elapsed: 52.7840 learning rate: 0.000967, scenario: 0, slope: -0.00016429158499527241, fluctuations: 0.0\n",
      "step: 40050 loss: 1.070535 time elapsed: 52.7983 learning rate: 0.000967, scenario: 0, slope: -0.00015981570765348795, fluctuations: 0.0\n",
      "step: 40060 loss: 1.069142 time elapsed: 52.8143 learning rate: 0.000967, scenario: 0, slope: -0.0001556136395336313, fluctuations: 0.0\n",
      "step: 40070 loss: 1.067778 time elapsed: 52.8282 learning rate: 0.000967, scenario: 0, slope: -0.0001516653855046766, fluctuations: 0.0\n",
      "step: 40080 loss: 1.066443 time elapsed: 52.8415 learning rate: 0.000967, scenario: 0, slope: -0.00014795269661811757, fluctuations: 0.0\n",
      "step: 40090 loss: 1.065135 time elapsed: 52.8554 learning rate: 0.000967, scenario: 0, slope: -0.00014445889730652366, fluctuations: 0.0\n",
      "step: 40100 loss: 1.063852 time elapsed: 52.8698 learning rate: 0.000967, scenario: 0, slope: -0.0001414889918240557, fluctuations: 0.0\n",
      "step: 40110 loss: 1.062593 time elapsed: 52.8832 learning rate: 0.000967, scenario: 0, slope: -0.00013806822131235145, fluctuations: 0.0\n",
      "step: 40120 loss: 1.061356 time elapsed: 52.8957 learning rate: 0.000967, scenario: 0, slope: -0.00013514454923422062, fluctuations: 0.0\n",
      "step: 40130 loss: 1.060140 time elapsed: 52.9093 learning rate: 0.000967, scenario: 0, slope: -0.0001323859410222819, fluctuations: 0.0\n",
      "step: 40140 loss: 1.058944 time elapsed: 52.9223 learning rate: 0.000967, scenario: 0, slope: -0.00012978156915414217, fluctuations: 0.0\n",
      "step: 40150 loss: 1.057768 time elapsed: 52.9349 learning rate: 0.000967, scenario: 0, slope: -0.00012732146289712345, fluctuations: 0.0\n",
      "step: 40160 loss: 1.056609 time elapsed: 52.9474 learning rate: 0.000967, scenario: 0, slope: -0.00012499642817048262, fluctuations: 0.0\n",
      "step: 40170 loss: 1.055468 time elapsed: 52.9606 learning rate: 0.000967, scenario: 0, slope: -0.00012279797564363192, fluctuations: 0.0\n",
      "step: 40180 loss: 1.054343 time elapsed: 52.9753 learning rate: 0.000967, scenario: 0, slope: -0.00012071825621972732, fluctuations: 0.0\n",
      "step: 40190 loss: 1.053233 time elapsed: 52.9893 learning rate: 0.000967, scenario: 0, slope: -0.0001187500031437153, fluctuations: 0.0\n",
      "step: 40200 loss: 1.052138 time elapsed: 53.0030 learning rate: 0.000967, scenario: 0, slope: -0.00011706830130201784, fluctuations: 0.0\n",
      "step: 40210 loss: 1.051056 time elapsed: 53.0176 learning rate: 0.000967, scenario: 0, slope: -0.00011512143431958652, fluctuations: 0.0\n",
      "step: 40220 loss: 1.049988 time elapsed: 53.0304 learning rate: 0.000967, scenario: 0, slope: -0.00011344905522726202, fluctuations: 0.0\n",
      "step: 40230 loss: 1.048932 time elapsed: 53.0432 learning rate: 0.000967, scenario: 0, slope: -0.00011186393631116705, fluctuations: 0.0\n",
      "step: 40240 loss: 1.047888 time elapsed: 53.0559 learning rate: 0.000967, scenario: 0, slope: -0.00011036104157399617, fluctuations: 0.0\n",
      "step: 40250 loss: 1.046855 time elapsed: 53.0685 learning rate: 0.000967, scenario: 0, slope: -0.00010893567506746026, fluctuations: 0.0\n",
      "step: 40260 loss: 1.045833 time elapsed: 53.0812 learning rate: 0.000967, scenario: 0, slope: -0.00010758345350250716, fluctuations: 0.0\n",
      "step: 40270 loss: 1.044821 time elapsed: 53.0936 learning rate: 0.000967, scenario: 0, slope: -0.00010630028155193225, fluctuations: 0.0\n",
      "step: 40280 loss: 1.043815 time elapsed: 53.1062 learning rate: 0.001007, scenario: 1, slope: -0.00010508469540018238, fluctuations: 0.0\n",
      "step: 40290 loss: 1.042745 time elapsed: 53.1187 learning rate: 0.001112, scenario: 1, slope: -0.0001041341944171644, fluctuations: 0.0\n",
      "step: 40300 loss: 1.041575 time elapsed: 53.1311 learning rate: 0.001216, scenario: 1, slope: -0.00010391577694695362, fluctuations: 0.0\n",
      "step: 40310 loss: 1.040307 time elapsed: 53.1442 learning rate: 0.001343, scenario: 1, slope: -0.00010492795087965153, fluctuations: 0.0\n",
      "step: 40320 loss: 1.038974 time elapsed: 53.1576 learning rate: 0.001343, scenario: 0, slope: -0.00010731098160256911, fluctuations: 0.0\n",
      "step: 40330 loss: 1.037656 time elapsed: 53.1725 learning rate: 0.001343, scenario: 0, slope: -0.00011078372778965263, fluctuations: 0.0\n",
      "step: 40340 loss: 1.036353 time elapsed: 53.1879 learning rate: 0.001343, scenario: 0, slope: -0.00011486536351399929, fluctuations: 0.0\n",
      "step: 40350 loss: 1.035064 time elapsed: 53.2031 learning rate: 0.001343, scenario: 0, slope: -0.00011908766917887306, fluctuations: 0.0\n",
      "step: 40360 loss: 1.033789 time elapsed: 53.2180 learning rate: 0.001343, scenario: 0, slope: -0.00012299485238643552, fluctuations: 0.0\n",
      "step: 40370 loss: 1.032527 time elapsed: 53.2321 learning rate: 0.001343, scenario: 0, slope: -0.00012614287167909684, fluctuations: 0.0\n",
      "step: 40380 loss: 1.031276 time elapsed: 53.2458 learning rate: 0.001343, scenario: 0, slope: -0.0001281010777061926, fluctuations: 0.0\n",
      "step: 40390 loss: 1.030037 time elapsed: 53.2597 learning rate: 0.001343, scenario: 0, slope: -0.0001286762178756795, fluctuations: 0.0\n",
      "step: 40400 loss: 1.028808 time elapsed: 53.2730 learning rate: 0.001343, scenario: 0, slope: -0.00012824363237888526, fluctuations: 0.0\n",
      "step: 40410 loss: 1.027589 time elapsed: 53.2872 learning rate: 0.001343, scenario: 0, slope: -0.0001270053395586152, fluctuations: 0.0\n",
      "step: 40420 loss: 1.026379 time elapsed: 53.3008 learning rate: 0.001343, scenario: 0, slope: -0.00012576758552822174, fluctuations: 0.0\n",
      "step: 40430 loss: 1.025178 time elapsed: 53.3143 learning rate: 0.001343, scenario: 0, slope: -0.00012460338572792027, fluctuations: 0.0\n",
      "step: 40440 loss: 1.023986 time elapsed: 53.3282 learning rate: 0.001343, scenario: 0, slope: -0.00012350891300757559, fluctuations: 0.0\n",
      "step: 40450 loss: 1.022801 time elapsed: 53.3416 learning rate: 0.001343, scenario: 0, slope: -0.00012247982466949046, fluctuations: 0.0\n",
      "step: 40460 loss: 1.021624 time elapsed: 53.3546 learning rate: 0.001343, scenario: 0, slope: -0.0001215118430770863, fluctuations: 0.0\n",
      "step: 40470 loss: 1.020453 time elapsed: 53.3672 learning rate: 0.001343, scenario: 0, slope: -0.00012060093642170906, fluctuations: 0.0\n",
      "step: 40480 loss: 1.019289 time elapsed: 53.3810 learning rate: 0.001343, scenario: 0, slope: -0.00011974336158273261, fluctuations: 0.0\n",
      "step: 40490 loss: 1.018131 time elapsed: 53.3952 learning rate: 0.001343, scenario: 0, slope: -0.00011893566069071255, fluctuations: 0.0\n",
      "step: 40500 loss: 1.016979 time elapsed: 53.4088 learning rate: 0.001343, scenario: 0, slope: -0.0001182487300250006, fluctuations: 0.0\n",
      "step: 40510 loss: 1.015833 time elapsed: 53.4234 learning rate: 0.001343, scenario: 0, slope: -0.00011745736516438604, fluctuations: 0.0\n",
      "step: 40520 loss: 1.014691 time elapsed: 53.4379 learning rate: 0.001343, scenario: 0, slope: -0.00011678110729863422, fluctuations: 0.0\n",
      "step: 40530 loss: 1.013555 time elapsed: 53.4510 learning rate: 0.001343, scenario: 0, slope: -0.00011614335646158973, fluctuations: 0.0\n",
      "step: 40540 loss: 1.012423 time elapsed: 53.4638 learning rate: 0.001343, scenario: 0, slope: -0.00011554178732490839, fluctuations: 0.0\n",
      "step: 40550 loss: 1.011296 time elapsed: 53.4764 learning rate: 0.001343, scenario: 0, slope: -0.00011497424629505198, fluctuations: 0.0\n",
      "step: 40560 loss: 1.010172 time elapsed: 53.4891 learning rate: 0.001343, scenario: 0, slope: -0.00011443873686832633, fluctuations: 0.0\n",
      "step: 40570 loss: 1.009053 time elapsed: 53.5017 learning rate: 0.001343, scenario: 0, slope: -0.00011393340637853192, fluctuations: 0.0\n",
      "step: 40580 loss: 1.007937 time elapsed: 53.5143 learning rate: 0.001343, scenario: 0, slope: -0.00011345653399958014, fluctuations: 0.0\n",
      "step: 40590 loss: 1.006824 time elapsed: 53.5267 learning rate: 0.001343, scenario: 0, slope: -0.0001130065198767221, fluctuations: 0.0\n",
      "step: 40600 loss: 1.005715 time elapsed: 53.5391 learning rate: 0.001343, scenario: 0, slope: -0.00011262323838342912, fluctuations: 0.0\n",
      "step: 40610 loss: 1.004609 time elapsed: 53.5528 learning rate: 0.001343, scenario: 0, slope: -0.00011218121361629902, fluctuations: 0.0\n",
      "step: 40620 loss: 1.003506 time elapsed: 53.5653 learning rate: 0.001343, scenario: 0, slope: -0.00011180324238849498, fluctuations: 0.0\n",
      "step: 40630 loss: 1.002405 time elapsed: 53.5776 learning rate: 0.001343, scenario: 0, slope: -0.00011144675571817558, fluctuations: 0.0\n",
      "step: 40640 loss: 1.001307 time elapsed: 53.5915 learning rate: 0.001343, scenario: 0, slope: -0.00011111062765919466, fluctuations: 0.0\n",
      "step: 40650 loss: 1.000212 time elapsed: 53.6057 learning rate: 0.001343, scenario: 0, slope: -0.00011079380605442542, fluctuations: 0.0\n",
      "step: 40660 loss: 0.999118 time elapsed: 53.6196 learning rate: 0.001343, scenario: 0, slope: -0.00011049530693846988, fluctuations: 0.0\n",
      "step: 40670 loss: 0.998027 time elapsed: 53.6334 learning rate: 0.001343, scenario: 0, slope: -0.00011021420942400366, fluctuations: 0.0\n",
      "step: 40680 loss: 0.996938 time elapsed: 53.6470 learning rate: 0.001343, scenario: 0, slope: -0.00010994965102510327, fluctuations: 0.0\n",
      "step: 40690 loss: 0.995851 time elapsed: 53.6607 learning rate: 0.001343, scenario: 0, slope: -0.00010970082337520105, fluctuations: 0.0\n",
      "step: 40700 loss: 0.994765 time elapsed: 53.6731 learning rate: 0.001343, scenario: 0, slope: -0.00010948970067351802, fluctuations: 0.0\n",
      "step: 40710 loss: 0.993681 time elapsed: 53.6862 learning rate: 0.001343, scenario: 0, slope: -0.00010924737422827878, fluctuations: 0.0\n",
      "step: 40720 loss: 0.992598 time elapsed: 53.6984 learning rate: 0.001343, scenario: 0, slope: -0.00010904137285841485, fluctuations: 0.0\n",
      "step: 40730 loss: 0.991517 time elapsed: 53.7106 learning rate: 0.001343, scenario: 0, slope: -0.00010884833614156554, fluctuations: 0.0\n",
      "step: 40740 loss: 0.990438 time elapsed: 53.7232 learning rate: 0.001343, scenario: 0, slope: -0.00010866767346896814, fluctuations: 0.0\n",
      "step: 40750 loss: 0.989359 time elapsed: 53.7358 learning rate: 0.001343, scenario: 0, slope: -0.00010849882909195127, fluctuations: 0.0\n",
      "step: 40760 loss: 0.988282 time elapsed: 53.7484 learning rate: 0.001343, scenario: 0, slope: -0.00010834127816941088, fluctuations: 0.0\n",
      "step: 40770 loss: 0.987207 time elapsed: 53.7611 learning rate: 0.001343, scenario: 0, slope: -0.00010819257475614491, fluctuations: 0.0\n",
      "step: 40780 loss: 0.989836 time elapsed: 53.7737 learning rate: 0.001343, scenario: 0, slope: -0.00010407316688340288, fluctuations: 0.0\n",
      "step: 40790 loss: 43.884982 time elapsed: 53.7861 learning rate: 0.001323, scenario: -1, slope: 0.04170406785451883, fluctuations: 0.0\n",
      "step: 40800 loss: 32.359201 time elapsed: 53.7984 learning rate: 0.001208, scenario: -1, slope: 0.17862863696677125, fluctuations: 0.02\n",
      "step: 40810 loss: 3.569775 time elapsed: 53.8120 learning rate: 0.001093, scenario: -1, slope: 0.17791832386165232, fluctuations: 0.06\n",
      "step: 40820 loss: 2.637408 time elapsed: 53.8262 learning rate: 0.000988, scenario: -1, slope: 0.1496781820216177, fluctuations: 0.1\n",
      "step: 40830 loss: 1.512671 time elapsed: 53.8404 learning rate: 0.000894, scenario: -1, slope: 0.11469334305360891, fluctuations: 0.15\n",
      "step: 40840 loss: 1.288356 time elapsed: 53.8544 learning rate: 0.000808, scenario: -1, slope: 0.061330825307781306, fluctuations: 0.19\n",
      "step: 40850 loss: 1.049188 time elapsed: 53.8688 learning rate: 0.000731, scenario: -1, slope: 0.007472880657045819, fluctuations: 0.24\n",
      "step: 40860 loss: 0.991370 time elapsed: 53.8818 learning rate: 0.000724, scenario: 0, slope: -0.05272308254293471, fluctuations: 0.29\n",
      "step: 40870 loss: 0.999520 time elapsed: 53.8955 learning rate: 0.000724, scenario: 0, slope: -0.12030646635449088, fluctuations: 0.33\n",
      "step: 40880 loss: 0.989922 time elapsed: 53.9090 learning rate: 0.000724, scenario: 0, slope: -0.20150409152737048, fluctuations: 0.38\n",
      "step: 40890 loss: 0.986506 time elapsed: 53.9229 learning rate: 0.000724, scenario: 0, slope: -0.250660770071263, fluctuations: 0.43\n",
      "step: 40900 loss: 0.985958 time elapsed: 53.9364 learning rate: 0.000724, scenario: 0, slope: -0.06907434260486567, fluctuations: 0.44\n",
      "step: 40910 loss: 0.984907 time elapsed: 53.9507 learning rate: 0.000724, scenario: 0, slope: -0.017668298658200406, fluctuations: 0.45\n",
      "step: 40920 loss: 0.983955 time elapsed: 53.9677 learning rate: 0.000724, scenario: 0, slope: -0.003926030844720374, fluctuations: 0.43\n",
      "step: 40930 loss: 0.983184 time elapsed: 53.9809 learning rate: 0.000724, scenario: 0, slope: -0.0009080762024572331, fluctuations: 0.39\n",
      "step: 40940 loss: 0.982455 time elapsed: 53.9948 learning rate: 0.000724, scenario: 0, slope: -0.0003035722489593029, fluctuations: 0.34\n",
      "step: 40950 loss: 0.981742 time elapsed: 54.0096 learning rate: 0.000724, scenario: 0, slope: -0.00020087581944032155, fluctuations: 0.29\n",
      "step: 40960 loss: 0.981051 time elapsed: 54.0244 learning rate: 0.000724, scenario: 0, slope: -0.0001407642183944436, fluctuations: 0.24\n",
      "step: 40970 loss: 0.980380 time elapsed: 54.0387 learning rate: 0.000688, scenario: -1, slope: -8.562189729736087e-05, fluctuations: 0.2\n",
      "step: 40980 loss: 0.979774 time elapsed: 54.0529 learning rate: 0.000622, scenario: -1, slope: -7.977422447257814e-05, fluctuations: 0.15\n",
      "step: 40990 loss: 0.979233 time elapsed: 54.0678 learning rate: 0.000563, scenario: -1, slope: -7.391622500264179e-05, fluctuations: 0.1\n",
      "step: 41000 loss: 0.978750 time elapsed: 54.0808 learning rate: 0.000514, scenario: -1, slope: -6.925488075865309e-05, fluctuations: 0.06\n",
      "step: 41010 loss: 0.978312 time elapsed: 54.0943 learning rate: 0.000465, scenario: -1, slope: -6.519633687630608e-05, fluctuations: 0.01\n",
      "step: 41020 loss: 0.977892 time elapsed: 54.1070 learning rate: 0.000511, scenario: 1, slope: -6.111598658866046e-05, fluctuations: 0.0\n",
      "step: 41030 loss: 0.977431 time elapsed: 54.1198 learning rate: 0.000565, scenario: 1, slope: -5.70313597137358e-05, fluctuations: 0.0\n",
      "step: 41040 loss: 0.976925 time elapsed: 54.1321 learning rate: 0.000624, scenario: 1, slope: -5.3498214511534134e-05, fluctuations: 0.0\n",
      "step: 41050 loss: 0.976369 time elapsed: 54.1447 learning rate: 0.000689, scenario: 1, slope: -5.087800222027376e-05, fluctuations: 0.0\n",
      "step: 41060 loss: 0.975759 time elapsed: 54.1572 learning rate: 0.000761, scenario: 1, slope: -4.951957699899899e-05, fluctuations: 0.0\n",
      "step: 41070 loss: 0.975089 time elapsed: 54.1696 learning rate: 0.000841, scenario: 1, slope: -4.974284698892285e-05, fluctuations: 0.0\n",
      "step: 41080 loss: 0.974354 time elapsed: 54.1822 learning rate: 0.000929, scenario: 1, slope: -5.16604258172956e-05, fluctuations: 0.0\n",
      "step: 41090 loss: 0.973548 time elapsed: 54.1948 learning rate: 0.001026, scenario: 1, slope: -5.510988768530534e-05, fluctuations: 0.0\n",
      "step: 41100 loss: 0.972663 time elapsed: 54.2068 learning rate: 0.001122, scenario: 1, slope: -5.9325182309410277e-05, fluctuations: 0.0\n",
      "step: 41110 loss: 0.971700 time elapsed: 54.2203 learning rate: 0.001239, scenario: 1, slope: -6.555246388987524e-05, fluctuations: 0.0\n",
      "step: 41120 loss: 0.970644 time elapsed: 54.2348 learning rate: 0.001369, scenario: 1, slope: -7.188065152544196e-05, fluctuations: 0.0\n",
      "step: 41130 loss: 0.969486 time elapsed: 54.2486 learning rate: 0.001512, scenario: 1, slope: -7.878722111833896e-05, fluctuations: 0.0\n",
      "step: 41140 loss: 0.968213 time elapsed: 54.2626 learning rate: 0.001671, scenario: 1, slope: -8.634288779295576e-05, fluctuations: 0.0\n",
      "step: 41150 loss: 5.160845 time elapsed: 54.2769 learning rate: 0.001782, scenario: -1, slope: 0.002802281897481375, fluctuations: 0.0\n",
      "step: 41160 loss: 192.535198 time elapsed: 54.2898 learning rate: 0.001612, scenario: -1, slope: 1.694995784971424, fluctuations: 0.03\n",
      "step: 41170 loss: 201.998155 time elapsed: 54.3027 learning rate: 0.001458, scenario: -1, slope: 2.043543559628711, fluctuations: 0.07\n",
      "step: 41180 loss: 58.465201 time elapsed: 54.3157 learning rate: 0.001318, scenario: -1, slope: 1.734653804107361, fluctuations: 0.11\n",
      "step: 41190 loss: 22.545056 time elapsed: 54.3284 learning rate: 0.001192, scenario: -1, slope: 1.2925731312264412, fluctuations: 0.15\n",
      "step: 41200 loss: 3.524090 time elapsed: 54.3404 learning rate: 0.001089, scenario: -1, slope: 0.8611773995836239, fluctuations: 0.19\n",
      "step: 41210 loss: 3.295017 time elapsed: 54.3535 learning rate: 0.000985, scenario: -1, slope: 0.18576880287907593, fluctuations: 0.22\n",
      "step: 41220 loss: 1.446181 time elapsed: 54.3663 learning rate: 0.000956, scenario: 0, slope: -0.38369888020759196, fluctuations: 0.26\n",
      "step: 41230 loss: 1.546732 time elapsed: 54.3787 learning rate: 0.000956, scenario: 0, slope: -1.027053782842443, fluctuations: 0.29\n",
      "step: 41240 loss: 1.376779 time elapsed: 54.3918 learning rate: 0.000956, scenario: 0, slope: -1.8132978302140699, fluctuations: 0.33\n",
      "step: 41250 loss: 1.274091 time elapsed: 54.4049 learning rate: 0.000956, scenario: 0, slope: -2.9129593612630043, fluctuations: 0.36\n",
      "step: 41260 loss: 1.255110 time elapsed: 54.4176 learning rate: 0.000956, scenario: 0, slope: -1.1136628295560977, fluctuations: 0.35\n",
      "step: 41270 loss: 1.224023 time elapsed: 54.4317 learning rate: 0.000956, scenario: 0, slope: -0.22710137824067514, fluctuations: 0.35\n",
      "step: 41280 loss: 1.203349 time elapsed: 54.4469 learning rate: 0.000956, scenario: 0, slope: -0.06876719422314945, fluctuations: 0.32\n",
      "step: 41290 loss: 1.187321 time elapsed: 54.4618 learning rate: 0.000956, scenario: 0, slope: -0.02952750161534787, fluctuations: 0.28\n",
      "step: 41300 loss: 1.173001 time elapsed: 54.4767 learning rate: 0.000956, scenario: 0, slope: -0.013158187997167656, fluctuations: 0.25\n",
      "step: 41310 loss: 1.161039 time elapsed: 54.4919 learning rate: 0.000956, scenario: 0, slope: -0.00597179236626479, fluctuations: 0.21\n",
      "step: 41320 loss: 1.150485 time elapsed: 54.5053 learning rate: 0.000956, scenario: 0, slope: -0.003891215222560296, fluctuations: 0.17\n",
      "step: 41330 loss: 1.141192 time elapsed: 54.5192 learning rate: 0.000956, scenario: 0, slope: -0.0023666612853772877, fluctuations: 0.14\n",
      "step: 41340 loss: 1.132911 time elapsed: 54.5322 learning rate: 0.000956, scenario: 0, slope: -0.001703764337549604, fluctuations: 0.11\n",
      "step: 41350 loss: 1.125473 time elapsed: 54.5458 learning rate: 0.000956, scenario: 0, slope: -0.001417239974753068, fluctuations: 0.07\n",
      "step: 41360 loss: 1.118747 time elapsed: 54.5605 learning rate: 0.000956, scenario: 0, slope: -0.001188871972118544, fluctuations: 0.04\n",
      "step: 41370 loss: 1.112622 time elapsed: 54.5743 learning rate: 0.000956, scenario: 0, slope: -0.0010427261224377187, fluctuations: 0.01\n",
      "step: 41380 loss: 1.107015 time elapsed: 54.5879 learning rate: 0.000956, scenario: 0, slope: -0.0009226454446266979, fluctuations: 0.0\n",
      "step: 41390 loss: 1.101855 time elapsed: 54.6017 learning rate: 0.000956, scenario: 0, slope: -0.0008187884519293524, fluctuations: 0.0\n",
      "step: 41400 loss: 1.097083 time elapsed: 54.6145 learning rate: 0.000956, scenario: 0, slope: -0.0007415868082575693, fluctuations: 0.0\n",
      "step: 41410 loss: 1.092653 time elapsed: 54.6278 learning rate: 0.000956, scenario: 0, slope: -0.000662770943842622, fluctuations: 0.0\n",
      "step: 41420 loss: 1.088525 time elapsed: 54.6411 learning rate: 0.000956, scenario: 0, slope: -0.000602719465809143, fluctuations: 0.0\n",
      "step: 41430 loss: 1.084664 time elapsed: 54.6553 learning rate: 0.000956, scenario: 0, slope: -0.0005514239544101717, fluctuations: 0.0\n",
      "step: 41440 loss: 1.081044 time elapsed: 54.6693 learning rate: 0.000956, scenario: 0, slope: -0.000507206756622507, fluctuations: 0.0\n",
      "step: 41450 loss: 1.077639 time elapsed: 54.6836 learning rate: 0.000956, scenario: 0, slope: -0.00046877683949427755, fluctuations: 0.0\n",
      "step: 41460 loss: 1.074428 time elapsed: 54.6975 learning rate: 0.000956, scenario: 0, slope: -0.0004351192808703504, fluctuations: 0.0\n",
      "step: 41470 loss: 1.071394 time elapsed: 54.7106 learning rate: 0.000956, scenario: 0, slope: -0.0004054348107671149, fluctuations: 0.0\n",
      "step: 41480 loss: 1.068519 time elapsed: 54.7232 learning rate: 0.000956, scenario: 0, slope: -0.0003790863174878253, fluctuations: 0.0\n",
      "step: 41490 loss: 1.065791 time elapsed: 54.7358 learning rate: 0.000956, scenario: 0, slope: -0.0003555633466990933, fluctuations: 0.0\n",
      "step: 41500 loss: 1.063196 time elapsed: 54.7482 learning rate: 0.000956, scenario: 0, slope: -0.00033646582734422767, fluctuations: 0.0\n",
      "step: 41510 loss: 1.060724 time elapsed: 54.7616 learning rate: 0.000956, scenario: 0, slope: -0.0003154201347924817, fluctuations: 0.0\n",
      "step: 41520 loss: 1.058365 time elapsed: 54.7746 learning rate: 0.000956, scenario: 0, slope: -0.0002981876642310976, fluctuations: 0.0\n",
      "step: 41530 loss: 1.056109 time elapsed: 54.7868 learning rate: 0.000956, scenario: 0, slope: -0.00028252701073987793, fluctuations: 0.0\n",
      "step: 41540 loss: 1.053950 time elapsed: 54.7990 learning rate: 0.000956, scenario: 0, slope: -0.00026824683166987916, fluctuations: 0.0\n",
      "step: 41550 loss: 1.051879 time elapsed: 54.8112 learning rate: 0.000956, scenario: 0, slope: -0.00025518585357690713, fluctuations: 0.0\n",
      "step: 41560 loss: 1.049890 time elapsed: 54.8251 learning rate: 0.000956, scenario: 0, slope: -0.00024320708998718906, fluctuations: 0.0\n",
      "step: 41570 loss: 1.047978 time elapsed: 54.8386 learning rate: 0.000956, scenario: 0, slope: -0.00023219334263751944, fluctuations: 0.0\n",
      "step: 41580 loss: 1.046137 time elapsed: 54.8529 learning rate: 0.000956, scenario: 0, slope: -0.000222043674289009, fluctuations: 0.0\n",
      "step: 41590 loss: 1.044362 time elapsed: 54.8671 learning rate: 0.000956, scenario: 0, slope: -0.00021267062131979117, fluctuations: 0.0\n",
      "step: 41600 loss: 1.042649 time elapsed: 54.8811 learning rate: 0.000956, scenario: 0, slope: -0.00020483568864753627, fluctuations: 0.0\n",
      "step: 41610 loss: 1.040994 time elapsed: 54.8956 learning rate: 0.000956, scenario: 0, slope: -0.0001959589849946239, fluctuations: 0.0\n",
      "step: 41620 loss: 1.039392 time elapsed: 54.9094 learning rate: 0.000956, scenario: 0, slope: -0.0001884949372979827, fluctuations: 0.0\n",
      "step: 41630 loss: 1.037841 time elapsed: 54.9226 learning rate: 0.000956, scenario: 0, slope: -0.00018155394955837848, fluctuations: 0.0\n",
      "step: 41640 loss: 1.036337 time elapsed: 54.9354 learning rate: 0.000956, scenario: 0, slope: -0.00017509001079525852, fluctuations: 0.0\n",
      "step: 41650 loss: 1.034877 time elapsed: 54.9481 learning rate: 0.000956, scenario: 0, slope: -0.0001690621725850498, fluctuations: 0.0\n",
      "step: 41660 loss: 1.033459 time elapsed: 54.9610 learning rate: 0.000956, scenario: 0, slope: -0.00016343387523535443, fluctuations: 0.0\n",
      "step: 41670 loss: 1.032079 time elapsed: 54.9734 learning rate: 0.000956, scenario: 0, slope: -0.000158172380894526, fluctuations: 0.0\n",
      "step: 41680 loss: 1.030737 time elapsed: 54.9859 learning rate: 0.000956, scenario: 0, slope: -0.00015324829356437857, fluctuations: 0.0\n",
      "step: 41690 loss: 1.029429 time elapsed: 54.9985 learning rate: 0.000956, scenario: 0, slope: -0.00014863515027852613, fluctuations: 0.0\n",
      "step: 41700 loss: 1.028153 time elapsed: 55.0108 learning rate: 0.000956, scenario: 0, slope: -0.0001447293932375723, fluctuations: 0.0\n",
      "step: 41710 loss: 1.026908 time elapsed: 55.0236 learning rate: 0.000956, scenario: 0, slope: -0.00014024845727821883, fluctuations: 0.0\n",
      "step: 41720 loss: 1.025692 time elapsed: 55.0362 learning rate: 0.000956, scenario: 0, slope: -0.00013643373181242656, fluctuations: 0.0\n",
      "step: 41730 loss: 1.024503 time elapsed: 55.0487 learning rate: 0.000956, scenario: 0, slope: -0.00013284711229196483, fluctuations: 0.0\n",
      "step: 41740 loss: 1.023340 time elapsed: 55.0612 learning rate: 0.000956, scenario: 0, slope: -0.00012947241444908518, fluctuations: 0.0\n",
      "step: 41750 loss: 1.022201 time elapsed: 55.0745 learning rate: 0.000956, scenario: 0, slope: -0.0001262948799027028, fluctuations: 0.0\n",
      "step: 41760 loss: 1.021086 time elapsed: 55.0886 learning rate: 0.000956, scenario: 0, slope: -0.00012330102526353747, fluctuations: 0.0\n",
      "step: 41770 loss: 1.019991 time elapsed: 55.1024 learning rate: 0.000956, scenario: 0, slope: -0.00012047850952652696, fluctuations: 0.0\n",
      "step: 41780 loss: 1.018917 time elapsed: 55.1166 learning rate: 0.000956, scenario: 0, slope: -0.00011781601725990623, fluctuations: 0.0\n",
      "step: 41790 loss: 1.017863 time elapsed: 55.1300 learning rate: 0.000956, scenario: 0, slope: -0.00011530315548961622, fluctuations: 0.0\n",
      "step: 41800 loss: 1.016827 time elapsed: 55.1426 learning rate: 0.000956, scenario: 0, slope: -0.00011316159536256817, fluctuations: 0.0\n",
      "step: 41810 loss: 1.015808 time elapsed: 55.1558 learning rate: 0.000956, scenario: 0, slope: -0.00011068882698859365, fluctuations: 0.0\n",
      "step: 41820 loss: 1.014806 time elapsed: 55.1684 learning rate: 0.000956, scenario: 0, slope: -0.00010857041639045516, fluctuations: 0.0\n",
      "step: 41830 loss: 1.013819 time elapsed: 55.1808 learning rate: 0.000956, scenario: 0, slope: -0.00010656761304490561, fluctuations: 0.0\n",
      "step: 41840 loss: 1.012847 time elapsed: 55.1931 learning rate: 0.000956, scenario: 0, slope: -0.00010467345743455144, fluctuations: 0.0\n",
      "step: 41850 loss: 1.011889 time elapsed: 55.2055 learning rate: 0.000956, scenario: 0, slope: -0.00010288149753256229, fluctuations: 0.0\n",
      "step: 41860 loss: 1.010934 time elapsed: 55.2178 learning rate: 0.001014, scenario: 1, slope: -0.0001011968249457757, fluctuations: 0.0\n",
      "step: 41870 loss: 1.009909 time elapsed: 55.2300 learning rate: 0.001121, scenario: 1, slope: -9.987961937220811e-05, fluctuations: 0.0\n",
      "step: 41880 loss: 1.008794 time elapsed: 55.2422 learning rate: 0.001238, scenario: 1, slope: -9.939483572643759e-05, fluctuations: 0.0\n",
      "step: 41890 loss: 1.007582 time elapsed: 55.2547 learning rate: 0.001367, scenario: 1, slope: -0.00010013108496016496, fluctuations: 0.0\n",
      "step: 41900 loss: 1.006267 time elapsed: 55.2672 learning rate: 0.001466, scenario: 0, slope: -0.00010208333184741366, fluctuations: 0.0\n",
      "step: 41910 loss: 1.004930 time elapsed: 55.2808 learning rate: 0.001466, scenario: 0, slope: -0.00010612553763155124, fluctuations: 0.0\n",
      "step: 41920 loss: 1.003616 time elapsed: 55.2950 learning rate: 0.001466, scenario: 0, slope: -0.00011080143779580491, fluctuations: 0.0\n",
      "step: 41930 loss: 1.002324 time elapsed: 55.3091 learning rate: 0.001466, scenario: 0, slope: -0.00011580294175261674, fluctuations: 0.0\n",
      "step: 41940 loss: 1.001054 time elapsed: 55.3229 learning rate: 0.001466, scenario: 0, slope: -0.0001205455216245499, fluctuations: 0.0\n",
      "step: 41950 loss: 0.999802 time elapsed: 55.3372 learning rate: 0.001466, scenario: 0, slope: -0.00012446595631450088, fluctuations: 0.0\n",
      "step: 41960 loss: 0.998568 time elapsed: 55.3497 learning rate: 0.001466, scenario: 0, slope: -0.0001270323089005694, fluctuations: 0.0\n",
      "step: 41970 loss: 0.997351 time elapsed: 55.3622 learning rate: 0.001466, scenario: 0, slope: -0.00012803122227659033, fluctuations: 0.0\n",
      "step: 41980 loss: 0.996149 time elapsed: 55.3746 learning rate: 0.001466, scenario: 0, slope: -0.00012765834496600393, fluctuations: 0.0\n",
      "step: 41990 loss: 0.994961 time elapsed: 55.3871 learning rate: 0.001466, scenario: 0, slope: -0.00012629341087463133, fluctuations: 0.0\n",
      "step: 42000 loss: 0.993786 time elapsed: 55.3993 learning rate: 0.001466, scenario: 0, slope: -0.00012467556733417013, fluctuations: 0.0\n",
      "step: 42010 loss: 0.992624 time elapsed: 55.4122 learning rate: 0.001466, scenario: 0, slope: -0.00012276575193878846, fluctuations: 0.0\n",
      "step: 42020 loss: 0.991472 time elapsed: 55.4250 learning rate: 0.001466, scenario: 0, slope: -0.00012116226500554925, fluctuations: 0.0\n",
      "step: 42030 loss: 0.990332 time elapsed: 55.4372 learning rate: 0.001466, scenario: 0, slope: -0.00011967616496411908, fluctuations: 0.0\n",
      "step: 42040 loss: 0.989200 time elapsed: 55.4494 learning rate: 0.001466, scenario: 0, slope: -0.00011829860288168876, fluctuations: 0.0\n",
      "step: 42050 loss: 0.988078 time elapsed: 55.4617 learning rate: 0.001466, scenario: 0, slope: -0.00011702121712285968, fluctuations: 0.0\n",
      "step: 42060 loss: 0.986965 time elapsed: 55.4739 learning rate: 0.001466, scenario: 0, slope: -0.00011583628712909202, fluctuations: 0.0\n",
      "step: 42070 loss: 0.985859 time elapsed: 55.4876 learning rate: 0.001466, scenario: 0, slope: -0.0001147367400690351, fluctuations: 0.0\n",
      "step: 42080 loss: 0.984760 time elapsed: 55.5016 learning rate: 0.001466, scenario: 0, slope: -0.00011371611128838407, fluctuations: 0.0\n",
      "step: 42090 loss: 0.983668 time elapsed: 55.5158 learning rate: 0.001466, scenario: 0, slope: -0.0001127684932369721, fluctuations: 0.0\n",
      "step: 42100 loss: 0.982583 time elapsed: 55.5296 learning rate: 0.001466, scenario: 0, slope: -0.00011197358715948753, fluctuations: 0.0\n",
      "step: 42110 loss: 0.981503 time elapsed: 55.5437 learning rate: 0.001466, scenario: 0, slope: -0.0001110711419976007, fluctuations: 0.0\n",
      "step: 42120 loss: 0.980429 time elapsed: 55.5563 learning rate: 0.001466, scenario: 0, slope: -0.00011031193859532011, fluctuations: 0.0\n",
      "step: 42130 loss: 0.979359 time elapsed: 55.5695 learning rate: 0.001466, scenario: 0, slope: -0.00010960672364127672, fluctuations: 0.0\n",
      "step: 42140 loss: 0.978294 time elapsed: 55.5827 learning rate: 0.001466, scenario: 0, slope: -0.00010895168846999799, fluctuations: 0.0\n",
      "step: 42150 loss: 0.977234 time elapsed: 55.5954 learning rate: 0.001466, scenario: 0, slope: -0.00010834333503249022, fluctuations: 0.0\n",
      "step: 42160 loss: 0.976177 time elapsed: 55.6079 learning rate: 0.001466, scenario: 0, slope: -0.00010777844774799592, fluctuations: 0.0\n",
      "step: 42170 loss: 0.975125 time elapsed: 55.6203 learning rate: 0.001466, scenario: 0, slope: -0.00010725406816203244, fluctuations: 0.0\n",
      "step: 42180 loss: 0.974075 time elapsed: 55.6327 learning rate: 0.001466, scenario: 0, slope: -0.00010676747210373572, fluctuations: 0.0\n",
      "step: 42190 loss: 0.973029 time elapsed: 55.6451 learning rate: 0.001466, scenario: 0, slope: -0.0001063161490710797, fluctuations: 0.0\n",
      "step: 42200 loss: 0.971986 time elapsed: 55.6575 learning rate: 0.001466, scenario: 0, slope: -0.00010593819929922633, fluctuations: 0.0\n",
      "step: 42210 loss: 0.970945 time elapsed: 55.6704 learning rate: 0.001466, scenario: 0, slope: -0.00010551023845245818, fluctuations: 0.0\n",
      "step: 42220 loss: 0.969907 time elapsed: 55.6831 learning rate: 0.001466, scenario: 0, slope: -0.00010515153929866653, fluctuations: 0.0\n",
      "step: 42230 loss: 0.968871 time elapsed: 55.6971 learning rate: 0.001466, scenario: 0, slope: -0.00010481986096027321, fluctuations: 0.0\n",
      "step: 42240 loss: 0.967838 time elapsed: 55.7112 learning rate: 0.001466, scenario: 0, slope: -0.00010451351483723807, fluctuations: 0.0\n",
      "step: 42250 loss: 0.966806 time elapsed: 55.7251 learning rate: 0.001466, scenario: 0, slope: -0.00010423093752665562, fluctuations: 0.0\n",
      "step: 42260 loss: 0.965776 time elapsed: 55.7390 learning rate: 0.001466, scenario: 0, slope: -0.00010397068047224578, fluctuations: 0.0\n",
      "step: 42270 loss: 0.964747 time elapsed: 55.7519 learning rate: 0.001466, scenario: 0, slope: -0.00010373140053710942, fluctuations: 0.0\n",
      "step: 42280 loss: 0.963720 time elapsed: 55.7642 learning rate: 0.001466, scenario: 0, slope: -0.000103511843306772, fluctuations: 0.0\n",
      "step: 42290 loss: 0.962703 time elapsed: 55.7768 learning rate: 0.001466, scenario: 0, slope: -0.00010330096450501771, fluctuations: 0.0\n",
      "step: 42300 loss: 0.980077 time elapsed: 55.7890 learning rate: 0.001481, scenario: 1, slope: -9.40756646737105e-05, fluctuations: 0.0\n",
      "step: 42310 loss: 72.319629 time elapsed: 55.8018 learning rate: 0.001401, scenario: -1, slope: 0.09673945251978916, fluctuations: 0.0\n",
      "step: 42320 loss: 2.552640 time elapsed: 55.8143 learning rate: 0.001267, scenario: -1, slope: 0.13095549074049137, fluctuations: 0.03\n",
      "step: 42330 loss: 4.885847 time elapsed: 55.8271 learning rate: 0.001146, scenario: -1, slope: 0.12231292403896787, fluctuations: 0.06\n",
      "step: 42340 loss: 1.877133 time elapsed: 55.8397 learning rate: 0.001036, scenario: -1, slope: 0.09935554627142988, fluctuations: 0.1\n",
      "step: 42350 loss: 1.373963 time elapsed: 55.8520 learning rate: 0.000937, scenario: -1, slope: 0.06815839793103061, fluctuations: 0.14\n",
      "step: 42360 loss: 1.081804 time elapsed: 55.8643 learning rate: 0.000847, scenario: -1, slope: 0.03492102485823635, fluctuations: 0.19\n",
      "step: 42370 loss: 1.002663 time elapsed: 55.8768 learning rate: 0.000774, scenario: 0, slope: -0.004023981890425328, fluctuations: 0.24\n",
      "step: 42380 loss: 0.977246 time elapsed: 55.8896 learning rate: 0.000774, scenario: 0, slope: -0.047686497312382094, fluctuations: 0.29\n",
      "step: 42390 loss: 0.963545 time elapsed: 55.9024 learning rate: 0.000774, scenario: 0, slope: -0.09691314205009713, fluctuations: 0.34\n",
      "step: 42400 loss: 0.959723 time elapsed: 55.9163 learning rate: 0.000774, scenario: 0, slope: -0.1489911628757716, fluctuations: 0.38\n",
      "step: 42410 loss: 0.958801 time elapsed: 55.9308 learning rate: 0.000774, scenario: 0, slope: -0.1401872344093166, fluctuations: 0.43\n",
      "step: 42420 loss: 0.958216 time elapsed: 55.9448 learning rate: 0.000774, scenario: 0, slope: -0.021621452988208233, fluctuations: 0.46\n",
      "step: 42430 loss: 0.957603 time elapsed: 55.9583 learning rate: 0.000774, scenario: 0, slope: -0.006685830460204996, fluctuations: 0.44\n",
      "step: 42440 loss: 0.956983 time elapsed: 55.9709 learning rate: 0.000774, scenario: 0, slope: -0.0018290558362859261, fluctuations: 0.4\n",
      "step: 42450 loss: 0.956377 time elapsed: 55.9835 learning rate: 0.000774, scenario: 0, slope: -0.0002577074687452002, fluctuations: 0.36\n",
      "step: 42460 loss: 0.955783 time elapsed: 55.9959 learning rate: 0.000774, scenario: 0, slope: -0.00019094657148467032, fluctuations: 0.31\n",
      "step: 42470 loss: 0.955199 time elapsed: 56.0083 learning rate: 0.000759, scenario: -1, slope: -8.206795418779731e-05, fluctuations: 0.26\n",
      "step: 42480 loss: 0.954641 time elapsed: 56.0209 learning rate: 0.000707, scenario: -1, slope: -7.517496053864477e-05, fluctuations: 0.21\n",
      "step: 42490 loss: 0.954133 time elapsed: 56.0333 learning rate: 0.000640, scenario: -1, slope: -6.697990377470267e-05, fluctuations: 0.16\n",
      "step: 42500 loss: 0.953675 time elapsed: 56.0457 learning rate: 0.000584, scenario: -1, slope: -5.888729311271202e-05, fluctuations: 0.12\n",
      "step: 42510 loss: 0.953258 time elapsed: 56.0588 learning rate: 0.000528, scenario: -1, slope: -5.6951844956179505e-05, fluctuations: 0.06\n",
      "step: 42520 loss: 0.952881 time elapsed: 56.0713 learning rate: 0.000478, scenario: -1, slope: -5.435926505943819e-05, fluctuations: 0.01\n",
      "step: 42530 loss: 0.952522 time elapsed: 56.0837 learning rate: 0.000515, scenario: 1, slope: -5.125970529112315e-05, fluctuations: 0.0\n",
      "step: 42540 loss: 0.952128 time elapsed: 56.0960 learning rate: 0.000569, scenario: 1, slope: -4.812677823874561e-05, fluctuations: 0.0\n",
      "step: 42550 loss: 0.951693 time elapsed: 56.1092 learning rate: 0.000628, scenario: 1, slope: -4.536202381892532e-05, fluctuations: 0.0\n",
      "step: 42560 loss: 0.951212 time elapsed: 56.1232 learning rate: 0.000694, scenario: 1, slope: -4.331538739265591e-05, fluctuations: 0.0\n",
      "step: 42570 loss: 0.950682 time elapsed: 56.1369 learning rate: 0.000767, scenario: 1, slope: -4.2347989370340245e-05, fluctuations: 0.0\n",
      "step: 42580 loss: 0.950096 time elapsed: 56.1506 learning rate: 0.000847, scenario: 1, slope: -4.274452831114153e-05, fluctuations: 0.0\n",
      "step: 42590 loss: 0.949450 time elapsed: 56.1664 learning rate: 0.000935, scenario: 1, slope: -4.457215440493257e-05, fluctuations: 0.0\n",
      "step: 42600 loss: 0.948738 time elapsed: 56.1791 learning rate: 0.001023, scenario: 1, slope: -4.735910587352677e-05, fluctuations: 0.0\n",
      "step: 42610 loss: 0.947958 time elapsed: 56.1923 learning rate: 0.001130, scenario: 1, slope: -5.203796776506875e-05, fluctuations: 0.0\n",
      "step: 42620 loss: 0.947100 time elapsed: 56.2049 learning rate: 0.001248, scenario: 1, slope: -5.724763890979064e-05, fluctuations: 0.0\n",
      "step: 42630 loss: 0.946154 time elapsed: 56.2176 learning rate: 0.001379, scenario: 1, slope: -6.308554017109154e-05, fluctuations: 0.0\n",
      "step: 42640 loss: 0.945110 time elapsed: 56.2301 learning rate: 0.001523, scenario: 1, slope: -6.949264911973102e-05, fluctuations: 0.0\n",
      "step: 42650 loss: 0.943961 time elapsed: 56.2426 learning rate: 0.001683, scenario: 1, slope: -7.653316290791781e-05, fluctuations: 0.0\n",
      "step: 42660 loss: 2.265628 time elapsed: 56.2553 learning rate: 0.001795, scenario: -1, slope: 0.0008357686188293063, fluctuations: 0.0\n",
      "step: 42670 loss: 267.237238 time elapsed: 56.2692 learning rate: 0.001623, scenario: -1, slope: 1.2824885639506427, fluctuations: 0.03\n",
      "step: 42680 loss: 23.487031 time elapsed: 56.2833 learning rate: 0.001468, scenario: -1, slope: 1.4992401300057803, fluctuations: 0.08\n",
      "step: 42690 loss: 19.691607 time elapsed: 56.2965 learning rate: 0.001328, scenario: -1, slope: 1.3741151884461555, fluctuations: 0.12\n",
      "step: 42700 loss: 4.030960 time elapsed: 56.3116 learning rate: 0.001213, scenario: -1, slope: 1.1139741270976427, fluctuations: 0.16\n",
      "step: 42710 loss: 3.009537 time elapsed: 56.3280 learning rate: 0.001097, scenario: -1, slope: 0.6433903853276502, fluctuations: 0.2\n",
      "step: 42720 loss: 3.352895 time elapsed: 56.3433 learning rate: 0.000992, scenario: -1, slope: 0.17022852870900287, fluctuations: 0.23\n",
      "step: 42730 loss: 1.592947 time elapsed: 56.3586 learning rate: 0.000953, scenario: 0, slope: -0.2797175358163919, fluctuations: 0.27\n",
      "step: 42740 loss: 1.331604 time elapsed: 56.3729 learning rate: 0.000953, scenario: 0, slope: -0.7879982127962656, fluctuations: 0.3\n",
      "step: 42750 loss: 1.304407 time elapsed: 56.3868 learning rate: 0.000953, scenario: 0, slope: -1.389755354917789, fluctuations: 0.33\n",
      "step: 42760 loss: 1.203528 time elapsed: 56.4006 learning rate: 0.000953, scenario: 0, slope: -2.3042677644051373, fluctuations: 0.37\n",
      "step: 42770 loss: 1.184947 time elapsed: 56.4137 learning rate: 0.000953, scenario: 0, slope: -0.9326092991198612, fluctuations: 0.36\n",
      "step: 42780 loss: 1.162709 time elapsed: 56.4276 learning rate: 0.000953, scenario: 0, slope: -0.3490888683346565, fluctuations: 0.35\n",
      "step: 42790 loss: 1.147008 time elapsed: 56.4411 learning rate: 0.000953, scenario: 0, slope: -0.10344067458189035, fluctuations: 0.32\n",
      "step: 42800 loss: 1.134834 time elapsed: 56.4548 learning rate: 0.000953, scenario: 0, slope: -0.026415702536078813, fluctuations: 0.29\n",
      "step: 42810 loss: 1.124045 time elapsed: 56.4693 learning rate: 0.000953, scenario: 0, slope: -0.011119896430801478, fluctuations: 0.25\n",
      "step: 42820 loss: 1.114795 time elapsed: 56.4826 learning rate: 0.000953, scenario: 0, slope: -0.004931191594874435, fluctuations: 0.21\n",
      "step: 42830 loss: 1.106502 time elapsed: 56.4960 learning rate: 0.000953, scenario: 0, slope: -0.0025968875568269096, fluctuations: 0.18\n",
      "step: 42840 loss: 1.099056 time elapsed: 56.5093 learning rate: 0.000953, scenario: 0, slope: -0.0018981417134726762, fluctuations: 0.14\n",
      "step: 42850 loss: 1.092286 time elapsed: 56.5242 learning rate: 0.000953, scenario: 0, slope: -0.0013247744751169715, fluctuations: 0.11\n",
      "step: 42860 loss: 1.086094 time elapsed: 56.5391 learning rate: 0.000953, scenario: 0, slope: -0.001075077334508713, fluctuations: 0.08\n",
      "step: 42870 loss: 1.080397 time elapsed: 56.5533 learning rate: 0.000953, scenario: 0, slope: -0.0009271630425611522, fluctuations: 0.04\n",
      "step: 42880 loss: 1.075130 time elapsed: 56.5678 learning rate: 0.000953, scenario: 0, slope: -0.0008222822933215985, fluctuations: 0.01\n",
      "step: 42890 loss: 1.070242 time elapsed: 56.5821 learning rate: 0.000953, scenario: 0, slope: -0.0007395175549031041, fluctuations: 0.0\n",
      "step: 42900 loss: 1.065689 time elapsed: 56.5956 learning rate: 0.000953, scenario: 0, slope: -0.0006749587848652529, fluctuations: 0.0\n",
      "step: 42910 loss: 1.061433 time elapsed: 56.6094 learning rate: 0.000953, scenario: 0, slope: -0.0006098737149704285, fluctuations: 0.0\n",
      "step: 42920 loss: 1.057445 time elapsed: 56.6221 learning rate: 0.000953, scenario: 0, slope: -0.0005601900500860268, fluctuations: 0.0\n",
      "step: 42930 loss: 1.053698 time elapsed: 56.6349 learning rate: 0.000953, scenario: 0, slope: -0.0005173664212372372, fluctuations: 0.0\n",
      "step: 42940 loss: 1.050167 time elapsed: 56.6473 learning rate: 0.000953, scenario: 0, slope: -0.00047996452050578766, fluctuations: 0.0\n",
      "step: 42950 loss: 1.046833 time elapsed: 56.6596 learning rate: 0.000953, scenario: 0, slope: -0.0004469873067578948, fluctuations: 0.0\n",
      "step: 42960 loss: 1.043679 time elapsed: 56.6720 learning rate: 0.000953, scenario: 0, slope: -0.0004176874708429517, fluctuations: 0.0\n",
      "step: 42970 loss: 1.040687 time elapsed: 56.6846 learning rate: 0.000953, scenario: 0, slope: -0.00039149447284014317, fluctuations: 0.0\n",
      "step: 42980 loss: 1.037845 time elapsed: 56.6971 learning rate: 0.000953, scenario: 0, slope: -0.0003679591630701512, fluctuations: 0.0\n",
      "step: 42990 loss: 1.035139 time elapsed: 56.7098 learning rate: 0.000953, scenario: 0, slope: -0.0003467210944762789, fluctuations: 0.0\n",
      "step: 43000 loss: 1.032558 time elapsed: 56.7221 learning rate: 0.000953, scenario: 0, slope: -0.00032932630040771385, fluctuations: 0.0\n",
      "step: 43010 loss: 1.030092 time elapsed: 56.7357 learning rate: 0.000953, scenario: 0, slope: -0.00031000801963274835, fluctuations: 0.0\n",
      "step: 43020 loss: 1.027733 time elapsed: 56.7505 learning rate: 0.000953, scenario: 0, slope: -0.0002940829803437186, fluctuations: 0.0\n",
      "step: 43030 loss: 1.025471 time elapsed: 56.7646 learning rate: 0.000953, scenario: 0, slope: -0.0002795358540639858, fluctuations: 0.0\n",
      "step: 43040 loss: 1.023299 time elapsed: 56.7792 learning rate: 0.000953, scenario: 0, slope: -0.0002662169659827232, fluctuations: 0.0\n",
      "step: 43050 loss: 1.021212 time elapsed: 56.7940 learning rate: 0.000953, scenario: 0, slope: -0.000253996991453381, fluctuations: 0.0\n",
      "step: 43060 loss: 1.019201 time elapsed: 56.8074 learning rate: 0.000953, scenario: 0, slope: -0.0002427634457542971, fluctuations: 0.0\n",
      "step: 43070 loss: 1.017263 time elapsed: 56.8211 learning rate: 0.000953, scenario: 0, slope: -0.00023241793394396923, fluctuations: 0.0\n",
      "step: 43080 loss: 1.015391 time elapsed: 56.8367 learning rate: 0.000953, scenario: 0, slope: -0.0002228739650166321, fluctuations: 0.0\n",
      "step: 43090 loss: 1.013582 time elapsed: 56.8505 learning rate: 0.000953, scenario: 0, slope: -0.00021405519053053676, fluctuations: 0.0\n",
      "step: 43100 loss: 1.011830 time elapsed: 56.8633 learning rate: 0.000953, scenario: 0, slope: -0.00020668226782836308, fluctuations: 0.0\n",
      "step: 43110 loss: 1.010133 time elapsed: 56.8775 learning rate: 0.000953, scenario: 0, slope: -0.00019833017393671782, fluctuations: 0.0\n",
      "step: 43120 loss: 1.008485 time elapsed: 56.8910 learning rate: 0.000953, scenario: 0, slope: -0.00019131021894106686, fluctuations: 0.0\n",
      "step: 43130 loss: 1.006885 time elapsed: 56.9038 learning rate: 0.000953, scenario: 0, slope: -0.00018478621402866885, fluctuations: 0.0\n",
      "step: 43140 loss: 1.005330 time elapsed: 56.9164 learning rate: 0.000953, scenario: 0, slope: -0.00017871526792649897, fluctuations: 0.0\n",
      "step: 43150 loss: 1.003815 time elapsed: 56.9288 learning rate: 0.000953, scenario: 0, slope: -0.00017305888550907253, fluctuations: 0.0\n",
      "step: 43160 loss: 1.002339 time elapsed: 56.9425 learning rate: 0.000953, scenario: 0, slope: -0.00016778245111897535, fluctuations: 0.0\n",
      "step: 43170 loss: 1.000900 time elapsed: 56.9569 learning rate: 0.000953, scenario: 0, slope: -0.0001628547820567767, fluctuations: 0.0\n",
      "step: 43180 loss: 0.999495 time elapsed: 56.9709 learning rate: 0.000953, scenario: 0, slope: -0.00015824774078692454, fluctuations: 0.0\n",
      "step: 43190 loss: 0.998122 time elapsed: 56.9849 learning rate: 0.000953, scenario: 0, slope: -0.0001539358966744226, fluctuations: 0.0\n",
      "step: 43200 loss: 0.996779 time elapsed: 56.9992 learning rate: 0.000953, scenario: 0, slope: -0.00015028855906405974, fluctuations: 0.0\n",
      "step: 43210 loss: 0.995465 time elapsed: 57.0126 learning rate: 0.000953, scenario: 0, slope: -0.00014610787083814574, fluctuations: 0.0\n",
      "step: 43220 loss: 0.994177 time elapsed: 57.0257 learning rate: 0.000953, scenario: 0, slope: -0.00014255187178674608, fluctuations: 0.0\n",
      "step: 43230 loss: 0.992915 time elapsed: 57.0384 learning rate: 0.000953, scenario: 0, slope: -0.00013921100370355242, fluctuations: 0.0\n",
      "step: 43240 loss: 0.991677 time elapsed: 57.0528 learning rate: 0.000953, scenario: 0, slope: -0.00013606957762709739, fluctuations: 0.0\n",
      "step: 43250 loss: 0.990461 time elapsed: 57.0664 learning rate: 0.000953, scenario: 0, slope: -0.0001331132859246082, fluctuations: 0.0\n",
      "step: 43260 loss: 0.989266 time elapsed: 57.0789 learning rate: 0.000953, scenario: 0, slope: -0.0001303290614858853, fluctuations: 0.0\n",
      "step: 43270 loss: 0.988092 time elapsed: 57.0920 learning rate: 0.000953, scenario: 0, slope: -0.00012770495261941515, fluctuations: 0.0\n",
      "step: 43280 loss: 0.986937 time elapsed: 57.1045 learning rate: 0.000953, scenario: 0, slope: -0.00012523001179121562, fluctuations: 0.0\n",
      "step: 43290 loss: 0.985799 time elapsed: 57.1174 learning rate: 0.000953, scenario: 0, slope: -0.00012289419659388578, fluctuations: 0.0\n",
      "step: 43300 loss: 0.984679 time elapsed: 57.1306 learning rate: 0.000953, scenario: 0, slope: -0.00012090327487798861, fluctuations: 0.0\n",
      "step: 43310 loss: 0.983575 time elapsed: 57.1438 learning rate: 0.000953, scenario: 0, slope: -0.00011860377943917366, fluctuations: 0.0\n",
      "step: 43320 loss: 0.982487 time elapsed: 57.1579 learning rate: 0.000953, scenario: 0, slope: -0.00011663287129408692, fluctuations: 0.0\n",
      "step: 43330 loss: 0.981412 time elapsed: 57.1722 learning rate: 0.000953, scenario: 0, slope: -0.00011476834372144681, fluctuations: 0.0\n",
      "step: 43340 loss: 0.980352 time elapsed: 57.1867 learning rate: 0.000953, scenario: 0, slope: -0.00011300353308333723, fluctuations: 0.0\n",
      "step: 43350 loss: 0.979305 time elapsed: 57.2008 learning rate: 0.000953, scenario: 0, slope: -0.00011133227555352009, fluctuations: 0.0\n",
      "step: 43360 loss: 0.978270 time elapsed: 57.2143 learning rate: 0.000953, scenario: 0, slope: -0.0001097488624554954, fluctuations: 0.0\n",
      "step: 43370 loss: 0.977247 time elapsed: 57.2279 learning rate: 0.000953, scenario: 0, slope: -0.00010824800027570079, fluctuations: 0.0\n",
      "step: 43380 loss: 0.976235 time elapsed: 57.2419 learning rate: 0.000953, scenario: 0, slope: -0.0001068247748205878, fluctuations: 0.0\n",
      "step: 43390 loss: 0.975234 time elapsed: 57.2556 learning rate: 0.000953, scenario: 0, slope: -0.00010547461904659082, fluctuations: 0.0\n",
      "step: 43400 loss: 0.974243 time elapsed: 57.2692 learning rate: 0.000953, scenario: 0, slope: -0.00010431843563781811, fluctuations: 0.0\n",
      "step: 43410 loss: 0.973261 time elapsed: 57.2833 learning rate: 0.000953, scenario: 0, slope: -0.0001029768135020144, fluctuations: 0.0\n",
      "step: 43420 loss: 0.972289 time elapsed: 57.2966 learning rate: 0.000953, scenario: 0, slope: -0.00010182151922999113, fluctuations: 0.0\n",
      "step: 43430 loss: 0.971326 time elapsed: 57.3104 learning rate: 0.000953, scenario: 0, slope: -0.0001007239609368674, fluctuations: 0.0\n",
      "step: 43440 loss: 0.970370 time elapsed: 57.3239 learning rate: 0.000953, scenario: 0, slope: -9.96809265197652e-05, fluctuations: 0.0\n",
      "step: 43450 loss: 0.969423 time elapsed: 57.3375 learning rate: 0.000953, scenario: 0, slope: -9.868941473040563e-05, fluctuations: 0.0\n",
      "step: 43460 loss: 0.968483 time elapsed: 57.3515 learning rate: 0.000962, scenario: 1, slope: -9.774661932381101e-05, fluctuations: 0.0\n",
      "step: 43470 loss: 0.967507 time elapsed: 57.3657 learning rate: 0.001063, scenario: 1, slope: -9.69392359955522e-05, fluctuations: 0.0\n",
      "step: 43480 loss: 0.966438 time elapsed: 57.3809 learning rate: 0.001174, scenario: 1, slope: -9.669258133949605e-05, fluctuations: 0.0\n",
      "step: 43490 loss: 0.965268 time elapsed: 57.3963 learning rate: 0.001297, scenario: 1, slope: -9.745132437682492e-05, fluctuations: 0.0\n",
      "step: 43500 loss: 0.964034 time elapsed: 57.4114 learning rate: 0.001297, scenario: 0, slope: -9.923432013399975e-05, fluctuations: 0.0\n",
      "step: 43510 loss: 0.962810 time elapsed: 57.4268 learning rate: 0.001297, scenario: 0, slope: -0.00010255370625376523, fluctuations: 0.0\n",
      "step: 43520 loss: 0.961599 time elapsed: 57.4405 learning rate: 0.001297, scenario: 0, slope: -0.00010621925090127135, fluctuations: 0.0\n",
      "step: 43530 loss: 0.960399 time elapsed: 57.4536 learning rate: 0.001297, scenario: 0, slope: -0.00011007816329054798, fluctuations: 0.0\n",
      "step: 43540 loss: 0.959209 time elapsed: 57.4659 learning rate: 0.001297, scenario: 0, slope: -0.00011373007385935883, fluctuations: 0.0\n",
      "step: 43550 loss: 0.958030 time elapsed: 57.4783 learning rate: 0.001297, scenario: 0, slope: -0.00011678360599989891, fluctuations: 0.0\n",
      "step: 43560 loss: 0.956860 time elapsed: 57.4906 learning rate: 0.001297, scenario: 0, slope: -0.00011885583150605973, fluctuations: 0.0\n",
      "step: 43570 loss: 0.955699 time elapsed: 57.5026 learning rate: 0.001297, scenario: 0, slope: -0.00011967040561653854, fluctuations: 0.0\n",
      "step: 43580 loss: 0.954547 time elapsed: 57.5146 learning rate: 0.001297, scenario: 0, slope: -0.00011941265983931525, fluctuations: 0.0\n",
      "step: 43590 loss: 0.953402 time elapsed: 57.5269 learning rate: 0.001297, scenario: 0, slope: -0.00011852616125089013, fluctuations: 0.0\n",
      "step: 43600 loss: 0.952265 time elapsed: 57.5388 learning rate: 0.001297, scenario: 0, slope: -0.0001176392470512682, fluctuations: 0.0\n",
      "step: 43610 loss: 0.951136 time elapsed: 57.5517 learning rate: 0.001297, scenario: 0, slope: -0.00011661483157698299, fluctuations: 0.0\n",
      "step: 43620 loss: 0.950013 time elapsed: 57.5642 learning rate: 0.001297, scenario: 0, slope: -0.0001157379091683883, fluctuations: 0.0\n",
      "step: 43630 loss: 0.948896 time elapsed: 57.5783 learning rate: 0.001297, scenario: 0, slope: -0.00011490975305331633, fluctuations: 0.0\n",
      "step: 43640 loss: 0.947786 time elapsed: 57.5922 learning rate: 0.001297, scenario: 0, slope: -0.00011412743182424387, fluctuations: 0.0\n",
      "step: 43650 loss: 0.946681 time elapsed: 57.6064 learning rate: 0.001297, scenario: 0, slope: -0.00011338815820886977, fluctuations: 0.0\n",
      "step: 43660 loss: 0.945581 time elapsed: 57.6204 learning rate: 0.001297, scenario: 0, slope: -0.0001126893300453237, fluctuations: 0.0\n",
      "step: 43670 loss: 0.944487 time elapsed: 57.6349 learning rate: 0.001297, scenario: 0, slope: -0.0001120285325950737, fluctuations: 0.0\n",
      "step: 43680 loss: 0.943398 time elapsed: 57.6477 learning rate: 0.001297, scenario: 0, slope: -0.00011140352850594656, fluctuations: 0.0\n",
      "step: 43690 loss: 0.942313 time elapsed: 57.6608 learning rate: 0.001297, scenario: 0, slope: -0.00011081224452025355, fluctuations: 0.0\n",
      "step: 43700 loss: 0.941232 time elapsed: 57.6730 learning rate: 0.001297, scenario: 0, slope: -0.00011030732765534368, fluctuations: 0.0\n",
      "step: 43710 loss: 0.940156 time elapsed: 57.6857 learning rate: 0.001297, scenario: 0, slope: -0.00010972328423662014, fluctuations: 0.0\n",
      "step: 43720 loss: 0.939084 time elapsed: 57.6981 learning rate: 0.001297, scenario: 0, slope: -0.00010922216482654466, fluctuations: 0.0\n",
      "step: 43730 loss: 0.938015 time elapsed: 57.7103 learning rate: 0.001297, scenario: 0, slope: -0.00010874785707107508, fluctuations: 0.0\n",
      "step: 43740 loss: 0.936949 time elapsed: 57.7228 learning rate: 0.001297, scenario: 0, slope: -0.00010829892438262101, fluctuations: 0.0\n",
      "step: 43750 loss: 0.935887 time elapsed: 57.7353 learning rate: 0.001297, scenario: 0, slope: -0.00010787402758031861, fluctuations: 0.0\n",
      "step: 43760 loss: 0.934829 time elapsed: 57.7478 learning rate: 0.001297, scenario: 0, slope: -0.0001074719169976215, fluctuations: 0.0\n",
      "step: 43770 loss: 0.933773 time elapsed: 57.7606 learning rate: 0.001297, scenario: 0, slope: -0.00010709142532018837, fluctuations: 0.0\n",
      "step: 43780 loss: 0.932720 time elapsed: 57.7732 learning rate: 0.001297, scenario: 0, slope: -0.00010673146108057097, fluctuations: 0.0\n",
      "step: 43790 loss: 0.931669 time elapsed: 57.7860 learning rate: 0.001297, scenario: 0, slope: -0.00010639100274402231, fluctuations: 0.0\n",
      "step: 43800 loss: 0.930621 time elapsed: 57.8010 learning rate: 0.001297, scenario: 0, slope: -0.00010610047558754876, fluctuations: 0.0\n",
      "step: 43810 loss: 0.929575 time elapsed: 57.8165 learning rate: 0.001297, scenario: 0, slope: -0.00010576483549288347, fluctuations: 0.0\n",
      "step: 43820 loss: 0.928532 time elapsed: 57.8306 learning rate: 0.001297, scenario: 0, slope: -0.0001054773870860789, fluctuations: 0.0\n",
      "step: 43830 loss: 0.927490 time elapsed: 57.8462 learning rate: 0.001297, scenario: 0, slope: -0.0001052059570484018, fluctuations: 0.0\n",
      "step: 43840 loss: 0.926451 time elapsed: 57.8598 learning rate: 0.001297, scenario: 0, slope: -0.00010494980169329275, fluctuations: 0.0\n",
      "step: 43850 loss: 0.925413 time elapsed: 57.8734 learning rate: 0.001297, scenario: 0, slope: -0.00010470822129659471, fluctuations: 0.0\n",
      "step: 43860 loss: 0.924377 time elapsed: 57.8865 learning rate: 0.001297, scenario: 0, slope: -0.00010448055697482625, fluctuations: 0.0\n",
      "step: 43870 loss: 0.923343 time elapsed: 57.8999 learning rate: 0.001297, scenario: 0, slope: -0.00010426618782455862, fluctuations: 0.0\n",
      "step: 43880 loss: 0.922310 time elapsed: 57.9128 learning rate: 0.001297, scenario: 0, slope: -0.00010406452829750122, fluctuations: 0.0\n",
      "step: 43890 loss: 0.921279 time elapsed: 57.9255 learning rate: 0.001297, scenario: 0, slope: -0.00010387502578916355, fluctuations: 0.0\n",
      "step: 43900 loss: 0.920249 time elapsed: 57.9379 learning rate: 0.001297, scenario: 0, slope: -0.00010371443584941238, fluctuations: 0.0\n",
      "step: 43910 loss: 0.919220 time elapsed: 57.9510 learning rate: 0.001297, scenario: 0, slope: -0.00010353043300455912, fluctuations: 0.0\n",
      "step: 43920 loss: 0.918192 time elapsed: 57.9635 learning rate: 0.001297, scenario: 0, slope: -0.00010337438315008088, fluctuations: 0.0\n",
      "step: 43930 loss: 0.917165 time elapsed: 57.9758 learning rate: 0.001297, scenario: 0, slope: -0.00010322856754195298, fluctuations: 0.0\n",
      "step: 43940 loss: 0.916139 time elapsed: 57.9885 learning rate: 0.001297, scenario: 0, slope: -0.00010309256833043422, fluctuations: 0.0\n",
      "step: 43950 loss: 0.915114 time elapsed: 58.0025 learning rate: 0.001297, scenario: 0, slope: -0.00010296598965136391, fluctuations: 0.0\n",
      "step: 43960 loss: 0.914090 time elapsed: 58.0167 learning rate: 0.001297, scenario: 0, slope: -0.00010284845625406016, fluctuations: 0.0\n",
      "step: 43970 loss: 0.913067 time elapsed: 58.0307 learning rate: 0.001297, scenario: 0, slope: -0.00010273961222723409, fluctuations: 0.0\n",
      "step: 43980 loss: 0.912044 time elapsed: 58.0446 learning rate: 0.001297, scenario: 0, slope: -0.00010263911662095317, fluctuations: 0.0\n",
      "step: 43990 loss: 0.911027 time elapsed: 58.0581 learning rate: 0.001297, scenario: 0, slope: -0.00010254168042109786, fluctuations: 0.0\n",
      "step: 44000 loss: 0.921864 time elapsed: 58.0713 learning rate: 0.001297, scenario: 0, slope: -9.685773797399556e-05, fluctuations: 0.0\n",
      "step: 44010 loss: 35.048330 time elapsed: 58.0845 learning rate: 0.001227, scenario: -1, slope: 0.04473704380787359, fluctuations: 0.0\n",
      "step: 44020 loss: 10.788653 time elapsed: 58.0969 learning rate: 0.001110, scenario: -1, slope: 0.0794084192503711, fluctuations: 0.02\n",
      "step: 44030 loss: 2.880958 time elapsed: 58.1094 learning rate: 0.001004, scenario: -1, slope: 0.07185776589100423, fluctuations: 0.06\n",
      "step: 44040 loss: 1.646903 time elapsed: 58.1223 learning rate: 0.000908, scenario: -1, slope: 0.056382529833939266, fluctuations: 0.09\n",
      "step: 44050 loss: 1.161510 time elapsed: 58.1347 learning rate: 0.000821, scenario: -1, slope: 0.03879625211052768, fluctuations: 0.13\n",
      "step: 44060 loss: 0.911274 time elapsed: 58.1474 learning rate: 0.000743, scenario: -1, slope: 0.020168543930072082, fluctuations: 0.18\n",
      "step: 44070 loss: 0.909581 time elapsed: 58.1598 learning rate: 0.000685, scenario: 0, slope: -0.0020402809378861097, fluctuations: 0.23\n",
      "step: 44080 loss: 0.907928 time elapsed: 58.1723 learning rate: 0.000685, scenario: 0, slope: -0.02738663904141261, fluctuations: 0.28\n",
      "step: 44090 loss: 0.906956 time elapsed: 58.1848 learning rate: 0.000685, scenario: 0, slope: -0.0561056219046923, fluctuations: 0.33\n",
      "step: 44100 loss: 0.906173 time elapsed: 58.1973 learning rate: 0.000685, scenario: 0, slope: -0.0856932186549915, fluctuations: 0.37\n",
      "step: 44110 loss: 0.905466 time elapsed: 58.2118 learning rate: 0.000685, scenario: 0, slope: -0.09541946837335231, fluctuations: 0.42\n",
      "step: 44120 loss: 0.904805 time elapsed: 58.2256 learning rate: 0.000685, scenario: 0, slope: -0.011361911207941786, fluctuations: 0.4\n",
      "step: 44130 loss: 0.904179 time elapsed: 58.2396 learning rate: 0.000685, scenario: 0, slope: -0.002394936392812899, fluctuations: 0.37\n",
      "step: 44140 loss: 0.903581 time elapsed: 58.2534 learning rate: 0.000685, scenario: 0, slope: -0.0008929723992232945, fluctuations: 0.33\n",
      "step: 44150 loss: 0.903002 time elapsed: 58.2667 learning rate: 0.000685, scenario: 0, slope: -0.0002251223316311836, fluctuations: 0.29\n",
      "step: 44160 loss: 0.902435 time elapsed: 58.2795 learning rate: 0.000672, scenario: 0, slope: -0.0001390043664743757, fluctuations: 0.24\n",
      "step: 44170 loss: 0.901897 time elapsed: 58.2924 learning rate: 0.000626, scenario: -1, slope: -8.646567435519349e-05, fluctuations: 0.19\n",
      "step: 44180 loss: 0.901404 time elapsed: 58.3051 learning rate: 0.000566, scenario: -1, slope: -6.803398649073035e-05, fluctuations: 0.14\n",
      "step: 44190 loss: 0.900959 time elapsed: 58.3176 learning rate: 0.000512, scenario: -1, slope: -6.028038744766129e-05, fluctuations: 0.09\n",
      "step: 44200 loss: 0.900558 time elapsed: 58.3301 learning rate: 0.000468, scenario: -1, slope: -5.596148081233372e-05, fluctuations: 0.05\n",
      "step: 44210 loss: 0.900191 time elapsed: 58.3432 learning rate: 0.000447, scenario: 1, slope: -5.300325540733695e-05, fluctuations: 0.0\n",
      "step: 44220 loss: 0.899814 time elapsed: 58.3559 learning rate: 0.000494, scenario: 1, slope: -4.977717629065207e-05, fluctuations: 0.0\n",
      "step: 44230 loss: 0.899398 time elapsed: 58.3688 learning rate: 0.000545, scenario: 1, slope: -4.693911124442876e-05, fluctuations: 0.0\n",
      "step: 44240 loss: 0.898939 time elapsed: 58.3813 learning rate: 0.000602, scenario: 1, slope: -4.4640190338570676e-05, fluctuations: 0.0\n",
      "step: 44250 loss: 0.898430 time elapsed: 58.3940 learning rate: 0.000665, scenario: 1, slope: -4.316871904260384e-05, fluctuations: 0.0\n",
      "step: 44260 loss: 0.897869 time elapsed: 58.4072 learning rate: 0.000735, scenario: 1, slope: -4.2840992790292755e-05, fluctuations: 0.0\n",
      "step: 44270 loss: 0.897248 time elapsed: 58.4224 learning rate: 0.000812, scenario: 1, slope: -4.389365435419893e-05, fluctuations: 0.0\n",
      "step: 44280 loss: 0.896563 time elapsed: 58.4376 learning rate: 0.000897, scenario: 1, slope: -4.637189363180708e-05, fluctuations: 0.0\n",
      "step: 44290 loss: 0.895806 time elapsed: 58.4529 learning rate: 0.000991, scenario: 1, slope: -5.015350197813586e-05, fluctuations: 0.0\n",
      "step: 44300 loss: 0.894970 time elapsed: 58.4679 learning rate: 0.001083, scenario: 1, slope: -5.450683934969277e-05, fluctuations: 0.0\n",
      "step: 44310 loss: 0.894055 time elapsed: 58.4820 learning rate: 0.001197, scenario: 1, slope: -6.07406938581457e-05, fluctuations: 0.0\n",
      "step: 44320 loss: 0.893047 time elapsed: 58.4955 learning rate: 0.001322, scenario: 1, slope: -6.702580074306296e-05, fluctuations: 0.0\n",
      "step: 44330 loss: 0.891954 time elapsed: 58.5090 learning rate: 0.001460, scenario: 1, slope: -7.390511782782866e-05, fluctuations: 0.0\n",
      "step: 44340 loss: 702.258934 time elapsed: 58.5225 learning rate: 0.001438, scenario: -1, slope: 0.5368936026973046, fluctuations: 0.0\n",
      "step: 44350 loss: 235.927447 time elapsed: 58.5361 learning rate: 0.001300, scenario: -1, slope: 1.0817233335713925, fluctuations: 0.04\n",
      "step: 44360 loss: 12.446419 time elapsed: 58.5492 learning rate: 0.001176, scenario: -1, slope: 0.9883822044741032, fluctuations: 0.09\n",
      "step: 44370 loss: 22.862874 time elapsed: 58.5626 learning rate: 0.001064, scenario: -1, slope: 0.8363290006679566, fluctuations: 0.13\n",
      "step: 44380 loss: 8.104844 time elapsed: 58.5760 learning rate: 0.000962, scenario: -1, slope: 0.5864221071549042, fluctuations: 0.17\n",
      "step: 44390 loss: 1.986802 time elapsed: 58.5901 learning rate: 0.000870, scenario: -1, slope: 0.3043661523618663, fluctuations: 0.21\n",
      "step: 44400 loss: 1.474215 time elapsed: 58.6041 learning rate: 0.000795, scenario: -1, slope: 0.026364831890966015, fluctuations: 0.24\n",
      "step: 44410 loss: 1.218428 time elapsed: 58.6200 learning rate: 0.000795, scenario: 0, slope: -0.31709530599315683, fluctuations: 0.28\n",
      "step: 44420 loss: 1.074756 time elapsed: 58.6352 learning rate: 0.000795, scenario: 0, slope: -0.6714859306187553, fluctuations: 0.32\n",
      "step: 44430 loss: 1.036632 time elapsed: 58.6501 learning rate: 0.000795, scenario: 0, slope: -1.1194135861239296, fluctuations: 0.35\n",
      "step: 44440 loss: 1.007816 time elapsed: 58.6646 learning rate: 0.000795, scenario: 0, slope: -1.762151860662847, fluctuations: 0.38\n",
      "step: 44450 loss: 0.997956 time elapsed: 58.6799 learning rate: 0.000795, scenario: 0, slope: -0.24463608138987003, fluctuations: 0.37\n",
      "step: 44460 loss: 0.987026 time elapsed: 58.6942 learning rate: 0.000795, scenario: 0, slope: -0.13214217860058133, fluctuations: 0.36\n",
      "step: 44470 loss: 0.980935 time elapsed: 58.7081 learning rate: 0.000795, scenario: 0, slope: -0.03226104382801687, fluctuations: 0.33\n",
      "step: 44480 loss: 0.974946 time elapsed: 58.7219 learning rate: 0.000795, scenario: 0, slope: -0.01170213753471835, fluctuations: 0.29\n",
      "step: 44490 loss: 0.970240 time elapsed: 58.7357 learning rate: 0.000795, scenario: 0, slope: -0.005633934351034626, fluctuations: 0.25\n",
      "step: 44500 loss: 0.966005 time elapsed: 58.7486 learning rate: 0.000795, scenario: 0, slope: -0.002195900601614405, fluctuations: 0.22\n",
      "step: 44510 loss: 0.962289 time elapsed: 58.7625 learning rate: 0.000795, scenario: 0, slope: -0.0012608582577098148, fluctuations: 0.18\n",
      "step: 44520 loss: 0.958965 time elapsed: 58.7762 learning rate: 0.000795, scenario: 0, slope: -0.0007478481303445023, fluctuations: 0.15\n",
      "step: 44530 loss: 0.955963 time elapsed: 58.7891 learning rate: 0.000795, scenario: 0, slope: -0.0005970260636516244, fluctuations: 0.11\n",
      "step: 44540 loss: 0.953234 time elapsed: 58.8019 learning rate: 0.000795, scenario: 0, slope: -0.0004777997973065936, fluctuations: 0.08\n",
      "step: 44550 loss: 0.950734 time elapsed: 58.8146 learning rate: 0.000795, scenario: 0, slope: -0.0004156026663900164, fluctuations: 0.04\n",
      "step: 44560 loss: 0.948430 time elapsed: 58.8280 learning rate: 0.000795, scenario: 0, slope: -0.0003692783685945406, fluctuations: 0.01\n",
      "step: 44570 loss: 0.946295 time elapsed: 58.8421 learning rate: 0.000795, scenario: 0, slope: -0.0003298723276945749, fluctuations: 0.0\n",
      "step: 44580 loss: 0.944306 time elapsed: 58.8563 learning rate: 0.000795, scenario: 0, slope: -0.0002967988740394532, fluctuations: 0.0\n",
      "step: 44590 loss: 0.942445 time elapsed: 58.8703 learning rate: 0.000795, scenario: 0, slope: -0.00026943471679758644, fluctuations: 0.0\n",
      "step: 44600 loss: 0.940697 time elapsed: 58.8846 learning rate: 0.000795, scenario: 0, slope: -0.0002486444941845276, fluctuations: 0.0\n",
      "step: 44610 loss: 0.939048 time elapsed: 58.9003 learning rate: 0.000795, scenario: 0, slope: -0.0002270466340547558, fluctuations: 0.0\n",
      "step: 44620 loss: 0.937487 time elapsed: 58.9133 learning rate: 0.000795, scenario: 0, slope: -0.0002103410907819042, fluctuations: 0.0\n",
      "step: 44630 loss: 0.936005 time elapsed: 58.9271 learning rate: 0.000795, scenario: 0, slope: -0.00019588920599781145, fluctuations: 0.0\n",
      "step: 44640 loss: 0.934594 time elapsed: 58.9403 learning rate: 0.000795, scenario: 0, slope: -0.00018329268846742016, fluctuations: 0.0\n",
      "step: 44650 loss: 0.933246 time elapsed: 58.9541 learning rate: 0.000795, scenario: 0, slope: -0.00017223998377608886, fluctuations: 0.0\n",
      "step: 44660 loss: 0.931956 time elapsed: 58.9679 learning rate: 0.000795, scenario: 0, slope: -0.00016248246935032034, fluctuations: 0.0\n",
      "step: 44670 loss: 0.930719 time elapsed: 58.9809 learning rate: 0.000795, scenario: 0, slope: -0.00015382076942673626, fluctuations: 0.0\n",
      "step: 44680 loss: 0.929528 time elapsed: 58.9952 learning rate: 0.000795, scenario: 0, slope: -0.00014609333263238927, fluctuations: 0.0\n",
      "step: 44690 loss: 0.928381 time elapsed: 59.0091 learning rate: 0.000795, scenario: 0, slope: -0.00013916810212157043, fluctuations: 0.0\n",
      "step: 44700 loss: 0.927274 time elapsed: 59.0225 learning rate: 0.000795, scenario: 0, slope: -0.0001335309412717969, fluctuations: 0.0\n",
      "step: 44710 loss: 0.926203 time elapsed: 59.0370 learning rate: 0.000795, scenario: 0, slope: -0.0001273073840425912, fluctuations: 0.0\n",
      "step: 44720 loss: 0.925165 time elapsed: 59.0514 learning rate: 0.000795, scenario: 0, slope: -0.00012220578961585386, fluctuations: 0.0\n",
      "step: 44730 loss: 0.924158 time elapsed: 59.0656 learning rate: 0.000795, scenario: 0, slope: -0.00011756763793808216, fluctuations: 0.0\n",
      "step: 44740 loss: 0.923180 time elapsed: 59.0797 learning rate: 0.000795, scenario: 0, slope: -0.00011333873271763708, fluctuations: 0.0\n",
      "step: 44750 loss: 0.922227 time elapsed: 59.0939 learning rate: 0.000795, scenario: 0, slope: -0.00010947277457962937, fluctuations: 0.0\n",
      "step: 44760 loss: 0.921298 time elapsed: 59.1074 learning rate: 0.000795, scenario: 0, slope: -0.00010592998507079889, fluctuations: 0.0\n",
      "step: 44770 loss: 0.920392 time elapsed: 59.1202 learning rate: 0.000795, scenario: 0, slope: -0.00010267601043821054, fluctuations: 0.0\n",
      "step: 44780 loss: 0.919507 time elapsed: 59.1329 learning rate: 0.000795, scenario: 0, slope: -9.968104101420711e-05, fluctuations: 0.0\n",
      "step: 44790 loss: 0.918641 time elapsed: 59.1455 learning rate: 0.000795, scenario: 0, slope: -9.691909788076237e-05, fluctuations: 0.0\n",
      "step: 44800 loss: 0.917793 time elapsed: 59.1580 learning rate: 0.000795, scenario: 0, slope: -9.461374049349384e-05, fluctuations: 0.0\n",
      "step: 44810 loss: 0.916962 time elapsed: 59.1708 learning rate: 0.000819, scenario: 1, slope: -9.200662574552377e-05, fluctuations: 0.0\n",
      "step: 44820 loss: 0.916092 time elapsed: 59.1833 learning rate: 0.000904, scenario: 1, slope: -8.995199567059273e-05, fluctuations: 0.0\n",
      "step: 44830 loss: 0.915150 time elapsed: 59.1957 learning rate: 0.000999, scenario: 1, slope: -8.858369181921666e-05, fluctuations: 0.0\n",
      "step: 44840 loss: 0.914131 time elapsed: 59.2081 learning rate: 0.001104, scenario: 1, slope: -8.824479086117052e-05, fluctuations: 0.0\n",
      "step: 44850 loss: 0.913032 time elapsed: 59.2203 learning rate: 0.001219, scenario: 1, slope: -8.919420004104749e-05, fluctuations: 0.0\n",
      "step: 44860 loss: 0.911845 time elapsed: 59.2329 learning rate: 0.001347, scenario: 1, slope: -9.160292350940181e-05, fluctuations: 0.0\n",
      "step: 44870 loss: 0.910603 time elapsed: 59.2456 learning rate: 0.001360, scenario: 0, slope: -9.548983228836254e-05, fluctuations: 0.0\n",
      "step: 44880 loss: 0.909387 time elapsed: 59.2599 learning rate: 0.001360, scenario: 0, slope: -0.00010035280310555922, fluctuations: 0.0\n",
      "step: 44890 loss: 0.908198 time elapsed: 59.2738 learning rate: 0.001360, scenario: 0, slope: -0.00010550172309017936, fluctuations: 0.0\n",
      "step: 44900 loss: 0.907036 time elapsed: 59.2876 learning rate: 0.001360, scenario: 0, slope: -0.00010983665699935742, fluctuations: 0.0\n",
      "step: 44910 loss: 0.905896 time elapsed: 59.3023 learning rate: 0.001360, scenario: 0, slope: -0.00011406176397551136, fluctuations: 0.0\n",
      "step: 44920 loss: 0.904777 time elapsed: 59.3177 learning rate: 0.001360, scenario: 0, slope: -0.00011640059855071792, fluctuations: 0.0\n",
      "step: 44930 loss: 0.903676 time elapsed: 59.3315 learning rate: 0.001360, scenario: 0, slope: -0.00011726424134801818, fluctuations: 0.0\n",
      "step: 44940 loss: 0.902593 time elapsed: 59.3444 learning rate: 0.001360, scenario: 0, slope: -0.00011680883871493466, fluctuations: 0.0\n",
      "step: 44950 loss: 0.901525 time elapsed: 59.3596 learning rate: 0.001360, scenario: 0, slope: -0.00011534156250531099, fluctuations: 0.0\n",
      "step: 44960 loss: 0.900471 time elapsed: 59.3733 learning rate: 0.001360, scenario: 0, slope: -0.00011332419547282104, fluctuations: 0.0\n",
      "step: 44970 loss: 0.899430 time elapsed: 59.3869 learning rate: 0.001360, scenario: 0, slope: -0.00011131197788764218, fluctuations: 0.0\n",
      "step: 44980 loss: 0.898401 time elapsed: 59.4003 learning rate: 0.001360, scenario: 0, slope: -0.00010948711425418613, fluctuations: 0.0\n",
      "step: 44990 loss: 0.897382 time elapsed: 59.4133 learning rate: 0.001360, scenario: 0, slope: -0.00010783283397922033, fluctuations: 0.0\n",
      "step: 45000 loss: 0.896372 time elapsed: 59.4267 learning rate: 0.001360, scenario: 0, slope: -0.0001064758044213787, fluctuations: 0.0\n",
      "step: 45010 loss: 0.895372 time elapsed: 59.4409 learning rate: 0.001360, scenario: 0, slope: -0.00010496982739102401, fluctuations: 0.0\n",
      "step: 45020 loss: 0.894379 time elapsed: 59.4556 learning rate: 0.001360, scenario: 0, slope: -0.00010373198310269836, fluctuations: 0.0\n",
      "step: 45030 loss: 0.893394 time elapsed: 59.4705 learning rate: 0.001360, scenario: 0, slope: -0.00010260650462452527, fluctuations: 0.0\n",
      "step: 45040 loss: 0.892415 time elapsed: 59.4850 learning rate: 0.001360, scenario: 0, slope: -0.00010158260581630686, fluctuations: 0.0\n",
      "step: 45050 loss: 0.891442 time elapsed: 59.4990 learning rate: 0.001360, scenario: 0, slope: -0.0001006507063395005, fluctuations: 0.0\n",
      "step: 45060 loss: 0.890475 time elapsed: 59.5133 learning rate: 0.001360, scenario: 0, slope: -9.980227664259821e-05, fluctuations: 0.0\n",
      "step: 45070 loss: 0.889512 time elapsed: 59.5269 learning rate: 0.001360, scenario: 0, slope: -9.90297043880803e-05, fluctuations: 0.0\n",
      "step: 45080 loss: 0.888554 time elapsed: 59.5393 learning rate: 0.001360, scenario: 0, slope: -9.832618034435156e-05, fluctuations: 0.0\n",
      "step: 45090 loss: 0.887601 time elapsed: 59.5520 learning rate: 0.001360, scenario: 0, slope: -9.76856008729628e-05, fluctuations: 0.0\n",
      "step: 45100 loss: 0.886651 time elapsed: 59.5643 learning rate: 0.001360, scenario: 0, slope: -9.715835469030196e-05, fluctuations: 0.0\n",
      "step: 45110 loss: 0.885704 time elapsed: 59.5772 learning rate: 0.001360, scenario: 0, slope: -9.657189882317482e-05, fluctuations: 0.0\n",
      "step: 45120 loss: 0.884760 time elapsed: 59.5897 learning rate: 0.001360, scenario: 0, slope: -9.608940017674097e-05, fluctuations: 0.0\n",
      "step: 45130 loss: 0.883820 time elapsed: 59.6024 learning rate: 0.001360, scenario: 0, slope: -9.56509773219826e-05, fluctuations: 0.0\n",
      "step: 45140 loss: 0.882881 time elapsed: 59.6149 learning rate: 0.001360, scenario: 0, slope: -9.525300510535757e-05, fluctuations: 0.0\n",
      "step: 45150 loss: 0.881945 time elapsed: 59.6273 learning rate: 0.001360, scenario: 0, slope: -9.4892202744401e-05, fluctuations: 0.0\n",
      "step: 45160 loss: 0.881011 time elapsed: 59.6397 learning rate: 0.001360, scenario: 0, slope: -9.456559733863369e-05, fluctuations: 0.0\n",
      "step: 45170 loss: 0.880079 time elapsed: 59.6518 learning rate: 0.001360, scenario: 0, slope: -9.427049170105238e-05, fluctuations: 0.0\n",
      "step: 45180 loss: 0.879148 time elapsed: 59.6653 learning rate: 0.001360, scenario: 0, slope: -9.400443592648271e-05, fluctuations: 0.0\n",
      "step: 45190 loss: 0.878219 time elapsed: 59.6796 learning rate: 0.001360, scenario: 0, slope: -9.376520220498909e-05, fluctuations: 0.0\n",
      "step: 45200 loss: 0.877291 time elapsed: 59.6932 learning rate: 0.001360, scenario: 0, slope: -9.357114470951265e-05, fluctuations: 0.0\n",
      "step: 45210 loss: 0.876364 time elapsed: 59.7075 learning rate: 0.001360, scenario: 0, slope: -9.335926848846827e-05, fluctuations: 0.0\n",
      "step: 45220 loss: 0.875437 time elapsed: 59.7221 learning rate: 0.001360, scenario: 0, slope: -9.318903420262095e-05, fluctuations: 0.0\n",
      "step: 45230 loss: 0.874512 time elapsed: 59.7355 learning rate: 0.001360, scenario: 0, slope: -9.303851987223632e-05, fluctuations: 0.0\n",
      "step: 45240 loss: 0.873587 time elapsed: 59.7488 learning rate: 0.001360, scenario: 0, slope: -9.290631798092385e-05, fluctuations: 0.0\n",
      "step: 45250 loss: 0.872663 time elapsed: 59.7612 learning rate: 0.001360, scenario: 0, slope: -9.279114059624764e-05, fluctuations: 0.0\n",
      "step: 45260 loss: 0.871739 time elapsed: 59.7736 learning rate: 0.001360, scenario: 0, slope: -9.269180805024882e-05, fluctuations: 0.0\n",
      "step: 45270 loss: 0.870815 time elapsed: 59.7862 learning rate: 0.001360, scenario: 0, slope: -9.260723878681824e-05, fluctuations: 0.0\n",
      "step: 45280 loss: 0.869892 time elapsed: 59.7986 learning rate: 0.001360, scenario: 0, slope: -9.253644024389572e-05, fluctuations: 0.0\n",
      "step: 45290 loss: 0.868969 time elapsed: 59.8112 learning rate: 0.001360, scenario: 0, slope: -9.247850065712748e-05, fluctuations: 0.0\n",
      "step: 45300 loss: 0.868045 time elapsed: 59.8234 learning rate: 0.001360, scenario: 0, slope: -9.243665513757526e-05, fluctuations: 0.0\n",
      "step: 45310 loss: 0.867122 time elapsed: 59.8370 learning rate: 0.001360, scenario: 0, slope: -9.239791175323932e-05, fluctuations: 0.0\n",
      "step: 45320 loss: 0.866198 time elapsed: 59.8500 learning rate: 0.001360, scenario: 0, slope: -9.237378008299225e-05, fluctuations: 0.0\n",
      "step: 45330 loss: 0.865275 time elapsed: 59.8628 learning rate: 0.001360, scenario: 0, slope: -9.235953126117776e-05, fluctuations: 0.0\n",
      "step: 45340 loss: 0.864350 time elapsed: 59.8776 learning rate: 0.001360, scenario: 0, slope: -9.235456036410904e-05, fluctuations: 0.0\n",
      "step: 45350 loss: 0.863426 time elapsed: 59.8924 learning rate: 0.001360, scenario: 0, slope: -9.235830853645571e-05, fluctuations: 0.0\n",
      "step: 45360 loss: 0.862501 time elapsed: 59.9067 learning rate: 0.001360, scenario: 0, slope: -9.237025878271635e-05, fluctuations: 0.0\n",
      "step: 45370 loss: 0.861576 time elapsed: 59.9210 learning rate: 0.001360, scenario: 0, slope: -9.238970549721684e-05, fluctuations: 0.0\n",
      "step: 45380 loss: 0.861026 time elapsed: 59.9356 learning rate: 0.001360, scenario: 0, slope: -9.200751124278449e-05, fluctuations: 0.0\n",
      "step: 45390 loss: 3.061430 time elapsed: 59.9485 learning rate: 0.001353, scenario: -1, slope: 0.0019696552730269143, fluctuations: 0.0\n",
      "step: 45400 loss: 22.158368 time elapsed: 59.9613 learning rate: 0.001236, scenario: -1, slope: 0.0898123192485561, fluctuations: 0.01\n",
      "step: 45410 loss: 4.263546 time elapsed: 59.9754 learning rate: 0.001118, scenario: -1, slope: 0.09946965822032633, fluctuations: 0.04\n",
      "step: 45420 loss: 1.722994 time elapsed: 59.9885 learning rate: 0.001011, scenario: -1, slope: 0.0883090170664019, fluctuations: 0.08\n",
      "step: 45430 loss: 1.070525 time elapsed: 60.0018 learning rate: 0.000914, scenario: -1, slope: 0.06721440183867455, fluctuations: 0.12\n",
      "step: 45440 loss: 0.929622 time elapsed: 60.0148 learning rate: 0.000827, scenario: -1, slope: 0.04025279525678172, fluctuations: 0.16\n",
      "step: 45450 loss: 0.877779 time elapsed: 60.0281 learning rate: 0.000748, scenario: -1, slope: 0.012468842630092947, fluctuations: 0.21\n",
      "step: 45460 loss: 0.866156 time elapsed: 60.0411 learning rate: 0.000718, scenario: 0, slope: -0.01993150755302838, fluctuations: 0.26\n",
      "step: 45470 loss: 0.859708 time elapsed: 60.0538 learning rate: 0.000718, scenario: 0, slope: -0.056483057414242706, fluctuations: 0.31\n",
      "step: 45480 loss: 0.857581 time elapsed: 60.0663 learning rate: 0.000718, scenario: 0, slope: -0.09894084642028654, fluctuations: 0.36\n",
      "step: 45490 loss: 0.856879 time elapsed: 60.0791 learning rate: 0.000718, scenario: 0, slope: -0.15066468450292603, fluctuations: 0.41\n",
      "step: 45500 loss: 0.856372 time elapsed: 60.0933 learning rate: 0.000718, scenario: 0, slope: -0.056458429054815296, fluctuations: 0.43\n",
      "step: 45510 loss: 0.855851 time elapsed: 60.1083 learning rate: 0.000718, scenario: 0, slope: -0.011078161706212907, fluctuations: 0.45\n",
      "step: 45520 loss: 0.855330 time elapsed: 60.1222 learning rate: 0.000718, scenario: 0, slope: -0.0017103514403917735, fluctuations: 0.42\n",
      "step: 45530 loss: 0.854816 time elapsed: 60.1361 learning rate: 0.000718, scenario: 0, slope: -0.0006327117223124739, fluctuations: 0.38\n",
      "step: 45540 loss: 0.854309 time elapsed: 60.1497 learning rate: 0.000718, scenario: 0, slope: -0.00031752235848414123, fluctuations: 0.33\n",
      "step: 45550 loss: 0.853806 time elapsed: 60.1625 learning rate: 0.000718, scenario: 0, slope: -0.00015073420811245755, fluctuations: 0.28\n",
      "step: 45560 loss: 0.853304 time elapsed: 60.1751 learning rate: 0.000704, scenario: -1, slope: -7.926155242817547e-05, fluctuations: 0.23\n",
      "step: 45570 loss: 0.852831 time elapsed: 60.1876 learning rate: 0.000637, scenario: -1, slope: -5.778985013307952e-05, fluctuations: 0.18\n",
      "step: 45580 loss: 0.852403 time elapsed: 60.2004 learning rate: 0.000576, scenario: -1, slope: -5.221081661264397e-05, fluctuations: 0.13\n",
      "step: 45590 loss: 0.852015 time elapsed: 60.2132 learning rate: 0.000521, scenario: -1, slope: -4.995078529409571e-05, fluctuations: 0.08\n",
      "step: 45600 loss: 0.851665 time elapsed: 60.2256 learning rate: 0.000476, scenario: -1, slope: -4.8101981479267304e-05, fluctuations: 0.04\n",
      "step: 45610 loss: 0.851341 time elapsed: 60.2386 learning rate: 0.000473, scenario: 1, slope: -4.600685059442668e-05, fluctuations: 0.0\n",
      "step: 45620 loss: 0.850999 time elapsed: 60.2514 learning rate: 0.000523, scenario: 1, slope: -4.357806323342641e-05, fluctuations: 0.0\n",
      "step: 45630 loss: 0.850619 time elapsed: 60.2637 learning rate: 0.000577, scenario: 1, slope: -4.127990324098049e-05, fluctuations: 0.0\n",
      "step: 45640 loss: 0.850200 time elapsed: 60.2766 learning rate: 0.000638, scenario: 1, slope: -3.941825999598699e-05, fluctuations: 0.0\n",
      "step: 45650 loss: 0.849736 time elapsed: 60.2892 learning rate: 0.000704, scenario: 1, slope: -3.831587518458799e-05, fluctuations: 0.0\n",
      "step: 45660 loss: 0.849222 time elapsed: 60.3021 learning rate: 0.000778, scenario: 1, slope: -3.8282921396049394e-05, fluctuations: 0.0\n",
      "step: 45670 loss: 0.848654 time elapsed: 60.3167 learning rate: 0.000860, scenario: 1, slope: -3.9537046932757185e-05, fluctuations: 0.0\n",
      "step: 45680 loss: 0.848026 time elapsed: 60.3307 learning rate: 0.000950, scenario: 1, slope: -4.205067675721781e-05, fluctuations: 0.0\n",
      "step: 45690 loss: 0.847332 time elapsed: 60.3446 learning rate: 0.001049, scenario: 1, slope: -4.569999900938201e-05, fluctuations: 0.0\n",
      "step: 45700 loss: 0.846565 time elapsed: 60.3583 learning rate: 0.001147, scenario: 1, slope: -4.9802356607009555e-05, fluctuations: 0.0\n",
      "step: 45710 loss: 0.845726 time elapsed: 60.3724 learning rate: 0.001267, scenario: 1, slope: -5.55844135318012e-05, fluctuations: 0.0\n",
      "step: 45720 loss: 0.844799 time elapsed: 60.3852 learning rate: 0.001400, scenario: 1, slope: -6.139296246323906e-05, fluctuations: 0.0\n",
      "step: 45730 loss: 0.843775 time elapsed: 60.3983 learning rate: 0.001546, scenario: 1, slope: -6.777004352326645e-05, fluctuations: 0.0\n",
      "step: 45740 loss: 0.981040 time elapsed: 60.4109 learning rate: 0.001683, scenario: -1, slope: 1.9379328362368253e-05, fluctuations: 0.0\n",
      "step: 45750 loss: 758.894928 time elapsed: 60.4236 learning rate: 0.001522, scenario: -1, slope: 1.5404286258707482, fluctuations: 0.02\n",
      "step: 45760 loss: 42.249878 time elapsed: 60.4364 learning rate: 0.001376, scenario: -1, slope: 1.356513216149373, fluctuations: 0.07\n",
      "step: 45770 loss: 37.237433 time elapsed: 60.4489 learning rate: 0.001245, scenario: -1, slope: 1.322949740499965, fluctuations: 0.11\n",
      "step: 45780 loss: 3.226431 time elapsed: 60.4616 learning rate: 0.001126, scenario: -1, slope: 1.0550091792082241, fluctuations: 0.15\n",
      "step: 45790 loss: 8.651615 time elapsed: 60.4740 learning rate: 0.001018, scenario: -1, slope: 0.6462491154624733, fluctuations: 0.18\n",
      "step: 45800 loss: 2.044357 time elapsed: 60.4866 learning rate: 0.000930, scenario: -1, slope: 0.31464855917579637, fluctuations: 0.22\n",
      "step: 45810 loss: 1.874992 time elapsed: 60.4997 learning rate: 0.000884, scenario: 0, slope: -0.18338427843654714, fluctuations: 0.25\n",
      "step: 45820 loss: 1.456381 time elapsed: 60.5137 learning rate: 0.000884, scenario: 0, slope: -0.6443503938303018, fluctuations: 0.28\n",
      "step: 45830 loss: 1.189294 time elapsed: 60.5283 learning rate: 0.000884, scenario: 0, slope: -1.1986523475345925, fluctuations: 0.32\n",
      "step: 45840 loss: 1.125850 time elapsed: 60.5421 learning rate: 0.000884, scenario: 0, slope: -1.964930308490015, fluctuations: 0.35\n",
      "step: 45850 loss: 1.104342 time elapsed: 60.5564 learning rate: 0.000884, scenario: 0, slope: -0.6902570562584914, fluctuations: 0.35\n",
      "step: 45860 loss: 1.079903 time elapsed: 60.5708 learning rate: 0.000884, scenario: 0, slope: -0.45603827762638016, fluctuations: 0.34\n",
      "step: 45870 loss: 1.061886 time elapsed: 60.5841 learning rate: 0.000884, scenario: 0, slope: -0.12461326287192719, fluctuations: 0.31\n",
      "step: 45880 loss: 1.049054 time elapsed: 60.5972 learning rate: 0.000884, scenario: 0, slope: -0.04705218403157849, fluctuations: 0.27\n",
      "step: 45890 loss: 1.037780 time elapsed: 60.6103 learning rate: 0.000884, scenario: 0, slope: -0.014657199104893664, fluctuations: 0.24\n",
      "step: 45900 loss: 1.027789 time elapsed: 60.6234 learning rate: 0.000884, scenario: 0, slope: -0.0069842319399177075, fluctuations: 0.21\n",
      "step: 45910 loss: 1.018998 time elapsed: 60.6365 learning rate: 0.000884, scenario: 0, slope: -0.0033404828769879563, fluctuations: 0.17\n",
      "step: 45920 loss: 1.011085 time elapsed: 60.6493 learning rate: 0.000884, scenario: 0, slope: -0.0019125546129458587, fluctuations: 0.14\n",
      "step: 45930 loss: 1.003901 time elapsed: 60.6621 learning rate: 0.000884, scenario: 0, slope: -0.0014255659416174437, fluctuations: 0.11\n",
      "step: 45940 loss: 0.997345 time elapsed: 60.6749 learning rate: 0.000884, scenario: 0, slope: -0.0011759777255215891, fluctuations: 0.07\n",
      "step: 45950 loss: 0.991323 time elapsed: 60.6875 learning rate: 0.000884, scenario: 0, slope: -0.0009890252428997892, fluctuations: 0.04\n",
      "step: 45960 loss: 0.985764 time elapsed: 60.7001 learning rate: 0.000884, scenario: 0, slope: -0.0008752497729184499, fluctuations: 0.01\n",
      "step: 45970 loss: 0.980612 time elapsed: 60.7128 learning rate: 0.000884, scenario: 0, slope: -0.0007857024081826283, fluctuations: 0.0\n",
      "step: 45980 loss: 0.975818 time elapsed: 60.7269 learning rate: 0.000884, scenario: 0, slope: -0.0007090375993327334, fluctuations: 0.0\n",
      "step: 45990 loss: 0.971343 time elapsed: 60.7411 learning rate: 0.000884, scenario: 0, slope: -0.0006457330901411173, fluctuations: 0.0\n",
      "step: 46000 loss: 0.967153 time elapsed: 60.7551 learning rate: 0.000884, scenario: 0, slope: -0.000597194568157361, fluctuations: 0.0\n",
      "step: 46010 loss: 0.963219 time elapsed: 60.7699 learning rate: 0.000884, scenario: 0, slope: -0.0005461247425390053, fluctuations: 0.0\n",
      "step: 46020 loss: 0.959516 time elapsed: 60.7848 learning rate: 0.000884, scenario: 0, slope: -0.0005059523286556059, fluctuations: 0.0\n",
      "step: 46030 loss: 0.956022 time elapsed: 60.7975 learning rate: 0.000884, scenario: 0, slope: -0.0004706094603336157, fluctuations: 0.0\n",
      "step: 46040 loss: 0.952718 time elapsed: 60.8105 learning rate: 0.000884, scenario: 0, slope: -0.0004392710881490413, fluctuations: 0.0\n",
      "step: 46050 loss: 0.949588 time elapsed: 60.8231 learning rate: 0.000884, scenario: 0, slope: -0.000411306424227757, fluctuations: 0.0\n",
      "step: 46060 loss: 0.946617 time elapsed: 60.8356 learning rate: 0.000884, scenario: 0, slope: -0.0003862168398851, fluctuations: 0.0\n",
      "step: 46070 loss: 0.943790 time elapsed: 60.8483 learning rate: 0.000884, scenario: 0, slope: -0.0003636014903077007, fluctuations: 0.0\n",
      "step: 46080 loss: 0.941097 time elapsed: 60.8609 learning rate: 0.000884, scenario: 0, slope: -0.0003431335035132256, fluctuations: 0.0\n",
      "step: 46090 loss: 0.938527 time elapsed: 60.8734 learning rate: 0.000884, scenario: 0, slope: -0.00032454248250293885, fluctuations: 0.0\n",
      "step: 46100 loss: 0.936070 time elapsed: 60.8857 learning rate: 0.000884, scenario: 0, slope: -0.00030922770069843013, fluctuations: 0.0\n",
      "step: 46110 loss: 0.933718 time elapsed: 60.9005 learning rate: 0.000884, scenario: 0, slope: -0.00029212185544405316, fluctuations: 0.0\n",
      "step: 46120 loss: 0.931462 time elapsed: 60.9137 learning rate: 0.000884, scenario: 0, slope: -0.00027793840176190037, fluctuations: 0.0\n",
      "step: 46130 loss: 0.929296 time elapsed: 60.9264 learning rate: 0.000884, scenario: 0, slope: -0.00026491212129720775, fluctuations: 0.0\n",
      "step: 46140 loss: 0.927214 time elapsed: 60.9404 learning rate: 0.000884, scenario: 0, slope: -0.0002529221491524449, fluctuations: 0.0\n",
      "step: 46150 loss: 0.925210 time elapsed: 60.9549 learning rate: 0.000884, scenario: 0, slope: -0.0002418633602787554, fluctuations: 0.0\n",
      "step: 46160 loss: 0.923277 time elapsed: 60.9686 learning rate: 0.000884, scenario: 0, slope: -0.00023164382921319615, fluctuations: 0.0\n",
      "step: 46170 loss: 0.921412 time elapsed: 60.9823 learning rate: 0.000884, scenario: 0, slope: -0.0002221827894429464, fluctuations: 0.0\n",
      "step: 46180 loss: 0.919610 time elapsed: 60.9966 learning rate: 0.000884, scenario: 0, slope: -0.00021340897750935837, fluctuations: 0.0\n",
      "step: 46190 loss: 0.917867 time elapsed: 61.0104 learning rate: 0.000884, scenario: 0, slope: -0.0002052592770022664, fluctuations: 0.0\n",
      "step: 46200 loss: 0.916179 time elapsed: 61.0230 learning rate: 0.000884, scenario: 0, slope: -0.00019841167407602642, fluctuations: 0.0\n",
      "step: 46210 loss: 0.914543 time elapsed: 61.0359 learning rate: 0.000884, scenario: 0, slope: -0.0001906139455608715, fluctuations: 0.0\n",
      "step: 46220 loss: 0.912956 time elapsed: 61.0482 learning rate: 0.000884, scenario: 0, slope: -0.0001840236332388162, fluctuations: 0.0\n",
      "step: 46230 loss: 0.911414 time elapsed: 61.0605 learning rate: 0.000884, scenario: 0, slope: -0.00017786662387997353, fluctuations: 0.0\n",
      "step: 46240 loss: 0.909915 time elapsed: 61.0742 learning rate: 0.000884, scenario: 0, slope: -0.00017210696405932984, fluctuations: 0.0\n",
      "step: 46250 loss: 0.908456 time elapsed: 61.0882 learning rate: 0.000884, scenario: 0, slope: -0.0001667123011766058, fluctuations: 0.0\n",
      "step: 46260 loss: 0.907036 time elapsed: 61.1024 learning rate: 0.000884, scenario: 0, slope: -0.0001616534670228222, fluctuations: 0.0\n",
      "step: 46270 loss: 0.905651 time elapsed: 61.1162 learning rate: 0.000884, scenario: 0, slope: -0.00015690411701879255, fluctuations: 0.0\n",
      "step: 46280 loss: 0.904301 time elapsed: 61.1301 learning rate: 0.000884, scenario: 0, slope: -0.00015244041620881326, fluctuations: 0.0\n",
      "step: 46290 loss: 0.902982 time elapsed: 61.1445 learning rate: 0.000884, scenario: 0, slope: -0.00014824076480342697, fluctuations: 0.0\n",
      "step: 46300 loss: 0.901694 time elapsed: 61.1598 learning rate: 0.000884, scenario: 0, slope: -0.00014467059920206794, fluctuations: 0.0\n",
      "step: 46310 loss: 0.900435 time elapsed: 61.1756 learning rate: 0.000884, scenario: 0, slope: -0.0001405569711116189, fluctuations: 0.0\n",
      "step: 46320 loss: 0.899202 time elapsed: 61.1909 learning rate: 0.000884, scenario: 0, slope: -0.00013703877849733701, fluctuations: 0.0\n",
      "step: 46330 loss: 0.897996 time elapsed: 61.2059 learning rate: 0.000884, scenario: 0, slope: -0.0001337161821982523, fluctuations: 0.0\n",
      "step: 46340 loss: 0.896814 time elapsed: 61.2204 learning rate: 0.000884, scenario: 0, slope: -0.00013057566828316136, fluctuations: 0.0\n",
      "step: 46350 loss: 0.895655 time elapsed: 61.2339 learning rate: 0.000884, scenario: 0, slope: -0.00012760487608442435, fluctuations: 0.0\n",
      "step: 46360 loss: 0.894518 time elapsed: 61.2472 learning rate: 0.000884, scenario: 0, slope: -0.00012479248247924404, fluctuations: 0.0\n",
      "step: 46370 loss: 0.893402 time elapsed: 61.2613 learning rate: 0.000884, scenario: 0, slope: -0.0001221280989065178, fluctuations: 0.0\n",
      "step: 46380 loss: 0.892307 time elapsed: 61.2752 learning rate: 0.000884, scenario: 0, slope: -0.00011960217963809429, fluctuations: 0.0\n",
      "step: 46390 loss: 0.891230 time elapsed: 61.2895 learning rate: 0.000884, scenario: 0, slope: -0.00011720594001562156, fluctuations: 0.0\n",
      "step: 46400 loss: 0.890171 time elapsed: 61.3036 learning rate: 0.000884, scenario: 0, slope: -0.00011515349553827797, fluctuations: 0.0\n",
      "step: 46410 loss: 0.889129 time elapsed: 61.3182 learning rate: 0.000884, scenario: 0, slope: -0.00011277073674609034, fluctuations: 0.0\n",
      "step: 46420 loss: 0.888104 time elapsed: 61.3312 learning rate: 0.000884, scenario: 0, slope: -0.0001107173912218811, fluctuations: 0.0\n",
      "step: 46430 loss: 0.887094 time elapsed: 61.3457 learning rate: 0.000884, scenario: 0, slope: -0.00010876485161352863, fluctuations: 0.0\n",
      "step: 46440 loss: 0.886099 time elapsed: 61.3614 learning rate: 0.000884, scenario: 0, slope: -0.00010690718931315093, fluctuations: 0.0\n",
      "step: 46450 loss: 0.885118 time elapsed: 61.3770 learning rate: 0.000884, scenario: 0, slope: -0.00010513890098632027, fluctuations: 0.0\n",
      "step: 46460 loss: 0.884151 time elapsed: 61.3922 learning rate: 0.000884, scenario: 0, slope: -0.00010345487146959255, fluctuations: 0.0\n",
      "step: 46470 loss: 0.883197 time elapsed: 61.4074 learning rate: 0.000884, scenario: 0, slope: -0.00010185034053802956, fluctuations: 0.0\n",
      "step: 46480 loss: 0.882255 time elapsed: 61.4228 learning rate: 0.000884, scenario: 0, slope: -0.00010032087310792834, fluctuations: 0.0\n",
      "step: 46490 loss: 0.881325 time elapsed: 61.4365 learning rate: 0.000884, scenario: 0, slope: -9.886233248315615e-05, fluctuations: 0.0\n",
      "step: 46500 loss: 0.880406 time elapsed: 61.4501 learning rate: 0.000884, scenario: 0, slope: -9.760709078335146e-05, fluctuations: 0.0\n",
      "step: 46510 loss: 0.879498 time elapsed: 61.4639 learning rate: 0.000884, scenario: 0, slope: -9.614283486095934e-05, fluctuations: 0.0\n",
      "step: 46520 loss: 0.878601 time elapsed: 61.4779 learning rate: 0.000884, scenario: 0, slope: -9.48748915774511e-05, fluctuations: 0.0\n",
      "step: 46530 loss: 0.877713 time elapsed: 61.4923 learning rate: 0.000884, scenario: 0, slope: -9.366386528866845e-05, fluctuations: 0.0\n",
      "step: 46540 loss: 0.876834 time elapsed: 61.5069 learning rate: 0.000884, scenario: 0, slope: -9.250679422936598e-05, fluctuations: 0.0\n",
      "step: 46550 loss: 0.875965 time elapsed: 61.5209 learning rate: 0.000884, scenario: 0, slope: -9.140090146412653e-05, fluctuations: 0.0\n",
      "step: 46560 loss: 0.875104 time elapsed: 61.5352 learning rate: 0.000884, scenario: 0, slope: -9.034358161868579e-05, fluctuations: 0.0\n",
      "step: 46570 loss: 0.874252 time elapsed: 61.5485 learning rate: 0.000884, scenario: 0, slope: -8.933238876179068e-05, fluctuations: 0.0\n",
      "step: 46580 loss: 0.873407 time elapsed: 61.5619 learning rate: 0.000884, scenario: 0, slope: -8.836502531099429e-05, fluctuations: 0.0\n",
      "step: 46590 loss: 0.872546 time elapsed: 61.5779 learning rate: 0.000967, scenario: 1, slope: -8.748032694490147e-05, fluctuations: 0.0\n",
      "step: 46600 loss: 0.871607 time elapsed: 61.5931 learning rate: 0.001058, scenario: 1, slope: -8.702618463768131e-05, fluctuations: 0.0\n",
      "step: 46610 loss: 0.870590 time elapsed: 61.6089 learning rate: 0.001169, scenario: 1, slope: -8.73386337918418e-05, fluctuations: 0.0\n",
      "step: 46620 loss: 0.869491 time elapsed: 61.6241 learning rate: 0.001216, scenario: 0, slope: -8.876245857811571e-05, fluctuations: 0.0\n",
      "step: 46630 loss: 0.868388 time elapsed: 61.6389 learning rate: 0.001216, scenario: 0, slope: -9.123048935240764e-05, fluctuations: 0.0\n",
      "step: 46640 loss: 0.867299 time elapsed: 61.6528 learning rate: 0.001216, scenario: 0, slope: -9.43626069999015e-05, fluctuations: 0.0\n",
      "step: 46650 loss: 0.866223 time elapsed: 61.6660 learning rate: 0.001216, scenario: 0, slope: -9.777478776838235e-05, fluctuations: 0.0\n",
      "step: 46660 loss: 0.865158 time elapsed: 61.6795 learning rate: 0.001216, scenario: 0, slope: -0.00010109300478811888, fluctuations: 0.0\n",
      "step: 46670 loss: 0.864104 time elapsed: 61.6938 learning rate: 0.001216, scenario: 0, slope: -0.00010395292202838306, fluctuations: 0.0\n",
      "step: 46680 loss: 0.863061 time elapsed: 61.7073 learning rate: 0.001216, scenario: 0, slope: -0.00010599938448270364, fluctuations: 0.0\n",
      "step: 46690 loss: 0.862028 time elapsed: 61.7217 learning rate: 0.001216, scenario: 0, slope: -0.00010693026889211216, fluctuations: 0.0\n",
      "step: 46700 loss: 0.861004 time elapsed: 61.7354 learning rate: 0.001216, scenario: 0, slope: -0.00010686932155638357, fluctuations: 0.0\n",
      "step: 46710 loss: 0.859990 time elapsed: 61.7495 learning rate: 0.001216, scenario: 0, slope: -0.0001059969289432423, fluctuations: 0.0\n",
      "step: 46720 loss: 0.858984 time elapsed: 61.7630 learning rate: 0.001216, scenario: 0, slope: -0.00010491626728133313, fluctuations: 0.0\n",
      "step: 46730 loss: 0.857987 time elapsed: 61.7772 learning rate: 0.001216, scenario: 0, slope: -0.00010387699807730072, fluctuations: 0.0\n",
      "step: 46740 loss: 0.856997 time elapsed: 61.7934 learning rate: 0.001216, scenario: 0, slope: -0.00010289039045424736, fluctuations: 0.0\n",
      "step: 46750 loss: 0.856015 time elapsed: 61.8091 learning rate: 0.001216, scenario: 0, slope: -0.00010195391747325936, fluctuations: 0.0\n",
      "step: 46760 loss: 0.855039 time elapsed: 61.8240 learning rate: 0.001216, scenario: 0, slope: -0.00010106487297068655, fluctuations: 0.0\n",
      "step: 46770 loss: 0.854071 time elapsed: 61.8392 learning rate: 0.001216, scenario: 0, slope: -0.00010022062045232625, fluctuations: 0.0\n",
      "step: 46780 loss: 0.853108 time elapsed: 61.8545 learning rate: 0.001216, scenario: 0, slope: -9.941866872442497e-05, fluctuations: 0.0\n",
      "step: 46790 loss: 0.852152 time elapsed: 61.8688 learning rate: 0.001216, scenario: 0, slope: -9.865668794129903e-05, fluctuations: 0.0\n",
      "step: 46800 loss: 0.851202 time elapsed: 61.8820 learning rate: 0.001216, scenario: 0, slope: -9.800328188733611e-05, fluctuations: 0.0\n",
      "step: 46810 loss: 0.850257 time elapsed: 61.8958 learning rate: 0.001216, scenario: 0, slope: -9.724409773651947e-05, fluctuations: 0.0\n",
      "step: 46820 loss: 0.849317 time elapsed: 61.9094 learning rate: 0.001216, scenario: 0, slope: -9.658957576283902e-05, fluctuations: 0.0\n",
      "step: 46830 loss: 0.848382 time elapsed: 61.9223 learning rate: 0.001216, scenario: 0, slope: -9.596717769450075e-05, fluctuations: 0.0\n",
      "step: 46840 loss: 0.847452 time elapsed: 61.9347 learning rate: 0.001216, scenario: 0, slope: -9.537525706568917e-05, fluctuations: 0.0\n",
      "step: 46850 loss: 0.846526 time elapsed: 61.9473 learning rate: 0.001216, scenario: 0, slope: -9.481227393491746e-05, fluctuations: 0.0\n",
      "step: 46860 loss: 0.845605 time elapsed: 61.9598 learning rate: 0.001216, scenario: 0, slope: -9.427678651545554e-05, fluctuations: 0.0\n",
      "step: 46870 loss: 0.844687 time elapsed: 61.9722 learning rate: 0.001216, scenario: 0, slope: -9.37674435711524e-05, fluctuations: 0.0\n",
      "step: 46880 loss: 0.843774 time elapsed: 61.9845 learning rate: 0.001216, scenario: 0, slope: -9.328297750455367e-05, fluctuations: 0.0\n",
      "step: 46890 loss: 0.842864 time elapsed: 61.9973 learning rate: 0.001216, scenario: 0, slope: -9.282219806834284e-05, fluctuations: 0.0\n",
      "step: 46900 loss: 0.841958 time elapsed: 62.0113 learning rate: 0.001216, scenario: 0, slope: -9.242682270668113e-05, fluctuations: 0.0\n",
      "step: 46910 loss: 0.841054 time elapsed: 62.0261 learning rate: 0.001216, scenario: 0, slope: -9.196729098253414e-05, fluctuations: 0.0\n",
      "step: 46920 loss: 0.840155 time elapsed: 62.0399 learning rate: 0.001216, scenario: 0, slope: -9.157112049928195e-05, fluctuations: 0.0\n",
      "step: 46930 loss: 0.839258 time elapsed: 62.0536 learning rate: 0.001216, scenario: 0, slope: -9.119454184069934e-05, fluctuations: 0.0\n",
      "step: 46940 loss: 0.838363 time elapsed: 62.0683 learning rate: 0.001216, scenario: 0, slope: -9.083667491968864e-05, fluctuations: 0.0\n",
      "step: 46950 loss: 0.837472 time elapsed: 62.0813 learning rate: 0.001216, scenario: 0, slope: -9.049668924128371e-05, fluctuations: 0.0\n",
      "step: 46960 loss: 0.836583 time elapsed: 62.0945 learning rate: 0.001216, scenario: 0, slope: -9.017380053446171e-05, fluctuations: 0.0\n",
      "step: 46970 loss: 0.835696 time elapsed: 62.1080 learning rate: 0.001216, scenario: 0, slope: -8.986726765483322e-05, fluctuations: 0.0\n",
      "step: 46980 loss: 0.834812 time elapsed: 62.1221 learning rate: 0.001216, scenario: 0, slope: -8.957638973261966e-05, fluctuations: 0.0\n",
      "step: 46990 loss: 0.833930 time elapsed: 62.1360 learning rate: 0.001216, scenario: 0, slope: -8.930050354334748e-05, fluctuations: 0.0\n",
      "step: 47000 loss: 0.833050 time elapsed: 62.1490 learning rate: 0.001216, scenario: 0, slope: -8.906450416678026e-05, fluctuations: 0.0\n",
      "step: 47010 loss: 0.832172 time elapsed: 62.1628 learning rate: 0.001216, scenario: 0, slope: -8.879122731602478e-05, fluctuations: 0.0\n",
      "step: 47020 loss: 0.831296 time elapsed: 62.1762 learning rate: 0.001216, scenario: 0, slope: -8.855667811962797e-05, fluctuations: 0.0\n",
      "step: 47030 loss: 0.830421 time elapsed: 62.1901 learning rate: 0.001216, scenario: 0, slope: -8.833479834346139e-05, fluctuations: 0.0\n",
      "step: 47040 loss: 0.829548 time elapsed: 62.2046 learning rate: 0.001216, scenario: 0, slope: -8.812508002664372e-05, fluctuations: 0.0\n",
      "step: 47050 loss: 0.828676 time elapsed: 62.2200 learning rate: 0.001216, scenario: 0, slope: -8.792702443514591e-05, fluctuations: 0.0\n",
      "step: 47060 loss: 0.827841 time elapsed: 62.2353 learning rate: 0.001216, scenario: 0, slope: -8.770338375432859e-05, fluctuations: 0.0\n",
      "step: 47070 loss: 0.999949 time elapsed: 62.2496 learning rate: 0.001259, scenario: -1, slope: 7.605607604825303e-05, fluctuations: 0.0\n",
      "step: 47080 loss: 7.809585 time elapsed: 62.2638 learning rate: 0.001139, scenario: -1, slope: 0.11813950651447855, fluctuations: 0.01\n",
      "step: 47090 loss: 11.921110 time elapsed: 62.2781 learning rate: 0.001030, scenario: -1, slope: 0.14883613456112763, fluctuations: 0.04\n",
      "step: 47100 loss: 2.955334 time elapsed: 62.2916 learning rate: 0.000941, scenario: -1, slope: 0.1356294858536843, fluctuations: 0.08\n",
      "step: 47110 loss: 2.117416 time elapsed: 62.3056 learning rate: 0.000851, scenario: -1, slope: 0.10281589960513841, fluctuations: 0.12\n",
      "step: 47120 loss: 1.037993 time elapsed: 62.3192 learning rate: 0.000769, scenario: -1, slope: 0.06943012074407214, fluctuations: 0.17\n",
      "step: 47130 loss: 0.897246 time elapsed: 62.3325 learning rate: 0.000696, scenario: -1, slope: 0.029844639146194313, fluctuations: 0.22\n",
      "step: 47140 loss: 0.873249 time elapsed: 62.3462 learning rate: 0.000662, scenario: 0, slope: -0.01867530970289682, fluctuations: 0.26\n",
      "step: 47150 loss: 0.831824 time elapsed: 62.3601 learning rate: 0.000662, scenario: 0, slope: -0.06827146905523698, fluctuations: 0.31\n",
      "step: 47160 loss: 0.829041 time elapsed: 62.3734 learning rate: 0.000662, scenario: 0, slope: -0.12685458437986996, fluctuations: 0.36\n",
      "step: 47170 loss: 0.828916 time elapsed: 62.3874 learning rate: 0.000662, scenario: 0, slope: -0.19884199440896594, fluctuations: 0.4\n",
      "step: 47180 loss: 0.826761 time elapsed: 62.4011 learning rate: 0.000662, scenario: 0, slope: -0.11068574444600723, fluctuations: 0.43\n",
      "step: 47190 loss: 0.825747 time elapsed: 62.4157 learning rate: 0.000662, scenario: 0, slope: -0.019023614227664923, fluctuations: 0.45\n",
      "step: 47200 loss: 0.825178 time elapsed: 62.4309 learning rate: 0.000662, scenario: 0, slope: -0.005108464710343957, fluctuations: 0.45\n",
      "step: 47210 loss: 0.824587 time elapsed: 62.4461 learning rate: 0.000662, scenario: 0, slope: -0.0011501350776613583, fluctuations: 0.4\n",
      "step: 47220 loss: 0.824004 time elapsed: 62.4609 learning rate: 0.000662, scenario: 0, slope: -0.0007494402750731307, fluctuations: 0.35\n",
      "step: 47230 loss: 0.823452 time elapsed: 62.4763 learning rate: 0.000662, scenario: 0, slope: -0.00019193403422733328, fluctuations: 0.31\n",
      "step: 47240 loss: 0.822918 time elapsed: 62.4902 learning rate: 0.000662, scenario: 0, slope: -0.00010029864411068975, fluctuations: 0.26\n",
      "step: 47250 loss: 0.822398 time elapsed: 62.5043 learning rate: 0.000636, scenario: -1, slope: -7.851912163814363e-05, fluctuations: 0.21\n",
      "step: 47260 loss: 0.821920 time elapsed: 62.5184 learning rate: 0.000575, scenario: -1, slope: -6.602555145634633e-05, fluctuations: 0.16\n",
      "step: 47270 loss: 0.821494 time elapsed: 62.5319 learning rate: 0.000520, scenario: -1, slope: -5.616957266897144e-05, fluctuations: 0.12\n",
      "step: 47280 loss: 0.821112 time elapsed: 62.5454 learning rate: 0.000470, scenario: -1, slope: -5.344330816783479e-05, fluctuations: 0.07\n",
      "step: 47290 loss: 0.820769 time elapsed: 62.5596 learning rate: 0.000425, scenario: -1, slope: -5.06661585235762e-05, fluctuations: 0.02\n",
      "step: 47300 loss: 0.820451 time elapsed: 62.5739 learning rate: 0.000436, scenario: 1, slope: -4.796359750902727e-05, fluctuations: 0.0\n",
      "step: 47310 loss: 0.820112 time elapsed: 62.5885 learning rate: 0.000481, scenario: 1, slope: -4.4436736200615836e-05, fluctuations: 0.0\n",
      "step: 47320 loss: 0.819740 time elapsed: 62.6022 learning rate: 0.000532, scenario: 1, slope: -4.1553575838569224e-05, fluctuations: 0.0\n",
      "step: 47330 loss: 0.819329 time elapsed: 62.6153 learning rate: 0.000587, scenario: 1, slope: -3.926274012266456e-05, fluctuations: 0.0\n",
      "step: 47340 loss: 0.818878 time elapsed: 62.6307 learning rate: 0.000649, scenario: 1, slope: -3.786080132799316e-05, fluctuations: 0.0\n",
      "step: 47350 loss: 0.818381 time elapsed: 62.6458 learning rate: 0.000717, scenario: 1, slope: -3.7628165944759344e-05, fluctuations: 0.0\n",
      "step: 47360 loss: 0.817835 time elapsed: 62.6611 learning rate: 0.000792, scenario: 1, slope: -3.8708099213027235e-05, fluctuations: 0.0\n",
      "step: 47370 loss: 0.817234 time elapsed: 62.6759 learning rate: 0.000875, scenario: 1, slope: -4.1015724271972034e-05, fluctuations: 0.0\n",
      "step: 47380 loss: 0.816573 time elapsed: 62.6909 learning rate: 0.000966, scenario: 1, slope: -4.439750845466425e-05, fluctuations: 0.0\n",
      "step: 47390 loss: 0.815846 time elapsed: 62.7050 learning rate: 0.001067, scenario: 1, slope: -4.864471592184019e-05, fluctuations: 0.0\n",
      "step: 47400 loss: 0.815047 time elapsed: 62.7193 learning rate: 0.001167, scenario: 1, slope: -5.299399252521352e-05, fluctuations: 0.0\n",
      "step: 47410 loss: 0.814176 time elapsed: 62.7337 learning rate: 0.001289, scenario: 1, slope: -5.883163947349471e-05, fluctuations: 0.0\n",
      "step: 47420 loss: 0.813219 time elapsed: 62.7473 learning rate: 0.001424, scenario: 1, slope: -6.464446192308766e-05, fluctuations: 0.0\n",
      "step: 47430 loss: 0.812196 time elapsed: 62.7607 learning rate: 0.001573, scenario: 1, slope: -7.097748395511282e-05, fluctuations: 0.0\n",
      "step: 47440 loss: 1342.401045 time elapsed: 62.7749 learning rate: 0.001549, scenario: -1, slope: 1.1000292436240695, fluctuations: 0.0\n",
      "step: 47450 loss: 6.492928 time elapsed: 62.7886 learning rate: 0.001401, scenario: -1, slope: 0.8928164546453375, fluctuations: 0.05\n",
      "step: 47460 loss: 113.509405 time elapsed: 62.8031 learning rate: 0.001267, scenario: -1, slope: 0.9979228881661893, fluctuations: 0.09\n",
      "step: 47470 loss: 35.514946 time elapsed: 62.8170 learning rate: 0.001146, scenario: -1, slope: 0.8200705857970231, fluctuations: 0.13\n",
      "step: 47480 loss: 11.743752 time elapsed: 62.8305 learning rate: 0.001036, scenario: -1, slope: 0.5760827223443354, fluctuations: 0.17\n",
      "step: 47490 loss: 1.248148 time elapsed: 62.8459 learning rate: 0.000937, scenario: -1, slope: 0.3025919434720718, fluctuations: 0.21\n",
      "step: 47500 loss: 2.324104 time elapsed: 62.8611 learning rate: 0.000856, scenario: -1, slope: 0.02958987104839245, fluctuations: 0.24\n",
      "step: 47510 loss: 1.261788 time elapsed: 62.8767 learning rate: 0.000856, scenario: 0, slope: -0.3122260283883343, fluctuations: 0.28\n",
      "step: 47520 loss: 1.080501 time elapsed: 62.8920 learning rate: 0.000856, scenario: 0, slope: -0.6670092719781441, fluctuations: 0.31\n",
      "step: 47530 loss: 1.074043 time elapsed: 62.9088 learning rate: 0.000856, scenario: 0, slope: -1.11571911145923, fluctuations: 0.34\n",
      "step: 47540 loss: 1.003857 time elapsed: 62.9242 learning rate: 0.000856, scenario: 0, slope: -1.1996657984709533, fluctuations: 0.37\n",
      "step: 47550 loss: 0.987083 time elapsed: 62.9380 learning rate: 0.000856, scenario: 0, slope: -0.5777898259728443, fluctuations: 0.35\n",
      "step: 47560 loss: 0.972061 time elapsed: 62.9520 learning rate: 0.000856, scenario: 0, slope: -0.13212163470147378, fluctuations: 0.35\n",
      "step: 47570 loss: 0.958908 time elapsed: 62.9655 learning rate: 0.000856, scenario: 0, slope: -0.04203614201387646, fluctuations: 0.32\n",
      "step: 47580 loss: 0.949587 time elapsed: 62.9789 learning rate: 0.000856, scenario: 0, slope: -0.017452130857487556, fluctuations: 0.28\n",
      "step: 47590 loss: 0.941172 time elapsed: 62.9926 learning rate: 0.000856, scenario: 0, slope: -0.008651863178013138, fluctuations: 0.24\n",
      "step: 47600 loss: 0.934110 time elapsed: 63.0059 learning rate: 0.000856, scenario: 0, slope: -0.004147197939002108, fluctuations: 0.21\n",
      "step: 47610 loss: 0.927903 time elapsed: 63.0205 learning rate: 0.000856, scenario: 0, slope: -0.0018437425806931702, fluctuations: 0.18\n",
      "step: 47620 loss: 0.922422 time elapsed: 63.0340 learning rate: 0.000856, scenario: 0, slope: -0.0013970507703703913, fluctuations: 0.14\n",
      "step: 47630 loss: 0.917531 time elapsed: 63.0467 learning rate: 0.000856, scenario: 0, slope: -0.0010055573526194827, fluctuations: 0.11\n",
      "step: 47640 loss: 0.913126 time elapsed: 63.0627 learning rate: 0.000856, scenario: 0, slope: -0.0008186125907753363, fluctuations: 0.08\n",
      "step: 47650 loss: 0.909131 time elapsed: 63.0783 learning rate: 0.000856, scenario: 0, slope: -0.0007046334864974714, fluctuations: 0.04\n",
      "step: 47660 loss: 0.905481 time elapsed: 63.0935 learning rate: 0.000856, scenario: 0, slope: -0.0006152362403700756, fluctuations: 0.01\n",
      "step: 47670 loss: 0.902127 time elapsed: 63.1080 learning rate: 0.000856, scenario: 0, slope: -0.0005448314796417023, fluctuations: 0.0\n",
      "step: 47680 loss: 0.899029 time elapsed: 63.1225 learning rate: 0.000856, scenario: 0, slope: -0.0004843571495844635, fluctuations: 0.0\n",
      "step: 47690 loss: 0.896152 time elapsed: 63.1362 learning rate: 0.000856, scenario: 0, slope: -0.00043510139892325843, fluctuations: 0.0\n",
      "step: 47700 loss: 0.893469 time elapsed: 63.1496 learning rate: 0.000856, scenario: 0, slope: -0.00039790699951468283, fluctuations: 0.0\n",
      "step: 47710 loss: 0.890956 time elapsed: 63.1633 learning rate: 0.000856, scenario: 0, slope: -0.0003596075970434192, fluctuations: 0.0\n",
      "step: 47720 loss: 0.888595 time elapsed: 63.1768 learning rate: 0.000856, scenario: 0, slope: -0.00033022479775596605, fluctuations: 0.0\n",
      "step: 47730 loss: 0.886369 time elapsed: 63.1913 learning rate: 0.000856, scenario: 0, slope: -0.0003049705312958888, fluctuations: 0.0\n",
      "step: 47740 loss: 0.884265 time elapsed: 63.2051 learning rate: 0.000856, scenario: 0, slope: -0.0002830833699560374, fluctuations: 0.0\n",
      "step: 47750 loss: 0.882268 time elapsed: 63.2185 learning rate: 0.000856, scenario: 0, slope: -0.0002639674047241793, fluctuations: 0.0\n",
      "step: 47760 loss: 0.880371 time elapsed: 63.2315 learning rate: 0.000856, scenario: 0, slope: -0.00024715508777526263, fluctuations: 0.0\n",
      "step: 47770 loss: 0.878562 time elapsed: 63.2449 learning rate: 0.000856, scenario: 0, slope: -0.00023227405434586776, fluctuations: 0.0\n",
      "step: 47780 loss: 0.876835 time elapsed: 63.2584 learning rate: 0.000856, scenario: 0, slope: -0.00021902565146645074, fluctuations: 0.0\n",
      "step: 47790 loss: 0.875183 time elapsed: 63.2734 learning rate: 0.000856, scenario: 0, slope: -0.000207167976911908, fluctuations: 0.0\n",
      "step: 47800 loss: 0.873598 time elapsed: 63.2886 learning rate: 0.000856, scenario: 0, slope: -0.00019752113785168236, fluctuations: 0.0\n",
      "step: 47810 loss: 0.872076 time elapsed: 63.3039 learning rate: 0.000856, scenario: 0, slope: -0.0001868696357660303, fluctuations: 0.0\n",
      "step: 47820 loss: 0.870611 time elapsed: 63.3186 learning rate: 0.000856, scenario: 0, slope: -0.00017813141017066563, fluctuations: 0.0\n",
      "step: 47830 loss: 0.869200 time elapsed: 63.3336 learning rate: 0.000856, scenario: 0, slope: -0.00017017604772838772, fluctuations: 0.0\n",
      "step: 47840 loss: 0.867838 time elapsed: 63.3469 learning rate: 0.000856, scenario: 0, slope: -0.00016290861496088337, fluctuations: 0.0\n",
      "step: 47850 loss: 0.866523 time elapsed: 63.3609 learning rate: 0.000856, scenario: 0, slope: -0.0001562486883202342, fluctuations: 0.0\n",
      "step: 47860 loss: 0.865249 time elapsed: 63.3738 learning rate: 0.000856, scenario: 0, slope: -0.0001501277256442101, fluctuations: 0.0\n",
      "step: 47870 loss: 0.864015 time elapsed: 63.3865 learning rate: 0.000856, scenario: 0, slope: -0.00014448698470562516, fluctuations: 0.0\n",
      "step: 47880 loss: 0.862819 time elapsed: 63.3990 learning rate: 0.000856, scenario: 0, slope: -0.00013927586183536125, fluctuations: 0.0\n",
      "step: 47890 loss: 0.861657 time elapsed: 63.4114 learning rate: 0.000856, scenario: 0, slope: -0.0001344505553852529, fluctuations: 0.0\n",
      "step: 47900 loss: 0.860527 time elapsed: 63.4242 learning rate: 0.000856, scenario: 0, slope: -0.00013040607350014722, fluctuations: 0.0\n",
      "step: 47910 loss: 0.859427 time elapsed: 63.4375 learning rate: 0.000856, scenario: 0, slope: -0.00012580989275784454, fluctuations: 0.0\n",
      "step: 47920 loss: 0.858356 time elapsed: 63.4499 learning rate: 0.000856, scenario: 0, slope: -0.00012193214326096186, fluctuations: 0.0\n",
      "step: 47930 loss: 0.857312 time elapsed: 63.4623 learning rate: 0.000856, scenario: 0, slope: -0.00011831409327601737, fluctuations: 0.0\n",
      "step: 47940 loss: 0.856293 time elapsed: 63.4747 learning rate: 0.000856, scenario: 0, slope: -0.00011493310393319913, fluctuations: 0.0\n",
      "step: 47950 loss: 0.855297 time elapsed: 63.4893 learning rate: 0.000856, scenario: 0, slope: -0.00011176911780688378, fluctuations: 0.0\n",
      "step: 47960 loss: 0.854324 time elapsed: 63.5037 learning rate: 0.000856, scenario: 0, slope: -0.00010880430538435493, fluctuations: 0.0\n",
      "step: 47970 loss: 0.853372 time elapsed: 63.5185 learning rate: 0.000856, scenario: 0, slope: -0.00010602276626050983, fluctuations: 0.0\n",
      "step: 47980 loss: 0.852439 time elapsed: 63.5331 learning rate: 0.000856, scenario: 0, slope: -0.00010341027558785484, fluctuations: 0.0\n",
      "step: 47990 loss: 0.851525 time elapsed: 63.5476 learning rate: 0.000856, scenario: 0, slope: -0.00010095406816474667, fluctuations: 0.0\n",
      "step: 48000 loss: 0.850629 time elapsed: 63.5613 learning rate: 0.000856, scenario: 0, slope: -9.886758344330125e-05, fluctuations: 0.0\n",
      "step: 48010 loss: 0.849749 time elapsed: 63.5757 learning rate: 0.000856, scenario: 0, slope: -9.646566024370923e-05, fluctuations: 0.0\n",
      "step: 48020 loss: 0.848886 time elapsed: 63.5890 learning rate: 0.000856, scenario: 0, slope: -9.441369555397388e-05, fluctuations: 0.0\n",
      "step: 48030 loss: 0.848037 time elapsed: 63.6029 learning rate: 0.000856, scenario: 0, slope: -9.247823311945156e-05, fluctuations: 0.0\n",
      "step: 48040 loss: 0.847202 time elapsed: 63.6166 learning rate: 0.000856, scenario: 0, slope: -9.065150984217409e-05, fluctuations: 0.0\n",
      "step: 48050 loss: 0.846380 time elapsed: 63.6298 learning rate: 0.000856, scenario: 0, slope: -8.892643908530277e-05, fluctuations: 0.0\n",
      "step: 48060 loss: 0.845571 time elapsed: 63.6431 learning rate: 0.000856, scenario: 0, slope: -8.72965350569391e-05, fluctuations: 0.0\n",
      "step: 48070 loss: 0.844775 time elapsed: 63.6571 learning rate: 0.000856, scenario: 0, slope: -8.575584713498856e-05, fluctuations: 0.0\n",
      "step: 48080 loss: 0.843977 time elapsed: 63.6709 learning rate: 0.000918, scenario: 1, slope: -8.431498603751422e-05, fluctuations: 0.0\n",
      "step: 48090 loss: 0.843117 time elapsed: 63.6843 learning rate: 0.001014, scenario: 1, slope: -8.322158250122469e-05, fluctuations: 0.0\n",
      "step: 48100 loss: 0.842182 time elapsed: 63.6988 learning rate: 0.001109, scenario: 1, slope: -8.28531715104763e-05, fluctuations: 0.0\n",
      "step: 48110 loss: 0.841175 time elapsed: 63.7142 learning rate: 0.001225, scenario: 1, slope: -8.351778778449559e-05, fluctuations: 0.0\n",
      "step: 48120 loss: 0.840085 time elapsed: 63.7289 learning rate: 0.001313, scenario: 0, slope: -8.540588194901649e-05, fluctuations: 0.0\n",
      "step: 48130 loss: 0.838976 time elapsed: 63.7437 learning rate: 0.001313, scenario: 0, slope: -8.850350295245147e-05, fluctuations: 0.0\n",
      "step: 48140 loss: 0.837887 time elapsed: 63.7599 learning rate: 0.001313, scenario: 0, slope: -9.234590790377971e-05, fluctuations: 0.0\n",
      "step: 48150 loss: 0.836817 time elapsed: 63.7737 learning rate: 0.001313, scenario: 0, slope: -9.642927866647348e-05, fluctuations: 0.0\n",
      "step: 48160 loss: 0.835765 time elapsed: 63.7881 learning rate: 0.001313, scenario: 0, slope: -0.00010026896907741672, fluctuations: 0.0\n",
      "step: 48170 loss: 0.834730 time elapsed: 63.8027 learning rate: 0.001313, scenario: 0, slope: -0.0001033984278613213, fluctuations: 0.0\n",
      "step: 48180 loss: 0.833709 time elapsed: 63.8162 learning rate: 0.001313, scenario: 0, slope: -0.00010538483783027549, fluctuations: 0.0\n",
      "step: 48190 loss: 0.832703 time elapsed: 63.8292 learning rate: 0.001313, scenario: 0, slope: -0.00010609526964145568, fluctuations: 0.0\n",
      "step: 48200 loss: 0.831709 time elapsed: 63.8428 learning rate: 0.001313, scenario: 0, slope: -0.00010579154428876783, fluctuations: 0.0\n",
      "step: 48210 loss: 0.830727 time elapsed: 63.8578 learning rate: 0.001313, scenario: 0, slope: -0.00010453871156190962, fluctuations: 0.0\n",
      "step: 48220 loss: 0.829757 time elapsed: 63.8713 learning rate: 0.001313, scenario: 0, slope: -0.0001030093187342646, fluctuations: 0.0\n",
      "step: 48230 loss: 0.828797 time elapsed: 63.8851 learning rate: 0.001313, scenario: 0, slope: -0.00010154005468211223, fluctuations: 0.0\n",
      "step: 48240 loss: 0.827846 time elapsed: 63.8984 learning rate: 0.001313, scenario: 0, slope: -0.00010017849490004148, fluctuations: 0.0\n",
      "step: 48250 loss: 0.826904 time elapsed: 63.9138 learning rate: 0.001313, scenario: 0, slope: -9.891710213115743e-05, fluctuations: 0.0\n",
      "step: 48260 loss: 0.825971 time elapsed: 63.9294 learning rate: 0.001313, scenario: 0, slope: -9.774827378716095e-05, fluctuations: 0.0\n",
      "step: 48270 loss: 0.825045 time elapsed: 63.9445 learning rate: 0.001313, scenario: 0, slope: -9.666483366960261e-05, fluctuations: 0.0\n",
      "step: 48280 loss: 0.824126 time elapsed: 63.9596 learning rate: 0.001313, scenario: 0, slope: -9.566015848271098e-05, fluctuations: 0.0\n",
      "step: 48290 loss: 0.823214 time elapsed: 63.9770 learning rate: 0.001313, scenario: 0, slope: -9.472818176834941e-05, fluctuations: 0.0\n",
      "step: 48300 loss: 0.822308 time elapsed: 63.9910 learning rate: 0.001313, scenario: 0, slope: -9.394696772581521e-05, fluctuations: 0.0\n",
      "step: 48310 loss: 0.821408 time elapsed: 64.0054 learning rate: 0.001313, scenario: 0, slope: -9.306062654502033e-05, fluctuations: 0.0\n",
      "step: 48320 loss: 0.820513 time elapsed: 64.0193 learning rate: 0.001313, scenario: 0, slope: -9.23153526208554e-05, fluctuations: 0.0\n",
      "step: 48330 loss: 0.819623 time elapsed: 64.0329 learning rate: 0.001313, scenario: 0, slope: -9.162330225273548e-05, fluctuations: 0.0\n",
      "step: 48340 loss: 0.818737 time elapsed: 64.0465 learning rate: 0.001313, scenario: 0, slope: -9.098059777173006e-05, fluctuations: 0.0\n",
      "step: 48350 loss: 0.817856 time elapsed: 64.0604 learning rate: 0.001313, scenario: 0, slope: -9.038368621585143e-05, fluctuations: 0.0\n",
      "step: 48360 loss: 0.816979 time elapsed: 64.0742 learning rate: 0.001313, scenario: 0, slope: -8.982930950675174e-05, fluctuations: 0.0\n",
      "step: 48370 loss: 0.816105 time elapsed: 64.0884 learning rate: 0.001313, scenario: 0, slope: -8.931447760462597e-05, fluctuations: 0.0\n",
      "step: 48380 loss: 0.815235 time elapsed: 64.1029 learning rate: 0.001313, scenario: 0, slope: -8.883644431914937e-05, fluctuations: 0.0\n",
      "step: 48390 loss: 0.814367 time elapsed: 64.1170 learning rate: 0.001313, scenario: 0, slope: -8.839268548613967e-05, fluctuations: 0.0\n",
      "step: 48400 loss: 0.813503 time elapsed: 64.1317 learning rate: 0.001313, scenario: 0, slope: -8.802068429703107e-05, fluctuations: 0.0\n",
      "step: 48410 loss: 0.812641 time elapsed: 64.1470 learning rate: 0.001313, scenario: 0, slope: -8.759888827166321e-05, fluctuations: 0.0\n",
      "step: 48420 loss: 0.811782 time elapsed: 64.1617 learning rate: 0.001313, scenario: 0, slope: -8.724474353084032e-05, fluctuations: 0.0\n",
      "step: 48430 loss: 0.810925 time elapsed: 64.1765 learning rate: 0.001313, scenario: 0, slope: -8.691662977257093e-05, fluctuations: 0.0\n",
      "step: 48440 loss: 0.810071 time elapsed: 64.1922 learning rate: 0.001313, scenario: 0, slope: -8.661287222038126e-05, fluctuations: 0.0\n",
      "step: 48450 loss: 0.809218 time elapsed: 64.2085 learning rate: 0.001313, scenario: 0, slope: -8.633192455325103e-05, fluctuations: 0.0\n",
      "step: 48460 loss: 0.808367 time elapsed: 64.2225 learning rate: 0.001313, scenario: 0, slope: -8.607235798104166e-05, fluctuations: 0.0\n",
      "step: 48470 loss: 0.807517 time elapsed: 64.2378 learning rate: 0.001313, scenario: 0, slope: -8.583285131563641e-05, fluctuations: 0.0\n",
      "step: 48480 loss: 0.806669 time elapsed: 64.2520 learning rate: 0.001313, scenario: 0, slope: -8.561218194138834e-05, fluctuations: 0.0\n",
      "step: 48490 loss: 0.805823 time elapsed: 64.2651 learning rate: 0.001313, scenario: 0, slope: -8.540921759816841e-05, fluctuations: 0.0\n",
      "step: 48500 loss: 0.804977 time elapsed: 64.2780 learning rate: 0.001313, scenario: 0, slope: -8.5240818572155e-05, fluctuations: 0.0\n",
      "step: 48510 loss: 0.804133 time elapsed: 64.2924 learning rate: 0.001313, scenario: 0, slope: -8.505228251498032e-05, fluctuations: 0.0\n",
      "step: 48520 loss: 0.803290 time elapsed: 64.3057 learning rate: 0.001313, scenario: 0, slope: -8.489643495691088e-05, fluctuations: 0.0\n",
      "step: 48530 loss: 0.802447 time elapsed: 64.3190 learning rate: 0.001313, scenario: 0, slope: -8.47545269110818e-05, fluctuations: 0.0\n",
      "step: 48540 loss: 0.801605 time elapsed: 64.3340 learning rate: 0.001313, scenario: 0, slope: -8.462577806411444e-05, fluctuations: 0.0\n",
      "step: 48550 loss: 0.800764 time elapsed: 64.3488 learning rate: 0.001313, scenario: 0, slope: -8.45094623796756e-05, fluctuations: 0.0\n",
      "step: 48560 loss: 0.799924 time elapsed: 64.3635 learning rate: 0.001313, scenario: 0, slope: -8.440490378320795e-05, fluctuations: 0.0\n",
      "step: 48570 loss: 0.799084 time elapsed: 64.3783 learning rate: 0.001313, scenario: 0, slope: -8.431147221742391e-05, fluctuations: 0.0\n",
      "step: 48580 loss: 0.798244 time elapsed: 64.3929 learning rate: 0.001313, scenario: 0, slope: -8.422858003263089e-05, fluctuations: 0.0\n",
      "step: 48590 loss: 0.797405 time elapsed: 64.4066 learning rate: 0.001313, scenario: 0, slope: -8.415567745385274e-05, fluctuations: 0.0\n",
      "step: 48600 loss: 0.796567 time elapsed: 64.4203 learning rate: 0.001313, scenario: 0, slope: -8.409751186063517e-05, fluctuations: 0.0\n",
      "step: 48610 loss: 0.798104 time elapsed: 64.4350 learning rate: 0.001313, scenario: 0, slope: -8.14635992728947e-05, fluctuations: 0.0\n",
      "step: 48620 loss: 24.060366 time elapsed: 64.4480 learning rate: 0.001293, scenario: -1, slope: 0.023254983206239192, fluctuations: 0.0\n",
      "step: 48630 loss: 3.748216 time elapsed: 64.4609 learning rate: 0.001170, scenario: -1, slope: 0.08890172342892365, fluctuations: 0.03\n",
      "step: 48640 loss: 4.032477 time elapsed: 64.4738 learning rate: 0.001058, scenario: -1, slope: 0.0885182720250699, fluctuations: 0.06\n",
      "step: 48650 loss: 1.134075 time elapsed: 64.4864 learning rate: 0.000957, scenario: -1, slope: 0.0746051184789295, fluctuations: 0.1\n",
      "step: 48660 loss: 0.977506 time elapsed: 64.4988 learning rate: 0.000865, scenario: -1, slope: 0.053840640057111144, fluctuations: 0.14\n",
      "step: 48670 loss: 0.886532 time elapsed: 64.5115 learning rate: 0.000782, scenario: -1, slope: 0.028692192812673076, fluctuations: 0.18\n",
      "step: 48680 loss: 0.829072 time elapsed: 64.5250 learning rate: 0.000708, scenario: -1, slope: 0.002759579515292869, fluctuations: 0.23\n",
      "step: 48690 loss: 0.799421 time elapsed: 64.5398 learning rate: 0.000701, scenario: 0, slope: -0.02690764708884728, fluctuations: 0.28\n",
      "step: 48700 loss: 0.796283 time elapsed: 64.5546 learning rate: 0.000701, scenario: 0, slope: -0.05717197379731483, fluctuations: 0.32\n",
      "step: 48710 loss: 0.793768 time elapsed: 64.5696 learning rate: 0.000701, scenario: 0, slope: -0.10125988396856327, fluctuations: 0.37\n",
      "step: 48720 loss: 0.792125 time elapsed: 64.5838 learning rate: 0.000701, scenario: 0, slope: -0.11820473537278167, fluctuations: 0.41\n",
      "step: 48730 loss: 0.791614 time elapsed: 64.5976 learning rate: 0.000701, scenario: 0, slope: -0.02943722385089492, fluctuations: 0.4\n",
      "step: 48740 loss: 0.791029 time elapsed: 64.6118 learning rate: 0.000701, scenario: 0, slope: -0.007537186710078357, fluctuations: 0.42\n",
      "step: 48750 loss: 0.790541 time elapsed: 64.6248 learning rate: 0.000701, scenario: 0, slope: -0.0015958649584281925, fluctuations: 0.39\n",
      "step: 48760 loss: 0.790094 time elapsed: 64.6379 learning rate: 0.000701, scenario: 0, slope: -0.0007149938126437117, fluctuations: 0.34\n",
      "step: 48770 loss: 0.789647 time elapsed: 64.6505 learning rate: 0.000701, scenario: 0, slope: -0.00015275361846221028, fluctuations: 0.3\n",
      "step: 48780 loss: 0.789204 time elapsed: 64.6631 learning rate: 0.000701, scenario: 0, slope: -8.720930941047661e-05, fluctuations: 0.25\n",
      "step: 48790 loss: 0.788767 time elapsed: 64.6757 learning rate: 0.000666, scenario: -1, slope: -6.729079377814517e-05, fluctuations: 0.2\n",
      "step: 48800 loss: 0.788363 time elapsed: 64.6887 learning rate: 0.000609, scenario: -1, slope: -5.090587989658661e-05, fluctuations: 0.16\n",
      "step: 48810 loss: 0.787994 time elapsed: 64.7020 learning rate: 0.000550, scenario: -1, slope: -4.640199345383924e-05, fluctuations: 0.11\n",
      "step: 48820 loss: 0.787661 time elapsed: 64.7148 learning rate: 0.000498, scenario: -1, slope: -4.384592825054483e-05, fluctuations: 0.07\n",
      "step: 48830 loss: 0.787358 time elapsed: 64.7277 learning rate: 0.000450, scenario: -1, slope: -4.162304009028634e-05, fluctuations: 0.04\n",
      "step: 48840 loss: 0.787083 time elapsed: 64.7417 learning rate: 0.000439, scenario: 1, slope: -3.9929160496978016e-05, fluctuations: 0.0\n",
      "step: 48850 loss: 0.786797 time elapsed: 64.7560 learning rate: 0.000485, scenario: 1, slope: -3.7690839430641506e-05, fluctuations: 0.0\n",
      "step: 48860 loss: 0.786479 time elapsed: 64.7701 learning rate: 0.000536, scenario: 1, slope: -3.552652883549264e-05, fluctuations: 0.0\n",
      "step: 48870 loss: 0.786128 time elapsed: 64.7837 learning rate: 0.000592, scenario: 1, slope: -3.3733353061138726e-05, fluctuations: 0.0\n",
      "step: 48880 loss: 0.785739 time elapsed: 64.7976 learning rate: 0.000653, scenario: 1, slope: -3.260629210523788e-05, fluctuations: 0.0\n",
      "step: 48890 loss: 0.785309 time elapsed: 64.8113 learning rate: 0.000722, scenario: 1, slope: -3.242855196574887e-05, fluctuations: 0.0\n",
      "step: 48900 loss: 0.784833 time elapsed: 64.8242 learning rate: 0.000789, scenario: 1, slope: -3.320804814585805e-05, fluctuations: 0.0\n",
      "step: 48910 loss: 0.784312 time elapsed: 64.8377 learning rate: 0.000872, scenario: 1, slope: -3.532679357329756e-05, fluctuations: 0.0\n",
      "step: 48920 loss: 0.783737 time elapsed: 64.8502 learning rate: 0.000963, scenario: 1, slope: -3.826367048010879e-05, fluctuations: 0.0\n",
      "step: 48930 loss: 0.783100 time elapsed: 64.8625 learning rate: 0.001064, scenario: 1, slope: -4.20128994187033e-05, fluctuations: 0.0\n",
      "step: 48940 loss: 0.782397 time elapsed: 64.8750 learning rate: 0.001175, scenario: 1, slope: -4.6373583358096697e-05, fluctuations: 0.0\n",
      "step: 48950 loss: 0.781620 time elapsed: 64.8873 learning rate: 0.001298, scenario: 1, slope: -5.120021263071001e-05, fluctuations: 0.0\n",
      "step: 48960 loss: 0.780763 time elapsed: 64.8996 learning rate: 0.001434, scenario: 1, slope: -5.6523932862756566e-05, fluctuations: 0.0\n",
      "step: 48970 loss: 0.779866 time elapsed: 64.9125 learning rate: 0.001584, scenario: 1, slope: -6.236432488287608e-05, fluctuations: 0.0\n",
      "step: 48980 loss: 923.854310 time elapsed: 64.9266 learning rate: 0.001529, scenario: -1, slope: 0.8273098471198274, fluctuations: 0.01\n",
      "step: 48990 loss: 314.465784 time elapsed: 64.9408 learning rate: 0.001383, scenario: -1, slope: 1.169789394265346, fluctuations: 0.05\n",
      "step: 49000 loss: 81.294984 time elapsed: 64.9550 learning rate: 0.001263, scenario: -1, slope: 1.1430678474601261, fluctuations: 0.09\n",
      "step: 49010 loss: 27.753728 time elapsed: 64.9695 learning rate: 0.001143, scenario: -1, slope: 0.9785954527271211, fluctuations: 0.13\n",
      "step: 49020 loss: 9.598216 time elapsed: 64.9835 learning rate: 0.001033, scenario: -1, slope: 0.6971536402035008, fluctuations: 0.17\n",
      "step: 49030 loss: 2.019838 time elapsed: 64.9982 learning rate: 0.000935, scenario: -1, slope: 0.37454956764046626, fluctuations: 0.21\n",
      "step: 49040 loss: 1.435773 time elapsed: 65.0114 learning rate: 0.000845, scenario: -1, slope: 0.0020922155707005074, fluctuations: 0.24\n",
      "step: 49050 loss: 1.383401 time elapsed: 65.0245 learning rate: 0.000845, scenario: 0, slope: -0.37598981161371897, fluctuations: 0.27\n",
      "step: 49060 loss: 1.147496 time elapsed: 65.0369 learning rate: 0.000845, scenario: 0, slope: -0.7957691044280109, fluctuations: 0.3\n",
      "step: 49070 loss: 1.024866 time elapsed: 65.0493 learning rate: 0.000845, scenario: 0, slope: -1.3601355047939363, fluctuations: 0.34\n",
      "step: 49080 loss: 0.994822 time elapsed: 65.0617 learning rate: 0.000845, scenario: 0, slope: -0.6892358989669393, fluctuations: 0.36\n",
      "step: 49090 loss: 0.979313 time elapsed: 65.0743 learning rate: 0.000845, scenario: 0, slope: -0.4875063198697342, fluctuations: 0.34\n",
      "step: 49100 loss: 0.963046 time elapsed: 65.0865 learning rate: 0.000845, scenario: 0, slope: -0.21664978723054784, fluctuations: 0.33\n",
      "step: 49110 loss: 0.950651 time elapsed: 65.1000 learning rate: 0.000845, scenario: 0, slope: -0.04900643726878895, fluctuations: 0.31\n",
      "step: 49120 loss: 0.941265 time elapsed: 65.1125 learning rate: 0.000845, scenario: 0, slope: -0.019219279709532567, fluctuations: 0.27\n",
      "step: 49130 loss: 0.933008 time elapsed: 65.1248 learning rate: 0.000845, scenario: 0, slope: -0.006634310008758326, fluctuations: 0.24\n",
      "step: 49140 loss: 0.925589 time elapsed: 65.1372 learning rate: 0.000845, scenario: 0, slope: -0.003997939362619418, fluctuations: 0.2\n",
      "step: 49150 loss: 0.918953 time elapsed: 65.1511 learning rate: 0.000845, scenario: 0, slope: -0.0020919136896274788, fluctuations: 0.17\n",
      "step: 49160 loss: 0.912931 time elapsed: 65.1653 learning rate: 0.000845, scenario: 0, slope: -0.0013133066953291439, fluctuations: 0.14\n",
      "step: 49170 loss: 0.907422 time elapsed: 65.1793 learning rate: 0.000845, scenario: 0, slope: -0.00099730701962697, fluctuations: 0.11\n",
      "step: 49180 loss: 0.902360 time elapsed: 65.1931 learning rate: 0.000845, scenario: 0, slope: -0.0008430628842839999, fluctuations: 0.07\n",
      "step: 49190 loss: 0.897684 time elapsed: 65.2070 learning rate: 0.000845, scenario: 0, slope: -0.0007292072576991794, fluctuations: 0.04\n",
      "step: 49200 loss: 0.893349 time elapsed: 65.2205 learning rate: 0.000845, scenario: 0, slope: -0.0006639543373557448, fluctuations: 0.01\n",
      "step: 49210 loss: 0.889313 time elapsed: 65.2336 learning rate: 0.000845, scenario: 0, slope: -0.0005954311665624582, fluctuations: 0.0\n",
      "step: 49220 loss: 0.885545 time elapsed: 65.2463 learning rate: 0.000845, scenario: 0, slope: -0.0005424470743187711, fluctuations: 0.0\n",
      "step: 49230 loss: 0.882017 time elapsed: 65.2586 learning rate: 0.000845, scenario: 0, slope: -0.0004977156172170106, fluctuations: 0.0\n",
      "step: 49240 loss: 0.878703 time elapsed: 65.2710 learning rate: 0.000845, scenario: 0, slope: -0.00045924506031293314, fluctuations: 0.0\n",
      "step: 49250 loss: 0.875584 time elapsed: 65.2833 learning rate: 0.000845, scenario: 0, slope: -0.0004256718082295575, fluctuations: 0.0\n",
      "step: 49260 loss: 0.872641 time elapsed: 65.2958 learning rate: 0.000845, scenario: 0, slope: -0.00039608603568163584, fluctuations: 0.0\n",
      "step: 49270 loss: 0.869858 time elapsed: 65.3082 learning rate: 0.000845, scenario: 0, slope: -0.0003698201363078104, fluctuations: 0.0\n",
      "step: 49280 loss: 0.867220 time elapsed: 65.3204 learning rate: 0.000845, scenario: 0, slope: -0.00034635543553318196, fluctuations: 0.0\n",
      "step: 49290 loss: 0.864716 time elapsed: 65.3328 learning rate: 0.000845, scenario: 0, slope: -0.00032528451126220115, fluctuations: 0.0\n",
      "step: 49300 loss: 0.862334 time elapsed: 65.3450 learning rate: 0.000845, scenario: 0, slope: -0.0003080949080736235, fluctuations: 0.0\n",
      "step: 49310 loss: 0.860065 time elapsed: 65.3584 learning rate: 0.000845, scenario: 0, slope: -0.00028907059110101524, fluctuations: 0.0\n",
      "step: 49320 loss: 0.857898 time elapsed: 65.3724 learning rate: 0.000845, scenario: 0, slope: -0.00027343505184935655, fluctuations: 0.0\n",
      "step: 49330 loss: 0.855827 time elapsed: 65.3864 learning rate: 0.000845, scenario: 0, slope: -0.00025918507228368044, fluctuations: 0.0\n",
      "step: 49340 loss: 0.853844 time elapsed: 65.4003 learning rate: 0.000845, scenario: 0, slope: -0.0002461616171364579, fluctuations: 0.0\n",
      "step: 49350 loss: 0.851943 time elapsed: 65.4138 learning rate: 0.000845, scenario: 0, slope: -0.00023422876067857195, fluctuations: 0.0\n",
      "step: 49360 loss: 0.850117 time elapsed: 65.4281 learning rate: 0.000845, scenario: 0, slope: -0.00022326948774985218, fluctuations: 0.0\n",
      "step: 49370 loss: 0.848361 time elapsed: 65.4408 learning rate: 0.000845, scenario: 0, slope: -0.00021318243354384776, fluctuations: 0.0\n",
      "step: 49380 loss: 0.846670 time elapsed: 65.4537 learning rate: 0.000845, scenario: 0, slope: -0.00020387930618889725, fluctuations: 0.0\n",
      "step: 49390 loss: 0.845040 time elapsed: 65.4661 learning rate: 0.000845, scenario: 0, slope: -0.00019528282643598883, fluctuations: 0.0\n",
      "step: 49400 loss: 0.843467 time elapsed: 65.4782 learning rate: 0.000845, scenario: 0, slope: -0.0001880938614223244, fluctuations: 0.0\n",
      "step: 49410 loss: 0.841947 time elapsed: 65.4913 learning rate: 0.000845, scenario: 0, slope: -0.0001799460629762955, fluctuations: 0.0\n",
      "step: 49420 loss: 0.840476 time elapsed: 65.5042 learning rate: 0.000845, scenario: 0, slope: -0.00017309274448499548, fluctuations: 0.0\n",
      "step: 49430 loss: 0.839052 time elapsed: 65.5166 learning rate: 0.000845, scenario: 0, slope: -0.0001667179463601392, fluctuations: 0.0\n",
      "step: 49440 loss: 0.837671 time elapsed: 65.5291 learning rate: 0.000845, scenario: 0, slope: -0.00016077965261107052, fluctuations: 0.0\n",
      "step: 49450 loss: 0.836331 time elapsed: 65.5414 learning rate: 0.000845, scenario: 0, slope: -0.0001552403294543519, fluctuations: 0.0\n",
      "step: 49460 loss: 0.835029 time elapsed: 65.5538 learning rate: 0.000845, scenario: 0, slope: -0.00015006636351126544, fluctuations: 0.0\n",
      "step: 49470 loss: 0.833763 time elapsed: 65.5673 learning rate: 0.000845, scenario: 0, slope: -0.00014522758193091282, fluctuations: 0.0\n",
      "step: 49480 loss: 0.832531 time elapsed: 65.5812 learning rate: 0.000845, scenario: 0, slope: -0.00014069684044223512, fluctuations: 0.0\n",
      "step: 49490 loss: 0.831331 time elapsed: 65.5949 learning rate: 0.000845, scenario: 0, slope: -0.00013644966813355293, fluctuations: 0.0\n",
      "step: 49500 loss: 0.830161 time elapsed: 65.6083 learning rate: 0.000845, scenario: 0, slope: -0.00013285135037663796, fluctuations: 0.0\n",
      "step: 49510 loss: 0.829020 time elapsed: 65.6232 learning rate: 0.000845, scenario: 0, slope: -0.00012871970929789612, fluctuations: 0.0\n",
      "step: 49520 loss: 0.827905 time elapsed: 65.6373 learning rate: 0.000845, scenario: 0, slope: -0.00012519877545609005, fluctuations: 0.0\n",
      "step: 49530 loss: 0.826816 time elapsed: 65.6502 learning rate: 0.000845, scenario: 0, slope: -0.00012188467957084003, fluctuations: 0.0\n",
      "step: 49540 loss: 0.825750 time elapsed: 65.6626 learning rate: 0.000845, scenario: 0, slope: -0.00011876242631399775, fluctuations: 0.0\n",
      "step: 49550 loss: 0.824707 time elapsed: 65.6749 learning rate: 0.000845, scenario: 0, slope: -0.00011581834699128173, fluctuations: 0.0\n",
      "step: 49560 loss: 0.823686 time elapsed: 65.6874 learning rate: 0.000845, scenario: 0, slope: -0.00011303996142103709, fluctuations: 0.0\n",
      "step: 49570 loss: 0.822685 time elapsed: 65.7000 learning rate: 0.000845, scenario: 0, slope: -0.00011041585607124501, fluctuations: 0.0\n",
      "step: 49580 loss: 0.821703 time elapsed: 65.7126 learning rate: 0.000845, scenario: 0, slope: -0.00010793557634598449, fluctuations: 0.0\n",
      "step: 49590 loss: 0.820740 time elapsed: 65.7252 learning rate: 0.000845, scenario: 0, slope: -0.00010558953122033712, fluctuations: 0.0\n",
      "step: 49600 loss: 0.819794 time elapsed: 65.7371 learning rate: 0.000845, scenario: 0, slope: -0.00010358556319485851, fluctuations: 0.0\n",
      "step: 49610 loss: 0.818865 time elapsed: 65.7498 learning rate: 0.000845, scenario: 0, slope: -0.00010126560058849591, fluctuations: 0.0\n",
      "step: 49620 loss: 0.817951 time elapsed: 65.7622 learning rate: 0.000845, scenario: 0, slope: -9.927213595235912e-05, fluctuations: 0.0\n",
      "step: 49630 loss: 0.817052 time elapsed: 65.7745 learning rate: 0.000845, scenario: 0, slope: -9.738162137074012e-05, fluctuations: 0.0\n",
      "step: 49640 loss: 0.816168 time elapsed: 65.7886 learning rate: 0.000845, scenario: 0, slope: -9.558768800397225e-05, fluctuations: 0.0\n",
      "step: 49650 loss: 0.815296 time elapsed: 65.8022 learning rate: 0.000845, scenario: 0, slope: -9.38844441728928e-05, fluctuations: 0.0\n",
      "step: 49660 loss: 0.814438 time elapsed: 65.8158 learning rate: 0.000845, scenario: 0, slope: -9.226643296261777e-05, fluctuations: 0.0\n",
      "step: 49670 loss: 0.813592 time elapsed: 65.8298 learning rate: 0.000845, scenario: 0, slope: -9.072859423999258e-05, fluctuations: 0.0\n",
      "step: 49680 loss: 0.812758 time elapsed: 65.8443 learning rate: 0.000845, scenario: 0, slope: -8.926623057088198e-05, fluctuations: 0.0\n",
      "step: 49690 loss: 0.811935 time elapsed: 65.8575 learning rate: 0.000845, scenario: 0, slope: -8.787497658743372e-05, fluctuations: 0.0\n",
      "step: 49700 loss: 0.811122 time elapsed: 65.8703 learning rate: 0.000845, scenario: 0, slope: -8.668028458197869e-05, fluctuations: 0.0\n",
      "step: 49710 loss: 0.810320 time elapsed: 65.8833 learning rate: 0.000845, scenario: 0, slope: -8.528983376538311e-05, fluctuations: 0.0\n",
      "step: 49720 loss: 0.809527 time elapsed: 65.8956 learning rate: 0.000845, scenario: 0, slope: -8.408863952811303e-05, fluctuations: 0.0\n",
      "step: 49730 loss: 0.808743 time elapsed: 65.9078 learning rate: 0.000845, scenario: 0, slope: -8.294390136094652e-05, fluctuations: 0.0\n",
      "step: 49740 loss: 0.807969 time elapsed: 65.9203 learning rate: 0.000845, scenario: 0, slope: -8.185255027305885e-05, fluctuations: 0.0\n",
      "step: 49750 loss: 0.807186 time elapsed: 65.9326 learning rate: 0.000915, scenario: 1, slope: -8.083678980441735e-05, fluctuations: 0.0\n",
      "step: 49760 loss: 0.806336 time elapsed: 65.9449 learning rate: 0.001011, scenario: 1, slope: -8.017122228459777e-05, fluctuations: 0.0\n",
      "step: 49770 loss: 0.805409 time elapsed: 65.9573 learning rate: 0.001117, scenario: 1, slope: -8.023276236265841e-05, fluctuations: 0.0\n",
      "step: 49780 loss: 0.804398 time elapsed: 65.9695 learning rate: 0.001221, scenario: 0, slope: -8.133038203271435e-05, fluctuations: 0.0\n",
      "step: 49790 loss: 0.803346 time elapsed: 65.9835 learning rate: 0.001221, scenario: 0, slope: -8.359725867349814e-05, fluctuations: 0.0\n",
      "step: 49800 loss: 0.802310 time elapsed: 65.9978 learning rate: 0.001221, scenario: 0, slope: -8.637198565781678e-05, fluctuations: 0.0\n",
      "step: 49810 loss: 0.801288 time elapsed: 66.0121 learning rate: 0.001221, scenario: 0, slope: -9.024544685817493e-05, fluctuations: 0.0\n",
      "step: 49820 loss: 0.800280 time elapsed: 66.0258 learning rate: 0.001221, scenario: 0, slope: -9.379851221087303e-05, fluctuations: 0.0\n",
      "step: 49830 loss: 0.799285 time elapsed: 66.0395 learning rate: 0.001221, scenario: 0, slope: -9.697303046065061e-05, fluctuations: 0.0\n",
      "step: 49840 loss: 0.798302 time elapsed: 66.0534 learning rate: 0.001221, scenario: 0, slope: -9.938551833023421e-05, fluctuations: 0.0\n",
      "step: 49850 loss: 0.797331 time elapsed: 66.0658 learning rate: 0.001221, scenario: 0, slope: -0.00010069039261179901, fluctuations: 0.0\n",
      "step: 49860 loss: 0.796371 time elapsed: 66.0781 learning rate: 0.001221, scenario: 0, slope: -0.0001008623577070737, fluctuations: 0.0\n",
      "step: 49870 loss: 0.795421 time elapsed: 66.0907 learning rate: 0.001221, scenario: 0, slope: -0.00010015896822790612, fluctuations: 0.0\n",
      "step: 49880 loss: 0.794482 time elapsed: 66.1032 learning rate: 0.001221, scenario: 0, slope: -9.898363106174398e-05, fluctuations: 0.0\n",
      "step: 49890 loss: 0.793551 time elapsed: 66.1157 learning rate: 0.001221, scenario: 0, slope: -9.777992569674982e-05, fluctuations: 0.0\n",
      "step: 49900 loss: 0.792629 time elapsed: 66.1276 learning rate: 0.001221, scenario: 0, slope: -9.675455655924922e-05, fluctuations: 0.0\n",
      "step: 49910 loss: 0.791715 time elapsed: 66.1404 learning rate: 0.001221, scenario: 0, slope: -9.557217530071216e-05, fluctuations: 0.0\n",
      "step: 49920 loss: 0.790809 time elapsed: 66.1528 learning rate: 0.001221, scenario: 0, slope: -9.456088604315501e-05, fluctuations: 0.0\n",
      "step: 49930 loss: 0.789911 time elapsed: 66.1653 learning rate: 0.001221, scenario: 0, slope: -9.360627160932131e-05, fluctuations: 0.0\n",
      "step: 49940 loss: 0.789019 time elapsed: 66.1798 learning rate: 0.001221, scenario: 0, slope: -9.270477923670039e-05, fluctuations: 0.0\n",
      "step: 49950 loss: 0.788134 time elapsed: 66.1933 learning rate: 0.001221, scenario: 0, slope: -9.185310839069524e-05, fluctuations: 0.0\n",
      "step: 49960 loss: 0.787255 time elapsed: 66.2074 learning rate: 0.001221, scenario: 0, slope: -9.10482061768348e-05, fluctuations: 0.0\n",
      "step: 49970 loss: 0.786382 time elapsed: 66.2213 learning rate: 0.001221, scenario: 0, slope: -9.028725083592633e-05, fluctuations: 0.0\n",
      "step: 49980 loss: 0.785515 time elapsed: 66.2353 learning rate: 0.001221, scenario: 0, slope: -8.95676326562301e-05, fluctuations: 0.0\n",
      "step: 49990 loss: 0.784653 time elapsed: 66.2491 learning rate: 0.001221, scenario: 0, slope: -8.88869353882359e-05, fluctuations: 0.0\n",
      "step: 50000 loss: 0.783796 time elapsed: 66.2624 learning rate: 0.001221, scenario: 0, slope: -8.830573052070562e-05, fluctuations: 0.0\n",
      "step: 50010 loss: 0.782943 time elapsed: 66.2755 learning rate: 0.001221, scenario: 0, slope: -8.763350452724438e-05, fluctuations: 0.0\n",
      "step: 50020 loss: 0.782096 time elapsed: 66.2882 learning rate: 0.001221, scenario: 0, slope: -8.705675923238324e-05, fluctuations: 0.0\n",
      "step: 50030 loss: 0.781252 time elapsed: 66.3008 learning rate: 0.001221, scenario: 0, slope: -8.651088481325963e-05, fluctuations: 0.0\n",
      "step: 50040 loss: 0.780412 time elapsed: 66.3130 learning rate: 0.001221, scenario: 0, slope: -8.59942056443204e-05, fluctuations: 0.0\n",
      "step: 50050 loss: 0.779577 time elapsed: 66.3253 learning rate: 0.001221, scenario: 0, slope: -8.550515863772457e-05, fluctuations: 0.0\n",
      "step: 50060 loss: 0.778744 time elapsed: 66.3374 learning rate: 0.001221, scenario: 0, slope: -8.504228402693776e-05, fluctuations: 0.0\n",
      "step: 50070 loss: 0.777916 time elapsed: 66.3498 learning rate: 0.001221, scenario: 0, slope: -8.460421704434871e-05, fluctuations: 0.0\n",
      "step: 50080 loss: 0.777090 time elapsed: 66.3620 learning rate: 0.001221, scenario: 0, slope: -8.418968039531996e-05, fluctuations: 0.0\n",
      "step: 50090 loss: 0.776268 time elapsed: 66.3745 learning rate: 0.001221, scenario: 0, slope: -8.379747744299613e-05, fluctuations: 0.0\n",
      "step: 50100 loss: 0.775448 time elapsed: 66.3866 learning rate: 0.001221, scenario: 0, slope: -8.346266118453758e-05, fluctuations: 0.0\n",
      "step: 50110 loss: 0.774631 time elapsed: 66.3997 learning rate: 0.001221, scenario: 0, slope: -8.307565285031358e-05, fluctuations: 0.0\n",
      "step: 50120 loss: 0.773816 time elapsed: 66.4139 learning rate: 0.001221, scenario: 0, slope: -8.274398836878191e-05, fluctuations: 0.0\n",
      "step: 50130 loss: 0.773004 time elapsed: 66.4279 learning rate: 0.001221, scenario: 0, slope: -8.243056214143622e-05, fluctuations: 0.0\n",
      "step: 50140 loss: 0.772195 time elapsed: 66.4417 learning rate: 0.001221, scenario: 0, slope: -8.213449858334213e-05, fluctuations: 0.0\n",
      "step: 50150 loss: 0.771387 time elapsed: 66.4556 learning rate: 0.001221, scenario: 0, slope: -8.185497308649515e-05, fluctuations: 0.0\n",
      "step: 50160 loss: 0.770581 time elapsed: 66.4697 learning rate: 0.001221, scenario: 0, slope: -8.159120846944273e-05, fluctuations: 0.0\n",
      "step: 50170 loss: 0.769778 time elapsed: 66.4825 learning rate: 0.001221, scenario: 0, slope: -8.13424717238316e-05, fluctuations: 0.0\n",
      "step: 50180 loss: 0.768976 time elapsed: 66.4954 learning rate: 0.001221, scenario: 0, slope: -8.110807102808596e-05, fluctuations: 0.0\n",
      "step: 50190 loss: 0.768176 time elapsed: 66.5078 learning rate: 0.001221, scenario: 0, slope: -8.088735300262947e-05, fluctuations: 0.0\n",
      "step: 50200 loss: 0.767377 time elapsed: 66.5204 learning rate: 0.001221, scenario: 0, slope: -8.069989442538778e-05, fluctuations: 0.0\n",
      "step: 50210 loss: 0.766579 time elapsed: 66.5333 learning rate: 0.001221, scenario: 0, slope: -8.048452869703531e-05, fluctuations: 0.0\n",
      "step: 50220 loss: 0.765784 time elapsed: 66.5456 learning rate: 0.001221, scenario: 0, slope: -8.030128610493432e-05, fluctuations: 0.0\n",
      "step: 50230 loss: 0.764989 time elapsed: 66.5580 learning rate: 0.001221, scenario: 0, slope: -8.012944942543696e-05, fluctuations: 0.0\n",
      "step: 50240 loss: 0.764195 time elapsed: 66.5702 learning rate: 0.001221, scenario: 0, slope: -7.996852329286251e-05, fluctuations: 0.0\n",
      "step: 50250 loss: 0.763403 time elapsed: 66.5824 learning rate: 0.001221, scenario: 0, slope: -7.981803825675332e-05, fluctuations: 0.0\n",
      "step: 50260 loss: 0.762611 time elapsed: 66.5949 learning rate: 0.001221, scenario: 0, slope: -7.96775492031281e-05, fluctuations: 0.0\n",
      "step: 50270 loss: 0.761821 time elapsed: 66.6085 learning rate: 0.001221, scenario: 0, slope: -7.954663388923891e-05, fluctuations: 0.0\n",
      "step: 50280 loss: 0.761031 time elapsed: 66.6227 learning rate: 0.001221, scenario: 0, slope: -7.942489158102262e-05, fluctuations: 0.0\n",
      "step: 50290 loss: 0.760242 time elapsed: 66.6367 learning rate: 0.001221, scenario: 0, slope: -7.931194178515884e-05, fluctuations: 0.0\n",
      "step: 50300 loss: 0.759454 time elapsed: 66.6503 learning rate: 0.001221, scenario: 0, slope: -7.921750546647109e-05, fluctuations: 0.0\n",
      "step: 50310 loss: 0.758667 time elapsed: 66.6644 learning rate: 0.001221, scenario: 0, slope: -7.911099195289327e-05, fluctuations: 0.0\n",
      "step: 50320 loss: 0.757880 time elapsed: 66.6785 learning rate: 0.001221, scenario: 0, slope: -7.902232025593267e-05, fluctuations: 0.0\n",
      "step: 50330 loss: 0.757095 time elapsed: 66.6913 learning rate: 0.001221, scenario: 0, slope: -7.893928473426866e-05, fluctuations: 0.0\n",
      "step: 50340 loss: 0.759381 time elapsed: 66.7037 learning rate: 0.001234, scenario: 1, slope: -7.553479226800225e-05, fluctuations: 0.0\n",
      "step: 50350 loss: 33.656212 time elapsed: 66.7160 learning rate: 0.001191, scenario: -1, slope: 0.03390683157684207, fluctuations: 0.0\n",
      "step: 50360 loss: 20.629983 time elapsed: 66.7285 learning rate: 0.001077, scenario: -1, slope: 0.12193546103562418, fluctuations: 0.02\n",
      "step: 50370 loss: 0.795276 time elapsed: 66.7411 learning rate: 0.000974, scenario: -1, slope: 0.10794569717770681, fluctuations: 0.06\n",
      "step: 50380 loss: 0.804562 time elapsed: 66.7532 learning rate: 0.000881, scenario: -1, slope: 0.09081898970720227, fluctuations: 0.1\n",
      "step: 50390 loss: 1.170183 time elapsed: 66.7656 learning rate: 0.000796, scenario: -1, slope: 0.06456023103711155, fluctuations: 0.14\n",
      "step: 50400 loss: 0.768259 time elapsed: 66.7778 learning rate: 0.000728, scenario: -1, slope: 0.0374849937524626, fluctuations: 0.18\n",
      "step: 50410 loss: 0.763051 time elapsed: 66.7905 learning rate: 0.000658, scenario: -1, slope: 0.002839462855259688, fluctuations: 0.24\n",
      "step: 50420 loss: 0.769400 time elapsed: 66.8027 learning rate: 0.000658, scenario: 0, slope: -0.036136407480215044, fluctuations: 0.28\n",
      "step: 50430 loss: 0.762741 time elapsed: 66.8155 learning rate: 0.000658, scenario: 0, slope: -0.07728905437169538, fluctuations: 0.33\n",
      "step: 50440 loss: 0.757261 time elapsed: 66.8295 learning rate: 0.000658, scenario: 0, slope: -0.1274965636545769, fluctuations: 0.38\n",
      "step: 50450 loss: 0.755212 time elapsed: 66.8429 learning rate: 0.000658, scenario: 0, slope: -0.1433544683706878, fluctuations: 0.43\n",
      "step: 50460 loss: 0.754474 time elapsed: 66.8565 learning rate: 0.000658, scenario: 0, slope: -0.021393743925963573, fluctuations: 0.45\n",
      "step: 50470 loss: 0.753965 time elapsed: 66.8706 learning rate: 0.000658, scenario: 0, slope: -0.010364906669247744, fluctuations: 0.46\n",
      "step: 50480 loss: 0.753467 time elapsed: 66.8851 learning rate: 0.000658, scenario: 0, slope: -0.002857101912327389, fluctuations: 0.43\n",
      "step: 50490 loss: 0.752974 time elapsed: 66.8975 learning rate: 0.000658, scenario: 0, slope: -0.000645061293826292, fluctuations: 0.39\n",
      "step: 50500 loss: 0.752493 time elapsed: 66.9101 learning rate: 0.000658, scenario: 0, slope: -0.00012623074425078957, fluctuations: 0.35\n",
      "step: 50510 loss: 0.752023 time elapsed: 66.9245 learning rate: 0.000658, scenario: 0, slope: -0.0001445569552567592, fluctuations: 0.29\n",
      "step: 50520 loss: 0.751561 time elapsed: 66.9378 learning rate: 0.000638, scenario: -1, slope: -6.271550759645406e-05, fluctuations: 0.25\n",
      "step: 50530 loss: 0.751134 time elapsed: 66.9507 learning rate: 0.000577, scenario: -1, slope: -5.353037720749175e-05, fluctuations: 0.2\n",
      "step: 50540 loss: 0.750751 time elapsed: 66.9631 learning rate: 0.000522, scenario: -1, slope: -5.087862003863532e-05, fluctuations: 0.15\n",
      "step: 50550 loss: 0.750406 time elapsed: 66.9754 learning rate: 0.000472, scenario: -1, slope: -4.6908781932753015e-05, fluctuations: 0.1\n",
      "step: 50560 loss: 0.750096 time elapsed: 66.9877 learning rate: 0.000427, scenario: -1, slope: -4.4482745086003724e-05, fluctuations: 0.05\n",
      "step: 50570 loss: 0.749816 time elapsed: 67.0002 learning rate: 0.000392, scenario: 1, slope: -4.2268311581191054e-05, fluctuations: 0.0\n",
      "step: 50580 loss: 0.749540 time elapsed: 67.0125 learning rate: 0.000433, scenario: 1, slope: -3.9471577773985084e-05, fluctuations: 0.0\n",
      "step: 50590 loss: 0.749237 time elapsed: 67.0262 learning rate: 0.000479, scenario: 1, slope: -3.6791073440151476e-05, fluctuations: 0.0\n",
      "step: 50600 loss: 0.748901 time elapsed: 67.0401 learning rate: 0.000523, scenario: 1, slope: -3.469243413037933e-05, fluctuations: 0.0\n",
      "step: 50610 loss: 0.748535 time elapsed: 67.0545 learning rate: 0.000578, scenario: 1, slope: -3.2832868898672265e-05, fluctuations: 0.0\n",
      "step: 50620 loss: 0.748131 time elapsed: 67.0685 learning rate: 0.000639, scenario: 1, slope: -3.210380334035929e-05, fluctuations: 0.0\n",
      "step: 50630 loss: 0.747685 time elapsed: 67.0826 learning rate: 0.000705, scenario: 1, slope: -3.24853347909047e-05, fluctuations: 0.0\n",
      "step: 50640 loss: 0.747193 time elapsed: 67.0958 learning rate: 0.000779, scenario: 1, slope: -3.39537702501205e-05, fluctuations: 0.0\n",
      "step: 50650 loss: 0.746651 time elapsed: 67.1087 learning rate: 0.000861, scenario: 1, slope: -3.6417345537985717e-05, fluctuations: 0.0\n",
      "step: 50660 loss: 0.746054 time elapsed: 67.1211 learning rate: 0.000951, scenario: 1, slope: -3.973597544791445e-05, fluctuations: 0.0\n",
      "step: 50670 loss: 0.745395 time elapsed: 67.1335 learning rate: 0.001050, scenario: 1, slope: -4.371864141646815e-05, fluctuations: 0.0\n",
      "step: 50680 loss: 0.744669 time elapsed: 67.1459 learning rate: 0.001160, scenario: 1, slope: -4.81717247823342e-05, fluctuations: 0.0\n",
      "step: 50690 loss: 0.743869 time elapsed: 67.1584 learning rate: 0.001281, scenario: 1, slope: -5.308678326525224e-05, fluctuations: 0.0\n",
      "step: 50700 loss: 0.742988 time elapsed: 67.1707 learning rate: 0.001402, scenario: 1, slope: -5.7946553946337094e-05, fluctuations: 0.0\n",
      "step: 50710 loss: 0.742055 time elapsed: 67.1837 learning rate: 0.001548, scenario: 1, slope: -6.445304602635244e-05, fluctuations: 0.0\n",
      "step: 50720 loss: 1493.123497 time elapsed: 67.1962 learning rate: 0.001524, scenario: -1, slope: 1.7101975263549563, fluctuations: 0.0\n",
      "step: 50730 loss: 401.318405 time elapsed: 67.2086 learning rate: 0.001379, scenario: -1, slope: 1.4385170837428465, fluctuations: 0.05\n",
      "step: 50740 loss: 80.067470 time elapsed: 67.2211 learning rate: 0.001247, scenario: -1, slope: 1.3053666679942857, fluctuations: 0.1\n",
      "step: 50750 loss: 36.703955 time elapsed: 67.2346 learning rate: 0.001128, scenario: -1, slope: 1.0776653090916042, fluctuations: 0.13\n",
      "step: 50760 loss: 7.486480 time elapsed: 67.2487 learning rate: 0.001020, scenario: -1, slope: 0.7471741332950801, fluctuations: 0.17\n",
      "step: 50770 loss: 3.949691 time elapsed: 67.2626 learning rate: 0.000922, scenario: -1, slope: 0.39132975920074886, fluctuations: 0.21\n",
      "step: 50780 loss: 1.188180 time elapsed: 67.2765 learning rate: 0.000843, scenario: 0, slope: -0.026001735877069432, fluctuations: 0.24\n",
      "step: 50790 loss: 1.186947 time elapsed: 67.2901 learning rate: 0.000843, scenario: 0, slope: -0.4502355194524674, fluctuations: 0.27\n",
      "step: 50800 loss: 1.148767 time elapsed: 67.3042 learning rate: 0.000843, scenario: 0, slope: -0.8737355618855265, fluctuations: 0.3\n",
      "step: 50810 loss: 1.038891 time elapsed: 67.3177 learning rate: 0.000843, scenario: 0, slope: -1.5330116010963184, fluctuations: 0.33\n",
      "step: 50820 loss: 0.970130 time elapsed: 67.3303 learning rate: 0.000843, scenario: 0, slope: -0.90773936545221, fluctuations: 0.36\n",
      "step: 50830 loss: 0.943271 time elapsed: 67.3428 learning rate: 0.000843, scenario: 0, slope: -0.42038731423247794, fluctuations: 0.34\n",
      "step: 50840 loss: 0.928582 time elapsed: 67.3550 learning rate: 0.000843, scenario: 0, slope: -0.14168198102738816, fluctuations: 0.33\n",
      "step: 50850 loss: 0.915119 time elapsed: 67.3673 learning rate: 0.000843, scenario: 0, slope: -0.04785673384423511, fluctuations: 0.29\n",
      "step: 50860 loss: 0.903376 time elapsed: 67.3795 learning rate: 0.000843, scenario: 0, slope: -0.025885338478620835, fluctuations: 0.25\n",
      "step: 50870 loss: 0.893760 time elapsed: 67.3919 learning rate: 0.000843, scenario: 0, slope: -0.007820552388797699, fluctuations: 0.22\n",
      "step: 50880 loss: 0.885567 time elapsed: 67.4042 learning rate: 0.000843, scenario: 0, slope: -0.003907293375572918, fluctuations: 0.19\n",
      "step: 50890 loss: 0.878364 time elapsed: 67.4166 learning rate: 0.000843, scenario: 0, slope: -0.0024948262095265896, fluctuations: 0.15\n",
      "step: 50900 loss: 0.872001 time elapsed: 67.4289 learning rate: 0.000843, scenario: 0, slope: -0.0017042406871432187, fluctuations: 0.12\n",
      "step: 50910 loss: 0.866347 time elapsed: 67.4433 learning rate: 0.000843, scenario: 0, slope: -0.0011564669177722394, fluctuations: 0.09\n",
      "step: 50920 loss: 0.861278 time elapsed: 67.4576 learning rate: 0.000843, scenario: 0, slope: -0.0009476214147732849, fluctuations: 0.06\n",
      "step: 50930 loss: 0.856701 time elapsed: 67.4717 learning rate: 0.000843, scenario: 0, slope: -0.0008231139282153445, fluctuations: 0.02\n",
      "step: 50940 loss: 0.852544 time elapsed: 67.4856 learning rate: 0.000843, scenario: 0, slope: -0.000718306829505074, fluctuations: 0.0\n",
      "step: 50950 loss: 0.848747 time elapsed: 67.4996 learning rate: 0.000843, scenario: 0, slope: -0.0006295425286573325, fluctuations: 0.0\n",
      "step: 50960 loss: 0.845260 time elapsed: 67.5140 learning rate: 0.000843, scenario: 0, slope: -0.0005584580835820905, fluctuations: 0.0\n",
      "step: 50970 loss: 0.842042 time elapsed: 67.5269 learning rate: 0.000843, scenario: 0, slope: -0.0004998019819092714, fluctuations: 0.0\n",
      "step: 50980 loss: 0.839057 time elapsed: 67.5395 learning rate: 0.000843, scenario: 0, slope: -0.00045063082053507417, fluctuations: 0.0\n",
      "step: 50990 loss: 0.836278 time elapsed: 67.5520 learning rate: 0.000843, scenario: 0, slope: -0.00040901254985689655, fluctuations: 0.0\n",
      "step: 51000 loss: 0.833680 time elapsed: 67.5643 learning rate: 0.000843, scenario: 0, slope: -0.00037679863637988734, fluctuations: 0.0\n",
      "step: 51010 loss: 0.831241 time elapsed: 67.5772 learning rate: 0.000843, scenario: 0, slope: -0.00034293713840210035, fluctuations: 0.0\n",
      "step: 51020 loss: 0.828945 time elapsed: 67.5896 learning rate: 0.000843, scenario: 0, slope: -0.0003164970841500267, fluctuations: 0.0\n",
      "step: 51030 loss: 0.826777 time elapsed: 67.6020 learning rate: 0.000843, scenario: 0, slope: -0.00029347675220339303, fluctuations: 0.0\n",
      "step: 51040 loss: 0.824723 time elapsed: 67.6143 learning rate: 0.000843, scenario: 0, slope: -0.0002733195426283352, fluctuations: 0.0\n",
      "step: 51050 loss: 0.822772 time elapsed: 67.6265 learning rate: 0.000843, scenario: 0, slope: -0.00025557374337519564, fluctuations: 0.0\n",
      "step: 51060 loss: 0.820914 time elapsed: 67.6388 learning rate: 0.000843, scenario: 0, slope: -0.000239869958272155, fluctuations: 0.0\n",
      "step: 51070 loss: 0.819141 time elapsed: 67.6520 learning rate: 0.000843, scenario: 0, slope: -0.00022590425775414143, fluctuations: 0.0\n",
      "step: 51080 loss: 0.817445 time elapsed: 67.6660 learning rate: 0.000843, scenario: 0, slope: -0.00021342532697650818, fluctuations: 0.0\n",
      "step: 51090 loss: 0.815821 time elapsed: 67.6799 learning rate: 0.000843, scenario: 0, slope: -0.0002022243284219449, fluctuations: 0.0\n",
      "step: 51100 loss: 0.814261 time elapsed: 67.6935 learning rate: 0.000843, scenario: 0, slope: -0.0001930913610142853, fluctuations: 0.0\n",
      "step: 51110 loss: 0.812761 time elapsed: 67.7080 learning rate: 0.000843, scenario: 0, slope: -0.00018298691118147977, fluctuations: 0.0\n",
      "step: 51120 loss: 0.811317 time elapsed: 67.7220 learning rate: 0.000843, scenario: 0, slope: -0.00017468119335961339, fluctuations: 0.0\n",
      "step: 51130 loss: 0.809924 time elapsed: 67.7347 learning rate: 0.000843, scenario: 0, slope: -0.000167105656701035, fluctuations: 0.0\n",
      "step: 51140 loss: 0.808579 time elapsed: 67.7470 learning rate: 0.000843, scenario: 0, slope: -0.00016017190109730376, fluctuations: 0.0\n",
      "step: 51150 loss: 0.807277 time elapsed: 67.7593 learning rate: 0.000843, scenario: 0, slope: -0.00015380456676066456, fluctuations: 0.0\n",
      "step: 51160 loss: 0.806018 time elapsed: 67.7717 learning rate: 0.000843, scenario: 0, slope: -0.00014793916208883206, fluctuations: 0.0\n",
      "step: 51170 loss: 0.804797 time elapsed: 67.7839 learning rate: 0.000843, scenario: 0, slope: -0.0001425202887043722, fluctuations: 0.0\n",
      "step: 51180 loss: 0.803612 time elapsed: 67.7961 learning rate: 0.000843, scenario: 0, slope: -0.00013750018543944373, fluctuations: 0.0\n",
      "step: 51190 loss: 0.802461 time elapsed: 67.8085 learning rate: 0.000843, scenario: 0, slope: -0.00013283752926066884, fluctuations: 0.0\n",
      "step: 51200 loss: 0.801342 time elapsed: 67.8205 learning rate: 0.000843, scenario: 0, slope: -0.00012891700538391837, fluctuations: 0.0\n",
      "step: 51210 loss: 0.800254 time elapsed: 67.8335 learning rate: 0.000843, scenario: 0, slope: -0.00012444567713102263, fluctuations: 0.0\n",
      "step: 51220 loss: 0.799193 time elapsed: 67.8458 learning rate: 0.000843, scenario: 0, slope: -0.00012065791502375737, fluctuations: 0.0\n",
      "step: 51230 loss: 0.798160 time elapsed: 67.8583 learning rate: 0.000843, scenario: 0, slope: -0.00011710920873433286, fluctuations: 0.0\n",
      "step: 51240 loss: 0.797151 time elapsed: 67.8722 learning rate: 0.000843, scenario: 0, slope: -0.00011377849309240166, fluctuations: 0.0\n",
      "step: 51250 loss: 0.796167 time elapsed: 67.8862 learning rate: 0.000843, scenario: 0, slope: -0.0001106471811489849, fluctuations: 0.0\n",
      "step: 51260 loss: 0.795206 time elapsed: 67.9000 learning rate: 0.000843, scenario: 0, slope: -0.00010769882146608366, fluctuations: 0.0\n",
      "step: 51270 loss: 0.794265 time elapsed: 67.9136 learning rate: 0.000843, scenario: 0, slope: -0.00010491880754299213, fluctuations: 0.0\n",
      "step: 51280 loss: 0.793346 time elapsed: 67.9278 learning rate: 0.000843, scenario: 0, slope: -0.00010229413072937032, fluctuations: 0.0\n",
      "step: 51290 loss: 0.792446 time elapsed: 67.9411 learning rate: 0.000843, scenario: 0, slope: -9.981316955289115e-05, fluctuations: 0.0\n",
      "step: 51300 loss: 0.791564 time elapsed: 67.9539 learning rate: 0.000843, scenario: 0, slope: -9.769455150538446e-05, fluctuations: 0.0\n",
      "step: 51310 loss: 0.790699 time elapsed: 67.9669 learning rate: 0.000843, scenario: 0, slope: -9.524178951610978e-05, fluctuations: 0.0\n",
      "step: 51320 loss: 0.789852 time elapsed: 67.9792 learning rate: 0.000843, scenario: 0, slope: -9.313356807378337e-05, fluctuations: 0.0\n",
      "step: 51330 loss: 0.789020 time elapsed: 67.9916 learning rate: 0.000843, scenario: 0, slope: -9.113321080011265e-05, fluctuations: 0.0\n",
      "step: 51340 loss: 0.788203 time elapsed: 68.0044 learning rate: 0.000843, scenario: 0, slope: -8.923379159841035e-05, fluctuations: 0.0\n",
      "step: 51350 loss: 0.787401 time elapsed: 68.0167 learning rate: 0.000843, scenario: 0, slope: -8.742900812283944e-05, fluctuations: 0.0\n",
      "step: 51360 loss: 0.786612 time elapsed: 68.0289 learning rate: 0.000843, scenario: 0, slope: -8.571310858856892e-05, fluctuations: 0.0\n",
      "step: 51370 loss: 0.785836 time elapsed: 68.0412 learning rate: 0.000843, scenario: 0, slope: -8.408082841587806e-05, fluctuations: 0.0\n",
      "step: 51380 loss: 0.785073 time elapsed: 68.0535 learning rate: 0.000843, scenario: 0, slope: -8.252733530391695e-05, fluctuations: 0.0\n",
      "step: 51390 loss: 0.784322 time elapsed: 68.0671 learning rate: 0.000843, scenario: 0, slope: -8.104818154174172e-05, fluctuations: 0.0\n",
      "step: 51400 loss: 0.783582 time elapsed: 68.0807 learning rate: 0.000843, scenario: 0, slope: -7.977710436236304e-05, fluctuations: 0.0\n",
      "step: 51410 loss: 0.782842 time elapsed: 68.0952 learning rate: 0.000903, scenario: 1, slope: -7.831170698144855e-05, fluctuations: 0.0\n",
      "step: 51420 loss: 0.782044 time elapsed: 68.1091 learning rate: 0.000998, scenario: 1, slope: -7.729638692103202e-05, fluctuations: 0.0\n",
      "step: 51430 loss: 0.781177 time elapsed: 68.1230 learning rate: 0.001102, scenario: 1, slope: -7.694988318740381e-05, fluctuations: 0.0\n",
      "step: 51440 loss: 0.780236 time elapsed: 68.1370 learning rate: 0.001218, scenario: 1, slope: -7.756342296162448e-05, fluctuations: 0.0\n",
      "step: 51450 loss: 0.779217 time elapsed: 68.1498 learning rate: 0.001305, scenario: 0, slope: -7.935302986014333e-05, fluctuations: 0.0\n",
      "step: 51460 loss: 0.778182 time elapsed: 68.1624 learning rate: 0.001305, scenario: 0, slope: -8.2287558879871e-05, fluctuations: 0.0\n",
      "step: 51470 loss: 0.777168 time elapsed: 68.1748 learning rate: 0.001305, scenario: 0, slope: -8.592056983577925e-05, fluctuations: 0.0\n",
      "step: 51480 loss: 0.776174 time elapsed: 68.1871 learning rate: 0.001305, scenario: 0, slope: -8.977016426990234e-05, fluctuations: 0.0\n",
      "step: 51490 loss: 0.775198 time elapsed: 68.1995 learning rate: 0.001305, scenario: 0, slope: -9.337364297787849e-05, fluctuations: 0.0\n",
      "step: 51500 loss: 0.774238 time elapsed: 68.2118 learning rate: 0.001305, scenario: 0, slope: -9.603869197796704e-05, fluctuations: 0.0\n",
      "step: 51510 loss: 0.773295 time elapsed: 68.2247 learning rate: 0.001305, scenario: 0, slope: -9.809751948662843e-05, fluctuations: 0.0\n",
      "step: 51520 loss: 0.772366 time elapsed: 68.2372 learning rate: 0.001305, scenario: 0, slope: -9.867460492837757e-05, fluctuations: 0.0\n",
      "step: 51530 loss: 0.771451 time elapsed: 68.2496 learning rate: 0.001305, scenario: 0, slope: -9.817979859229827e-05, fluctuations: 0.0\n",
      "step: 51540 loss: 0.770549 time elapsed: 68.2622 learning rate: 0.001305, scenario: 0, slope: -9.691310217754077e-05, fluctuations: 0.0\n",
      "step: 51550 loss: 0.769660 time elapsed: 68.2755 learning rate: 0.001305, scenario: 0, slope: -9.531199141458263e-05, fluctuations: 0.0\n",
      "step: 51560 loss: 0.768781 time elapsed: 68.2892 learning rate: 0.001305, scenario: 0, slope: -9.376717917613778e-05, fluctuations: 0.0\n",
      "step: 51570 loss: 0.767913 time elapsed: 68.3029 learning rate: 0.001305, scenario: 0, slope: -9.232422534237725e-05, fluctuations: 0.0\n",
      "step: 51580 loss: 0.767055 time elapsed: 68.3167 learning rate: 0.001305, scenario: 0, slope: -9.097688920349628e-05, fluctuations: 0.0\n",
      "step: 51590 loss: 0.766206 time elapsed: 68.3305 learning rate: 0.001305, scenario: 0, slope: -8.9718686282897e-05, fluctuations: 0.0\n",
      "step: 51600 loss: 0.765365 time elapsed: 68.3447 learning rate: 0.001305, scenario: 0, slope: -8.865736501792837e-05, fluctuations: 0.0\n",
      "step: 51610 loss: 0.764533 time elapsed: 68.3580 learning rate: 0.001305, scenario: 0, slope: -8.744522014790781e-05, fluctuations: 0.0\n",
      "step: 51620 loss: 0.763708 time elapsed: 68.3705 learning rate: 0.001305, scenario: 0, slope: -8.641877932476387e-05, fluctuations: 0.0\n",
      "step: 51630 loss: 0.762890 time elapsed: 68.3832 learning rate: 0.001305, scenario: 0, slope: -8.545911977818356e-05, fluctuations: 0.0\n",
      "step: 51640 loss: 0.762079 time elapsed: 68.3956 learning rate: 0.001305, scenario: 0, slope: -8.4561670871672e-05, fluctuations: 0.0\n",
      "step: 51650 loss: 0.761274 time elapsed: 68.4088 learning rate: 0.001305, scenario: 0, slope: -8.372221617612275e-05, fluctuations: 0.0\n",
      "step: 51660 loss: 0.760475 time elapsed: 68.4212 learning rate: 0.001305, scenario: 0, slope: -8.293686356939807e-05, fluctuations: 0.0\n",
      "step: 51670 loss: 0.759681 time elapsed: 68.4335 learning rate: 0.001305, scenario: 0, slope: -8.220201772064293e-05, fluctuations: 0.0\n",
      "step: 51680 loss: 0.758893 time elapsed: 68.4459 learning rate: 0.001305, scenario: 0, slope: -8.151435503978332e-05, fluctuations: 0.0\n",
      "step: 51690 loss: 0.758109 time elapsed: 68.4584 learning rate: 0.001305, scenario: 0, slope: -8.087080093794908e-05, fluctuations: 0.0\n",
      "step: 51700 loss: 0.757329 time elapsed: 68.4708 learning rate: 0.001305, scenario: 0, slope: -8.032695826256113e-05, fluctuations: 0.0\n",
      "step: 51710 loss: 0.756554 time elapsed: 68.4845 learning rate: 0.001305, scenario: 0, slope: -7.970484316736365e-05, fluctuations: 0.0\n",
      "step: 51720 loss: 0.755782 time elapsed: 68.4986 learning rate: 0.001305, scenario: 0, slope: -7.917735881433243e-05, fluctuations: 0.0\n",
      "step: 51730 loss: 0.755014 time elapsed: 68.5128 learning rate: 0.001305, scenario: 0, slope: -7.868378908621636e-05, fluctuations: 0.0\n",
      "step: 51740 loss: 0.754250 time elapsed: 68.5263 learning rate: 0.001305, scenario: 0, slope: -7.82220298074889e-05, fluctuations: 0.0\n",
      "step: 51750 loss: 0.753489 time elapsed: 68.5402 learning rate: 0.001305, scenario: 0, slope: -7.779012674953162e-05, fluctuations: 0.0\n",
      "step: 51760 loss: 0.752730 time elapsed: 68.5542 learning rate: 0.001305, scenario: 0, slope: -7.738626383027883e-05, fluctuations: 0.0\n",
      "step: 51770 loss: 0.751975 time elapsed: 68.5668 learning rate: 0.001305, scenario: 0, slope: -7.700875232648513e-05, fluctuations: 0.0\n",
      "step: 51780 loss: 0.751221 time elapsed: 68.5793 learning rate: 0.001305, scenario: 0, slope: -7.665602100346065e-05, fluctuations: 0.0\n",
      "step: 51790 loss: 0.750471 time elapsed: 68.5918 learning rate: 0.001305, scenario: 0, slope: -7.632660707756358e-05, fluctuations: 0.0\n",
      "step: 51800 loss: 0.749722 time elapsed: 68.6041 learning rate: 0.001305, scenario: 0, slope: -7.60489427728696e-05, fluctuations: 0.0\n",
      "step: 51810 loss: 0.748976 time elapsed: 68.6168 learning rate: 0.001305, scenario: 0, slope: -7.573237353713726e-05, fluctuations: 0.0\n",
      "step: 51820 loss: 0.748224 time elapsed: 68.6289 learning rate: 0.001386, scenario: 1, slope: -7.547386599687663e-05, fluctuations: 0.0\n",
      "step: 51830 loss: 0.747407 time elapsed: 68.6410 learning rate: 0.001531, scenario: 1, slope: -7.545456585499463e-05, fluctuations: 0.0\n",
      "step: 51840 loss: 0.746533 time elapsed: 68.6531 learning rate: 0.001546, scenario: 0, slope: -7.60137878934732e-05, fluctuations: 0.0\n",
      "step: 51850 loss: 0.745659 time elapsed: 68.6655 learning rate: 0.001546, scenario: 0, slope: -7.713355402613384e-05, fluctuations: 0.0\n",
      "step: 51860 loss: 1.185583 time elapsed: 68.6780 learning rate: 0.001585, scenario: -1, slope: 0.00025815919157778596, fluctuations: 0.0\n",
      "step: 51870 loss: 22.131271 time elapsed: 68.6917 learning rate: 0.001433, scenario: -1, slope: 0.49818419607992215, fluctuations: 0.02\n",
      "step: 51880 loss: 52.452624 time elapsed: 68.7058 learning rate: 0.001296, scenario: -1, slope: 0.6268216716003143, fluctuations: 0.06\n",
      "step: 51890 loss: 17.980272 time elapsed: 68.7203 learning rate: 0.001172, scenario: -1, slope: 0.5548434067770005, fluctuations: 0.11\n",
      "step: 51900 loss: 4.744632 time elapsed: 68.7343 learning rate: 0.001071, scenario: -1, slope: 0.4667107653904155, fluctuations: 0.16\n",
      "step: 51910 loss: 1.804381 time elapsed: 68.7490 learning rate: 0.000969, scenario: -1, slope: 0.29543303514750463, fluctuations: 0.21\n",
      "step: 51920 loss: 0.896729 time elapsed: 68.7630 learning rate: 0.000876, scenario: -1, slope: 0.10627702235006219, fluctuations: 0.25\n",
      "step: 51930 loss: 0.802973 time elapsed: 68.7785 learning rate: 0.000833, scenario: 0, slope: -0.08631387188616597, fluctuations: 0.29\n",
      "step: 51940 loss: 0.796315 time elapsed: 68.7916 learning rate: 0.000833, scenario: 0, slope: -0.2904841771885652, fluctuations: 0.33\n",
      "step: 51950 loss: 0.787732 time elapsed: 68.8045 learning rate: 0.000833, scenario: 0, slope: -0.5336021762881, fluctuations: 0.37\n",
      "step: 51960 loss: 0.779707 time elapsed: 68.8174 learning rate: 0.000833, scenario: 0, slope: -0.8832724341486001, fluctuations: 0.41\n",
      "step: 51970 loss: 0.775601 time elapsed: 68.8315 learning rate: 0.000833, scenario: 0, slope: -0.42697404410054224, fluctuations: 0.42\n",
      "step: 51980 loss: 0.772991 time elapsed: 68.8446 learning rate: 0.000833, scenario: 0, slope: -0.05529590485174262, fluctuations: 0.42\n",
      "step: 51990 loss: 0.770902 time elapsed: 68.8574 learning rate: 0.000833, scenario: 0, slope: -0.024228700714355456, fluctuations: 0.4\n",
      "step: 52000 loss: 0.769272 time elapsed: 68.8700 learning rate: 0.000833, scenario: 0, slope: -0.014287057561357225, fluctuations: 0.35\n",
      "step: 52010 loss: 0.767880 time elapsed: 68.8832 learning rate: 0.000833, scenario: 0, slope: -0.0030381409734593218, fluctuations: 0.31\n",
      "step: 52020 loss: 0.766600 time elapsed: 68.8964 learning rate: 0.000833, scenario: 0, slope: -0.001365019615240598, fluctuations: 0.26\n",
      "step: 52030 loss: 0.765409 time elapsed: 68.9108 learning rate: 0.000833, scenario: 0, slope: -0.000602500306808058, fluctuations: 0.22\n",
      "step: 52040 loss: 0.764292 time elapsed: 68.9248 learning rate: 0.000833, scenario: 0, slope: -0.00030495194037802174, fluctuations: 0.18\n",
      "step: 52050 loss: 0.763236 time elapsed: 68.9389 learning rate: 0.000833, scenario: 0, slope: -0.00017913134139166802, fluctuations: 0.15\n",
      "step: 52060 loss: 0.762234 time elapsed: 68.9530 learning rate: 0.000833, scenario: 0, slope: -0.00014656417320424347, fluctuations: 0.11\n",
      "step: 52070 loss: 0.761280 time elapsed: 68.9687 learning rate: 0.000833, scenario: 0, slope: -0.00012934137966154753, fluctuations: 0.07\n",
      "step: 52080 loss: 0.760369 time elapsed: 68.9827 learning rate: 0.000833, scenario: 0, slope: -0.00011866909700747486, fluctuations: 0.03\n",
      "step: 52090 loss: 0.759497 time elapsed: 68.9961 learning rate: 0.000833, scenario: 0, slope: -0.00011135803528009612, fluctuations: 0.0\n",
      "step: 52100 loss: 0.758659 time elapsed: 69.0091 learning rate: 0.000833, scenario: 0, slope: -0.0001051846003340124, fluctuations: 0.0\n",
      "step: 52110 loss: 0.757853 time elapsed: 69.0227 learning rate: 0.000833, scenario: 0, slope: -9.900559755203753e-05, fluctuations: 0.0\n",
      "step: 52120 loss: 0.757075 time elapsed: 69.0351 learning rate: 0.000833, scenario: 0, slope: -9.419357874974024e-05, fluctuations: 0.0\n",
      "step: 52130 loss: 0.756322 time elapsed: 69.0477 learning rate: 0.000833, scenario: 0, slope: -8.995803752118368e-05, fluctuations: 0.0\n",
      "step: 52140 loss: 0.755593 time elapsed: 69.0605 learning rate: 0.000833, scenario: 0, slope: -8.618353335421864e-05, fluctuations: 0.0\n",
      "step: 52150 loss: 0.754886 time elapsed: 69.0731 learning rate: 0.000833, scenario: 0, slope: -8.279565906744798e-05, fluctuations: 0.0\n",
      "step: 52160 loss: 0.754198 time elapsed: 69.0857 learning rate: 0.000833, scenario: 0, slope: -7.973974016041426e-05, fluctuations: 0.0\n",
      "step: 52170 loss: 0.753528 time elapsed: 69.0988 learning rate: 0.000833, scenario: 0, slope: -7.697331979373824e-05, fluctuations: 0.0\n",
      "step: 52180 loss: 0.752864 time elapsed: 69.1129 learning rate: 0.000893, scenario: 1, slope: -7.447533946616546e-05, fluctuations: 0.0\n",
      "step: 52190 loss: 0.752155 time elapsed: 69.1272 learning rate: 0.000987, scenario: 1, slope: -7.242507310069635e-05, fluctuations: 0.0\n",
      "step: 52200 loss: 0.751392 time elapsed: 69.1408 learning rate: 0.001079, scenario: 1, slope: -7.120292617073373e-05, fluctuations: 0.0\n",
      "step: 52210 loss: 0.750578 time elapsed: 69.1555 learning rate: 0.001192, scenario: 1, slope: -7.074257016423783e-05, fluctuations: 0.0\n",
      "step: 52220 loss: 0.749703 time elapsed: 69.1699 learning rate: 0.001317, scenario: 1, slope: -7.145680180012803e-05, fluctuations: 0.0\n",
      "step: 52230 loss: 0.748763 time elapsed: 69.1833 learning rate: 0.001454, scenario: 1, slope: -7.335226984323051e-05, fluctuations: 0.0\n",
      "step: 52240 loss: 0.747754 time elapsed: 69.1959 learning rate: 0.001559, scenario: 0, slope: -7.645583505027998e-05, fluctuations: 0.0\n",
      "step: 52250 loss: 0.746738 time elapsed: 69.2086 learning rate: 0.001559, scenario: 0, slope: -8.055798722704007e-05, fluctuations: 0.0\n",
      "step: 52260 loss: 0.745748 time elapsed: 69.2211 learning rate: 0.001559, scenario: 0, slope: -8.50554067966807e-05, fluctuations: 0.0\n",
      "step: 52270 loss: 0.744780 time elapsed: 69.2336 learning rate: 0.001559, scenario: 0, slope: -8.932823014293202e-05, fluctuations: 0.0\n",
      "step: 52280 loss: 0.743833 time elapsed: 69.2460 learning rate: 0.001559, scenario: 0, slope: -9.280460763437943e-05, fluctuations: 0.0\n",
      "step: 52290 loss: 0.742903 time elapsed: 69.2586 learning rate: 0.001559, scenario: 0, slope: -9.517628986024927e-05, fluctuations: 0.0\n",
      "step: 52300 loss: 0.741988 time elapsed: 69.2708 learning rate: 0.001559, scenario: 0, slope: -9.63288223516372e-05, fluctuations: 0.0\n",
      "step: 52310 loss: 0.741086 time elapsed: 69.2834 learning rate: 0.001559, scenario: 0, slope: -9.653375747291847e-05, fluctuations: 0.0\n",
      "step: 52320 loss: 0.740195 time elapsed: 69.2956 learning rate: 0.001559, scenario: 0, slope: -9.573391295778271e-05, fluctuations: 0.0\n",
      "step: 52330 loss: 0.739315 time elapsed: 69.3087 learning rate: 0.001559, scenario: 0, slope: -9.430372399960835e-05, fluctuations: 0.0\n",
      "step: 52340 loss: 0.738444 time elapsed: 69.3226 learning rate: 0.001559, scenario: 0, slope: -9.266772829432278e-05, fluctuations: 0.0\n",
      "step: 52350 loss: 0.737581 time elapsed: 69.3362 learning rate: 0.001559, scenario: 0, slope: -9.119264951025801e-05, fluctuations: 0.0\n",
      "step: 52360 loss: 0.736725 time elapsed: 69.3501 learning rate: 0.001559, scenario: 0, slope: -8.990156905806558e-05, fluctuations: 0.0\n",
      "step: 52370 loss: 0.735875 time elapsed: 69.3639 learning rate: 0.001559, scenario: 0, slope: -8.876930116245578e-05, fluctuations: 0.0\n",
      "step: 52380 loss: 0.735031 time elapsed: 69.3778 learning rate: 0.001559, scenario: 0, slope: -8.777366544210686e-05, fluctuations: 0.0\n",
      "step: 52390 loss: 0.734192 time elapsed: 69.3907 learning rate: 0.001559, scenario: 0, slope: -8.689564907876724e-05, fluctuations: 0.0\n",
      "step: 52400 loss: 0.733357 time elapsed: 69.4029 learning rate: 0.001559, scenario: 0, slope: -8.619260117916324e-05, fluctuations: 0.0\n",
      "step: 52410 loss: 0.732527 time elapsed: 69.4160 learning rate: 0.001559, scenario: 0, slope: -8.543035762213078e-05, fluctuations: 0.0\n",
      "step: 52420 loss: 0.731700 time elapsed: 69.4284 learning rate: 0.001559, scenario: 0, slope: -8.481781499748631e-05, fluctuations: 0.0\n",
      "step: 52430 loss: 0.730876 time elapsed: 69.4406 learning rate: 0.001559, scenario: 0, slope: -8.427164430062498e-05, fluctuations: 0.0\n",
      "step: 52440 loss: 0.730055 time elapsed: 69.4529 learning rate: 0.001559, scenario: 0, slope: -8.378348650802841e-05, fluctuations: 0.0\n",
      "step: 52450 loss: 0.729237 time elapsed: 69.4652 learning rate: 0.001559, scenario: 0, slope: -8.334622116817263e-05, fluctuations: 0.0\n",
      "step: 52460 loss: 0.728421 time elapsed: 69.4776 learning rate: 0.001559, scenario: 0, slope: -8.295376917276445e-05, fluctuations: 0.0\n",
      "step: 52470 loss: 0.727608 time elapsed: 69.4897 learning rate: 0.001559, scenario: 0, slope: -8.26009279923244e-05, fluctuations: 0.0\n",
      "step: 52480 loss: 0.726796 time elapsed: 69.5018 learning rate: 0.001559, scenario: 0, slope: -8.228323395222524e-05, fluctuations: 0.0\n",
      "step: 52490 loss: 0.725986 time elapsed: 69.5149 learning rate: 0.001559, scenario: 0, slope: -8.199684701215336e-05, fluctuations: 0.0\n",
      "step: 52500 loss: 0.725178 time elapsed: 69.5285 learning rate: 0.001559, scenario: 0, slope: -8.176311868586444e-05, fluctuations: 0.0\n",
      "step: 52510 loss: 0.724371 time elapsed: 69.5430 learning rate: 0.001559, scenario: 0, slope: -8.150518916655747e-05, fluctuations: 0.0\n",
      "step: 52520 loss: 0.723566 time elapsed: 69.5569 learning rate: 0.001559, scenario: 0, slope: -8.129456351644939e-05, fluctuations: 0.0\n",
      "step: 52530 loss: 0.722762 time elapsed: 69.5705 learning rate: 0.001559, scenario: 0, slope: -8.110441069381533e-05, fluctuations: 0.0\n",
      "step: 52540 loss: 0.721958 time elapsed: 69.5849 learning rate: 0.001559, scenario: 0, slope: -8.093283770518756e-05, fluctuations: 0.0\n",
      "step: 52550 loss: 0.721156 time elapsed: 69.5977 learning rate: 0.001559, scenario: 0, slope: -8.077818494699307e-05, fluctuations: 0.0\n",
      "step: 52560 loss: 0.720355 time elapsed: 69.6102 learning rate: 0.001559, scenario: 0, slope: -8.063899230921834e-05, fluctuations: 0.0\n",
      "step: 52570 loss: 0.719554 time elapsed: 69.6225 learning rate: 0.001559, scenario: 0, slope: -8.05139706025338e-05, fluctuations: 0.0\n",
      "step: 52580 loss: 0.718754 time elapsed: 69.6351 learning rate: 0.001559, scenario: 0, slope: -8.040197744358562e-05, fluctuations: 0.0\n",
      "step: 52590 loss: 0.717954 time elapsed: 69.6473 learning rate: 0.001559, scenario: 0, slope: -8.030199687585377e-05, fluctuations: 0.0\n",
      "step: 52600 loss: 0.717155 time elapsed: 69.6598 learning rate: 0.001559, scenario: 0, slope: -8.022153366586584e-05, fluctuations: 0.0\n",
      "step: 52610 loss: 0.716356 time elapsed: 69.6729 learning rate: 0.001559, scenario: 0, slope: -8.01345409666278e-05, fluctuations: 0.0\n",
      "step: 52620 loss: 0.715558 time elapsed: 69.6870 learning rate: 0.001559, scenario: 0, slope: -8.006552333787872e-05, fluctuations: 0.0\n",
      "step: 52630 loss: 0.714759 time elapsed: 69.7014 learning rate: 0.001559, scenario: 0, slope: -8.000541075234065e-05, fluctuations: 0.0\n",
      "step: 52640 loss: 0.713961 time elapsed: 69.7169 learning rate: 0.001559, scenario: 0, slope: -7.99536073114263e-05, fluctuations: 0.0\n",
      "step: 52650 loss: 0.713164 time elapsed: 69.7312 learning rate: 0.001559, scenario: 0, slope: -7.990957201305839e-05, fluctuations: 0.0\n",
      "step: 52660 loss: 0.712366 time elapsed: 69.7450 learning rate: 0.001559, scenario: 0, slope: -7.987281216852748e-05, fluctuations: 0.0\n",
      "step: 52670 loss: 0.711568 time elapsed: 69.7589 learning rate: 0.001559, scenario: 0, slope: -7.984287775212844e-05, fluctuations: 0.0\n",
      "step: 52680 loss: 0.710771 time elapsed: 69.7728 learning rate: 0.001559, scenario: 0, slope: -7.981935622094097e-05, fluctuations: 0.0\n",
      "step: 52690 loss: 0.709973 time elapsed: 69.7864 learning rate: 0.001559, scenario: 0, slope: -7.980146523498453e-05, fluctuations: 0.0\n",
      "step: 52700 loss: 0.709959 time elapsed: 69.8010 learning rate: 0.001559, scenario: 0, slope: -7.940759690994005e-05, fluctuations: 0.0\n",
      "step: 52710 loss: 6.485222 time elapsed: 69.8143 learning rate: 0.001536, scenario: -1, slope: 0.005431380376349705, fluctuations: 0.0\n",
      "step: 52720 loss: 32.564160 time elapsed: 69.8268 learning rate: 0.001389, scenario: -1, slope: 0.16559021213066505, fluctuations: 0.01\n",
      "step: 52730 loss: 9.512260 time elapsed: 69.8392 learning rate: 0.001256, scenario: -1, slope: 0.15351521926237202, fluctuations: 0.04\n",
      "step: 52740 loss: 1.000958 time elapsed: 69.8517 learning rate: 0.001136, scenario: -1, slope: 0.1285645393403475, fluctuations: 0.08\n",
      "step: 52750 loss: 0.858392 time elapsed: 69.8643 learning rate: 0.001027, scenario: -1, slope: 0.09592459662819573, fluctuations: 0.12\n",
      "step: 52760 loss: 0.942234 time elapsed: 69.8767 learning rate: 0.000929, scenario: -1, slope: 0.05470300798204413, fluctuations: 0.16\n",
      "step: 52770 loss: 0.768418 time elapsed: 69.8890 learning rate: 0.000840, scenario: -1, slope: 0.011910143908366052, fluctuations: 0.21\n",
      "step: 52780 loss: 0.730669 time elapsed: 69.9014 learning rate: 0.000823, scenario: 0, slope: -0.03710467088465343, fluctuations: 0.26\n",
      "step: 52790 loss: 0.717012 time elapsed: 69.9138 learning rate: 0.000823, scenario: 0, slope: -0.09206733064628855, fluctuations: 0.31\n",
      "step: 52800 loss: 0.711493 time elapsed: 69.9271 learning rate: 0.000823, scenario: 0, slope: -0.15070308160847112, fluctuations: 0.36\n",
      "step: 52810 loss: 0.709203 time elapsed: 69.9416 learning rate: 0.000823, scenario: 0, slope: -0.23146425622901023, fluctuations: 0.41\n",
      "step: 52820 loss: 0.708091 time elapsed: 69.9554 learning rate: 0.000823, scenario: 0, slope: -0.039816739640901606, fluctuations: 0.44\n",
      "step: 52830 loss: 0.707406 time elapsed: 69.9689 learning rate: 0.000823, scenario: 0, slope: -0.010075398486773857, fluctuations: 0.46\n",
      "step: 52840 loss: 0.706875 time elapsed: 69.9827 learning rate: 0.000823, scenario: 0, slope: -0.004361010459232438, fluctuations: 0.42\n",
      "step: 52850 loss: 0.706398 time elapsed: 69.9970 learning rate: 0.000823, scenario: 0, slope: -0.0011314171212497381, fluctuations: 0.38\n",
      "step: 52860 loss: 0.705939 time elapsed: 70.0102 learning rate: 0.000823, scenario: 0, slope: -0.000162288755539834, fluctuations: 0.34\n",
      "step: 52870 loss: 0.705488 time elapsed: 70.0234 learning rate: 0.000823, scenario: 0, slope: -0.00011806602392891129, fluctuations: 0.29\n",
      "step: 52880 loss: 0.705039 time elapsed: 70.0360 learning rate: 0.000807, scenario: -1, slope: -6.576721848897823e-05, fluctuations: 0.24\n",
      "step: 52890 loss: 0.704612 time elapsed: 70.0485 learning rate: 0.000737, scenario: -1, slope: -5.227447040413333e-05, fluctuations: 0.19\n",
      "step: 52900 loss: 0.704227 time elapsed: 70.0607 learning rate: 0.000673, scenario: -1, slope: -5.0133010709042495e-05, fluctuations: 0.14\n",
      "step: 52910 loss: 0.703875 time elapsed: 70.0734 learning rate: 0.000609, scenario: -1, slope: -4.5360292224810416e-05, fluctuations: 0.09\n",
      "step: 52920 loss: 0.703557 time elapsed: 70.0855 learning rate: 0.000551, scenario: -1, slope: -4.350763282879514e-05, fluctuations: 0.04\n",
      "step: 52930 loss: 0.703268 time elapsed: 70.0977 learning rate: 0.000537, scenario: 1, slope: -4.16169835931474e-05, fluctuations: 0.0\n",
      "step: 52940 loss: 0.702966 time elapsed: 70.1099 learning rate: 0.000593, scenario: 1, slope: -3.930705552091891e-05, fluctuations: 0.0\n",
      "step: 52950 loss: 0.702633 time elapsed: 70.1221 learning rate: 0.000655, scenario: 1, slope: -3.7144397208880576e-05, fluctuations: 0.0\n",
      "step: 52960 loss: 0.702264 time elapsed: 70.1353 learning rate: 0.000724, scenario: 1, slope: -3.536952089946517e-05, fluctuations: 0.0\n",
      "step: 52970 loss: 0.701857 time elapsed: 70.1497 learning rate: 0.000800, scenario: 1, slope: -3.4255407422206544e-05, fluctuations: 0.0\n",
      "step: 52980 loss: 0.701407 time elapsed: 70.1634 learning rate: 0.000883, scenario: 1, slope: -3.4070887895518994e-05, fluctuations: 0.0\n",
      "step: 52990 loss: 0.700909 time elapsed: 70.1771 learning rate: 0.000976, scenario: 1, slope: -3.501304119600692e-05, fluctuations: 0.0\n",
      "step: 53000 loss: 0.700359 time elapsed: 70.1909 learning rate: 0.001067, scenario: 1, slope: -3.68212980032352e-05, fluctuations: 0.0\n",
      "step: 53010 loss: 0.699757 time elapsed: 70.2053 learning rate: 0.001179, scenario: 1, slope: -4.014422430917893e-05, fluctuations: 0.0\n",
      "step: 53020 loss: 0.699093 time elapsed: 70.2180 learning rate: 0.001302, scenario: 1, slope: -4.405408127691785e-05, fluctuations: 0.0\n",
      "step: 53030 loss: 0.698360 time elapsed: 70.2306 learning rate: 0.001438, scenario: 1, slope: -4.859098476624204e-05, fluctuations: 0.0\n",
      "step: 53040 loss: 0.697552 time elapsed: 70.2434 learning rate: 0.001589, scenario: 1, slope: -5.359995542834725e-05, fluctuations: 0.0\n",
      "step: 53050 loss: 0.696666 time elapsed: 70.2557 learning rate: 0.001755, scenario: 1, slope: -5.910668444387387e-05, fluctuations: 0.0\n",
      "step: 53060 loss: 371.419444 time elapsed: 70.2679 learning rate: 0.001763, scenario: -1, slope: 0.2643582976921805, fluctuations: 0.0\n",
      "step: 53070 loss: 263.929557 time elapsed: 70.2803 learning rate: 0.001594, scenario: -1, slope: 2.096792044806459, fluctuations: 0.03\n",
      "step: 53080 loss: 101.960277 time elapsed: 70.2927 learning rate: 0.001442, scenario: -1, slope: 2.1936886174267776, fluctuations: 0.07\n",
      "step: 53090 loss: 50.135101 time elapsed: 70.3050 learning rate: 0.001304, scenario: -1, slope: 1.8739326571372752, fluctuations: 0.11\n",
      "step: 53100 loss: 14.246169 time elapsed: 70.3174 learning rate: 0.001191, scenario: -1, slope: 1.4452865731997193, fluctuations: 0.15\n",
      "step: 53110 loss: 1.691898 time elapsed: 70.3303 learning rate: 0.001077, scenario: -1, slope: 0.7596091993716876, fluctuations: 0.19\n",
      "step: 53120 loss: 2.634284 time elapsed: 70.3438 learning rate: 0.000974, scenario: -1, slope: 0.06997744702949527, fluctuations: 0.22\n",
      "step: 53130 loss: 1.083412 time elapsed: 70.3577 learning rate: 0.000965, scenario: 0, slope: -0.597269380100448, fluctuations: 0.26\n",
      "step: 53140 loss: 1.171997 time elapsed: 70.3714 learning rate: 0.000965, scenario: 0, slope: -1.3502863481327887, fluctuations: 0.29\n",
      "step: 53150 loss: 0.999379 time elapsed: 70.3849 learning rate: 0.000965, scenario: 0, slope: -2.282042960121728, fluctuations: 0.33\n",
      "step: 53160 loss: 0.944747 time elapsed: 70.3986 learning rate: 0.000965, scenario: 0, slope: -3.1634742372592477, fluctuations: 0.36\n",
      "step: 53170 loss: 0.924946 time elapsed: 70.4128 learning rate: 0.000965, scenario: 0, slope: -0.5533511652312358, fluctuations: 0.36\n",
      "step: 53180 loss: 0.900089 time elapsed: 70.4252 learning rate: 0.000965, scenario: 0, slope: -0.20728323765815906, fluctuations: 0.35\n",
      "step: 53190 loss: 0.886854 time elapsed: 70.4379 learning rate: 0.000965, scenario: 0, slope: -0.06507893623245364, fluctuations: 0.33\n",
      "step: 53200 loss: 0.874060 time elapsed: 70.4502 learning rate: 0.000965, scenario: 0, slope: -0.0340614107071022, fluctuations: 0.29\n",
      "step: 53210 loss: 0.864129 time elapsed: 70.4634 learning rate: 0.000965, scenario: 0, slope: -0.012045901175089024, fluctuations: 0.25\n",
      "step: 53220 loss: 0.855434 time elapsed: 70.4758 learning rate: 0.000965, scenario: 0, slope: -0.004484768443873844, fluctuations: 0.22\n",
      "step: 53230 loss: 0.847969 time elapsed: 70.4884 learning rate: 0.000965, scenario: 0, slope: -0.002616520575667586, fluctuations: 0.19\n",
      "step: 53240 loss: 0.841409 time elapsed: 70.5006 learning rate: 0.000965, scenario: 0, slope: -0.0017098521205190316, fluctuations: 0.15\n",
      "step: 53250 loss: 0.835614 time elapsed: 70.5126 learning rate: 0.000965, scenario: 0, slope: -0.001241328475537373, fluctuations: 0.12\n",
      "step: 53260 loss: 0.830443 time elapsed: 70.5252 learning rate: 0.000965, scenario: 0, slope: -0.0010269247085478255, fluctuations: 0.08\n",
      "step: 53270 loss: 0.825795 time elapsed: 70.5376 learning rate: 0.000965, scenario: 0, slope: -0.0008490251047915408, fluctuations: 0.05\n",
      "step: 53280 loss: 0.821587 time elapsed: 70.5511 learning rate: 0.000965, scenario: 0, slope: -0.0007442171675088557, fluctuations: 0.01\n",
      "step: 53290 loss: 0.817750 time elapsed: 70.5654 learning rate: 0.000965, scenario: 0, slope: -0.0006512709810944282, fluctuations: 0.0\n",
      "step: 53300 loss: 0.814231 time elapsed: 70.5792 learning rate: 0.000965, scenario: 0, slope: -0.0005808953641890655, fluctuations: 0.0\n",
      "step: 53310 loss: 0.810984 time elapsed: 70.5935 learning rate: 0.000965, scenario: 0, slope: -0.0005108620661861602, fluctuations: 0.0\n",
      "step: 53320 loss: 0.807972 time elapsed: 70.6072 learning rate: 0.000965, scenario: 0, slope: -0.00045867780256421967, fluctuations: 0.0\n",
      "step: 53330 loss: 0.805164 time elapsed: 70.6214 learning rate: 0.000965, scenario: 0, slope: -0.000414996836637864, fluctuations: 0.0\n",
      "step: 53340 loss: 0.802534 time elapsed: 70.6340 learning rate: 0.000965, scenario: 0, slope: -0.00037811772974458905, fluctuations: 0.0\n",
      "step: 53350 loss: 0.800061 time elapsed: 70.6475 learning rate: 0.000965, scenario: 0, slope: -0.0003467465688682837, fluctuations: 0.0\n",
      "step: 53360 loss: 0.797727 time elapsed: 70.6601 learning rate: 0.000965, scenario: 0, slope: -0.00031987647378045066, fluctuations: 0.0\n",
      "step: 53370 loss: 0.795516 time elapsed: 70.6727 learning rate: 0.000965, scenario: 0, slope: -0.0002967094661252339, fluctuations: 0.0\n",
      "step: 53380 loss: 0.793416 time elapsed: 70.6852 learning rate: 0.000965, scenario: 0, slope: -0.0002766062205922408, fluctuations: 0.0\n",
      "step: 53390 loss: 0.791414 time elapsed: 70.6974 learning rate: 0.000965, scenario: 0, slope: -0.00025905072368554685, fluctuations: 0.0\n",
      "step: 53400 loss: 0.789503 time elapsed: 70.7095 learning rate: 0.000965, scenario: 0, slope: -0.0002450816177910498, fluctuations: 0.0\n",
      "step: 53410 loss: 0.787674 time elapsed: 70.7222 learning rate: 0.000965, scenario: 0, slope: -0.00022998712395598146, fluctuations: 0.0\n",
      "step: 53420 loss: 0.785918 time elapsed: 70.7349 learning rate: 0.000965, scenario: 0, slope: -0.00021786003189665424, fluctuations: 0.0\n",
      "step: 53430 loss: 0.784231 time elapsed: 70.7475 learning rate: 0.000965, scenario: 0, slope: -0.0002070151082530068, fluctuations: 0.0\n",
      "step: 53440 loss: 0.782607 time elapsed: 70.7611 learning rate: 0.000965, scenario: 0, slope: -0.0001972645490191819, fluctuations: 0.0\n",
      "step: 53450 loss: 0.781040 time elapsed: 70.7756 learning rate: 0.000965, scenario: 0, slope: -0.00018845316886260564, fluctuations: 0.0\n",
      "step: 53460 loss: 0.779528 time elapsed: 70.7894 learning rate: 0.000965, scenario: 0, slope: -0.00018045219561225515, fluctuations: 0.0\n",
      "step: 53470 loss: 0.778064 time elapsed: 70.8031 learning rate: 0.000965, scenario: 0, slope: -0.00017315431040067073, fluctuations: 0.0\n",
      "step: 53480 loss: 0.776648 time elapsed: 70.8169 learning rate: 0.000965, scenario: 0, slope: -0.00016646968075738287, fluctuations: 0.0\n",
      "step: 53490 loss: 0.775274 time elapsed: 70.8312 learning rate: 0.000965, scenario: 0, slope: -0.00016032278152530136, fluctuations: 0.0\n",
      "step: 53500 loss: 0.773942 time elapsed: 70.8437 learning rate: 0.000965, scenario: 0, slope: -0.0001551974147156988, fluctuations: 0.0\n",
      "step: 53510 loss: 0.772647 time elapsed: 70.8565 learning rate: 0.000965, scenario: 0, slope: -0.0001493967714036028, fluctuations: 0.0\n",
      "step: 53520 loss: 0.771388 time elapsed: 70.8691 learning rate: 0.000965, scenario: 0, slope: -0.00014451751514970942, fluctuations: 0.0\n",
      "step: 53530 loss: 0.770162 time elapsed: 70.8815 learning rate: 0.000965, scenario: 0, slope: -0.00013997267070833106, fluctuations: 0.0\n",
      "step: 53540 loss: 0.768969 time elapsed: 70.8936 learning rate: 0.000965, scenario: 0, slope: -0.00013572839152649463, fluctuations: 0.0\n",
      "step: 53550 loss: 0.767805 time elapsed: 70.9058 learning rate: 0.000965, scenario: 0, slope: -0.00013175547515080155, fluctuations: 0.0\n",
      "step: 53560 loss: 0.766669 time elapsed: 70.9181 learning rate: 0.000965, scenario: 0, slope: -0.0001280286151056418, fluctuations: 0.0\n",
      "step: 53570 loss: 0.765561 time elapsed: 70.9305 learning rate: 0.000965, scenario: 0, slope: -0.00012452578335482174, fluctuations: 0.0\n",
      "step: 53580 loss: 0.764478 time elapsed: 70.9426 learning rate: 0.000965, scenario: 0, slope: -0.0001212277190594745, fluctuations: 0.0\n",
      "step: 53590 loss: 0.763419 time elapsed: 70.9548 learning rate: 0.000965, scenario: 0, slope: -0.00011811750422103985, fluctuations: 0.0\n",
      "step: 53600 loss: 0.762383 time elapsed: 70.9675 learning rate: 0.000965, scenario: 0, slope: -0.0001154665477534767, fluctuations: 0.0\n",
      "step: 53610 loss: 0.761368 time elapsed: 70.9819 learning rate: 0.000965, scenario: 0, slope: -0.00011240260537888252, fluctuations: 0.0\n",
      "step: 53620 loss: 0.760375 time elapsed: 70.9957 learning rate: 0.000965, scenario: 0, slope: -0.00010977290509586705, fluctuations: 0.0\n",
      "step: 53630 loss: 0.759401 time elapsed: 71.0092 learning rate: 0.000965, scenario: 0, slope: -0.00010728056987799046, fluctuations: 0.0\n",
      "step: 53640 loss: 0.758446 time elapsed: 71.0233 learning rate: 0.000965, scenario: 0, slope: -0.00010491613073534816, fluctuations: 0.0\n",
      "step: 53650 loss: 0.757508 time elapsed: 71.0377 learning rate: 0.000965, scenario: 0, slope: -0.00010267104451479633, fluctuations: 0.0\n",
      "step: 53660 loss: 0.756588 time elapsed: 71.0526 learning rate: 0.000965, scenario: 0, slope: -0.0001005375717873528, fluctuations: 0.0\n",
      "step: 53670 loss: 0.755684 time elapsed: 71.0656 learning rate: 0.000965, scenario: 0, slope: -9.850867383658458e-05, fluctuations: 0.0\n",
      "step: 53680 loss: 0.754795 time elapsed: 71.0780 learning rate: 0.000965, scenario: 0, slope: -9.657792556207569e-05, fluctuations: 0.0\n",
      "step: 53690 loss: 0.753921 time elapsed: 71.0902 learning rate: 0.000965, scenario: 0, slope: -9.473944165441474e-05, fluctuations: 0.0\n",
      "step: 53700 loss: 0.753062 time elapsed: 71.1023 learning rate: 0.000965, scenario: 0, slope: -9.315921326690791e-05, fluctuations: 0.0\n",
      "step: 53710 loss: 0.752215 time elapsed: 71.1151 learning rate: 0.000965, scenario: 0, slope: -9.131805740626909e-05, fluctuations: 0.0\n",
      "step: 53720 loss: 0.751382 time elapsed: 71.1273 learning rate: 0.000965, scenario: 0, slope: -8.972556537808726e-05, fluctuations: 0.0\n",
      "step: 53730 loss: 0.750561 time elapsed: 71.1396 learning rate: 0.000965, scenario: 0, slope: -8.820606926630577e-05, fluctuations: 0.0\n",
      "step: 53740 loss: 0.749752 time elapsed: 71.1521 learning rate: 0.000965, scenario: 0, slope: -8.67556051538045e-05, fluctuations: 0.0\n",
      "step: 53750 loss: 0.748954 time elapsed: 71.1647 learning rate: 0.000965, scenario: 0, slope: -8.537048435546545e-05, fluctuations: 0.0\n",
      "step: 53760 loss: 0.748167 time elapsed: 71.1780 learning rate: 0.000965, scenario: 0, slope: -8.40472678900016e-05, fluctuations: 0.0\n",
      "step: 53770 loss: 0.747390 time elapsed: 71.1919 learning rate: 0.000965, scenario: 0, slope: -8.278274416496806e-05, fluctuations: 0.0\n",
      "step: 53780 loss: 0.746623 time elapsed: 71.2056 learning rate: 0.000965, scenario: 0, slope: -8.157390937481953e-05, fluctuations: 0.0\n",
      "step: 53790 loss: 0.745866 time elapsed: 71.2194 learning rate: 0.000965, scenario: 0, slope: -8.041795019545862e-05, fluctuations: 0.0\n",
      "step: 53800 loss: 0.745117 time elapsed: 71.2331 learning rate: 0.000965, scenario: 0, slope: -7.942061171822723e-05, fluctuations: 0.0\n",
      "step: 53810 loss: 0.744377 time elapsed: 71.2480 learning rate: 0.000965, scenario: 0, slope: -7.825426732050681e-05, fluctuations: 0.0\n",
      "step: 53820 loss: 0.743646 time elapsed: 71.2610 learning rate: 0.000965, scenario: 0, slope: -7.724173928180609e-05, fluctuations: 0.0\n",
      "step: 53830 loss: 0.742922 time elapsed: 71.2736 learning rate: 0.000965, scenario: 0, slope: -7.62724548684514e-05, fluctuations: 0.0\n",
      "step: 53840 loss: 0.742206 time elapsed: 71.2862 learning rate: 0.000965, scenario: 0, slope: -7.534435279979655e-05, fluctuations: 0.0\n",
      "step: 53850 loss: 0.741491 time elapsed: 71.2985 learning rate: 0.001024, scenario: 1, slope: -7.446381060363315e-05, fluctuations: 0.0\n",
      "step: 53860 loss: 0.740720 time elapsed: 71.3109 learning rate: 0.001131, scenario: 1, slope: -7.382903855947709e-05, fluctuations: 0.0\n",
      "step: 53870 loss: 0.739878 time elapsed: 71.3232 learning rate: 0.001249, scenario: 1, slope: -7.379450034033125e-05, fluctuations: 0.0\n",
      "step: 53880 loss: 0.738959 time elapsed: 71.3355 learning rate: 0.001366, scenario: 0, slope: -7.46604745545598e-05, fluctuations: 0.0\n",
      "step: 53890 loss: 0.738002 time elapsed: 71.3476 learning rate: 0.001366, scenario: 0, slope: -7.656777996889524e-05, fluctuations: 0.0\n",
      "step: 53900 loss: 0.737058 time elapsed: 71.3597 learning rate: 0.001366, scenario: 0, slope: -7.894950900799295e-05, fluctuations: 0.0\n",
      "step: 53910 loss: 0.736127 time elapsed: 71.3726 learning rate: 0.001366, scenario: 0, slope: -8.231526327171304e-05, fluctuations: 0.0\n",
      "step: 53920 loss: 0.735208 time elapsed: 71.3857 learning rate: 0.001366, scenario: 0, slope: -8.543725140338577e-05, fluctuations: 0.0\n",
      "step: 53930 loss: 0.734300 time elapsed: 71.3997 learning rate: 0.001366, scenario: 0, slope: -8.826313627667871e-05, fluctuations: 0.0\n",
      "step: 53940 loss: 0.733403 time elapsed: 71.4136 learning rate: 0.001366, scenario: 0, slope: -9.045905569273781e-05, fluctuations: 0.0\n",
      "step: 53950 loss: 0.732516 time elapsed: 71.4271 learning rate: 0.001366, scenario: 0, slope: -9.170907559542422e-05, fluctuations: 0.0\n",
      "step: 53960 loss: 0.731638 time elapsed: 71.4407 learning rate: 0.001366, scenario: 0, slope: -9.193215506106037e-05, fluctuations: 0.0\n",
      "step: 53970 loss: 0.730770 time elapsed: 71.4553 learning rate: 0.001366, scenario: 0, slope: -9.135354434173449e-05, fluctuations: 0.0\n",
      "step: 53980 loss: 0.729910 time elapsed: 71.4684 learning rate: 0.001366, scenario: 0, slope: -9.033868022924214e-05, fluctuations: 0.0\n",
      "step: 53990 loss: 0.729059 time elapsed: 71.4808 learning rate: 0.001366, scenario: 0, slope: -8.929077754530125e-05, fluctuations: 0.0\n",
      "step: 54000 loss: 0.728215 time elapsed: 71.4930 learning rate: 0.001366, scenario: 0, slope: -8.839471115548142e-05, fluctuations: 0.0\n",
      "step: 54010 loss: 0.727379 time elapsed: 71.5056 learning rate: 0.001366, scenario: 0, slope: -8.735745176324717e-05, fluctuations: 0.0\n",
      "step: 54020 loss: 0.726549 time elapsed: 71.5178 learning rate: 0.001366, scenario: 0, slope: -8.646683528935354e-05, fluctuations: 0.0\n",
      "step: 54030 loss: 0.725726 time elapsed: 71.5306 learning rate: 0.001366, scenario: 0, slope: -8.562314950911005e-05, fluctuations: 0.0\n",
      "step: 54040 loss: 0.724910 time elapsed: 71.5435 learning rate: 0.001366, scenario: 0, slope: -8.482369368858326e-05, fluctuations: 0.0\n",
      "step: 54050 loss: 0.724099 time elapsed: 71.5559 learning rate: 0.001366, scenario: 0, slope: -8.406593184672087e-05, fluctuations: 0.0\n",
      "step: 54060 loss: 0.723294 time elapsed: 71.5682 learning rate: 0.001366, scenario: 0, slope: -8.334749599986564e-05, fluctuations: 0.0\n",
      "step: 54070 loss: 0.722494 time elapsed: 71.5805 learning rate: 0.001366, scenario: 0, slope: -8.266617833137728e-05, fluctuations: 0.0\n",
      "step: 54080 loss: 0.721700 time elapsed: 71.5937 learning rate: 0.001366, scenario: 0, slope: -8.201992031991904e-05, fluctuations: 0.0\n",
      "step: 54090 loss: 0.720910 time elapsed: 71.6076 learning rate: 0.001366, scenario: 0, slope: -8.140680155215547e-05, fluctuations: 0.0\n",
      "step: 54100 loss: 0.720125 time elapsed: 71.6216 learning rate: 0.001366, scenario: 0, slope: -8.08818443847404e-05, fluctuations: 0.0\n",
      "step: 54110 loss: 0.719344 time elapsed: 71.6361 learning rate: 0.001366, scenario: 0, slope: -8.027292768641938e-05, fluctuations: 0.0\n",
      "step: 54120 loss: 0.718567 time elapsed: 71.6497 learning rate: 0.001366, scenario: 0, slope: -7.97489306993289e-05, fluctuations: 0.0\n",
      "step: 54130 loss: 0.717794 time elapsed: 71.6637 learning rate: 0.001366, scenario: 0, slope: -7.92515719457155e-05, fluctuations: 0.0\n",
      "step: 54140 loss: 0.717025 time elapsed: 71.6788 learning rate: 0.001366, scenario: 0, slope: -7.877947814137115e-05, fluctuations: 0.0\n",
      "step: 54150 loss: 0.716260 time elapsed: 71.6915 learning rate: 0.001366, scenario: 0, slope: -7.833136199067387e-05, fluctuations: 0.0\n",
      "step: 54160 loss: 0.715498 time elapsed: 71.7046 learning rate: 0.001366, scenario: 0, slope: -7.790597581122008e-05, fluctuations: 0.0\n",
      "step: 54170 loss: 0.714794 time elapsed: 71.7169 learning rate: 0.001366, scenario: 0, slope: -7.744163292299774e-05, fluctuations: 0.0\n",
      "step: 54180 loss: 0.904042 time elapsed: 71.7294 learning rate: 0.001415, scenario: -1, slope: 0.00010675786804475903, fluctuations: 0.0\n",
      "step: 54190 loss: 7.177815 time elapsed: 71.7423 learning rate: 0.001280, scenario: -1, slope: 0.09688407969781358, fluctuations: 0.01\n",
      "step: 54200 loss: 7.199942 time elapsed: 71.7554 learning rate: 0.001169, scenario: -1, slope: 0.11767552298277392, fluctuations: 0.04\n",
      "step: 54210 loss: 0.784107 time elapsed: 71.7685 learning rate: 0.001057, scenario: -1, slope: 0.10656789297631532, fluctuations: 0.08\n",
      "step: 54220 loss: 1.355425 time elapsed: 71.7809 learning rate: 0.000956, scenario: -1, slope: 0.08353888530795243, fluctuations: 0.12\n",
      "step: 54230 loss: 0.759904 time elapsed: 71.7933 learning rate: 0.000865, scenario: -1, slope: 0.05710676830714463, fluctuations: 0.17\n",
      "step: 54240 loss: 0.745830 time elapsed: 71.8071 learning rate: 0.000782, scenario: -1, slope: 0.024274182485162377, fluctuations: 0.22\n",
      "step: 54250 loss: 0.715591 time elapsed: 71.8209 learning rate: 0.000736, scenario: 0, slope: -0.013561846353933068, fluctuations: 0.27\n",
      "step: 54260 loss: 0.723784 time elapsed: 71.8347 learning rate: 0.000736, scenario: 0, slope: -0.05597209440624296, fluctuations: 0.31\n",
      "step: 54270 loss: 0.717282 time elapsed: 71.8486 learning rate: 0.000736, scenario: 0, slope: -0.1027848091304567, fluctuations: 0.36\n",
      "step: 54280 loss: 0.713177 time elapsed: 71.8624 learning rate: 0.000736, scenario: 0, slope: -0.16394206219838703, fluctuations: 0.41\n",
      "step: 54290 loss: 0.712573 time elapsed: 71.8761 learning rate: 0.000736, scenario: 0, slope: -0.08818824018248146, fluctuations: 0.44\n",
      "step: 54300 loss: 0.712147 time elapsed: 71.8889 learning rate: 0.000736, scenario: 0, slope: -0.016489914286259295, fluctuations: 0.45\n",
      "step: 54310 loss: 0.711570 time elapsed: 71.9019 learning rate: 0.000736, scenario: 0, slope: -0.006446298105085668, fluctuations: 0.45\n",
      "step: 54320 loss: 0.711049 time elapsed: 71.9145 learning rate: 0.000736, scenario: 0, slope: -0.0011510794163212229, fluctuations: 0.42\n",
      "step: 54330 loss: 0.710580 time elapsed: 71.9266 learning rate: 0.000736, scenario: 0, slope: -0.0005525897719990392, fluctuations: 0.37\n",
      "step: 54340 loss: 0.710127 time elapsed: 71.9390 learning rate: 0.000736, scenario: 0, slope: -0.00017084644011658085, fluctuations: 0.32\n",
      "step: 54350 loss: 0.709680 time elapsed: 71.9510 learning rate: 0.000736, scenario: 0, slope: -0.00010665800593698484, fluctuations: 0.27\n",
      "step: 54360 loss: 0.709241 time elapsed: 71.9633 learning rate: 0.000700, scenario: -1, slope: -5.417535533990787e-05, fluctuations: 0.23\n",
      "step: 54370 loss: 0.708838 time elapsed: 71.9757 learning rate: 0.000633, scenario: -1, slope: -5.1211490179119234e-05, fluctuations: 0.18\n",
      "step: 54380 loss: 0.708478 time elapsed: 71.9880 learning rate: 0.000573, scenario: -1, slope: -4.6990025497033584e-05, fluctuations: 0.13\n",
      "step: 54390 loss: 0.708154 time elapsed: 72.0005 learning rate: 0.000518, scenario: -1, slope: -4.461128668012652e-05, fluctuations: 0.08\n",
      "step: 54400 loss: 0.707862 time elapsed: 72.0133 learning rate: 0.000473, scenario: -1, slope: -4.2242700085054964e-05, fluctuations: 0.05\n",
      "step: 54410 loss: 0.707596 time elapsed: 72.0277 learning rate: 0.000443, scenario: 1, slope: -3.995852012305645e-05, fluctuations: 0.0\n",
      "step: 54420 loss: 0.707330 time elapsed: 72.0416 learning rate: 0.000490, scenario: 1, slope: -3.736572199486964e-05, fluctuations: 0.0\n",
      "step: 54430 loss: 0.707036 time elapsed: 72.0558 learning rate: 0.000541, scenario: 1, slope: -3.485295337922888e-05, fluctuations: 0.0\n",
      "step: 54440 loss: 0.706712 time elapsed: 72.0696 learning rate: 0.000597, scenario: 1, slope: -3.2711967314821666e-05, fluctuations: 0.0\n",
      "step: 54450 loss: 0.706355 time elapsed: 72.0836 learning rate: 0.000660, scenario: 1, slope: -3.123371212143901e-05, fluctuations: 0.0\n",
      "step: 54460 loss: 0.705962 time elapsed: 72.0971 learning rate: 0.000729, scenario: 1, slope: -3.069055665068942e-05, fluctuations: 0.0\n",
      "step: 54470 loss: 0.705529 time elapsed: 72.1106 learning rate: 0.000805, scenario: 1, slope: -3.121995213618082e-05, fluctuations: 0.0\n",
      "step: 54480 loss: 0.705052 time elapsed: 72.1231 learning rate: 0.000889, scenario: 1, slope: -3.277647816115804e-05, fluctuations: 0.0\n",
      "step: 54490 loss: 0.704528 time elapsed: 72.1359 learning rate: 0.000982, scenario: 1, slope: -3.5259919437746786e-05, fluctuations: 0.0\n",
      "step: 54500 loss: 0.703951 time elapsed: 72.1482 learning rate: 0.001074, scenario: 1, slope: -3.8166172258178194e-05, fluctuations: 0.0\n",
      "step: 54510 loss: 0.703323 time elapsed: 72.1610 learning rate: 0.001187, scenario: 1, slope: -4.2357477287710874e-05, fluctuations: 0.0\n",
      "step: 54520 loss: 0.702632 time elapsed: 72.1732 learning rate: 0.001311, scenario: 1, slope: -4.657568708901637e-05, fluctuations: 0.0\n",
      "step: 54530 loss: 0.701874 time elapsed: 72.1856 learning rate: 0.001448, scenario: 1, slope: -5.11784312483182e-05, fluctuations: 0.0\n",
      "step: 54540 loss: 0.701041 time elapsed: 72.1977 learning rate: 0.001600, scenario: 1, slope: -5.6209523231332335e-05, fluctuations: 0.0\n",
      "step: 54550 loss: 0.700889 time elapsed: 72.2098 learning rate: 0.001767, scenario: 1, slope: -6.117075207826655e-05, fluctuations: 0.0\n",
      "step: 54560 loss: 60.094646 time elapsed: 72.2231 learning rate: 0.001672, scenario: -1, slope: 0.8669727932271638, fluctuations: 0.01\n",
      "step: 54570 loss: 31.116078 time elapsed: 72.2368 learning rate: 0.001512, scenario: -1, slope: 1.1215973373399464, fluctuations: 0.06\n",
      "step: 54580 loss: 90.909399 time elapsed: 72.2505 learning rate: 0.001367, scenario: -1, slope: 1.1414961774912296, fluctuations: 0.1\n",
      "step: 54590 loss: 28.983488 time elapsed: 72.2644 learning rate: 0.001237, scenario: -1, slope: 0.9129997606373584, fluctuations: 0.14\n",
      "step: 54600 loss: 7.916948 time elapsed: 72.2778 learning rate: 0.001130, scenario: -1, slope: 0.6682698046073309, fluctuations: 0.18\n",
      "step: 54610 loss: 1.550359 time elapsed: 72.2923 learning rate: 0.001022, scenario: -1, slope: 0.2978191506872787, fluctuations: 0.22\n",
      "step: 54620 loss: 2.010162 time elapsed: 72.3052 learning rate: 0.000943, scenario: 0, slope: -0.0669732536130038, fluctuations: 0.25\n",
      "step: 54630 loss: 1.091632 time elapsed: 72.3178 learning rate: 0.000943, scenario: 0, slope: -0.4313453161783173, fluctuations: 0.29\n",
      "step: 54640 loss: 1.011977 time elapsed: 72.3303 learning rate: 0.000943, scenario: 0, slope: -0.8608342910108495, fluctuations: 0.32\n",
      "step: 54650 loss: 0.976517 time elapsed: 72.3426 learning rate: 0.000943, scenario: 0, slope: -1.4256671378808927, fluctuations: 0.35\n",
      "step: 54660 loss: 0.924864 time elapsed: 72.3548 learning rate: 0.000943, scenario: 0, slope: -1.4772296005159027, fluctuations: 0.37\n",
      "step: 54670 loss: 0.910538 time elapsed: 72.3670 learning rate: 0.000943, scenario: 0, slope: -0.45981583611992227, fluctuations: 0.35\n",
      "step: 54680 loss: 0.891769 time elapsed: 72.3792 learning rate: 0.000943, scenario: 0, slope: -0.11641427875870712, fluctuations: 0.34\n",
      "step: 54690 loss: 0.879840 time elapsed: 72.3913 learning rate: 0.000943, scenario: 0, slope: -0.04146520171342182, fluctuations: 0.3\n",
      "step: 54700 loss: 0.868902 time elapsed: 72.4031 learning rate: 0.000943, scenario: 0, slope: -0.0213199561456519, fluctuations: 0.26\n",
      "step: 54710 loss: 0.859390 time elapsed: 72.4162 learning rate: 0.000943, scenario: 0, slope: -0.007395877767855154, fluctuations: 0.22\n",
      "step: 54720 loss: 0.850900 time elapsed: 72.4294 learning rate: 0.000943, scenario: 0, slope: -0.002920777731794779, fluctuations: 0.19\n",
      "step: 54730 loss: 0.843250 time elapsed: 72.4434 learning rate: 0.000943, scenario: 0, slope: -0.0018313950993873367, fluctuations: 0.16\n",
      "step: 54740 loss: 0.836316 time elapsed: 72.4574 learning rate: 0.000943, scenario: 0, slope: -0.0014373866157215667, fluctuations: 0.12\n",
      "step: 54750 loss: 0.829990 time elapsed: 72.4713 learning rate: 0.000943, scenario: 0, slope: -0.001101325958991102, fluctuations: 0.09\n",
      "step: 54760 loss: 0.824198 time elapsed: 72.4852 learning rate: 0.000943, scenario: 0, slope: -0.000948765346945856, fluctuations: 0.05\n",
      "step: 54770 loss: 0.818874 time elapsed: 72.4993 learning rate: 0.000943, scenario: 0, slope: -0.000835634699000024, fluctuations: 0.02\n",
      "step: 54780 loss: 0.813964 time elapsed: 72.5122 learning rate: 0.000943, scenario: 0, slope: -0.0007556436868717118, fluctuations: 0.0\n",
      "step: 54790 loss: 0.809423 time elapsed: 72.5248 learning rate: 0.000943, scenario: 0, slope: -0.000682193358646174, fluctuations: 0.0\n",
      "step: 54800 loss: 0.805211 time elapsed: 72.5369 learning rate: 0.000943, scenario: 0, slope: -0.0006263401128176489, fluctuations: 0.0\n",
      "step: 54810 loss: 0.801294 time elapsed: 72.5500 learning rate: 0.000943, scenario: 0, slope: -0.0005677041707651658, fluctuations: 0.0\n",
      "step: 54820 loss: 0.797642 time elapsed: 72.5626 learning rate: 0.000943, scenario: 0, slope: -0.0005215078221008683, fluctuations: 0.0\n",
      "step: 54830 loss: 0.794229 time elapsed: 72.5750 learning rate: 0.000943, scenario: 0, slope: -0.0004808130682383617, fluctuations: 0.0\n",
      "step: 54840 loss: 0.791033 time elapsed: 72.5873 learning rate: 0.000943, scenario: 0, slope: -0.0004447088896542403, fluctuations: 0.0\n",
      "step: 54850 loss: 0.788033 time elapsed: 72.5995 learning rate: 0.000943, scenario: 0, slope: -0.0004125157986592992, fluctuations: 0.0\n",
      "step: 54860 loss: 0.785212 time elapsed: 72.6118 learning rate: 0.000943, scenario: 0, slope: -0.0003836850884240257, fluctuations: 0.0\n",
      "step: 54870 loss: 0.782553 time elapsed: 72.6241 learning rate: 0.000943, scenario: 0, slope: -0.0003577720811249412, fluctuations: 0.0\n",
      "step: 54880 loss: 0.780043 time elapsed: 72.6372 learning rate: 0.000943, scenario: 0, slope: -0.00033440673663593187, fluctuations: 0.0\n",
      "step: 54890 loss: 0.777668 time elapsed: 72.6513 learning rate: 0.000943, scenario: 0, slope: -0.0003132784189860777, fluctuations: 0.0\n",
      "step: 54900 loss: 0.775418 time elapsed: 72.6652 learning rate: 0.000943, scenario: 0, slope: -0.00029595704454659743, fluctuations: 0.0\n",
      "step: 54910 loss: 0.773281 time elapsed: 72.6798 learning rate: 0.000943, scenario: 0, slope: -0.0002767176372288281, fluctuations: 0.0\n",
      "step: 54920 loss: 0.771249 time elapsed: 72.6936 learning rate: 0.000943, scenario: 0, slope: -0.000260866811748548, fluctuations: 0.0\n",
      "step: 54930 loss: 0.769313 time elapsed: 72.7078 learning rate: 0.000943, scenario: 0, slope: -0.0002464039929753731, fluctuations: 0.0\n",
      "step: 54940 loss: 0.767466 time elapsed: 72.7206 learning rate: 0.000943, scenario: 0, slope: -0.00023318377864742907, fluctuations: 0.0\n",
      "step: 54950 loss: 0.765701 time elapsed: 72.7331 learning rate: 0.000943, scenario: 0, slope: -0.0002210791172861918, fluctuations: 0.0\n",
      "step: 54960 loss: 0.764010 time elapsed: 72.7452 learning rate: 0.000943, scenario: 0, slope: -0.00020997848594499942, fluctuations: 0.0\n",
      "step: 54970 loss: 0.762390 time elapsed: 72.7576 learning rate: 0.000943, scenario: 0, slope: -0.00019978358519100466, fluctuations: 0.0\n",
      "step: 54980 loss: 0.760834 time elapsed: 72.7701 learning rate: 0.000943, scenario: 0, slope: -0.00019040744512420428, fluctuations: 0.0\n",
      "step: 54990 loss: 0.759338 time elapsed: 72.7827 learning rate: 0.000943, scenario: 0, slope: -0.0001817728577781522, fluctuations: 0.0\n",
      "step: 55000 loss: 0.757897 time elapsed: 72.7950 learning rate: 0.000943, scenario: 0, slope: -0.00017457877616851037, fluctuations: 0.0\n",
      "step: 55010 loss: 0.756507 time elapsed: 72.8076 learning rate: 0.000943, scenario: 0, slope: -0.0001664606894220292, fluctuations: 0.0\n",
      "step: 55020 loss: 0.755165 time elapsed: 72.8198 learning rate: 0.000943, scenario: 0, slope: -0.00015966675334881707, fluctuations: 0.0\n",
      "step: 55030 loss: 0.753868 time elapsed: 72.8322 learning rate: 0.000943, scenario: 0, slope: -0.00015337994662706998, fluctuations: 0.0\n",
      "step: 55040 loss: 0.752612 time elapsed: 72.8453 learning rate: 0.000943, scenario: 0, slope: -0.0001475559264984637, fluctuations: 0.0\n",
      "step: 55050 loss: 0.751395 time elapsed: 72.8591 learning rate: 0.000943, scenario: 0, slope: -0.0001421547451629017, fluctuations: 0.0\n",
      "step: 55060 loss: 0.750213 time elapsed: 72.8726 learning rate: 0.000943, scenario: 0, slope: -0.0001371403508245726, fluctuations: 0.0\n",
      "step: 55070 loss: 0.749065 time elapsed: 72.8863 learning rate: 0.000943, scenario: 0, slope: -0.00013248015498140017, fluctuations: 0.0\n",
      "step: 55080 loss: 0.747949 time elapsed: 72.8997 learning rate: 0.000943, scenario: 0, slope: -0.00012814465560513947, fluctuations: 0.0\n",
      "step: 55090 loss: 0.746862 time elapsed: 72.9133 learning rate: 0.000943, scenario: 0, slope: -0.00012410710774138407, fluctuations: 0.0\n",
      "step: 55100 loss: 0.745802 time elapsed: 72.9262 learning rate: 0.000943, scenario: 0, slope: -0.00012070795159354864, fluctuations: 0.0\n",
      "step: 55110 loss: 0.744769 time elapsed: 72.9391 learning rate: 0.000943, scenario: 0, slope: -0.00011683097302446504, fluctuations: 0.0\n",
      "step: 55120 loss: 0.743759 time elapsed: 72.9517 learning rate: 0.000943, scenario: 0, slope: -0.00011355024953105232, fluctuations: 0.0\n",
      "step: 55130 loss: 0.742772 time elapsed: 72.9642 learning rate: 0.000943, scenario: 0, slope: -0.00011048278121146415, fluctuations: 0.0\n",
      "step: 55140 loss: 0.741807 time elapsed: 72.9764 learning rate: 0.000943, scenario: 0, slope: -0.00010761189977511794, fluctuations: 0.0\n",
      "step: 55150 loss: 0.740862 time elapsed: 72.9886 learning rate: 0.000943, scenario: 0, slope: -0.00010492239484831542, fluctuations: 0.0\n",
      "step: 55160 loss: 0.739935 time elapsed: 73.0010 learning rate: 0.000943, scenario: 0, slope: -0.00010240037441268646, fluctuations: 0.0\n",
      "step: 55170 loss: 0.739027 time elapsed: 73.0133 learning rate: 0.000943, scenario: 0, slope: -0.00010003314023439183, fluctuations: 0.0\n",
      "step: 55180 loss: 0.738135 time elapsed: 73.0256 learning rate: 0.000943, scenario: 0, slope: -9.780907647566086e-05, fluctuations: 0.0\n",
      "step: 55190 loss: 0.737260 time elapsed: 73.0377 learning rate: 0.000943, scenario: 0, slope: -9.571754992354235e-05, fluctuations: 0.0\n",
      "step: 55200 loss: 0.736399 time elapsed: 73.0516 learning rate: 0.000943, scenario: 0, slope: -9.394042801059894e-05, fluctuations: 0.0\n",
      "step: 55210 loss: 0.735553 time elapsed: 73.0669 learning rate: 0.000943, scenario: 0, slope: -9.189396069413924e-05, fluctuations: 0.0\n",
      "step: 55220 loss: 0.734720 time elapsed: 73.0808 learning rate: 0.000943, scenario: 0, slope: -9.014478339407062e-05, fluctuations: 0.0\n",
      "step: 55230 loss: 0.733900 time elapsed: 73.0945 learning rate: 0.000943, scenario: 0, slope: -8.849377634221876e-05, fluctuations: 0.0\n",
      "step: 55240 loss: 0.733092 time elapsed: 73.1083 learning rate: 0.000943, scenario: 0, slope: -8.693404327497462e-05, fluctuations: 0.0\n",
      "step: 55250 loss: 0.732295 time elapsed: 73.1220 learning rate: 0.000943, scenario: 0, slope: -8.545925051976728e-05, fluctuations: 0.0\n",
      "step: 55260 loss: 0.731510 time elapsed: 73.1361 learning rate: 0.000943, scenario: 0, slope: -8.406357860013353e-05, fluctuations: 0.0\n",
      "step: 55270 loss: 0.730735 time elapsed: 73.1491 learning rate: 0.000943, scenario: 0, slope: -8.274167827510474e-05, fluctuations: 0.0\n",
      "step: 55280 loss: 0.729970 time elapsed: 73.1620 learning rate: 0.000943, scenario: 0, slope: -8.148863053089379e-05, fluctuations: 0.0\n",
      "step: 55290 loss: 0.729214 time elapsed: 73.1744 learning rate: 0.000943, scenario: 0, slope: -8.029991010168165e-05, fluctuations: 0.0\n",
      "step: 55300 loss: 0.728467 time elapsed: 73.1866 learning rate: 0.000943, scenario: 0, slope: -7.928161238340613e-05, fluctuations: 0.0\n",
      "step: 55310 loss: 0.727729 time elapsed: 73.1995 learning rate: 0.000943, scenario: 0, slope: -7.809912178581883e-05, fluctuations: 0.0\n",
      "step: 55320 loss: 0.726999 time elapsed: 73.2120 learning rate: 0.000943, scenario: 0, slope: -7.707968613977737e-05, fluctuations: 0.0\n",
      "step: 55330 loss: 0.726276 time elapsed: 73.2242 learning rate: 0.000943, scenario: 0, slope: -7.610978875480317e-05, fluctuations: 0.0\n",
      "step: 55340 loss: 0.725561 time elapsed: 73.2365 learning rate: 0.000943, scenario: 0, slope: -7.518642607974567e-05, fluctuations: 0.0\n",
      "step: 55350 loss: 0.724853 time elapsed: 73.2488 learning rate: 0.000943, scenario: 0, slope: -7.430682588680452e-05, fluctuations: 0.0\n",
      "step: 55360 loss: 0.724152 time elapsed: 73.2617 learning rate: 0.000943, scenario: 0, slope: -7.3468427454206e-05, fluctuations: 0.0\n",
      "step: 55370 loss: 0.723451 time elapsed: 73.2755 learning rate: 0.001001, scenario: 1, slope: -7.267702195502284e-05, fluctuations: 0.0\n",
      "step: 55380 loss: 0.722694 time elapsed: 73.2892 learning rate: 0.001105, scenario: 1, slope: -7.21267406481212e-05, fluctuations: 0.0\n",
      "step: 55390 loss: 0.721867 time elapsed: 73.3028 learning rate: 0.001221, scenario: 1, slope: -7.216583784131602e-05, fluctuations: 0.0\n",
      "step: 55400 loss: 0.720964 time elapsed: 73.3162 learning rate: 0.001322, scenario: 0, slope: -7.2951182272107e-05, fluctuations: 0.0\n",
      "step: 55410 loss: 0.720030 time elapsed: 73.3307 learning rate: 0.001322, scenario: 0, slope: -7.501327067150253e-05, fluctuations: 0.0\n",
      "step: 55420 loss: 0.719109 time elapsed: 73.3439 learning rate: 0.001322, scenario: 0, slope: -7.765032849528045e-05, fluctuations: 0.0\n",
      "step: 55430 loss: 0.718198 time elapsed: 73.3566 learning rate: 0.001322, scenario: 0, slope: -8.065739057190953e-05, fluctuations: 0.0\n",
      "step: 55440 loss: 0.717298 time elapsed: 73.3690 learning rate: 0.001322, scenario: 0, slope: -8.369910931916276e-05, fluctuations: 0.0\n",
      "step: 55450 loss: 0.716407 time elapsed: 73.3812 learning rate: 0.001322, scenario: 0, slope: -8.644905005146262e-05, fluctuations: 0.0\n",
      "step: 55460 loss: 0.715526 time elapsed: 73.3934 learning rate: 0.001322, scenario: 0, slope: -8.858924385239366e-05, fluctuations: 0.0\n",
      "step: 55470 loss: 0.714653 time elapsed: 73.4058 learning rate: 0.001322, scenario: 0, slope: -8.981824720802234e-05, fluctuations: 0.0\n",
      "step: 55480 loss: 0.713789 time elapsed: 73.4180 learning rate: 0.001322, scenario: 0, slope: -9.006414523474492e-05, fluctuations: 0.0\n",
      "step: 55490 loss: 0.712933 time elapsed: 73.4304 learning rate: 0.001322, scenario: 0, slope: -8.95555264332394e-05, fluctuations: 0.0\n",
      "step: 55500 loss: 0.712085 time elapsed: 73.4423 learning rate: 0.001322, scenario: 0, slope: -8.875324750270747e-05, fluctuations: 0.0\n",
      "step: 55510 loss: 0.711243 time elapsed: 73.4552 learning rate: 0.001322, scenario: 0, slope: -8.774858389782317e-05, fluctuations: 0.0\n",
      "step: 55520 loss: 0.710408 time elapsed: 73.4687 learning rate: 0.001322, scenario: 0, slope: -8.688518822825664e-05, fluctuations: 0.0\n",
      "step: 55530 loss: 0.709580 time elapsed: 73.4830 learning rate: 0.001322, scenario: 0, slope: -8.606704235155171e-05, fluctuations: 0.0\n",
      "step: 55540 loss: 0.708758 time elapsed: 73.4969 learning rate: 0.001322, scenario: 0, slope: -8.529158409068731e-05, fluctuations: 0.0\n",
      "step: 55550 loss: 0.707942 time elapsed: 73.5103 learning rate: 0.001322, scenario: 0, slope: -8.45562861125558e-05, fluctuations: 0.0\n",
      "step: 55560 loss: 0.707131 time elapsed: 73.5242 learning rate: 0.001322, scenario: 0, slope: -8.385876134030326e-05, fluctuations: 0.0\n",
      "step: 55570 loss: 0.706325 time elapsed: 73.5378 learning rate: 0.001322, scenario: 0, slope: -8.319678769720523e-05, fluctuations: 0.0\n",
      "step: 55580 loss: 0.705524 time elapsed: 73.5513 learning rate: 0.001322, scenario: 0, slope: -8.256830589925168e-05, fluctuations: 0.0\n",
      "step: 55590 loss: 0.704728 time elapsed: 73.5644 learning rate: 0.001322, scenario: 0, slope: -8.197140895683842e-05, fluctuations: 0.0\n",
      "step: 55600 loss: 0.703937 time elapsed: 73.5766 learning rate: 0.001322, scenario: 0, slope: -8.145974357274295e-05, fluctuations: 0.0\n",
      "step: 55610 loss: 0.703150 time elapsed: 73.5894 learning rate: 0.001322, scenario: 0, slope: -8.08654290530557e-05, fluctuations: 0.0\n",
      "step: 55620 loss: 0.702366 time elapsed: 73.6018 learning rate: 0.001322, scenario: 0, slope: -8.035318389720135e-05, fluctuations: 0.0\n",
      "step: 55630 loss: 0.701587 time elapsed: 73.6140 learning rate: 0.001322, scenario: 0, slope: -7.986617767116524e-05, fluctuations: 0.0\n",
      "step: 55640 loss: 0.700812 time elapsed: 73.6263 learning rate: 0.001322, scenario: 0, slope: -7.940309067377456e-05, fluctuations: 0.0\n",
      "step: 55650 loss: 0.700039 time elapsed: 73.6387 learning rate: 0.001322, scenario: 0, slope: -7.896269179842641e-05, fluctuations: 0.0\n",
      "step: 55660 loss: 0.699271 time elapsed: 73.6509 learning rate: 0.001322, scenario: 0, slope: -7.85438310122841e-05, fluctuations: 0.0\n",
      "step: 55670 loss: 0.698505 time elapsed: 73.6633 learning rate: 0.001322, scenario: 0, slope: -7.814543258201287e-05, fluctuations: 0.0\n",
      "step: 55680 loss: 0.697743 time elapsed: 73.6764 learning rate: 0.001322, scenario: 0, slope: -7.776648896976019e-05, fluctuations: 0.0\n",
      "step: 55690 loss: 0.696983 time elapsed: 73.6906 learning rate: 0.001322, scenario: 0, slope: -7.740605532826607e-05, fluctuations: 0.0\n",
      "step: 55700 loss: 0.696226 time elapsed: 73.7042 learning rate: 0.001322, scenario: 0, slope: -7.709675674815102e-05, fluctuations: 0.0\n",
      "step: 55710 loss: 0.695472 time elapsed: 73.7181 learning rate: 0.001322, scenario: 0, slope: -7.673722269074102e-05, fluctuations: 0.0\n",
      "step: 55720 loss: 0.694720 time elapsed: 73.7318 learning rate: 0.001322, scenario: 0, slope: -7.6427205081937e-05, fluctuations: 0.0\n",
      "step: 55730 loss: 0.693970 time elapsed: 73.7457 learning rate: 0.001322, scenario: 0, slope: -7.613245247703813e-05, fluctuations: 0.0\n",
      "step: 55740 loss: 0.693223 time elapsed: 73.7590 learning rate: 0.001322, scenario: 0, slope: -7.585226779971436e-05, fluctuations: 0.0\n",
      "step: 55750 loss: 0.692477 time elapsed: 73.7714 learning rate: 0.001322, scenario: 0, slope: -7.558599309216329e-05, fluctuations: 0.0\n",
      "step: 55760 loss: 0.691734 time elapsed: 73.7842 learning rate: 0.001322, scenario: 0, slope: -7.533300675289853e-05, fluctuations: 0.0\n",
      "step: 55770 loss: 0.690993 time elapsed: 73.7968 learning rate: 0.001322, scenario: 0, slope: -7.509272101794198e-05, fluctuations: 0.0\n",
      "step: 55780 loss: 0.690253 time elapsed: 73.8091 learning rate: 0.001322, scenario: 0, slope: -7.48645796600461e-05, fluctuations: 0.0\n",
      "step: 55790 loss: 0.689515 time elapsed: 73.8214 learning rate: 0.001322, scenario: 0, slope: -7.464805588300595e-05, fluctuations: 0.0\n",
      "step: 55800 loss: 0.688779 time elapsed: 73.8334 learning rate: 0.001322, scenario: 0, slope: -7.446270430877284e-05, fluctuations: 0.0\n",
      "step: 55810 loss: 0.688044 time elapsed: 73.8462 learning rate: 0.001322, scenario: 0, slope: -7.424788962050498e-05, fluctuations: 0.0\n",
      "step: 55820 loss: 0.687311 time elapsed: 73.8586 learning rate: 0.001322, scenario: 0, slope: -7.406332410157064e-05, fluctuations: 0.0\n",
      "step: 55830 loss: 0.686579 time elapsed: 73.8710 learning rate: 0.001322, scenario: 0, slope: -7.38885269650626e-05, fluctuations: 0.0\n",
      "step: 55840 loss: 0.685848 time elapsed: 73.8841 learning rate: 0.001322, scenario: 0, slope: -7.372309255250666e-05, fluctuations: 0.0\n",
      "step: 55850 loss: 0.685119 time elapsed: 73.8981 learning rate: 0.001322, scenario: 0, slope: -7.356663513632866e-05, fluctuations: 0.0\n",
      "step: 55860 loss: 0.684390 time elapsed: 73.9121 learning rate: 0.001322, scenario: 0, slope: -7.341878763354916e-05, fluctuations: 0.0\n",
      "step: 55870 loss: 0.683663 time elapsed: 73.9259 learning rate: 0.001322, scenario: 0, slope: -7.327910524544684e-05, fluctuations: 0.0\n",
      "step: 55880 loss: 0.683074 time elapsed: 73.9397 learning rate: 0.001322, scenario: 0, slope: -7.299680395553781e-05, fluctuations: 0.0\n",
      "step: 55890 loss: 1.287342 time elapsed: 73.9533 learning rate: 0.001329, scenario: -1, slope: 0.0004954421429506956, fluctuations: 0.0\n",
      "step: 55900 loss: 7.805757 time elapsed: 73.9666 learning rate: 0.001214, scenario: -1, slope: 0.07573115164549951, fluctuations: 0.01\n",
      "step: 55910 loss: 0.988835 time elapsed: 73.9821 learning rate: 0.001098, scenario: -1, slope: 0.08805251360895557, fluctuations: 0.04\n",
      "step: 55920 loss: 2.373539 time elapsed: 73.9953 learning rate: 0.000993, scenario: -1, slope: 0.0767103313983787, fluctuations: 0.07\n",
      "step: 55930 loss: 1.101096 time elapsed: 74.0080 learning rate: 0.000898, scenario: -1, slope: 0.0586018195115285, fluctuations: 0.11\n",
      "step: 55940 loss: 0.821927 time elapsed: 74.0206 learning rate: 0.000812, scenario: -1, slope: 0.035843468426749306, fluctuations: 0.15\n",
      "step: 55950 loss: 0.729599 time elapsed: 74.0330 learning rate: 0.000734, scenario: -1, slope: 0.01224782051762914, fluctuations: 0.2\n",
      "step: 55960 loss: 0.697524 time elapsed: 74.0454 learning rate: 0.000698, scenario: 0, slope: -0.01514632506701774, fluctuations: 0.25\n",
      "step: 55970 loss: 0.687695 time elapsed: 74.0583 learning rate: 0.000698, scenario: 0, slope: -0.04593122351265851, fluctuations: 0.3\n",
      "step: 55980 loss: 0.683385 time elapsed: 74.0709 learning rate: 0.000698, scenario: 0, slope: -0.08139556308450033, fluctuations: 0.35\n",
      "step: 55990 loss: 0.681487 time elapsed: 74.0833 learning rate: 0.000698, scenario: 0, slope: -0.12565213440755385, fluctuations: 0.4\n",
      "step: 56000 loss: 0.680524 time elapsed: 74.0974 learning rate: 0.000698, scenario: 0, slope: -0.04111398802854866, fluctuations: 0.44\n",
      "step: 56010 loss: 0.679907 time elapsed: 74.1116 learning rate: 0.000698, scenario: 0, slope: -0.008642220604022846, fluctuations: 0.46\n",
      "step: 56020 loss: 0.679418 time elapsed: 74.1256 learning rate: 0.000698, scenario: 0, slope: -0.0024035089912887663, fluctuations: 0.46\n",
      "step: 56030 loss: 0.678978 time elapsed: 74.1396 learning rate: 0.000698, scenario: 0, slope: -0.0006575777817937323, fluctuations: 0.42\n",
      "step: 56040 loss: 0.678558 time elapsed: 74.1534 learning rate: 0.000698, scenario: 0, slope: -0.00011809541137268628, fluctuations: 0.38\n",
      "step: 56050 loss: 0.678148 time elapsed: 74.1680 learning rate: 0.000698, scenario: 0, slope: -8.818687778293016e-05, fluctuations: 0.33\n",
      "step: 56060 loss: 0.677744 time elapsed: 74.1807 learning rate: 0.000671, scenario: -1, slope: -5.196737476780312e-05, fluctuations: 0.28\n",
      "step: 56070 loss: 0.677370 time elapsed: 74.1934 learning rate: 0.000607, scenario: -1, slope: -4.369446568839637e-05, fluctuations: 0.23\n",
      "step: 56080 loss: 0.677033 time elapsed: 74.2060 learning rate: 0.000549, scenario: -1, slope: -4.149780171451387e-05, fluctuations: 0.18\n",
      "step: 56090 loss: 0.676729 time elapsed: 74.2181 learning rate: 0.000496, scenario: -1, slope: -4.000840838161075e-05, fluctuations: 0.13\n",
      "step: 56100 loss: 0.676455 time elapsed: 74.2301 learning rate: 0.000453, scenario: -1, slope: -3.8821762949460465e-05, fluctuations: 0.08\n",
      "step: 56110 loss: 0.676204 time elapsed: 74.2429 learning rate: 0.000410, scenario: -1, slope: -3.6639658357247786e-05, fluctuations: 0.03\n",
      "step: 56120 loss: 0.675974 time elapsed: 74.2552 learning rate: 0.000416, scenario: 1, slope: -3.460862110829716e-05, fluctuations: 0.0\n",
      "step: 56130 loss: 0.675727 time elapsed: 74.2675 learning rate: 0.000459, scenario: 1, slope: -3.222288170497585e-05, fluctuations: 0.0\n",
      "step: 56140 loss: 0.675453 time elapsed: 74.2800 learning rate: 0.000508, scenario: 1, slope: -3.0039770280923706e-05, fluctuations: 0.0\n",
      "step: 56150 loss: 0.675151 time elapsed: 74.2923 learning rate: 0.000561, scenario: 1, slope: -2.8342361401364155e-05, fluctuations: 0.0\n",
      "step: 56160 loss: 0.674818 time elapsed: 74.3054 learning rate: 0.000619, scenario: 1, slope: -2.7408629085958435e-05, fluctuations: 0.0\n",
      "step: 56170 loss: 0.674449 time elapsed: 74.3191 learning rate: 0.000684, scenario: 1, slope: -2.7414818214699443e-05, fluctuations: 0.0\n",
      "step: 56180 loss: 0.674041 time elapsed: 74.3326 learning rate: 0.000756, scenario: 1, slope: -2.836321122033683e-05, fluctuations: 0.0\n",
      "step: 56190 loss: 0.673591 time elapsed: 74.3462 learning rate: 0.000835, scenario: 1, slope: -3.0200105242870427e-05, fluctuations: 0.0\n",
      "step: 56200 loss: 0.673095 time elapsed: 74.3599 learning rate: 0.000913, scenario: 1, slope: -3.2534228556534305e-05, fluctuations: 0.0\n",
      "step: 56210 loss: 0.672551 time elapsed: 74.3745 learning rate: 0.001008, scenario: 1, slope: -3.6102867218638766e-05, fluctuations: 0.0\n",
      "step: 56220 loss: 0.671952 time elapsed: 74.3876 learning rate: 0.001114, scenario: 1, slope: -3.9833919231641336e-05, fluctuations: 0.0\n",
      "step: 56230 loss: 0.671291 time elapsed: 74.4002 learning rate: 0.001230, scenario: 1, slope: -4.393127167379785e-05, fluctuations: 0.0\n",
      "step: 56240 loss: 0.670562 time elapsed: 74.4125 learning rate: 0.001359, scenario: 1, slope: -4.843003896372709e-05, fluctuations: 0.0\n",
      "step: 56250 loss: 0.669759 time elapsed: 74.4248 learning rate: 0.001501, scenario: 1, slope: -5.3375047080779545e-05, fluctuations: 0.0\n",
      "step: 56260 loss: 52.214334 time elapsed: 74.4374 learning rate: 0.001539, scenario: -1, slope: 0.035503084470610013, fluctuations: 0.0\n",
      "step: 56270 loss: 65.614552 time elapsed: 74.4500 learning rate: 0.001392, scenario: -1, slope: 0.9501049160059185, fluctuations: 0.04\n",
      "step: 56280 loss: 99.063563 time elapsed: 74.4624 learning rate: 0.001259, scenario: -1, slope: 1.031274040647745, fluctuations: 0.08\n",
      "step: 56290 loss: 15.400533 time elapsed: 74.4748 learning rate: 0.001138, scenario: -1, slope: 0.8885817144015621, fluctuations: 0.13\n",
      "step: 56300 loss: 10.456750 time elapsed: 74.4870 learning rate: 0.001040, scenario: -1, slope: 0.6680016722998956, fluctuations: 0.16\n",
      "step: 56310 loss: 4.687678 time elapsed: 74.4998 learning rate: 0.000940, scenario: -1, slope: 0.3588901783237884, fluctuations: 0.2\n",
      "step: 56320 loss: 1.323707 time elapsed: 74.5131 learning rate: 0.000850, scenario: -1, slope: 0.06875175609526743, fluctuations: 0.24\n",
      "step: 56330 loss: 1.090740 time elapsed: 74.5269 learning rate: 0.000842, scenario: 0, slope: -0.25388268188040825, fluctuations: 0.27\n",
      "step: 56340 loss: 1.020493 time elapsed: 74.5408 learning rate: 0.000842, scenario: 0, slope: -0.5988574507727715, fluctuations: 0.3\n",
      "step: 56350 loss: 0.842328 time elapsed: 74.5545 learning rate: 0.000842, scenario: 0, slope: -1.0391457462854834, fluctuations: 0.34\n",
      "step: 56360 loss: 0.838450 time elapsed: 74.5685 learning rate: 0.000842, scenario: 0, slope: -1.6295057198389882, fluctuations: 0.37\n",
      "step: 56370 loss: 0.813971 time elapsed: 74.5825 learning rate: 0.000842, scenario: 0, slope: -0.5737018560752943, fluctuations: 0.36\n",
      "step: 56380 loss: 0.799332 time elapsed: 74.5958 learning rate: 0.000842, scenario: 0, slope: -0.12401252062002203, fluctuations: 0.35\n",
      "step: 56390 loss: 0.791001 time elapsed: 74.6085 learning rate: 0.000842, scenario: 0, slope: -0.05117566735412281, fluctuations: 0.33\n",
      "step: 56400 loss: 0.782888 time elapsed: 74.6207 learning rate: 0.000842, scenario: 0, slope: -0.023292190399580522, fluctuations: 0.29\n",
      "step: 56410 loss: 0.776824 time elapsed: 74.6334 learning rate: 0.000842, scenario: 0, slope: -0.007516165437608445, fluctuations: 0.25\n",
      "step: 56420 loss: 0.771438 time elapsed: 74.6458 learning rate: 0.000842, scenario: 0, slope: -0.0031238381010748727, fluctuations: 0.22\n",
      "step: 56430 loss: 0.766822 time elapsed: 74.6582 learning rate: 0.000842, scenario: 0, slope: -0.001997291295049702, fluctuations: 0.18\n",
      "step: 56440 loss: 0.762749 time elapsed: 74.6706 learning rate: 0.000842, scenario: 0, slope: -0.0011131961968217237, fluctuations: 0.15\n",
      "step: 56450 loss: 0.759133 time elapsed: 74.6830 learning rate: 0.000842, scenario: 0, slope: -0.0008091765073275913, fluctuations: 0.12\n",
      "step: 56460 loss: 0.755888 time elapsed: 74.6952 learning rate: 0.000842, scenario: 0, slope: -0.0006381320564944743, fluctuations: 0.08\n",
      "step: 56470 loss: 0.752955 time elapsed: 74.7076 learning rate: 0.000842, scenario: 0, slope: -0.0005256536963944701, fluctuations: 0.05\n",
      "step: 56480 loss: 0.750282 time elapsed: 74.7210 learning rate: 0.000842, scenario: 0, slope: -0.00046241917692179426, fluctuations: 0.01\n",
      "step: 56490 loss: 0.747831 time elapsed: 74.7345 learning rate: 0.000842, scenario: 0, slope: -0.0004054145834057162, fluctuations: 0.0\n",
      "step: 56500 loss: 0.745569 time elapsed: 74.7476 learning rate: 0.000842, scenario: 0, slope: -0.00036290160948884855, fluctuations: 0.0\n",
      "step: 56510 loss: 0.743469 time elapsed: 74.7617 learning rate: 0.000842, scenario: 0, slope: -0.00032090814892959686, fluctuations: 0.0\n",
      "step: 56520 loss: 0.741510 time elapsed: 74.7753 learning rate: 0.000842, scenario: 0, slope: -0.0002897570281925637, fluctuations: 0.0\n",
      "step: 56530 loss: 0.739674 time elapsed: 74.7887 learning rate: 0.000842, scenario: 0, slope: -0.00026372535369518325, fluctuations: 0.0\n",
      "step: 56540 loss: 0.737945 time elapsed: 74.8028 learning rate: 0.000842, scenario: 0, slope: -0.0002417498568257841, fluctuations: 0.0\n",
      "step: 56550 loss: 0.736311 time elapsed: 74.8153 learning rate: 0.000842, scenario: 0, slope: -0.00022303789881711054, fluctuations: 0.0\n",
      "step: 56560 loss: 0.734761 time elapsed: 74.8276 learning rate: 0.000842, scenario: 0, slope: -0.00020697983824005187, fluctuations: 0.0\n",
      "step: 56570 loss: 0.733286 time elapsed: 74.8402 learning rate: 0.000842, scenario: 0, slope: -0.0001931005520132505, fluctuations: 0.0\n",
      "step: 56580 loss: 0.731879 time elapsed: 74.8525 learning rate: 0.000842, scenario: 0, slope: -0.00018102219026078334, fluctuations: 0.0\n",
      "step: 56590 loss: 0.730532 time elapsed: 74.8647 learning rate: 0.000842, scenario: 0, slope: -0.0001704424822123908, fluctuations: 0.0\n",
      "step: 56600 loss: 0.729240 time elapsed: 74.8767 learning rate: 0.000842, scenario: 0, slope: -0.0001619993403740547, fluctuations: 0.0\n",
      "step: 56610 loss: 0.727999 time elapsed: 74.8894 learning rate: 0.000842, scenario: 0, slope: -0.0001528484614267046, fluctuations: 0.0\n",
      "step: 56620 loss: 0.726803 time elapsed: 74.9017 learning rate: 0.000842, scenario: 0, slope: -0.00014547399972983166, fluctuations: 0.0\n",
      "step: 56630 loss: 0.725649 time elapsed: 74.9142 learning rate: 0.000842, scenario: 0, slope: -0.0001388610344062974, fluctuations: 0.0\n",
      "step: 56640 loss: 0.724534 time elapsed: 74.9272 learning rate: 0.000842, scenario: 0, slope: -0.0001328999744939589, fluctuations: 0.0\n",
      "step: 56650 loss: 0.723455 time elapsed: 74.9410 learning rate: 0.000842, scenario: 0, slope: -0.00012750004822556854, fluctuations: 0.0\n",
      "step: 56660 loss: 0.722409 time elapsed: 74.9548 learning rate: 0.000842, scenario: 0, slope: -0.00012258575082649552, fluctuations: 0.0\n",
      "step: 56670 loss: 0.721394 time elapsed: 74.9684 learning rate: 0.000842, scenario: 0, slope: -0.00011809401377106754, fluctuations: 0.0\n",
      "step: 56680 loss: 0.720408 time elapsed: 74.9819 learning rate: 0.000842, scenario: 0, slope: -0.00011397193882446554, fluctuations: 0.0\n",
      "step: 56690 loss: 0.719449 time elapsed: 74.9956 learning rate: 0.000842, scenario: 0, slope: -0.00011017497494362185, fluctuations: 0.0\n",
      "step: 56700 loss: 0.718514 time elapsed: 75.0092 learning rate: 0.000842, scenario: 0, slope: -0.00010700441566082361, fluctuations: 0.0\n",
      "step: 56710 loss: 0.717604 time elapsed: 75.0222 learning rate: 0.000842, scenario: 0, slope: -0.00010341135441145105, fluctuations: 0.0\n",
      "step: 56720 loss: 0.716716 time elapsed: 75.0346 learning rate: 0.000842, scenario: 0, slope: -0.00010038541314169704, fluctuations: 0.0\n",
      "step: 56730 loss: 0.715849 time elapsed: 75.0470 learning rate: 0.000842, scenario: 0, slope: -9.756424791313586e-05, fluctuations: 0.0\n",
      "step: 56740 loss: 0.715001 time elapsed: 75.0594 learning rate: 0.000842, scenario: 0, slope: -9.492774912023387e-05, fluctuations: 0.0\n",
      "step: 56750 loss: 0.714172 time elapsed: 75.0719 learning rate: 0.000842, scenario: 0, slope: -9.24585374475598e-05, fluctuations: 0.0\n",
      "step: 56760 loss: 0.713361 time elapsed: 75.0841 learning rate: 0.000842, scenario: 0, slope: -9.014152413254127e-05, fluctuations: 0.0\n",
      "step: 56770 loss: 0.712566 time elapsed: 75.0966 learning rate: 0.000842, scenario: 0, slope: -8.79635479517537e-05, fluctuations: 0.0\n",
      "step: 56780 loss: 0.711787 time elapsed: 75.1088 learning rate: 0.000842, scenario: 0, slope: -8.591307485367757e-05, fluctuations: 0.0\n",
      "step: 56790 loss: 0.711023 time elapsed: 75.1212 learning rate: 0.000842, scenario: 0, slope: -8.39799488805759e-05, fluctuations: 0.0\n",
      "step: 56800 loss: 0.710273 time elapsed: 75.1355 learning rate: 0.000842, scenario: 0, slope: -8.233301998589053e-05, fluctuations: 0.0\n",
      "step: 56810 loss: 0.709537 time elapsed: 75.1501 learning rate: 0.000842, scenario: 0, slope: -8.043079763601273e-05, fluctuations: 0.0\n",
      "step: 56820 loss: 0.708813 time elapsed: 75.1637 learning rate: 0.000842, scenario: 0, slope: -7.879965500970631e-05, fluctuations: 0.0\n",
      "step: 56830 loss: 0.708102 time elapsed: 75.1777 learning rate: 0.000842, scenario: 0, slope: -7.725536063636748e-05, fluctuations: 0.0\n",
      "step: 56840 loss: 0.707402 time elapsed: 75.1912 learning rate: 0.000842, scenario: 0, slope: -7.579215157144381e-05, fluctuations: 0.0\n",
      "step: 56850 loss: 0.706713 time elapsed: 75.2049 learning rate: 0.000842, scenario: 0, slope: -7.440481385159385e-05, fluctuations: 0.0\n",
      "step: 56860 loss: 0.706034 time elapsed: 75.2193 learning rate: 0.000842, scenario: 0, slope: -7.308861111269019e-05, fluctuations: 0.0\n",
      "step: 56870 loss: 0.705366 time elapsed: 75.2325 learning rate: 0.000842, scenario: 0, slope: -7.183922429707516e-05, fluctuations: 0.0\n",
      "step: 56880 loss: 0.704702 time elapsed: 75.2459 learning rate: 0.000885, scenario: 1, slope: -7.06565744999912e-05, fluctuations: 0.0\n",
      "step: 56890 loss: 0.703995 time elapsed: 75.2583 learning rate: 0.000977, scenario: 1, slope: -6.96952727863093e-05, fluctuations: 0.0\n",
      "step: 56900 loss: 0.703225 time elapsed: 75.2705 learning rate: 0.001069, scenario: 1, slope: -6.928778832093148e-05, fluctuations: 0.0\n",
      "step: 56910 loss: 0.702395 time elapsed: 75.2834 learning rate: 0.001181, scenario: 1, slope: -6.966390340905035e-05, fluctuations: 0.0\n",
      "step: 56920 loss: 0.701496 time elapsed: 75.2955 learning rate: 0.001291, scenario: 0, slope: -7.103979798521816e-05, fluctuations: 0.0\n",
      "step: 56930 loss: 0.700564 time elapsed: 75.3081 learning rate: 0.001291, scenario: 0, slope: -7.34665188208061e-05, fluctuations: 0.0\n",
      "step: 56940 loss: 0.699650 time elapsed: 75.3203 learning rate: 0.001291, scenario: 0, slope: -7.659931424362114e-05, fluctuations: 0.0\n",
      "step: 56950 loss: 0.698752 time elapsed: 75.3325 learning rate: 0.001291, scenario: 0, slope: -8.00127179230222e-05, fluctuations: 0.0\n",
      "step: 56960 loss: 0.697870 time elapsed: 75.3458 learning rate: 0.001291, scenario: 0, slope: -8.329733648517203e-05, fluctuations: 0.0\n",
      "step: 56970 loss: 0.697001 time elapsed: 75.3596 learning rate: 0.001291, scenario: 0, slope: -8.60591971051262e-05, fluctuations: 0.0\n",
      "step: 56980 loss: 0.696146 time elapsed: 75.3733 learning rate: 0.001291, scenario: 0, slope: -8.792270204817769e-05, fluctuations: 0.0\n",
      "step: 56990 loss: 0.695304 time elapsed: 75.3873 learning rate: 0.001291, scenario: 0, slope: -8.870464196243206e-05, fluctuations: 0.0\n",
      "step: 57000 loss: 0.694473 time elapsed: 75.4011 learning rate: 0.001291, scenario: 0, slope: -8.858007461628931e-05, fluctuations: 0.0\n",
      "step: 57010 loss: 0.693653 time elapsed: 75.4159 learning rate: 0.001291, scenario: 0, slope: -8.761410441461758e-05, fluctuations: 0.0\n",
      "step: 57020 loss: 0.692843 time elapsed: 75.4288 learning rate: 0.001291, scenario: 0, slope: -8.631234344183657e-05, fluctuations: 0.0\n",
      "step: 57030 loss: 0.692042 time elapsed: 75.4415 learning rate: 0.001291, scenario: 0, slope: -8.50150585090096e-05, fluctuations: 0.0\n",
      "step: 57040 loss: 0.691250 time elapsed: 75.4540 learning rate: 0.001291, scenario: 0, slope: -8.380560566607945e-05, fluctuations: 0.0\n",
      "step: 57050 loss: 0.690466 time elapsed: 75.4666 learning rate: 0.001291, scenario: 0, slope: -8.267857690841475e-05, fluctuations: 0.0\n",
      "step: 57060 loss: 0.689690 time elapsed: 75.4789 learning rate: 0.001291, scenario: 0, slope: -8.162824208169353e-05, fluctuations: 0.0\n",
      "step: 57070 loss: 0.688921 time elapsed: 75.4910 learning rate: 0.001291, scenario: 0, slope: -8.06490824796529e-05, fluctuations: 0.0\n",
      "step: 57080 loss: 0.688158 time elapsed: 75.5034 learning rate: 0.001291, scenario: 0, slope: -7.973594595793584e-05, fluctuations: 0.0\n",
      "step: 57090 loss: 0.687402 time elapsed: 75.5158 learning rate: 0.001291, scenario: 0, slope: -7.88840731910654e-05, fluctuations: 0.0\n",
      "step: 57100 loss: 0.686651 time elapsed: 75.5282 learning rate: 0.001291, scenario: 0, slope: -7.816613900192516e-05, fluctuations: 0.0\n",
      "step: 57110 loss: 0.685906 time elapsed: 75.5411 learning rate: 0.001291, scenario: 0, slope: -7.734693993121844e-05, fluctuations: 0.0\n",
      "step: 57120 loss: 0.685166 time elapsed: 75.5544 learning rate: 0.001291, scenario: 0, slope: -7.665393470290534e-05, fluctuations: 0.0\n",
      "step: 57130 loss: 0.684431 time elapsed: 75.5685 learning rate: 0.001291, scenario: 0, slope: -7.600664660554857e-05, fluctuations: 0.0\n",
      "step: 57140 loss: 0.683700 time elapsed: 75.5824 learning rate: 0.001291, scenario: 0, slope: -7.540192286042381e-05, fluctuations: 0.0\n",
      "step: 57150 loss: 0.682973 time elapsed: 75.5960 learning rate: 0.001291, scenario: 0, slope: -7.483685452765502e-05, fluctuations: 0.0\n",
      "step: 57160 loss: 0.682251 time elapsed: 75.6099 learning rate: 0.001291, scenario: 0, slope: -7.430875562750235e-05, fluctuations: 0.0\n",
      "step: 57170 loss: 0.681531 time elapsed: 75.6236 learning rate: 0.001291, scenario: 0, slope: -7.38151442776869e-05, fluctuations: 0.0\n",
      "step: 57180 loss: 0.680816 time elapsed: 75.6382 learning rate: 0.001291, scenario: 0, slope: -7.335372562981616e-05, fluctuations: 0.0\n",
      "step: 57190 loss: 0.680103 time elapsed: 75.6527 learning rate: 0.001291, scenario: 0, slope: -7.292237640744416e-05, fluctuations: 0.0\n",
      "step: 57200 loss: 0.679393 time elapsed: 75.6653 learning rate: 0.001291, scenario: 0, slope: -7.255824373723904e-05, fluctuations: 0.0\n",
      "step: 57210 loss: 0.678686 time elapsed: 75.6784 learning rate: 0.001291, scenario: 0, slope: -7.21421680645903e-05, fluctuations: 0.0\n",
      "step: 57220 loss: 0.677982 time elapsed: 75.6913 learning rate: 0.001291, scenario: 0, slope: -7.178980018868274e-05, fluctuations: 0.0\n",
      "step: 57230 loss: 0.677280 time elapsed: 75.7042 learning rate: 0.001291, scenario: 0, slope: -7.146046201992389e-05, fluctuations: 0.0\n",
      "step: 57240 loss: 0.676580 time elapsed: 75.7167 learning rate: 0.001291, scenario: 0, slope: -7.115270123670613e-05, fluctuations: 0.0\n",
      "step: 57250 loss: 0.675882 time elapsed: 75.7288 learning rate: 0.001291, scenario: 0, slope: -7.086516958356084e-05, fluctuations: 0.0\n",
      "step: 57260 loss: 0.675187 time elapsed: 75.7409 learning rate: 0.001291, scenario: 0, slope: -7.059661478493044e-05, fluctuations: 0.0\n",
      "step: 57270 loss: 0.674493 time elapsed: 75.7531 learning rate: 0.001291, scenario: 0, slope: -7.034587313921028e-05, fluctuations: 0.0\n",
      "step: 57280 loss: 0.673801 time elapsed: 75.7670 learning rate: 0.001291, scenario: 0, slope: -7.011186272884173e-05, fluctuations: 0.0\n",
      "step: 57290 loss: 0.673110 time elapsed: 75.7810 learning rate: 0.001291, scenario: 0, slope: -6.989357719019069e-05, fluctuations: 0.0\n",
      "step: 57300 loss: 0.672421 time elapsed: 75.7953 learning rate: 0.001291, scenario: 0, slope: -6.970978957075469e-05, fluctuations: 0.0\n",
      "step: 57310 loss: 0.671733 time elapsed: 75.8096 learning rate: 0.001291, scenario: 0, slope: -6.9500499177296e-05, fluctuations: 0.0\n",
      "step: 57320 loss: 0.671046 time elapsed: 75.8236 learning rate: 0.001291, scenario: 0, slope: -6.932402252744482e-05, fluctuations: 0.0\n",
      "step: 57330 loss: 0.670361 time elapsed: 75.8378 learning rate: 0.001291, scenario: 0, slope: -6.915989311164995e-05, fluctuations: 0.0\n",
      "step: 57340 loss: 0.669677 time elapsed: 75.8506 learning rate: 0.001291, scenario: 0, slope: -6.900740518859752e-05, fluctuations: 0.0\n",
      "step: 57350 loss: 0.668993 time elapsed: 75.8632 learning rate: 0.001291, scenario: 0, slope: -6.886590043059833e-05, fluctuations: 0.0\n",
      "step: 57360 loss: 0.668311 time elapsed: 75.8756 learning rate: 0.001291, scenario: 0, slope: -6.873476444244883e-05, fluctuations: 0.0\n",
      "step: 57370 loss: 0.667629 time elapsed: 75.8880 learning rate: 0.001291, scenario: 0, slope: -6.861342355043755e-05, fluctuations: 0.0\n",
      "step: 57380 loss: 0.666949 time elapsed: 75.9003 learning rate: 0.001291, scenario: 0, slope: -6.850134183937973e-05, fluctuations: 0.0\n",
      "step: 57390 loss: 0.666268 time elapsed: 75.9126 learning rate: 0.001291, scenario: 0, slope: -6.839801841727917e-05, fluctuations: 0.0\n",
      "step: 57400 loss: 0.665589 time elapsed: 75.9248 learning rate: 0.001291, scenario: 0, slope: -6.831212791256013e-05, fluctuations: 0.0\n",
      "step: 57410 loss: 0.664910 time elapsed: 75.9378 learning rate: 0.001291, scenario: 0, slope: -6.82158030217521e-05, fluctuations: 0.0\n",
      "step: 57420 loss: 0.664231 time elapsed: 75.9501 learning rate: 0.001291, scenario: 0, slope: -6.813606258767239e-05, fluctuations: 0.0\n",
      "step: 57430 loss: 0.663553 time elapsed: 75.9625 learning rate: 0.001291, scenario: 0, slope: -6.80633793687533e-05, fluctuations: 0.0\n",
      "step: 57440 loss: 0.662876 time elapsed: 75.9754 learning rate: 0.001291, scenario: 0, slope: -6.799739331173846e-05, fluctuations: 0.0\n",
      "step: 57450 loss: 0.662198 time elapsed: 75.9894 learning rate: 0.001291, scenario: 0, slope: -6.793776682071558e-05, fluctuations: 0.0\n",
      "step: 57460 loss: 0.661521 time elapsed: 76.0030 learning rate: 0.001291, scenario: 0, slope: -6.788418317746296e-05, fluctuations: 0.0\n",
      "step: 57470 loss: 0.660845 time elapsed: 76.0167 learning rate: 0.001291, scenario: 0, slope: -6.783634507840305e-05, fluctuations: 0.0\n",
      "step: 57480 loss: 0.660168 time elapsed: 76.0302 learning rate: 0.001291, scenario: 0, slope: -6.779397327901648e-05, fluctuations: 0.0\n",
      "step: 57490 loss: 0.659492 time elapsed: 76.0439 learning rate: 0.001291, scenario: 0, slope: -6.775680422219067e-05, fluctuations: 0.0\n",
      "step: 57500 loss: 0.658817 time elapsed: 76.0573 learning rate: 0.001291, scenario: 0, slope: -6.772689392327355e-05, fluctuations: 0.0\n",
      "step: 57510 loss: 0.661126 time elapsed: 76.0703 learning rate: 0.001317, scenario: 1, slope: -6.450124256124051e-05, fluctuations: 0.0\n",
      "step: 57520 loss: 61.079286 time elapsed: 76.0827 learning rate: 0.001272, scenario: -1, slope: 0.06214829444310213, fluctuations: 0.0\n",
      "step: 57530 loss: 12.712492 time elapsed: 76.0951 learning rate: 0.001150, scenario: -1, slope: 0.15072734576483685, fluctuations: 0.03\n",
      "step: 57540 loss: 4.064215 time elapsed: 76.1075 learning rate: 0.001040, scenario: -1, slope: 0.15091220523466192, fluctuations: 0.06\n",
      "step: 57550 loss: 2.616109 time elapsed: 76.1200 learning rate: 0.000941, scenario: -1, slope: 0.12622100669207492, fluctuations: 0.1\n",
      "step: 57560 loss: 1.053521 time elapsed: 76.1321 learning rate: 0.000851, scenario: -1, slope: 0.08335670096100724, fluctuations: 0.13\n",
      "step: 57570 loss: 0.860599 time elapsed: 76.1453 learning rate: 0.000769, scenario: -1, slope: 0.04723912620001968, fluctuations: 0.18\n",
      "step: 57580 loss: 0.697176 time elapsed: 76.1580 learning rate: 0.000696, scenario: -1, slope: 0.0030706462793830746, fluctuations: 0.23\n",
      "step: 57590 loss: 0.668807 time elapsed: 76.1704 learning rate: 0.000689, scenario: 0, slope: -0.04730095385712969, fluctuations: 0.28\n",
      "step: 57600 loss: 0.661843 time elapsed: 76.1857 learning rate: 0.000689, scenario: 0, slope: -0.09836977502884856, fluctuations: 0.31\n",
      "step: 57610 loss: 0.660828 time elapsed: 76.2004 learning rate: 0.000689, scenario: 0, slope: -0.1719655909452409, fluctuations: 0.36\n",
      "step: 57620 loss: 0.657852 time elapsed: 76.2145 learning rate: 0.000689, scenario: 0, slope: -0.23507595651521748, fluctuations: 0.4\n",
      "step: 57630 loss: 0.657517 time elapsed: 76.2283 learning rate: 0.000689, scenario: 0, slope: -0.03881418589667318, fluctuations: 0.42\n",
      "step: 57640 loss: 0.656944 time elapsed: 76.2420 learning rate: 0.000689, scenario: 0, slope: -0.01055823466503844, fluctuations: 0.4\n",
      "step: 57650 loss: 0.656531 time elapsed: 76.2558 learning rate: 0.000689, scenario: 0, slope: -0.0032689670374600162, fluctuations: 0.36\n",
      "step: 57660 loss: 0.656101 time elapsed: 76.2697 learning rate: 0.000689, scenario: 0, slope: -0.0006873138778439222, fluctuations: 0.33\n",
      "step: 57670 loss: 0.655710 time elapsed: 76.2825 learning rate: 0.000689, scenario: 0, slope: -0.00018604581100145365, fluctuations: 0.28\n",
      "step: 57680 loss: 0.655326 time elapsed: 76.2957 learning rate: 0.000689, scenario: 0, slope: -0.00014191033477717013, fluctuations: 0.23\n",
      "step: 57690 loss: 0.654946 time elapsed: 76.3083 learning rate: 0.000689, scenario: 0, slope: -8.721161761166315e-05, fluctuations: 0.18\n",
      "step: 57700 loss: 0.654577 time elapsed: 76.3207 learning rate: 0.000649, scenario: -1, slope: -5.227437434722029e-05, fluctuations: 0.15\n",
      "step: 57710 loss: 0.654239 time elapsed: 76.3334 learning rate: 0.000586, scenario: -1, slope: -4.170025183274463e-05, fluctuations: 0.1\n",
      "step: 57720 loss: 0.653935 time elapsed: 76.3457 learning rate: 0.000530, scenario: -1, slope: -3.977708520874859e-05, fluctuations: 0.05\n",
      "step: 57730 loss: 0.653661 time elapsed: 76.3582 learning rate: 0.000480, scenario: -1, slope: -3.757399597286125e-05, fluctuations: 0.02\n",
      "step: 57740 loss: 0.653405 time elapsed: 76.3704 learning rate: 0.000507, scenario: 1, slope: -3.577113571463532e-05, fluctuations: 0.0\n",
      "step: 57750 loss: 0.653126 time elapsed: 76.3826 learning rate: 0.000560, scenario: 1, slope: -3.3895970832467626e-05, fluctuations: 0.0\n",
      "step: 57760 loss: 0.652819 time elapsed: 76.3958 learning rate: 0.000618, scenario: 1, slope: -3.227158220367602e-05, fluctuations: 0.0\n",
      "step: 57770 loss: 0.652480 time elapsed: 76.4101 learning rate: 0.000683, scenario: 1, slope: -3.104000632169794e-05, fluctuations: 0.0\n",
      "step: 57780 loss: 0.652108 time elapsed: 76.4240 learning rate: 0.000754, scenario: 1, slope: -3.0403870903873927e-05, fluctuations: 0.0\n",
      "step: 57790 loss: 0.651697 time elapsed: 76.4376 learning rate: 0.000833, scenario: 1, slope: -3.05508215258702e-05, fluctuations: 0.0\n",
      "step: 57800 loss: 0.651245 time elapsed: 76.4511 learning rate: 0.000911, scenario: 1, slope: -3.148882219079337e-05, fluctuations: 0.0\n",
      "step: 57810 loss: 0.650752 time elapsed: 76.4651 learning rate: 0.001007, scenario: 1, slope: -3.3686624693264744e-05, fluctuations: 0.0\n",
      "step: 57820 loss: 0.650210 time elapsed: 76.4785 learning rate: 0.001112, scenario: 1, slope: -3.655735727365483e-05, fluctuations: 0.0\n",
      "step: 57830 loss: 0.649613 time elapsed: 76.4911 learning rate: 0.001228, scenario: 1, slope: -4.008503927419491e-05, fluctuations: 0.0\n",
      "step: 57840 loss: 0.648957 time elapsed: 76.5034 learning rate: 0.001357, scenario: 1, slope: -4.406680236541726e-05, fluctuations: 0.0\n",
      "step: 57850 loss: 0.648234 time elapsed: 76.5158 learning rate: 0.001499, scenario: 1, slope: -4.843904987462966e-05, fluctuations: 0.0\n",
      "step: 57860 loss: 0.647444 time elapsed: 76.5283 learning rate: 0.001655, scenario: 1, slope: -5.324145333191514e-05, fluctuations: 0.0\n",
      "step: 57870 loss: 758.333788 time elapsed: 76.5406 learning rate: 0.001663, scenario: -1, slope: 0.5420757722836723, fluctuations: 0.0\n",
      "step: 57880 loss: 291.319055 time elapsed: 76.5530 learning rate: 0.001504, scenario: -1, slope: 1.631375051202399, fluctuations: 0.04\n",
      "step: 57890 loss: 81.342316 time elapsed: 76.5655 learning rate: 0.001360, scenario: -1, slope: 1.6745253269505183, fluctuations: 0.09\n",
      "step: 57900 loss: 21.433592 time elapsed: 76.5778 learning rate: 0.001243, scenario: -1, slope: 1.434730438503146, fluctuations: 0.12\n",
      "step: 57910 loss: 13.394587 time elapsed: 76.5906 learning rate: 0.001124, scenario: -1, slope: 0.9855113193558502, fluctuations: 0.16\n",
      "step: 57920 loss: 2.891104 time elapsed: 76.6037 learning rate: 0.001016, scenario: -1, slope: 0.5414928608490123, fluctuations: 0.2\n",
      "step: 57930 loss: 2.298376 time elapsed: 76.6176 learning rate: 0.000919, scenario: -1, slope: 0.032523858897799456, fluctuations: 0.23\n",
      "step: 57940 loss: 1.083550 time elapsed: 76.6315 learning rate: 0.000910, scenario: 0, slope: -0.4665804015300871, fluctuations: 0.27\n",
      "step: 57950 loss: 0.970351 time elapsed: 76.6454 learning rate: 0.000910, scenario: 0, slope: -1.0418814955185798, fluctuations: 0.3\n",
      "step: 57960 loss: 0.936460 time elapsed: 76.6590 learning rate: 0.000910, scenario: 0, slope: -1.7511288283688475, fluctuations: 0.33\n",
      "step: 57970 loss: 0.869306 time elapsed: 76.6727 learning rate: 0.000910, scenario: 0, slope: -3.3304045781201226, fluctuations: 0.36\n",
      "step: 57980 loss: 0.846905 time elapsed: 76.6878 learning rate: 0.000910, scenario: 0, slope: -0.5736172531068762, fluctuations: 0.35\n",
      "step: 57990 loss: 0.834209 time elapsed: 76.7004 learning rate: 0.000910, scenario: 0, slope: -0.14756484995369895, fluctuations: 0.34\n",
      "step: 58000 loss: 0.820893 time elapsed: 76.7128 learning rate: 0.000910, scenario: 0, slope: -0.06692860700722908, fluctuations: 0.31\n",
      "step: 58010 loss: 0.810815 time elapsed: 76.7260 learning rate: 0.000910, scenario: 0, slope: -0.021621616062819253, fluctuations: 0.27\n",
      "step: 58020 loss: 0.802056 time elapsed: 76.7383 learning rate: 0.000910, scenario: 0, slope: -0.00982357151771283, fluctuations: 0.23\n",
      "step: 58030 loss: 0.794139 time elapsed: 76.7508 learning rate: 0.000910, scenario: 0, slope: -0.003472435472123549, fluctuations: 0.2\n",
      "step: 58040 loss: 0.787080 time elapsed: 76.7629 learning rate: 0.000910, scenario: 0, slope: -0.002043871417747911, fluctuations: 0.17\n",
      "step: 58050 loss: 0.780656 time elapsed: 76.7753 learning rate: 0.000910, scenario: 0, slope: -0.0014601664241182001, fluctuations: 0.13\n",
      "step: 58060 loss: 0.774791 time elapsed: 76.7875 learning rate: 0.000910, scenario: 0, slope: -0.0010615936346198161, fluctuations: 0.1\n",
      "step: 58070 loss: 0.769412 time elapsed: 76.8001 learning rate: 0.000910, scenario: 0, slope: -0.0008797824089526963, fluctuations: 0.07\n",
      "step: 58080 loss: 0.764458 time elapsed: 76.8140 learning rate: 0.000910, scenario: 0, slope: -0.0007804270659800146, fluctuations: 0.03\n",
      "step: 58090 loss: 0.759882 time elapsed: 76.8280 learning rate: 0.000910, scenario: 0, slope: -0.0007021145245940297, fluctuations: 0.0\n",
      "step: 58100 loss: 0.755642 time elapsed: 76.8416 learning rate: 0.000910, scenario: 0, slope: -0.0006391931037959834, fluctuations: 0.0\n",
      "step: 58110 loss: 0.751704 time elapsed: 76.8558 learning rate: 0.000910, scenario: 0, slope: -0.0005760644707761488, fluctuations: 0.0\n",
      "step: 58120 loss: 0.748038 time elapsed: 76.8701 learning rate: 0.000910, scenario: 0, slope: -0.0005274926337472565, fluctuations: 0.0\n",
      "step: 58130 loss: 0.744617 time elapsed: 76.8836 learning rate: 0.000910, scenario: 0, slope: -0.0004853088791486999, fluctuations: 0.0\n",
      "step: 58140 loss: 0.741418 time elapsed: 76.8967 learning rate: 0.000910, scenario: 0, slope: -0.00044812585555864244, fluctuations: 0.0\n",
      "step: 58150 loss: 0.738421 time elapsed: 76.9092 learning rate: 0.000910, scenario: 0, slope: -0.00041508081109254015, fluctuations: 0.0\n",
      "step: 58160 loss: 0.735607 time elapsed: 76.9218 learning rate: 0.000910, scenario: 0, slope: -0.000385529323400218, fluctuations: 0.0\n",
      "step: 58170 loss: 0.732960 time elapsed: 76.9342 learning rate: 0.000910, scenario: 0, slope: -0.0003589751737825367, fluctuations: 0.0\n",
      "step: 58180 loss: 0.730465 time elapsed: 76.9464 learning rate: 0.000910, scenario: 0, slope: -0.0003350255000111029, fluctuations: 0.0\n",
      "step: 58190 loss: 0.728110 time elapsed: 76.9585 learning rate: 0.000910, scenario: 0, slope: -0.00031335728706515225, fluctuations: 0.0\n",
      "step: 58200 loss: 0.725883 time elapsed: 76.9704 learning rate: 0.000910, scenario: 0, slope: -0.0002955834550317984, fluctuations: 0.0\n",
      "step: 58210 loss: 0.723772 time elapsed: 76.9831 learning rate: 0.000910, scenario: 0, slope: -0.00027583114081727275, fluctuations: 0.0\n",
      "step: 58220 loss: 0.721769 time elapsed: 77.0107 learning rate: 0.000910, scenario: 0, slope: -0.00025955093088399927, fluctuations: 0.0\n",
      "step: 58230 loss: 0.719865 time elapsed: 77.0251 learning rate: 0.000910, scenario: 0, slope: -0.00024469295557284326, fluctuations: 0.0\n",
      "step: 58240 loss: 0.718051 time elapsed: 77.0389 learning rate: 0.000910, scenario: 0, slope: -0.0002311109362764307, fluctuations: 0.0\n",
      "step: 58250 loss: 0.716322 time elapsed: 77.0529 learning rate: 0.000910, scenario: 0, slope: -0.00021867676561507118, fluctuations: 0.0\n",
      "step: 58260 loss: 0.714669 time elapsed: 77.0667 learning rate: 0.000910, scenario: 0, slope: -0.00020727762878129752, fluctuations: 0.0\n",
      "step: 58270 loss: 0.713087 time elapsed: 77.0804 learning rate: 0.000910, scenario: 0, slope: -0.00019681370906677809, fluctuations: 0.0\n",
      "step: 58280 loss: 0.711572 time elapsed: 77.0941 learning rate: 0.000910, scenario: 0, slope: -0.0001871963323439561, fluctuations: 0.0\n",
      "step: 58290 loss: 0.710117 time elapsed: 77.1072 learning rate: 0.000910, scenario: 0, slope: -0.0001783464484646875, fluctuations: 0.0\n",
      "step: 58300 loss: 0.708719 time elapsed: 77.1197 learning rate: 0.000910, scenario: 0, slope: -0.00017097918441331633, fluctuations: 0.0\n",
      "step: 58310 loss: 0.707373 time elapsed: 77.1325 learning rate: 0.000910, scenario: 0, slope: -0.00016267374364472371, fluctuations: 0.0\n",
      "step: 58320 loss: 0.706075 time elapsed: 77.1445 learning rate: 0.000910, scenario: 0, slope: -0.00015573061961749598, fluctuations: 0.0\n",
      "step: 58330 loss: 0.704823 time elapsed: 77.1567 learning rate: 0.000910, scenario: 0, slope: -0.00014931274206741888, fluctuations: 0.0\n",
      "step: 58340 loss: 0.703613 time elapsed: 77.1688 learning rate: 0.000910, scenario: 0, slope: -0.00014337388123710263, fluctuations: 0.0\n",
      "step: 58350 loss: 0.702441 time elapsed: 77.1812 learning rate: 0.000910, scenario: 0, slope: -0.00013787228109900793, fluctuations: 0.0\n",
      "step: 58360 loss: 0.701307 time elapsed: 77.1939 learning rate: 0.000910, scenario: 0, slope: -0.0001327701764198585, fluctuations: 0.0\n",
      "step: 58370 loss: 0.700206 time elapsed: 77.2059 learning rate: 0.000910, scenario: 0, slope: -0.00012803337162456196, fluctuations: 0.0\n",
      "step: 58380 loss: 0.699137 time elapsed: 77.2201 learning rate: 0.000910, scenario: 0, slope: -0.00012363087175222863, fluctuations: 0.0\n",
      "step: 58390 loss: 0.698097 time elapsed: 77.2337 learning rate: 0.000910, scenario: 0, slope: -0.00011953455761349956, fluctuations: 0.0\n",
      "step: 58400 loss: 0.697086 time elapsed: 77.2478 learning rate: 0.000910, scenario: 0, slope: -0.00011608851341965638, fluctuations: 0.0\n",
      "step: 58410 loss: 0.696100 time elapsed: 77.2628 learning rate: 0.000910, scenario: 0, slope: -0.0001121606984460516, fluctuations: 0.0\n",
      "step: 58420 loss: 0.695139 time elapsed: 77.2765 learning rate: 0.000910, scenario: 0, slope: -0.00010883886763998242, fluctuations: 0.0\n",
      "step: 58430 loss: 0.694201 time elapsed: 77.2905 learning rate: 0.000910, scenario: 0, slope: -0.00010573422193198657, fluctuations: 0.0\n",
      "step: 58440 loss: 0.693285 time elapsed: 77.3041 learning rate: 0.000910, scenario: 0, slope: -0.00010282930070275966, fluctuations: 0.0\n",
      "step: 58450 loss: 0.692389 time elapsed: 77.3180 learning rate: 0.000910, scenario: 0, slope: -0.00010010820445940226, fluctuations: 0.0\n",
      "step: 58460 loss: 0.691513 time elapsed: 77.3309 learning rate: 0.000910, scenario: 0, slope: -9.755644861180656e-05, fluctuations: 0.0\n",
      "step: 58470 loss: 0.690654 time elapsed: 77.3435 learning rate: 0.000910, scenario: 0, slope: -9.51608317226854e-05, fluctuations: 0.0\n",
      "step: 58480 loss: 0.689813 time elapsed: 77.3559 learning rate: 0.000910, scenario: 0, slope: -9.290931660753136e-05, fluctuations: 0.0\n",
      "step: 58490 loss: 0.688987 time elapsed: 77.3682 learning rate: 0.000910, scenario: 0, slope: -9.079092288191644e-05, fluctuations: 0.0\n",
      "step: 58500 loss: 0.688177 time elapsed: 77.3801 learning rate: 0.000910, scenario: 0, slope: -8.898988673817437e-05, fluctuations: 0.0\n",
      "step: 58510 loss: 0.687382 time elapsed: 77.3927 learning rate: 0.000910, scenario: 0, slope: -8.691428790931502e-05, fluctuations: 0.0\n",
      "step: 58520 loss: 0.686600 time elapsed: 77.4048 learning rate: 0.000910, scenario: 0, slope: -8.513853984360162e-05, fluctuations: 0.0\n",
      "step: 58530 loss: 0.685832 time elapsed: 77.4171 learning rate: 0.000910, scenario: 0, slope: -8.346074735486018e-05, fluctuations: 0.0\n",
      "step: 58540 loss: 0.685076 time elapsed: 77.4295 learning rate: 0.000910, scenario: 0, slope: -8.187392594506966e-05, fluctuations: 0.0\n",
      "step: 58550 loss: 0.684332 time elapsed: 77.4425 learning rate: 0.000910, scenario: 0, slope: -8.03716852074929e-05, fluctuations: 0.0\n",
      "step: 58560 loss: 0.683599 time elapsed: 77.4564 learning rate: 0.000910, scenario: 0, slope: -7.89481747219721e-05, fluctuations: 0.0\n",
      "step: 58570 loss: 0.682876 time elapsed: 77.4704 learning rate: 0.000910, scenario: 0, slope: -7.759803493324895e-05, fluctuations: 0.0\n",
      "step: 58580 loss: 0.682164 time elapsed: 77.4843 learning rate: 0.000910, scenario: 0, slope: -7.631635255170793e-05, fluctuations: 0.0\n",
      "step: 58590 loss: 0.681462 time elapsed: 77.4979 learning rate: 0.000910, scenario: 0, slope: -7.509862006323224e-05, fluctuations: 0.0\n",
      "step: 58600 loss: 0.680768 time elapsed: 77.5115 learning rate: 0.000910, scenario: 0, slope: -7.405391043821449e-05, fluctuations: 0.0\n",
      "step: 58610 loss: 0.680084 time elapsed: 77.5256 learning rate: 0.000910, scenario: 0, slope: -7.283878644243279e-05, fluctuations: 0.0\n",
      "step: 58620 loss: 0.679408 time elapsed: 77.5382 learning rate: 0.000910, scenario: 0, slope: -7.178938501689087e-05, fluctuations: 0.0\n",
      "step: 58630 loss: 0.678740 time elapsed: 77.5506 learning rate: 0.000910, scenario: 0, slope: -7.078927515492441e-05, fluctuations: 0.0\n",
      "step: 58640 loss: 0.678080 time elapsed: 77.5628 learning rate: 0.000910, scenario: 0, slope: -6.983549031299726e-05, fluctuations: 0.0\n",
      "step: 58650 loss: 0.677428 time elapsed: 77.5749 learning rate: 0.000910, scenario: 0, slope: -6.892529434831865e-05, fluctuations: 0.0\n",
      "step: 58660 loss: 0.676780 time elapsed: 77.5872 learning rate: 0.000947, scenario: 1, slope: -6.80576836196408e-05, fluctuations: 0.0\n",
      "step: 58670 loss: 0.676092 time elapsed: 77.5993 learning rate: 0.001046, scenario: 1, slope: -6.73596033696512e-05, fluctuations: 0.0\n",
      "step: 58680 loss: 0.675341 time elapsed: 77.6115 learning rate: 0.001155, scenario: 1, slope: -6.714891245539091e-05, fluctuations: 0.0\n",
      "step: 58690 loss: 0.674521 time elapsed: 77.6239 learning rate: 0.001276, scenario: 1, slope: -6.770890764420155e-05, fluctuations: 0.0\n",
      "step: 58700 loss: 0.673642 time elapsed: 77.6359 learning rate: 0.001315, scenario: 0, slope: -6.904624273765946e-05, fluctuations: 0.0\n",
      "step: 58710 loss: 0.672767 time elapsed: 77.6496 learning rate: 0.001315, scenario: 0, slope: -7.162334529759843e-05, fluctuations: 0.0\n",
      "step: 58720 loss: 0.671905 time elapsed: 77.6635 learning rate: 0.001315, scenario: 0, slope: -7.44969873227707e-05, fluctuations: 0.0\n",
      "step: 58730 loss: 0.671055 time elapsed: 77.6773 learning rate: 0.001315, scenario: 0, slope: -7.751769792141295e-05, fluctuations: 0.0\n",
      "step: 58740 loss: 0.670216 time elapsed: 77.6915 learning rate: 0.001315, scenario: 0, slope: -8.034939578132057e-05, fluctuations: 0.0\n",
      "step: 58750 loss: 0.669388 time elapsed: 77.7050 learning rate: 0.001315, scenario: 0, slope: -8.26661301082957e-05, fluctuations: 0.0\n",
      "step: 58760 loss: 0.668570 time elapsed: 77.7190 learning rate: 0.001315, scenario: 0, slope: -8.415305519810308e-05, fluctuations: 0.0\n",
      "step: 58770 loss: 0.667762 time elapsed: 77.7330 learning rate: 0.001315, scenario: 0, slope: -8.465034333032806e-05, fluctuations: 0.0\n",
      "step: 58780 loss: 0.666963 time elapsed: 77.7458 learning rate: 0.001315, scenario: 0, slope: -8.430814597149232e-05, fluctuations: 0.0\n",
      "step: 58790 loss: 0.666173 time elapsed: 77.7582 learning rate: 0.001315, scenario: 0, slope: -8.341597039783453e-05, fluctuations: 0.0\n",
      "step: 58800 loss: 0.665391 time elapsed: 77.7705 learning rate: 0.001315, scenario: 0, slope: -8.247143191866117e-05, fluctuations: 0.0\n",
      "step: 58810 loss: 0.664617 time elapsed: 77.7833 learning rate: 0.001315, scenario: 0, slope: -8.137066728076655e-05, fluctuations: 0.0\n",
      "step: 58820 loss: 0.663850 time elapsed: 77.7957 learning rate: 0.001315, scenario: 0, slope: -8.042506975698876e-05, fluctuations: 0.0\n",
      "step: 58830 loss: 0.663090 time elapsed: 77.8080 learning rate: 0.001315, scenario: 0, slope: -7.952904898079763e-05, fluctuations: 0.0\n",
      "step: 58840 loss: 0.662337 time elapsed: 77.8202 learning rate: 0.001315, scenario: 0, slope: -7.867970839444006e-05, fluctuations: 0.0\n",
      "step: 58850 loss: 0.661590 time elapsed: 77.8326 learning rate: 0.001315, scenario: 0, slope: -7.787427707119903e-05, fluctuations: 0.0\n",
      "step: 58860 loss: 0.660849 time elapsed: 77.8447 learning rate: 0.001315, scenario: 0, slope: -7.711016610583899e-05, fluctuations: 0.0\n",
      "step: 58870 loss: 0.660114 time elapsed: 77.8580 learning rate: 0.001315, scenario: 0, slope: -7.638497449127278e-05, fluctuations: 0.0\n",
      "step: 58880 loss: 0.659384 time elapsed: 77.8723 learning rate: 0.001315, scenario: 0, slope: -7.569647895897219e-05, fluctuations: 0.0\n",
      "step: 58890 loss: 0.658660 time elapsed: 77.8861 learning rate: 0.001315, scenario: 0, slope: -7.504261964240797e-05, fluctuations: 0.0\n",
      "step: 58900 loss: 0.657941 time elapsed: 77.8994 learning rate: 0.001315, scenario: 0, slope: -7.448217791327379e-05, fluctuations: 0.0\n",
      "step: 58910 loss: 0.657226 time elapsed: 77.9136 learning rate: 0.001315, scenario: 0, slope: -7.383130089771221e-05, fluctuations: 0.0\n",
      "step: 58920 loss: 0.656515 time elapsed: 77.9275 learning rate: 0.001315, scenario: 0, slope: -7.327041311306537e-05, fluctuations: 0.0\n",
      "step: 58930 loss: 0.655809 time elapsed: 77.9410 learning rate: 0.001315, scenario: 0, slope: -7.273728161965559e-05, fluctuations: 0.0\n",
      "step: 58940 loss: 0.655107 time elapsed: 77.9536 learning rate: 0.001315, scenario: 0, slope: -7.223046816470018e-05, fluctuations: 0.0\n",
      "step: 58950 loss: 0.654409 time elapsed: 77.9661 learning rate: 0.001315, scenario: 0, slope: -7.174862803838671e-05, fluctuations: 0.0\n",
      "step: 58960 loss: 0.653714 time elapsed: 77.9784 learning rate: 0.001315, scenario: 0, slope: -7.12905022387184e-05, fluctuations: 0.0\n",
      "step: 58970 loss: 0.653023 time elapsed: 77.9908 learning rate: 0.001315, scenario: 0, slope: -7.085491044787594e-05, fluctuations: 0.0\n",
      "step: 58980 loss: 0.652335 time elapsed: 78.0030 learning rate: 0.001315, scenario: 0, slope: -7.044074472363028e-05, fluctuations: 0.0\n",
      "step: 58990 loss: 0.651650 time elapsed: 78.0154 learning rate: 0.001315, scenario: 0, slope: -7.004696382264977e-05, fluctuations: 0.0\n",
      "step: 59000 loss: 0.650968 time elapsed: 78.0276 learning rate: 0.001315, scenario: 0, slope: -6.970917914762748e-05, fluctuations: 0.0\n",
      "step: 59010 loss: 0.650289 time elapsed: 78.0404 learning rate: 0.001315, scenario: 0, slope: -6.931669479496684e-05, fluctuations: 0.0\n",
      "step: 59020 loss: 0.649613 time elapsed: 78.0528 learning rate: 0.001315, scenario: 0, slope: -6.897841402606182e-05, fluctuations: 0.0\n",
      "step: 59030 loss: 0.648939 time elapsed: 78.0661 learning rate: 0.001315, scenario: 0, slope: -6.865692481409572e-05, fluctuations: 0.0\n",
      "step: 59040 loss: 0.648268 time elapsed: 78.0801 learning rate: 0.001315, scenario: 0, slope: -6.835145172317025e-05, fluctuations: 0.0\n",
      "step: 59050 loss: 0.647599 time elapsed: 78.0936 learning rate: 0.001315, scenario: 0, slope: -6.806126170097091e-05, fluctuations: 0.0\n",
      "step: 59060 loss: 0.646932 time elapsed: 78.1072 learning rate: 0.001315, scenario: 0, slope: -6.778566121348818e-05, fluctuations: 0.0\n",
      "step: 59070 loss: 0.646267 time elapsed: 78.1208 learning rate: 0.001315, scenario: 0, slope: -6.752399362537428e-05, fluctuations: 0.0\n",
      "step: 59080 loss: 0.645604 time elapsed: 78.1345 learning rate: 0.001315, scenario: 0, slope: -6.72756368009494e-05, fluctuations: 0.0\n",
      "step: 59090 loss: 0.644943 time elapsed: 78.1482 learning rate: 0.001315, scenario: 0, slope: -6.704000090272601e-05, fluctuations: 0.0\n",
      "step: 59100 loss: 0.644284 time elapsed: 78.1605 learning rate: 0.001315, scenario: 0, slope: -6.683834191233438e-05, fluctuations: 0.0\n",
      "step: 59110 loss: 0.643626 time elapsed: 78.1733 learning rate: 0.001315, scenario: 0, slope: -6.660468204295899e-05, fluctuations: 0.0\n",
      "step: 59120 loss: 0.642970 time elapsed: 78.1854 learning rate: 0.001315, scenario: 0, slope: -6.640396336246595e-05, fluctuations: 0.0\n",
      "step: 59130 loss: 0.642315 time elapsed: 78.1978 learning rate: 0.001315, scenario: 0, slope: -6.621378907179419e-05, fluctuations: 0.0\n",
      "step: 59140 loss: 0.641815 time elapsed: 78.2104 learning rate: 0.001315, scenario: 0, slope: -6.586700635582578e-05, fluctuations: 0.0\n",
      "step: 59150 loss: 2.057149 time elapsed: 78.2230 learning rate: 0.001348, scenario: -1, slope: 0.0011641634354188656, fluctuations: 0.0\n",
      "step: 59160 loss: 41.958665 time elapsed: 78.2353 learning rate: 0.001219, scenario: -1, slope: 0.16761967107558, fluctuations: 0.02\n",
      "step: 59170 loss: 2.495313 time elapsed: 78.2476 learning rate: 0.001102, scenario: -1, slope: 0.1778741888764763, fluctuations: 0.05\n",
      "step: 59180 loss: 1.882731 time elapsed: 78.2599 learning rate: 0.000997, scenario: -1, slope: 0.15716758950357712, fluctuations: 0.09\n",
      "step: 59190 loss: 1.682345 time elapsed: 78.2733 learning rate: 0.000902, scenario: -1, slope: 0.11884785784162076, fluctuations: 0.13\n",
      "step: 59200 loss: 1.068540 time elapsed: 78.2871 learning rate: 0.000824, scenario: -1, slope: 0.08468540936408347, fluctuations: 0.18\n",
      "step: 59210 loss: 0.772965 time elapsed: 78.3014 learning rate: 0.000745, scenario: -1, slope: 0.02561291423653165, fluctuations: 0.23\n",
      "step: 59220 loss: 0.647827 time elapsed: 78.3151 learning rate: 0.000708, scenario: 0, slope: -0.03116800438262292, fluctuations: 0.28\n",
      "step: 59230 loss: 0.659545 time elapsed: 78.3289 learning rate: 0.000708, scenario: 0, slope: -0.09477359710347713, fluctuations: 0.32\n",
      "step: 59240 loss: 0.647571 time elapsed: 78.3426 learning rate: 0.000708, scenario: 0, slope: -0.16778116281249328, fluctuations: 0.37\n",
      "step: 59250 loss: 0.645045 time elapsed: 78.3565 learning rate: 0.000708, scenario: 0, slope: -0.2673503828701516, fluctuations: 0.42\n",
      "step: 59260 loss: 0.644642 time elapsed: 78.3695 learning rate: 0.000708, scenario: 0, slope: -0.05789841986661986, fluctuations: 0.44\n",
      "step: 59270 loss: 0.643444 time elapsed: 78.3823 learning rate: 0.000708, scenario: 0, slope: -0.025016935049923812, fluctuations: 0.44\n",
      "step: 59280 loss: 0.642937 time elapsed: 78.3944 learning rate: 0.000708, scenario: 0, slope: -0.005837360058891048, fluctuations: 0.44\n",
      "step: 59290 loss: 0.642441 time elapsed: 78.4066 learning rate: 0.000708, scenario: 0, slope: -0.00041947944867112976, fluctuations: 0.4\n",
      "step: 59300 loss: 0.641950 time elapsed: 78.4195 learning rate: 0.000708, scenario: 0, slope: -0.0005721440114591494, fluctuations: 0.35\n",
      "step: 59310 loss: 0.641503 time elapsed: 78.4325 learning rate: 0.000708, scenario: 0, slope: -0.00019084871339045963, fluctuations: 0.3\n",
      "step: 59320 loss: 0.641071 time elapsed: 78.4450 learning rate: 0.000708, scenario: 0, slope: -0.00012403637329423139, fluctuations: 0.25\n",
      "step: 59330 loss: 0.640649 time elapsed: 78.4572 learning rate: 0.000701, scenario: -1, slope: -6.083228464371482e-05, fluctuations: 0.21\n",
      "step: 59340 loss: 0.640249 time elapsed: 78.4696 learning rate: 0.000647, scenario: -1, slope: -5.4407680572956446e-05, fluctuations: 0.16\n",
      "step: 59350 loss: 0.639893 time elapsed: 78.4830 learning rate: 0.000585, scenario: -1, slope: -4.8665830743554476e-05, fluctuations: 0.11\n",
      "step: 59360 loss: 0.639574 time elapsed: 78.4963 learning rate: 0.000529, scenario: -1, slope: -4.427033294520504e-05, fluctuations: 0.07\n",
      "step: 59370 loss: 0.639289 time elapsed: 78.5102 learning rate: 0.000479, scenario: -1, slope: -4.171237881697066e-05, fluctuations: 0.03\n",
      "step: 59380 loss: 0.639031 time elapsed: 78.5235 learning rate: 0.000476, scenario: 1, slope: -3.928399521489642e-05, fluctuations: 0.0\n",
      "step: 59390 loss: 0.638759 time elapsed: 78.5372 learning rate: 0.000526, scenario: 1, slope: -3.661833665311821e-05, fluctuations: 0.0\n",
      "step: 59400 loss: 0.638461 time elapsed: 78.5510 learning rate: 0.000575, scenario: 1, slope: -3.4442527092786145e-05, fluctuations: 0.0\n",
      "step: 59410 loss: 0.638136 time elapsed: 78.5648 learning rate: 0.000635, scenario: 1, slope: -3.2267068231036416e-05, fluctuations: 0.0\n",
      "step: 59420 loss: 0.637780 time elapsed: 78.5775 learning rate: 0.000702, scenario: 1, slope: -3.096572852441363e-05, fluctuations: 0.0\n",
      "step: 59430 loss: 0.637389 time elapsed: 78.5900 learning rate: 0.000775, scenario: 1, slope: -3.052515510111004e-05, fluctuations: 0.0\n",
      "step: 59440 loss: 0.636960 time elapsed: 78.6024 learning rate: 0.000856, scenario: 1, slope: -3.110668763248741e-05, fluctuations: 0.0\n",
      "step: 59450 loss: 0.636488 time elapsed: 78.6146 learning rate: 0.000946, scenario: 1, slope: -3.269692634862598e-05, fluctuations: 0.0\n",
      "step: 59460 loss: 0.635971 time elapsed: 78.6267 learning rate: 0.001045, scenario: 1, slope: -3.517253710235107e-05, fluctuations: 0.0\n",
      "step: 59470 loss: 0.635404 time elapsed: 78.6391 learning rate: 0.001154, scenario: 1, slope: -3.8366345507914066e-05, fluctuations: 0.0\n",
      "step: 59480 loss: 0.634782 time elapsed: 78.6513 learning rate: 0.001275, scenario: 1, slope: -4.2068359182618386e-05, fluctuations: 0.0\n",
      "step: 59490 loss: 0.634099 time elapsed: 78.6635 learning rate: 0.001408, scenario: 1, slope: -4.614824285320105e-05, fluctuations: 0.0\n",
      "step: 59500 loss: 0.633350 time elapsed: 78.6756 learning rate: 0.001540, scenario: 1, slope: -5.0164412280977075e-05, fluctuations: 0.0\n",
      "step: 59510 loss: 0.632540 time elapsed: 78.6897 learning rate: 0.001702, scenario: 1, slope: -5.553017712527921e-05, fluctuations: 0.0\n",
      "step: 59520 loss: 145.845691 time elapsed: 78.7038 learning rate: 0.001709, scenario: -1, slope: 0.10296683634800205, fluctuations: 0.0\n",
      "step: 59530 loss: 13.626675 time elapsed: 78.7176 learning rate: 0.001546, scenario: -1, slope: 1.1028757854893168, fluctuations: 0.04\n",
      "step: 59540 loss: 131.289018 time elapsed: 78.7315 learning rate: 0.001398, scenario: -1, slope: 1.246080160722565, fluctuations: 0.08\n",
      "step: 59550 loss: 21.757932 time elapsed: 78.7452 learning rate: 0.001264, scenario: -1, slope: 1.0617371765855923, fluctuations: 0.13\n",
      "step: 59560 loss: 11.066242 time elapsed: 78.7592 learning rate: 0.001144, scenario: -1, slope: 0.7883987267716556, fluctuations: 0.17\n",
      "step: 59570 loss: 5.112237 time elapsed: 78.7733 learning rate: 0.001034, scenario: -1, slope: 0.4115515742707649, fluctuations: 0.2\n",
      "step: 59580 loss: 1.259062 time elapsed: 78.7863 learning rate: 0.000935, scenario: -1, slope: 0.0608692353502547, fluctuations: 0.24\n",
      "step: 59590 loss: 1.185100 time elapsed: 78.7991 learning rate: 0.000926, scenario: 0, slope: -0.3275978094169448, fluctuations: 0.27\n",
      "step: 59600 loss: 1.022439 time elapsed: 78.8115 learning rate: 0.000926, scenario: 0, slope: -0.6998909965517018, fluctuations: 0.3\n",
      "step: 59610 loss: 0.847861 time elapsed: 78.8245 learning rate: 0.000926, scenario: 0, slope: -1.2716220787096442, fluctuations: 0.34\n",
      "step: 59620 loss: 0.844652 time elapsed: 78.8369 learning rate: 0.000926, scenario: 0, slope: -1.876909562708291, fluctuations: 0.37\n",
      "step: 59630 loss: 0.808832 time elapsed: 78.8492 learning rate: 0.000926, scenario: 0, slope: -0.7217444171136493, fluctuations: 0.36\n",
      "step: 59640 loss: 0.794642 time elapsed: 78.8613 learning rate: 0.000926, scenario: 0, slope: -0.13977395900801254, fluctuations: 0.35\n",
      "step: 59650 loss: 0.783043 time elapsed: 78.8736 learning rate: 0.000926, scenario: 0, slope: -0.050549631777606366, fluctuations: 0.31\n",
      "step: 59660 loss: 0.773406 time elapsed: 78.8858 learning rate: 0.000926, scenario: 0, slope: -0.017541938794494723, fluctuations: 0.27\n",
      "step: 59670 loss: 0.765820 time elapsed: 78.8989 learning rate: 0.000926, scenario: 0, slope: -0.008320985359704474, fluctuations: 0.23\n",
      "step: 59680 loss: 0.759155 time elapsed: 78.9126 learning rate: 0.000926, scenario: 0, slope: -0.003767645621260462, fluctuations: 0.2\n",
      "step: 59690 loss: 0.753473 time elapsed: 78.9264 learning rate: 0.000926, scenario: 0, slope: -0.0023599751828679376, fluctuations: 0.16\n",
      "step: 59700 loss: 0.748454 time elapsed: 78.9402 learning rate: 0.000926, scenario: 0, slope: -0.0014854864807550183, fluctuations: 0.13\n",
      "step: 59710 loss: 0.744014 time elapsed: 78.9545 learning rate: 0.000926, scenario: 0, slope: -0.0010173382610514032, fluctuations: 0.09\n",
      "step: 59720 loss: 0.740034 time elapsed: 78.9681 learning rate: 0.000926, scenario: 0, slope: -0.0007804373788838293, fluctuations: 0.06\n",
      "step: 59730 loss: 0.736443 time elapsed: 78.9813 learning rate: 0.000926, scenario: 0, slope: -0.0006551718792250467, fluctuations: 0.03\n",
      "step: 59740 loss: 0.733176 time elapsed: 78.9938 learning rate: 0.000926, scenario: 0, slope: -0.000574337140342116, fluctuations: 0.0\n",
      "step: 59750 loss: 0.730185 time elapsed: 79.0062 learning rate: 0.000926, scenario: 0, slope: -0.0004992272661499349, fluctuations: 0.0\n",
      "step: 59760 loss: 0.727429 time elapsed: 79.0187 learning rate: 0.000926, scenario: 0, slope: -0.0004408831427305856, fluctuations: 0.0\n",
      "step: 59770 loss: 0.724875 time elapsed: 79.0310 learning rate: 0.000926, scenario: 0, slope: -0.0003935928016837934, fluctuations: 0.0\n",
      "step: 59780 loss: 0.722496 time elapsed: 79.0431 learning rate: 0.000926, scenario: 0, slope: -0.00035474793673553304, fluctuations: 0.0\n",
      "step: 59790 loss: 0.720269 time elapsed: 79.0551 learning rate: 0.000926, scenario: 0, slope: -0.00032230115820737015, fluctuations: 0.0\n",
      "step: 59800 loss: 0.718176 time elapsed: 79.0674 learning rate: 0.000926, scenario: 0, slope: -0.0002974806288411782, fluctuations: 0.0\n",
      "step: 59810 loss: 0.716201 time elapsed: 79.0801 learning rate: 0.000926, scenario: 0, slope: -0.0002716603952270906, fluctuations: 0.0\n",
      "step: 59820 loss: 0.714330 time elapsed: 79.0922 learning rate: 0.000926, scenario: 0, slope: -0.00025168996443277386, fluctuations: 0.0\n",
      "step: 59830 loss: 0.712553 time elapsed: 79.1049 learning rate: 0.000926, scenario: 0, slope: -0.0002344340217009654, fluctuations: 0.0\n",
      "step: 59840 loss: 0.710859 time elapsed: 79.1184 learning rate: 0.000926, scenario: 0, slope: -0.00021942081098934294, fluctuations: 0.0\n",
      "step: 59850 loss: 0.709241 time elapsed: 79.1322 learning rate: 0.000926, scenario: 0, slope: -0.00020627236297030384, fluctuations: 0.0\n",
      "step: 59860 loss: 0.707691 time elapsed: 79.1459 learning rate: 0.000926, scenario: 0, slope: -0.00019468406937734104, fluctuations: 0.0\n",
      "step: 59870 loss: 0.706204 time elapsed: 79.1596 learning rate: 0.000926, scenario: 0, slope: -0.00018440846356212862, fluctuations: 0.0\n",
      "step: 59880 loss: 0.704773 time elapsed: 79.1731 learning rate: 0.000926, scenario: 0, slope: -0.0001752434215739806, fluctuations: 0.0\n",
      "step: 59890 loss: 0.703395 time elapsed: 79.1871 learning rate: 0.000926, scenario: 0, slope: -0.00016702297481085935, fluctuations: 0.0\n",
      "step: 59900 loss: 0.702066 time elapsed: 79.1998 learning rate: 0.000926, scenario: 0, slope: -0.00016031855433112273, fluctuations: 0.0\n",
      "step: 59910 loss: 0.700781 time elapsed: 79.2130 learning rate: 0.000926, scenario: 0, slope: -0.0001528918143304312, fluctuations: 0.0\n",
      "step: 59920 loss: 0.699538 time elapsed: 79.2266 learning rate: 0.000926, scenario: 0, slope: -0.0001467734424233982, fluctuations: 0.0\n",
      "step: 59930 loss: 0.698333 time elapsed: 79.2398 learning rate: 0.000926, scenario: 0, slope: -0.00014117642929270687, fluctuations: 0.0\n",
      "step: 59940 loss: 0.697165 time elapsed: 79.2523 learning rate: 0.000926, scenario: 0, slope: -0.00013603485402654628, fluctuations: 0.0\n",
      "step: 59950 loss: 0.696030 time elapsed: 79.2647 learning rate: 0.000926, scenario: 0, slope: -0.00013129328479958847, fluctuations: 0.0\n",
      "step: 59960 loss: 0.694927 time elapsed: 79.2770 learning rate: 0.000926, scenario: 0, slope: -0.0001269049322495018, fluctuations: 0.0\n",
      "step: 59970 loss: 0.693855 time elapsed: 79.2893 learning rate: 0.000926, scenario: 0, slope: -0.00012283014695078624, fluctuations: 0.0\n",
      "step: 59980 loss: 0.692810 time elapsed: 79.3015 learning rate: 0.000926, scenario: 0, slope: -0.00011903519305399804, fluctuations: 0.0\n",
      "step: 59990 loss: 0.691792 time elapsed: 79.3149 learning rate: 0.000926, scenario: 0, slope: -0.00011549124465810169, fluctuations: 0.0\n",
      "step: 60000 loss: 0.690798 time elapsed: 79.3285 learning rate: 0.000926, scenario: 0, slope: -0.00011249578069704988, fluctuations: 0.0\n",
      "step: 60010 loss: 0.689829 time elapsed: 79.3432 learning rate: 0.000926, scenario: 0, slope: -0.00010906081761374876, fluctuations: 0.0\n",
      "step: 60020 loss: 0.688882 time elapsed: 79.3567 learning rate: 0.000926, scenario: 0, slope: -0.00010613453344663312, fluctuations: 0.0\n",
      "step: 60030 loss: 0.687956 time elapsed: 79.3704 learning rate: 0.000926, scenario: 0, slope: -0.00010337862688636767, fluctuations: 0.0\n",
      "step: 60040 loss: 0.687050 time elapsed: 79.3841 learning rate: 0.000926, scenario: 0, slope: -0.0001007790281837579, fluctuations: 0.0\n",
      "step: 60050 loss: 0.686164 time elapsed: 79.3979 learning rate: 0.000926, scenario: 0, slope: -9.832336679472259e-05, fluctuations: 0.0\n",
      "step: 60060 loss: 0.685296 time elapsed: 79.4114 learning rate: 0.000926, scenario: 0, slope: -9.600071097804764e-05, fluctuations: 0.0\n",
      "step: 60070 loss: 0.684445 time elapsed: 79.4239 learning rate: 0.000926, scenario: 0, slope: -9.380135166248141e-05, fluctuations: 0.0\n",
      "step: 60080 loss: 0.683610 time elapsed: 79.4362 learning rate: 0.000926, scenario: 0, slope: -9.171662276733803e-05, fluctuations: 0.0\n",
      "step: 60090 loss: 0.682791 time elapsed: 79.4485 learning rate: 0.000926, scenario: 0, slope: -8.973875157004563e-05, fluctuations: 0.0\n",
      "step: 60100 loss: 0.681988 time elapsed: 79.4606 learning rate: 0.000926, scenario: 0, slope: -8.804422769987878e-05, fluctuations: 0.0\n",
      "step: 60110 loss: 0.681198 time elapsed: 79.4733 learning rate: 0.000926, scenario: 0, slope: -8.607622955405891e-05, fluctuations: 0.0\n",
      "step: 60120 loss: 0.680422 time elapsed: 79.4854 learning rate: 0.000926, scenario: 0, slope: -8.437947521263773e-05, fluctuations: 0.0\n",
      "step: 60130 loss: 0.679659 time elapsed: 79.4976 learning rate: 0.000926, scenario: 0, slope: -8.276521052890742e-05, fluctuations: 0.0\n",
      "step: 60140 loss: 0.678908 time elapsed: 79.5098 learning rate: 0.000926, scenario: 0, slope: -8.122861637245515e-05, fluctuations: 0.0\n",
      "step: 60150 loss: 0.678169 time elapsed: 79.5227 learning rate: 0.000926, scenario: 0, slope: -7.97652623998012e-05, fluctuations: 0.0\n",
      "step: 60160 loss: 0.677441 time elapsed: 79.5365 learning rate: 0.000926, scenario: 0, slope: -7.837106259198517e-05, fluctuations: 0.0\n",
      "step: 60170 loss: 0.676724 time elapsed: 79.5503 learning rate: 0.000926, scenario: 0, slope: -7.704223736817724e-05, fluctuations: 0.0\n",
      "step: 60180 loss: 0.676017 time elapsed: 79.5641 learning rate: 0.000926, scenario: 0, slope: -7.577528116389211e-05, fluctuations: 0.0\n",
      "step: 60190 loss: 0.675320 time elapsed: 79.5777 learning rate: 0.000926, scenario: 0, slope: -7.456693455777044e-05, fluctuations: 0.0\n",
      "step: 60200 loss: 0.674632 time elapsed: 79.5910 learning rate: 0.000926, scenario: 0, slope: -7.352701920076178e-05, fluctuations: 0.0\n",
      "step: 60210 loss: 0.673953 time elapsed: 79.6054 learning rate: 0.000926, scenario: 0, slope: -7.231412187938046e-05, fluctuations: 0.0\n",
      "step: 60220 loss: 0.673282 time elapsed: 79.6178 learning rate: 0.000926, scenario: 0, slope: -7.126416634701657e-05, fluctuations: 0.0\n",
      "step: 60230 loss: 0.672620 time elapsed: 79.6302 learning rate: 0.000926, scenario: 0, slope: -7.026180726317605e-05, fluctuations: 0.0\n",
      "step: 60240 loss: 0.671965 time elapsed: 79.6423 learning rate: 0.000926, scenario: 0, slope: -6.930471113402534e-05, fluctuations: 0.0\n",
      "step: 60250 loss: 0.671318 time elapsed: 79.6546 learning rate: 0.000926, scenario: 0, slope: -6.839068482497314e-05, fluctuations: 0.0\n",
      "step: 60260 loss: 0.670678 time elapsed: 79.6669 learning rate: 0.000954, scenario: 1, slope: -6.751804268916054e-05, fluctuations: 0.0\n",
      "step: 60270 loss: 0.670002 time elapsed: 79.6791 learning rate: 0.001054, scenario: 1, slope: -6.678823018475275e-05, fluctuations: 0.0\n",
      "step: 60280 loss: 0.669264 time elapsed: 79.6919 learning rate: 0.001164, scenario: 1, slope: -6.651068707181479e-05, fluctuations: 0.0\n",
      "step: 60290 loss: 0.668460 time elapsed: 79.7046 learning rate: 0.001286, scenario: 1, slope: -6.697207992079173e-05, fluctuations: 0.0\n",
      "step: 60300 loss: 0.667592 time elapsed: 79.7190 learning rate: 0.001338, scenario: 0, slope: -6.820244319583201e-05, fluctuations: 0.0\n",
      "step: 60310 loss: 0.666724 time elapsed: 79.7341 learning rate: 0.001338, scenario: 0, slope: -7.067471420918978e-05, fluctuations: 0.0\n",
      "step: 60320 loss: 0.665869 time elapsed: 79.7483 learning rate: 0.001338, scenario: 0, slope: -7.3487142183006e-05, fluctuations: 0.0\n",
      "step: 60330 loss: 0.665025 time elapsed: 79.7621 learning rate: 0.001338, scenario: 0, slope: -7.648206423940231e-05, fluctuations: 0.0\n",
      "step: 60340 loss: 0.664192 time elapsed: 79.7756 learning rate: 0.001338, scenario: 0, slope: -7.932617805619882e-05, fluctuations: 0.0\n",
      "step: 60350 loss: 0.663370 time elapsed: 79.7893 learning rate: 0.001338, scenario: 0, slope: -8.1696215710078e-05, fluctuations: 0.0\n",
      "step: 60360 loss: 0.662558 time elapsed: 79.8030 learning rate: 0.001338, scenario: 0, slope: -8.327870071782126e-05, fluctuations: 0.0\n",
      "step: 60370 loss: 0.661755 time elapsed: 79.8172 learning rate: 0.001338, scenario: 0, slope: -8.388517550918501e-05, fluctuations: 0.0\n",
      "step: 60380 loss: 0.660960 time elapsed: 79.8305 learning rate: 0.001338, scenario: 0, slope: -8.364299026952632e-05, fluctuations: 0.0\n",
      "step: 60390 loss: 0.660174 time elapsed: 79.8438 learning rate: 0.001338, scenario: 0, slope: -8.282671412838134e-05, fluctuations: 0.0\n",
      "step: 60400 loss: 0.659395 time elapsed: 79.8562 learning rate: 0.001338, scenario: 0, slope: -8.192152884317028e-05, fluctuations: 0.0\n",
      "step: 60410 loss: 0.658624 time elapsed: 79.8692 learning rate: 0.001338, scenario: 0, slope: -8.086685953766382e-05, fluctuations: 0.0\n",
      "step: 60420 loss: 0.657859 time elapsed: 79.8813 learning rate: 0.001338, scenario: 0, slope: -7.99669083895846e-05, fluctuations: 0.0\n",
      "step: 60430 loss: 0.657101 time elapsed: 79.8937 learning rate: 0.001338, scenario: 0, slope: -7.91197930418892e-05, fluctuations: 0.0\n",
      "step: 60440 loss: 0.656349 time elapsed: 79.9059 learning rate: 0.001338, scenario: 0, slope: -7.832227629858433e-05, fluctuations: 0.0\n",
      "step: 60450 loss: 0.655602 time elapsed: 79.9183 learning rate: 0.001338, scenario: 0, slope: -7.757123356174001e-05, fluctuations: 0.0\n",
      "step: 60460 loss: 0.654861 time elapsed: 79.9309 learning rate: 0.001338, scenario: 0, slope: -7.686372301773981e-05, fluctuations: 0.0\n",
      "step: 60470 loss: 0.654125 time elapsed: 79.9444 learning rate: 0.001338, scenario: 0, slope: -7.619699867004191e-05, fluctuations: 0.0\n",
      "step: 60480 loss: 0.653394 time elapsed: 79.9584 learning rate: 0.001338, scenario: 0, slope: -7.556850442619537e-05, fluctuations: 0.0\n",
      "step: 60490 loss: 0.652668 time elapsed: 79.9723 learning rate: 0.001338, scenario: 0, slope: -7.49758625090135e-05, fluctuations: 0.0\n",
      "step: 60500 loss: 0.651945 time elapsed: 79.9859 learning rate: 0.001338, scenario: 0, slope: -7.447130693378645e-05, fluctuations: 0.0\n",
      "step: 60510 loss: 0.651227 time elapsed: 80.0004 learning rate: 0.001338, scenario: 0, slope: -7.388944016884466e-05, fluctuations: 0.0\n",
      "step: 60520 loss: 0.650513 time elapsed: 80.0141 learning rate: 0.001338, scenario: 0, slope: -7.339168357651247e-05, fluctuations: 0.0\n",
      "step: 60530 loss: 0.649802 time elapsed: 80.0275 learning rate: 0.001338, scenario: 0, slope: -7.292180479873582e-05, fluctuations: 0.0\n",
      "step: 60540 loss: 0.649095 time elapsed: 80.0401 learning rate: 0.001338, scenario: 0, slope: -7.247813897757324e-05, fluctuations: 0.0\n",
      "step: 60550 loss: 0.648390 time elapsed: 80.0522 learning rate: 0.001338, scenario: 0, slope: -7.205913359172545e-05, fluctuations: 0.0\n",
      "step: 60560 loss: 0.647689 time elapsed: 80.0645 learning rate: 0.001338, scenario: 0, slope: -7.166334022291673e-05, fluctuations: 0.0\n",
      "step: 60570 loss: 0.646991 time elapsed: 80.0768 learning rate: 0.001338, scenario: 0, slope: -7.12894069866942e-05, fluctuations: 0.0\n",
      "step: 60580 loss: 0.646296 time elapsed: 80.0892 learning rate: 0.001338, scenario: 0, slope: -7.093607156678725e-05, fluctuations: 0.0\n",
      "step: 60590 loss: 0.645603 time elapsed: 80.1013 learning rate: 0.001338, scenario: 0, slope: -7.060215479785127e-05, fluctuations: 0.0\n",
      "step: 60600 loss: 0.644912 time elapsed: 80.1131 learning rate: 0.001338, scenario: 0, slope: -7.031732039873936e-05, fluctuations: 0.0\n",
      "step: 60610 loss: 0.644224 time elapsed: 80.1257 learning rate: 0.001338, scenario: 0, slope: -6.998824125328308e-05, fluctuations: 0.0\n",
      "step: 60620 loss: 0.643538 time elapsed: 80.1377 learning rate: 0.001338, scenario: 0, slope: -6.970625087599047e-05, fluctuations: 0.0\n",
      "step: 60630 loss: 0.642854 time elapsed: 80.1507 learning rate: 0.001338, scenario: 0, slope: -6.943968223259631e-05, fluctuations: 0.0\n",
      "step: 60640 loss: 0.642172 time elapsed: 80.1648 learning rate: 0.001338, scenario: 0, slope: -6.918769167789164e-05, fluctuations: 0.0\n",
      "step: 60650 loss: 0.641492 time elapsed: 80.1788 learning rate: 0.001338, scenario: 0, slope: -6.894948930582698e-05, fluctuations: 0.0\n",
      "step: 60660 loss: 0.640814 time elapsed: 80.1926 learning rate: 0.001338, scenario: 0, slope: -6.872433524450131e-05, fluctuations: 0.0\n",
      "step: 60670 loss: 0.640137 time elapsed: 80.2066 learning rate: 0.001338, scenario: 0, slope: -6.851153622086028e-05, fluctuations: 0.0\n",
      "step: 60680 loss: 0.639462 time elapsed: 80.2205 learning rate: 0.001338, scenario: 0, slope: -6.831044237460972e-05, fluctuations: 0.0\n",
      "step: 60690 loss: 0.638788 time elapsed: 80.2351 learning rate: 0.001338, scenario: 0, slope: -6.812044430081714e-05, fluctuations: 0.0\n",
      "step: 60700 loss: 0.638116 time elapsed: 80.2478 learning rate: 0.001338, scenario: 0, slope: -6.795845967638181e-05, fluctuations: 0.0\n",
      "step: 60710 loss: 0.637445 time elapsed: 80.2609 learning rate: 0.001338, scenario: 0, slope: -6.777148384993585e-05, fluctuations: 0.0\n",
      "step: 60720 loss: 0.636775 time elapsed: 80.2733 learning rate: 0.001338, scenario: 0, slope: -6.76114811913783e-05, fluctuations: 0.0\n",
      "step: 60730 loss: 0.636106 time elapsed: 80.2863 learning rate: 0.001338, scenario: 0, slope: -6.746048917012332e-05, fluctuations: 0.0\n",
      "step: 60740 loss: 0.635438 time elapsed: 80.2989 learning rate: 0.001338, scenario: 0, slope: -6.731806316627257e-05, fluctuations: 0.0\n",
      "step: 60750 loss: 0.634772 time elapsed: 80.3113 learning rate: 0.001338, scenario: 0, slope: -6.718378519393378e-05, fluctuations: 0.0\n",
      "step: 60760 loss: 0.634106 time elapsed: 80.3237 learning rate: 0.001338, scenario: 0, slope: -6.705726212930889e-05, fluctuations: 0.0\n",
      "step: 60770 loss: 0.633441 time elapsed: 80.3363 learning rate: 0.001338, scenario: 0, slope: -6.69381240613539e-05, fluctuations: 0.0\n",
      "step: 60780 loss: 0.632777 time elapsed: 80.3489 learning rate: 0.001338, scenario: 0, slope: -6.682602275749211e-05, fluctuations: 0.0\n",
      "step: 60790 loss: 0.632114 time elapsed: 80.3629 learning rate: 0.001338, scenario: 0, slope: -6.672063023147276e-05, fluctuations: 0.0\n",
      "step: 60800 loss: 0.631452 time elapsed: 80.3771 learning rate: 0.001338, scenario: 0, slope: -6.663125601776305e-05, fluctuations: 0.0\n",
      "step: 60810 loss: 0.630793 time elapsed: 80.3919 learning rate: 0.001338, scenario: 0, slope: -6.652588550556206e-05, fluctuations: 0.0\n",
      "step: 60820 loss: 0.635680 time elapsed: 80.4055 learning rate: 0.001351, scenario: 1, slope: -6.0483884788472974e-05, fluctuations: 0.0\n",
      "step: 60830 loss: 27.940720 time elapsed: 80.4193 learning rate: 0.001279, scenario: -1, slope: 0.03288389698827286, fluctuations: 0.0\n",
      "step: 60840 loss: 8.956021 time elapsed: 80.4333 learning rate: 0.001156, scenario: -1, slope: 0.070365920944716, fluctuations: 0.02\n",
      "step: 60850 loss: 3.059407 time elapsed: 80.4469 learning rate: 0.001046, scenario: -1, slope: 0.06462751811079352, fluctuations: 0.05\n",
      "step: 60860 loss: 1.055656 time elapsed: 80.4612 learning rate: 0.000946, scenario: -1, slope: 0.05255619778962958, fluctuations: 0.09\n",
      "step: 60870 loss: 0.838257 time elapsed: 80.4745 learning rate: 0.000855, scenario: -1, slope: 0.03687236752058992, fluctuations: 0.13\n",
      "step: 60880 loss: 0.643908 time elapsed: 80.4874 learning rate: 0.000774, scenario: -1, slope: 0.020102174313114403, fluctuations: 0.18\n",
      "step: 60890 loss: 0.641872 time elapsed: 80.5013 learning rate: 0.000714, scenario: 0, slope: -0.002930128938402205, fluctuations: 0.21\n",
      "step: 60900 loss: 0.633668 time elapsed: 80.5143 learning rate: 0.000714, scenario: 0, slope: -0.02166318177195845, fluctuations: 0.26\n",
      "step: 60910 loss: 0.629981 time elapsed: 80.5287 learning rate: 0.000714, scenario: 0, slope: -0.049032472837388534, fluctuations: 0.31\n",
      "step: 60920 loss: 0.628283 time elapsed: 80.5421 learning rate: 0.000714, scenario: 0, slope: -0.07930610116029468, fluctuations: 0.36\n",
      "step: 60930 loss: 0.627394 time elapsed: 80.5555 learning rate: 0.000714, scenario: 0, slope: -0.09424989506887185, fluctuations: 0.4\n",
      "step: 60940 loss: 0.626826 time elapsed: 80.5701 learning rate: 0.000714, scenario: 0, slope: -0.01464528522532363, fluctuations: 0.43\n",
      "step: 60950 loss: 0.626389 time elapsed: 80.5853 learning rate: 0.000714, scenario: 0, slope: -0.002902981465798822, fluctuations: 0.43\n",
      "step: 60960 loss: 0.626006 time elapsed: 80.6002 learning rate: 0.000714, scenario: 0, slope: -0.0010652603024702207, fluctuations: 0.39\n",
      "step: 60970 loss: 0.625643 time elapsed: 80.6150 learning rate: 0.000714, scenario: 0, slope: -0.0003274736037191956, fluctuations: 0.35\n",
      "step: 60980 loss: 0.625286 time elapsed: 80.6298 learning rate: 0.000714, scenario: 0, slope: -0.00018474876407775247, fluctuations: 0.3\n",
      "step: 60990 loss: 0.624931 time elapsed: 80.6448 learning rate: 0.000714, scenario: 0, slope: -7.069620231799679e-05, fluctuations: 0.27\n",
      "step: 61000 loss: 0.624584 time elapsed: 80.6594 learning rate: 0.000665, scenario: -1, slope: -4.984825303414524e-05, fluctuations: 0.22\n",
      "step: 61010 loss: 0.624265 time elapsed: 80.6734 learning rate: 0.000602, scenario: -1, slope: -3.798071138456916e-05, fluctuations: 0.17\n",
      "step: 61020 loss: 0.623976 time elapsed: 80.6874 learning rate: 0.000544, scenario: -1, slope: -3.5740173313627555e-05, fluctuations: 0.12\n",
      "step: 61030 loss: 0.623715 time elapsed: 80.7012 learning rate: 0.000492, scenario: -1, slope: -3.4501656114395244e-05, fluctuations: 0.07\n",
      "step: 61040 loss: 0.623478 time elapsed: 80.7138 learning rate: 0.000445, scenario: -1, slope: -3.3280768519783646e-05, fluctuations: 0.02\n",
      "step: 61050 loss: 0.623256 time elapsed: 80.7262 learning rate: 0.000470, scenario: 1, slope: -3.170446893287663e-05, fluctuations: 0.0\n",
      "step: 61060 loss: 0.623013 time elapsed: 80.7389 learning rate: 0.000519, scenario: 1, slope: -2.9918372311113245e-05, fluctuations: 0.0\n",
      "step: 61070 loss: 0.622744 time elapsed: 80.7514 learning rate: 0.000574, scenario: 1, slope: -2.828022417093082e-05, fluctuations: 0.0\n",
      "step: 61080 loss: 0.622446 time elapsed: 80.7639 learning rate: 0.000634, scenario: 1, slope: -2.7026704134665134e-05, fluctuations: 0.0\n",
      "step: 61090 loss: 0.622117 time elapsed: 80.7773 learning rate: 0.000700, scenario: 1, slope: -2.6393104777797447e-05, fluctuations: 0.0\n",
      "step: 61100 loss: 0.621753 time elapsed: 80.7912 learning rate: 0.000765, scenario: 1, slope: -2.6527644583081543e-05, fluctuations: 0.0\n",
      "step: 61110 loss: 0.621354 time elapsed: 80.8054 learning rate: 0.000845, scenario: 1, slope: -2.766738899410685e-05, fluctuations: 0.0\n",
      "step: 61120 loss: 0.620914 time elapsed: 80.8192 learning rate: 0.000934, scenario: 1, slope: -2.9580198380195004e-05, fluctuations: 0.0\n",
      "step: 61130 loss: 0.620428 time elapsed: 80.8327 learning rate: 0.001032, scenario: 1, slope: -3.223171780080658e-05, fluctuations: 0.0\n",
      "step: 61140 loss: 0.619890 time elapsed: 80.8463 learning rate: 0.001140, scenario: 1, slope: -3.548381762748943e-05, fluctuations: 0.0\n",
      "step: 61150 loss: 0.619297 time elapsed: 80.8599 learning rate: 0.001259, scenario: 1, slope: -3.916621365127184e-05, fluctuations: 0.0\n",
      "step: 61160 loss: 0.618642 time elapsed: 80.8729 learning rate: 0.001390, scenario: 1, slope: -4.3226898387318296e-05, fluctuations: 0.0\n",
      "step: 61170 loss: 0.618048 time elapsed: 80.8855 learning rate: 0.001536, scenario: 1, slope: -4.761172919161189e-05, fluctuations: 0.0\n",
      "step: 61180 loss: 711.822301 time elapsed: 80.8979 learning rate: 0.001483, scenario: -1, slope: 0.6694765542249893, fluctuations: 0.01\n",
      "step: 61190 loss: 246.889105 time elapsed: 80.9103 learning rate: 0.001341, scenario: -1, slope: 0.9987928283897796, fluctuations: 0.05\n",
      "step: 61200 loss: 4.252892 time elapsed: 80.9224 learning rate: 0.001225, scenario: -1, slope: 0.9207680114403158, fluctuations: 0.09\n",
      "step: 61210 loss: 3.644096 time elapsed: 80.9351 learning rate: 0.001108, scenario: -1, slope: 0.7384175405931087, fluctuations: 0.14\n",
      "step: 61220 loss: 1.517425 time elapsed: 80.9473 learning rate: 0.001002, scenario: -1, slope: 0.5206519303286122, fluctuations: 0.18\n",
      "step: 61230 loss: 3.407442 time elapsed: 80.9594 learning rate: 0.000906, scenario: -1, slope: 0.24108087259077482, fluctuations: 0.21\n",
      "step: 61240 loss: 0.934098 time elapsed: 80.9717 learning rate: 0.000828, scenario: 0, slope: -0.020941200546364946, fluctuations: 0.25\n",
      "step: 61250 loss: 1.052510 time elapsed: 80.9849 learning rate: 0.000828, scenario: 0, slope: -0.31199353845290617, fluctuations: 0.28\n",
      "step: 61260 loss: 0.817697 time elapsed: 80.9986 learning rate: 0.000828, scenario: 0, slope: -0.6393887540355194, fluctuations: 0.32\n",
      "step: 61270 loss: 0.769466 time elapsed: 81.0121 learning rate: 0.000828, scenario: 0, slope: -1.0677496769401509, fluctuations: 0.35\n",
      "step: 61280 loss: 0.753922 time elapsed: 81.0259 learning rate: 0.000828, scenario: 0, slope: -0.48913880736843046, fluctuations: 0.37\n",
      "step: 61290 loss: 0.735122 time elapsed: 81.0398 learning rate: 0.000828, scenario: 0, slope: -0.22852235760720518, fluctuations: 0.36\n",
      "step: 61300 loss: 0.728160 time elapsed: 81.0534 learning rate: 0.000828, scenario: 0, slope: -0.09521785535032436, fluctuations: 0.35\n",
      "step: 61310 loss: 0.720345 time elapsed: 81.0677 learning rate: 0.000828, scenario: 0, slope: -0.0491017230141945, fluctuations: 0.32\n",
      "step: 61320 loss: 0.714620 time elapsed: 81.0802 learning rate: 0.000828, scenario: 0, slope: -0.0162960169836984, fluctuations: 0.28\n",
      "step: 61330 loss: 0.709456 time elapsed: 81.0924 learning rate: 0.000828, scenario: 0, slope: -0.0048819968167758, fluctuations: 0.25\n",
      "step: 61340 loss: 0.704871 time elapsed: 81.1050 learning rate: 0.000828, scenario: 0, slope: -0.0026777021035146553, fluctuations: 0.21\n",
      "step: 61350 loss: 0.700757 time elapsed: 81.1178 learning rate: 0.000828, scenario: 0, slope: -0.00125163169140265, fluctuations: 0.18\n",
      "step: 61360 loss: 0.697010 time elapsed: 81.1306 learning rate: 0.000828, scenario: 0, slope: -0.0008033366013370796, fluctuations: 0.15\n",
      "step: 61370 loss: 0.693593 time elapsed: 81.1445 learning rate: 0.000828, scenario: 0, slope: -0.0006358549548294312, fluctuations: 0.11\n",
      "step: 61380 loss: 0.690458 time elapsed: 81.1582 learning rate: 0.000828, scenario: 0, slope: -0.0005136828714441154, fluctuations: 0.08\n",
      "step: 61390 loss: 0.687572 time elapsed: 81.1718 learning rate: 0.000828, scenario: 0, slope: -0.00045240881573015113, fluctuations: 0.04\n",
      "step: 61400 loss: 0.684907 time elapsed: 81.1853 learning rate: 0.000828, scenario: 0, slope: -0.00041110867268871813, fluctuations: 0.01\n",
      "step: 61410 loss: 0.682438 time elapsed: 81.2013 learning rate: 0.000828, scenario: 0, slope: -0.0003686222485563292, fluctuations: 0.0\n",
      "step: 61420 loss: 0.680145 time elapsed: 81.2164 learning rate: 0.000828, scenario: 0, slope: -0.00033558386559782104, fluctuations: 0.0\n",
      "step: 61430 loss: 0.678009 time elapsed: 81.2306 learning rate: 0.000828, scenario: 0, slope: -0.00030733077660022854, fluctuations: 0.0\n",
      "step: 61440 loss: 0.676015 time elapsed: 81.2455 learning rate: 0.000828, scenario: 0, slope: -0.0002827391097549393, fluctuations: 0.0\n",
      "step: 61450 loss: 0.674149 time elapsed: 81.2603 learning rate: 0.000828, scenario: 0, slope: -0.00026105871989481373, fluctuations: 0.0\n",
      "step: 61460 loss: 0.672399 time elapsed: 81.2758 learning rate: 0.000828, scenario: 0, slope: -0.00024181947939620028, fluctuations: 0.0\n",
      "step: 61470 loss: 0.670753 time elapsed: 81.2910 learning rate: 0.000828, scenario: 0, slope: -0.00022464843320342968, fluctuations: 0.0\n",
      "step: 61480 loss: 0.669202 time elapsed: 81.3048 learning rate: 0.000828, scenario: 0, slope: -0.00020926383475947647, fluctuations: 0.0\n",
      "step: 61490 loss: 0.667738 time elapsed: 81.3181 learning rate: 0.000828, scenario: 0, slope: -0.0001954344217293548, fluctuations: 0.0\n",
      "step: 61500 loss: 0.666351 time elapsed: 81.3310 learning rate: 0.000828, scenario: 0, slope: -0.00018415978262100642, fluctuations: 0.0\n",
      "step: 61510 loss: 0.665037 time elapsed: 81.3449 learning rate: 0.000828, scenario: 0, slope: -0.00017171007987879972, fluctuations: 0.0\n",
      "step: 61520 loss: 0.663787 time elapsed: 81.3572 learning rate: 0.000828, scenario: 0, slope: -0.00016151798775070438, fluctuations: 0.0\n",
      "step: 61530 loss: 0.662597 time elapsed: 81.3694 learning rate: 0.000828, scenario: 0, slope: -0.00015227595830316935, fluctuations: 0.0\n",
      "step: 61540 loss: 0.661461 time elapsed: 81.3819 learning rate: 0.000828, scenario: 0, slope: -0.0001438817924541298, fluctuations: 0.0\n",
      "step: 61550 loss: 0.660375 time elapsed: 81.3947 learning rate: 0.000828, scenario: 0, slope: -0.00013624622030218852, fluctuations: 0.0\n",
      "step: 61560 loss: 0.659335 time elapsed: 81.4078 learning rate: 0.000828, scenario: 0, slope: -0.00012929086907851472, fluctuations: 0.0\n",
      "step: 61570 loss: 0.658337 time elapsed: 81.4217 learning rate: 0.000828, scenario: 0, slope: -0.0001229466428647136, fluctuations: 0.0\n",
      "step: 61580 loss: 0.657377 time elapsed: 81.4355 learning rate: 0.000828, scenario: 0, slope: -0.00011715241047978209, fluctuations: 0.0\n",
      "step: 61590 loss: 0.656452 time elapsed: 81.4494 learning rate: 0.000828, scenario: 0, slope: -0.00011185392690621427, fluctuations: 0.0\n",
      "step: 61600 loss: 0.655561 time elapsed: 81.4627 learning rate: 0.000828, scenario: 0, slope: -0.00010746916111221699, fluctuations: 0.0\n",
      "step: 61610 loss: 0.654699 time elapsed: 81.4773 learning rate: 0.000828, scenario: 0, slope: -0.00010255640890927406, fluctuations: 0.0\n",
      "step: 61620 loss: 0.653865 time elapsed: 81.4912 learning rate: 0.000828, scenario: 0, slope: -9.847591333430956e-05, fluctuations: 0.0\n",
      "step: 61630 loss: 0.653056 time elapsed: 81.5049 learning rate: 0.000828, scenario: 0, slope: -9.472705315162232e-05, fluctuations: 0.0\n",
      "step: 61640 loss: 0.652271 time elapsed: 81.5176 learning rate: 0.000828, scenario: 0, slope: -9.1279001273712e-05, fluctuations: 0.0\n",
      "step: 61650 loss: 0.651508 time elapsed: 81.5301 learning rate: 0.000828, scenario: 0, slope: -8.810408935175043e-05, fluctuations: 0.0\n",
      "step: 61660 loss: 0.650766 time elapsed: 81.5423 learning rate: 0.000828, scenario: 0, slope: -8.517745199329361e-05, fluctuations: 0.0\n",
      "step: 61670 loss: 0.650042 time elapsed: 81.5551 learning rate: 0.000828, scenario: 0, slope: -8.247671616426126e-05, fluctuations: 0.0\n",
      "step: 61680 loss: 0.649335 time elapsed: 81.5674 learning rate: 0.000828, scenario: 0, slope: -7.998172908162911e-05, fluctuations: 0.0\n",
      "step: 61690 loss: 0.648645 time elapsed: 81.5796 learning rate: 0.000828, scenario: 0, slope: -7.767431909008126e-05, fluctuations: 0.0\n",
      "step: 61700 loss: 0.647970 time elapsed: 81.5917 learning rate: 0.000828, scenario: 0, slope: -7.574443936535492e-05, fluctuations: 0.0\n",
      "step: 61710 loss: 0.647310 time elapsed: 81.6046 learning rate: 0.000828, scenario: 0, slope: -7.355820966122414e-05, fluctuations: 0.0\n",
      "step: 61720 loss: 0.646662 time elapsed: 81.6181 learning rate: 0.000828, scenario: 0, slope: -7.172129572520816e-05, fluctuations: 0.0\n",
      "step: 61730 loss: 0.646026 time elapsed: 81.6322 learning rate: 0.000828, scenario: 0, slope: -7.001521888261547e-05, fluctuations: 0.0\n",
      "step: 61740 loss: 0.645402 time elapsed: 81.6458 learning rate: 0.000828, scenario: 0, slope: -6.842899821377213e-05, fluctuations: 0.0\n",
      "step: 61750 loss: 0.644789 time elapsed: 81.6590 learning rate: 0.000828, scenario: 0, slope: -6.69526804668244e-05, fluctuations: 0.0\n",
      "step: 61760 loss: 0.644186 time elapsed: 81.6728 learning rate: 0.000828, scenario: 0, slope: -6.557723693630014e-05, fluctuations: 0.0\n",
      "step: 61770 loss: 0.643586 time elapsed: 81.6865 learning rate: 0.000879, scenario: 1, slope: -6.430143089988987e-05, fluctuations: 0.0\n",
      "step: 61780 loss: 0.642943 time elapsed: 81.7004 learning rate: 0.000970, scenario: 1, slope: -6.328450693137884e-05, fluctuations: 0.0\n",
      "step: 61790 loss: 0.642245 time elapsed: 81.7131 learning rate: 0.001072, scenario: 1, slope: -6.281371321327519e-05, fluctuations: 0.0\n",
      "step: 61800 loss: 0.641487 time elapsed: 81.7253 learning rate: 0.001172, scenario: 1, slope: -6.305529474152567e-05, fluctuations: 0.0\n",
      "step: 61810 loss: 0.640672 time elapsed: 81.7383 learning rate: 0.001295, scenario: 1, slope: -6.438485635517756e-05, fluctuations: 0.0\n",
      "step: 61820 loss: 0.639815 time elapsed: 81.7509 learning rate: 0.001308, scenario: 0, slope: -6.663951295315044e-05, fluctuations: 0.0\n",
      "step: 61830 loss: 0.638973 time elapsed: 81.7633 learning rate: 0.001308, scenario: 0, slope: -6.961148660418644e-05, fluctuations: 0.0\n",
      "step: 61840 loss: 0.638146 time elapsed: 81.7757 learning rate: 0.001308, scenario: 0, slope: -7.288465808937697e-05, fluctuations: 0.0\n",
      "step: 61850 loss: 0.637334 time elapsed: 81.7878 learning rate: 0.001308, scenario: 0, slope: -7.605907623443176e-05, fluctuations: 0.0\n",
      "step: 61860 loss: 0.636535 time elapsed: 81.8002 learning rate: 0.001308, scenario: 0, slope: -7.875057248386752e-05, fluctuations: 0.0\n",
      "step: 61870 loss: 0.635748 time elapsed: 81.8126 learning rate: 0.001308, scenario: 0, slope: -8.05967349448111e-05, fluctuations: 0.0\n",
      "step: 61880 loss: 0.634973 time elapsed: 81.8259 learning rate: 0.001308, scenario: 0, slope: -8.143681427679901e-05, fluctuations: 0.0\n",
      "step: 61890 loss: 0.634209 time elapsed: 81.8397 learning rate: 0.001308, scenario: 0, slope: -8.136716432290108e-05, fluctuations: 0.0\n",
      "step: 61900 loss: 0.633454 time elapsed: 81.8532 learning rate: 0.001308, scenario: 0, slope: -8.069899220660848e-05, fluctuations: 0.0\n",
      "step: 61910 loss: 0.632709 time elapsed: 81.8673 learning rate: 0.001308, scenario: 0, slope: -7.943265908294632e-05, fluctuations: 0.0\n",
      "step: 61920 loss: 0.631972 time elapsed: 81.8812 learning rate: 0.001308, scenario: 0, slope: -7.82258310913623e-05, fluctuations: 0.0\n",
      "step: 61930 loss: 0.631243 time elapsed: 81.8950 learning rate: 0.001308, scenario: 0, slope: -7.710791112165979e-05, fluctuations: 0.0\n",
      "step: 61940 loss: 0.630521 time elapsed: 81.9090 learning rate: 0.001308, scenario: 0, slope: -7.607187492881429e-05, fluctuations: 0.0\n",
      "step: 61950 loss: 0.629807 time elapsed: 81.9218 learning rate: 0.001308, scenario: 0, slope: -7.511023822994149e-05, fluctuations: 0.0\n",
      "step: 61960 loss: 0.629099 time elapsed: 81.9341 learning rate: 0.001308, scenario: 0, slope: -7.42161264277154e-05, fluctuations: 0.0\n",
      "step: 61970 loss: 0.628397 time elapsed: 81.9463 learning rate: 0.001308, scenario: 0, slope: -7.33833871127575e-05, fluctuations: 0.0\n",
      "step: 61980 loss: 0.627700 time elapsed: 81.9587 learning rate: 0.001308, scenario: 0, slope: -7.260655857673162e-05, fluctuations: 0.0\n",
      "step: 61990 loss: 0.627010 time elapsed: 81.9707 learning rate: 0.001308, scenario: 0, slope: -7.18807987555087e-05, fluctuations: 0.0\n",
      "step: 62000 loss: 0.626324 time elapsed: 81.9828 learning rate: 0.001308, scenario: 0, slope: -7.126771609103897e-05, fluctuations: 0.0\n",
      "step: 62010 loss: 0.625643 time elapsed: 81.9954 learning rate: 0.001308, scenario: 0, slope: -7.056576629429633e-05, fluctuations: 0.0\n",
      "step: 62020 loss: 0.624966 time elapsed: 82.0080 learning rate: 0.001308, scenario: 0, slope: -6.996925772401984e-05, fluctuations: 0.0\n",
      "step: 62030 loss: 0.624294 time elapsed: 82.0203 learning rate: 0.001308, scenario: 0, slope: -6.940923158944405e-05, fluctuations: 0.0\n",
      "step: 62040 loss: 0.623625 time elapsed: 82.0332 learning rate: 0.001308, scenario: 0, slope: -6.88829497363318e-05, fluctuations: 0.0\n",
      "step: 62050 loss: 0.622961 time elapsed: 82.0471 learning rate: 0.001308, scenario: 0, slope: -6.838794880460386e-05, fluctuations: 0.0\n",
      "step: 62060 loss: 0.622300 time elapsed: 82.0608 learning rate: 0.001308, scenario: 0, slope: -6.792200654590445e-05, fluctuations: 0.0\n",
      "step: 62070 loss: 0.621642 time elapsed: 82.0745 learning rate: 0.001308, scenario: 0, slope: -6.748311280315412e-05, fluctuations: 0.0\n",
      "step: 62080 loss: 0.620988 time elapsed: 82.0883 learning rate: 0.001308, scenario: 0, slope: -6.706944445481408e-05, fluctuations: 0.0\n",
      "step: 62090 loss: 0.620336 time elapsed: 82.1019 learning rate: 0.001308, scenario: 0, slope: -6.667934373485738e-05, fluctuations: 0.0\n",
      "step: 62100 loss: 0.619687 time elapsed: 82.1159 learning rate: 0.001308, scenario: 0, slope: -6.634715163460026e-05, fluctuations: 0.0\n",
      "step: 62110 loss: 0.619041 time elapsed: 82.1290 learning rate: 0.001308, scenario: 0, slope: -6.596393054980826e-05, fluctuations: 0.0\n",
      "step: 62120 loss: 0.618398 time elapsed: 82.1415 learning rate: 0.001308, scenario: 0, slope: -6.563597207726892e-05, fluctuations: 0.0\n",
      "step: 62130 loss: 0.617757 time elapsed: 82.1538 learning rate: 0.001308, scenario: 0, slope: -6.53262625659398e-05, fluctuations: 0.0\n",
      "step: 62140 loss: 0.617118 time elapsed: 82.1661 learning rate: 0.001308, scenario: 0, slope: -6.50337332665662e-05, fluctuations: 0.0\n",
      "step: 62150 loss: 0.616481 time elapsed: 82.1782 learning rate: 0.001308, scenario: 0, slope: -6.47573986058293e-05, fluctuations: 0.0\n",
      "step: 62160 loss: 0.615846 time elapsed: 82.1905 learning rate: 0.001308, scenario: 0, slope: -6.44963478221037e-05, fluctuations: 0.0\n",
      "step: 62170 loss: 0.615214 time elapsed: 82.2027 learning rate: 0.001308, scenario: 0, slope: -6.424973760292926e-05, fluctuations: 0.0\n",
      "step: 62180 loss: 0.614583 time elapsed: 82.2149 learning rate: 0.001308, scenario: 0, slope: -6.401678559132595e-05, fluctuations: 0.0\n",
      "step: 62190 loss: 0.613953 time elapsed: 82.2270 learning rate: 0.001308, scenario: 0, slope: -6.379676464649675e-05, fluctuations: 0.0\n",
      "step: 62200 loss: 0.613325 time elapsed: 82.2398 learning rate: 0.001308, scenario: 0, slope: -6.360924134912761e-05, fluctuations: 0.0\n",
      "step: 62210 loss: 0.612699 time elapsed: 82.2542 learning rate: 0.001308, scenario: 0, slope: -6.339285354382091e-05, fluctuations: 0.0\n",
      "step: 62220 loss: 0.612074 time elapsed: 82.2681 learning rate: 0.001308, scenario: 0, slope: -6.32077422178283e-05, fluctuations: 0.0\n",
      "step: 62230 loss: 0.611451 time elapsed: 82.2823 learning rate: 0.001308, scenario: 0, slope: -6.303311203209271e-05, fluctuations: 0.0\n",
      "step: 62240 loss: 0.610829 time elapsed: 82.2960 learning rate: 0.001308, scenario: 0, slope: -6.28684460728557e-05, fluctuations: 0.0\n",
      "step: 62250 loss: 0.610208 time elapsed: 82.3096 learning rate: 0.001308, scenario: 0, slope: -6.271325940284897e-05, fluctuations: 0.0\n",
      "step: 62260 loss: 0.609588 time elapsed: 82.3236 learning rate: 0.001308, scenario: 0, slope: -6.256709649631731e-05, fluctuations: 0.0\n",
      "step: 62270 loss: 0.608969 time elapsed: 82.3362 learning rate: 0.001308, scenario: 0, slope: -6.242952893373123e-05, fluctuations: 0.0\n",
      "step: 62280 loss: 0.608351 time elapsed: 82.3486 learning rate: 0.001308, scenario: 0, slope: -6.23001533141654e-05, fluctuations: 0.0\n",
      "step: 62290 loss: 0.607734 time elapsed: 82.3607 learning rate: 0.001308, scenario: 0, slope: -6.217858163065901e-05, fluctuations: 0.0\n",
      "step: 62300 loss: 0.607126 time elapsed: 82.3727 learning rate: 0.001308, scenario: 0, slope: -6.207091279272619e-05, fluctuations: 0.0\n",
      "step: 62310 loss: 0.631730 time elapsed: 82.3856 learning rate: 0.001375, scenario: 1, slope: -3.729417027068266e-05, fluctuations: 0.0\n",
      "step: 62320 loss: 58.637961 time elapsed: 82.3981 learning rate: 0.001250, scenario: -1, slope: 0.11173358350137165, fluctuations: 0.01\n",
      "step: 62330 loss: 16.932703 time elapsed: 82.4103 learning rate: 0.001130, scenario: -1, slope: 0.14817540540582028, fluctuations: 0.03\n",
      "step: 62340 loss: 3.575117 time elapsed: 82.4224 learning rate: 0.001022, scenario: -1, slope: 0.13000612445327037, fluctuations: 0.07\n",
      "step: 62350 loss: 2.088267 time elapsed: 82.4347 learning rate: 0.000924, scenario: -1, slope: 0.10442941391460822, fluctuations: 0.11\n",
      "step: 62360 loss: 0.624163 time elapsed: 82.4476 learning rate: 0.000836, scenario: -1, slope: 0.0741275272728657, fluctuations: 0.16\n",
      "step: 62370 loss: 0.628773 time elapsed: 82.4616 learning rate: 0.000756, scenario: -1, slope: 0.036241203057267087, fluctuations: 0.21\n",
      "step: 62380 loss: 0.614850 time elapsed: 82.4755 learning rate: 0.000698, scenario: 0, slope: -0.007789996061147967, fluctuations: 0.26\n",
      "step: 62390 loss: 0.625281 time elapsed: 82.4893 learning rate: 0.000698, scenario: 0, slope: -0.05764260206300146, fluctuations: 0.3\n",
      "step: 62400 loss: 0.612985 time elapsed: 82.5031 learning rate: 0.000698, scenario: 0, slope: -0.10640076111142675, fluctuations: 0.35\n",
      "step: 62410 loss: 0.608606 time elapsed: 82.5173 learning rate: 0.000698, scenario: 0, slope: -0.1806129170735786, fluctuations: 0.4\n",
      "step: 62420 loss: 0.608457 time elapsed: 82.5310 learning rate: 0.000698, scenario: 0, slope: -0.07815102171512861, fluctuations: 0.44\n",
      "step: 62430 loss: 0.607868 time elapsed: 82.5439 learning rate: 0.000698, scenario: 0, slope: -0.018790193820548858, fluctuations: 0.45\n",
      "step: 62440 loss: 0.607227 time elapsed: 82.5563 learning rate: 0.000698, scenario: 0, slope: -0.0065934052544713625, fluctuations: 0.45\n",
      "step: 62450 loss: 0.606762 time elapsed: 82.5686 learning rate: 0.000698, scenario: 0, slope: -0.001324638221132661, fluctuations: 0.41\n",
      "step: 62460 loss: 0.606361 time elapsed: 82.5807 learning rate: 0.000698, scenario: 0, slope: -0.0005790483104080181, fluctuations: 0.36\n",
      "step: 62470 loss: 0.605971 time elapsed: 82.5931 learning rate: 0.000698, scenario: 0, slope: -0.0002202807385776839, fluctuations: 0.31\n",
      "step: 62480 loss: 0.605587 time elapsed: 82.6055 learning rate: 0.000698, scenario: 0, slope: -0.0001275687253154233, fluctuations: 0.26\n",
      "step: 62490 loss: 0.605212 time elapsed: 82.6180 learning rate: 0.000684, scenario: -1, slope: -5.2908876889795465e-05, fluctuations: 0.22\n",
      "step: 62500 loss: 0.604860 time elapsed: 82.6305 learning rate: 0.000631, scenario: -1, slope: -5.1799220447914226e-05, fluctuations: 0.17\n",
      "step: 62510 loss: 0.604543 time elapsed: 82.6434 learning rate: 0.000571, scenario: -1, slope: -4.2698602100153945e-05, fluctuations: 0.12\n",
      "step: 62520 loss: 0.604258 time elapsed: 82.6568 learning rate: 0.000516, scenario: -1, slope: -3.865816122151415e-05, fluctuations: 0.08\n",
      "step: 62530 loss: 0.604002 time elapsed: 82.6706 learning rate: 0.000467, scenario: -1, slope: -3.666169291258156e-05, fluctuations: 0.03\n",
      "step: 62540 loss: 0.603767 time elapsed: 82.6848 learning rate: 0.000474, scenario: 1, slope: -3.4754797265811075e-05, fluctuations: 0.0\n",
      "step: 62550 loss: 0.603517 time elapsed: 82.6986 learning rate: 0.000523, scenario: 1, slope: -3.2550609770117115e-05, fluctuations: 0.0\n",
      "step: 62560 loss: 0.603242 time elapsed: 82.7129 learning rate: 0.000578, scenario: 1, slope: -3.057129338837186e-05, fluctuations: 0.0\n",
      "step: 62570 loss: 0.602939 time elapsed: 82.7267 learning rate: 0.000638, scenario: 1, slope: -2.900373739236222e-05, fluctuations: 0.0\n",
      "step: 62580 loss: 0.602605 time elapsed: 82.7408 learning rate: 0.000705, scenario: 1, slope: -2.8044330396436933e-05, fluctuations: 0.0\n",
      "step: 62590 loss: 0.602238 time elapsed: 82.7535 learning rate: 0.000779, scenario: 1, slope: -2.7883814874423022e-05, fluctuations: 0.0\n",
      "step: 62600 loss: 0.601834 time elapsed: 82.7657 learning rate: 0.000852, scenario: 1, slope: -2.8535094130603886e-05, fluctuations: 0.0\n",
      "step: 62610 loss: 0.601394 time elapsed: 82.7788 learning rate: 0.000941, scenario: 1, slope: -3.031923580165661e-05, fluctuations: 0.0\n",
      "step: 62620 loss: 0.600910 time elapsed: 82.7916 learning rate: 0.001039, scenario: 1, slope: -3.276658457217355e-05, fluctuations: 0.0\n",
      "step: 62630 loss: 0.600379 time elapsed: 82.8042 learning rate: 0.001148, scenario: 1, slope: -3.584594489994758e-05, fluctuations: 0.0\n",
      "step: 62640 loss: 0.599794 time elapsed: 82.8167 learning rate: 0.001268, scenario: 1, slope: -3.9368592339752985e-05, fluctuations: 0.0\n",
      "step: 62650 loss: 0.599152 time elapsed: 82.8289 learning rate: 0.001401, scenario: 1, slope: -4.323684437557853e-05, fluctuations: 0.0\n",
      "step: 62660 loss: 0.598446 time elapsed: 82.8409 learning rate: 0.001547, scenario: 1, slope: -4.748183391465855e-05, fluctuations: 0.0\n",
      "step: 62670 loss: 0.617217 time elapsed: 82.8528 learning rate: 0.001709, scenario: 1, slope: -3.830121477456351e-05, fluctuations: 0.0\n",
      "step: 62680 loss: 453.196141 time elapsed: 82.8656 learning rate: 0.001554, scenario: -1, slope: 1.09022472941559, fluctuations: 0.02\n",
      "step: 62690 loss: 114.347309 time elapsed: 82.8793 learning rate: 0.001405, scenario: -1, slope: 1.4258466258410527, fluctuations: 0.06\n",
      "step: 62700 loss: 51.296526 time elapsed: 82.8931 learning rate: 0.001284, scenario: -1, slope: 1.3579771532785014, fluctuations: 0.1\n",
      "step: 62710 loss: 10.374453 time elapsed: 82.9073 learning rate: 0.001161, scenario: -1, slope: 1.0884319583781936, fluctuations: 0.15\n",
      "step: 62720 loss: 6.516716 time elapsed: 82.9211 learning rate: 0.001050, scenario: -1, slope: 0.6643110641204095, fluctuations: 0.18\n",
      "step: 62730 loss: 2.462620 time elapsed: 82.9348 learning rate: 0.000950, scenario: -1, slope: 0.26608213698049626, fluctuations: 0.22\n",
      "step: 62740 loss: 1.153046 time elapsed: 82.9486 learning rate: 0.000894, scenario: 0, slope: -0.15402284511417294, fluctuations: 0.26\n",
      "step: 62750 loss: 0.912099 time elapsed: 82.9611 learning rate: 0.000894, scenario: 0, slope: -0.6208081207975195, fluctuations: 0.29\n",
      "step: 62760 loss: 0.852551 time elapsed: 82.9737 learning rate: 0.000894, scenario: 0, slope: -1.1481257704801955, fluctuations: 0.32\n",
      "step: 62770 loss: 0.765365 time elapsed: 82.9859 learning rate: 0.000894, scenario: 0, slope: -1.8886150325578483, fluctuations: 0.36\n",
      "step: 62780 loss: 0.754876 time elapsed: 82.9979 learning rate: 0.000894, scenario: 0, slope: -0.7419599015743006, fluctuations: 0.37\n",
      "step: 62790 loss: 0.733139 time elapsed: 83.0103 learning rate: 0.000894, scenario: 0, slope: -0.2803857787679021, fluctuations: 0.36\n",
      "step: 62800 loss: 0.725067 time elapsed: 83.0224 learning rate: 0.000894, scenario: 0, slope: -0.1071611219101639, fluctuations: 0.35\n",
      "step: 62810 loss: 0.716221 time elapsed: 83.0351 learning rate: 0.000894, scenario: 0, slope: -0.02682387865289969, fluctuations: 0.31\n",
      "step: 62820 loss: 0.710028 time elapsed: 83.0473 learning rate: 0.000894, scenario: 0, slope: -0.009295782282822218, fluctuations: 0.27\n",
      "step: 62830 loss: 0.704485 time elapsed: 83.0595 learning rate: 0.000894, scenario: 0, slope: -0.004944980425852627, fluctuations: 0.23\n",
      "step: 62840 loss: 0.699851 time elapsed: 83.0727 learning rate: 0.000894, scenario: 0, slope: -0.0021097890452353926, fluctuations: 0.2\n",
      "step: 62850 loss: 0.695769 time elapsed: 83.0865 learning rate: 0.000894, scenario: 0, slope: -0.001427246367807923, fluctuations: 0.16\n",
      "step: 62860 loss: 0.692174 time elapsed: 83.1004 learning rate: 0.000894, scenario: 0, slope: -0.0008672601422439064, fluctuations: 0.13\n",
      "step: 62870 loss: 0.688960 time elapsed: 83.1143 learning rate: 0.000894, scenario: 0, slope: -0.0006870331760171686, fluctuations: 0.09\n",
      "step: 62880 loss: 0.686064 time elapsed: 83.1280 learning rate: 0.000894, scenario: 0, slope: -0.0005428773106419632, fluctuations: 0.06\n",
      "step: 62890 loss: 0.683432 time elapsed: 83.1417 learning rate: 0.000894, scenario: 0, slope: -0.00046800910851359516, fluctuations: 0.02\n",
      "step: 62900 loss: 0.681022 time elapsed: 83.1558 learning rate: 0.000894, scenario: 0, slope: -0.00041426663616176486, fluctuations: 0.0\n",
      "step: 62910 loss: 0.678799 time elapsed: 83.1691 learning rate: 0.000894, scenario: 0, slope: -0.0003583293958787821, fluctuations: 0.0\n",
      "step: 62920 loss: 0.676737 time elapsed: 83.1816 learning rate: 0.000894, scenario: 0, slope: -0.00031868741718087996, fluctuations: 0.0\n",
      "step: 62930 loss: 0.674813 time elapsed: 83.1939 learning rate: 0.000894, scenario: 0, slope: -0.0002866444802893378, fluctuations: 0.0\n",
      "step: 62940 loss: 0.673009 time elapsed: 83.2064 learning rate: 0.000894, scenario: 0, slope: -0.0002601670874161311, fluctuations: 0.0\n",
      "step: 62950 loss: 0.671309 time elapsed: 83.2186 learning rate: 0.000894, scenario: 0, slope: -0.00023805648533994247, fluctuations: 0.0\n",
      "step: 62960 loss: 0.669700 time elapsed: 83.2307 learning rate: 0.000894, scenario: 0, slope: -0.00021938608606249013, fluctuations: 0.0\n",
      "step: 62970 loss: 0.668173 time elapsed: 83.2429 learning rate: 0.000894, scenario: 0, slope: -0.00020348479129407609, fluctuations: 0.0\n",
      "step: 62980 loss: 0.666719 time elapsed: 83.2552 learning rate: 0.000894, scenario: 0, slope: -0.00018982913886382872, fluctuations: 0.0\n",
      "step: 62990 loss: 0.665329 time elapsed: 83.2673 learning rate: 0.000894, scenario: 0, slope: -0.00017801202842962813, fluctuations: 0.0\n",
      "step: 63000 loss: 0.663997 time elapsed: 83.2803 learning rate: 0.000894, scenario: 0, slope: -0.00016868034107046993, fluctuations: 0.0\n",
      "step: 63010 loss: 0.662718 time elapsed: 83.2948 learning rate: 0.000894, scenario: 0, slope: -0.00015866701171568108, fluctuations: 0.0\n",
      "step: 63020 loss: 0.661488 time elapsed: 83.3086 learning rate: 0.000894, scenario: 0, slope: -0.00015067357760780201, fluctuations: 0.0\n",
      "step: 63030 loss: 0.660301 time elapsed: 83.3223 learning rate: 0.000894, scenario: 0, slope: -0.00014356216640066714, fluctuations: 0.0\n",
      "step: 63040 loss: 0.659155 time elapsed: 83.3359 learning rate: 0.000894, scenario: 0, slope: -0.00013719587836214672, fluctuations: 0.0\n",
      "step: 63050 loss: 0.658046 time elapsed: 83.3494 learning rate: 0.000894, scenario: 0, slope: -0.0001314627270828624, fluctuations: 0.0\n",
      "step: 63060 loss: 0.656972 time elapsed: 83.3646 learning rate: 0.000894, scenario: 0, slope: -0.0001262707115623358, fluctuations: 0.0\n",
      "step: 63070 loss: 0.655930 time elapsed: 83.3776 learning rate: 0.000894, scenario: 0, slope: -0.00012154393818282017, fluctuations: 0.0\n",
      "step: 63080 loss: 0.654918 time elapsed: 83.3904 learning rate: 0.000894, scenario: 0, slope: -0.00011721954742910628, fluctuations: 0.0\n",
      "step: 63090 loss: 0.653934 time elapsed: 83.4037 learning rate: 0.000894, scenario: 0, slope: -0.00011324526493584441, fluctuations: 0.0\n",
      "step: 63100 loss: 0.652977 time elapsed: 83.4179 learning rate: 0.000894, scenario: 0, slope: -0.0001099315238450312, fluctuations: 0.0\n",
      "step: 63110 loss: 0.652044 time elapsed: 83.4311 learning rate: 0.000894, scenario: 0, slope: -0.00010617946585688309, fluctuations: 0.0\n",
      "step: 63120 loss: 0.651135 time elapsed: 83.4436 learning rate: 0.000894, scenario: 0, slope: -0.00010302051031352581, fluctuations: 0.0\n",
      "step: 63130 loss: 0.650248 time elapsed: 83.4566 learning rate: 0.000894, scenario: 0, slope: -0.0001000744816707021, fluctuations: 0.0\n",
      "step: 63140 loss: 0.649381 time elapsed: 83.4693 learning rate: 0.000894, scenario: 0, slope: -9.731919489625652e-05, fluctuations: 0.0\n",
      "step: 63150 loss: 0.648534 time elapsed: 83.4822 learning rate: 0.000894, scenario: 0, slope: -9.473569185224137e-05, fluctuations: 0.0\n",
      "step: 63160 loss: 0.647706 time elapsed: 83.4968 learning rate: 0.000894, scenario: 0, slope: -9.230768790391895e-05, fluctuations: 0.0\n",
      "step: 63170 loss: 0.646895 time elapsed: 83.5109 learning rate: 0.000894, scenario: 0, slope: -9.002112031869024e-05, fluctuations: 0.0\n",
      "step: 63180 loss: 0.646102 time elapsed: 83.5245 learning rate: 0.000894, scenario: 0, slope: -8.786377906133837e-05, fluctuations: 0.0\n",
      "step: 63190 loss: 0.645324 time elapsed: 83.5383 learning rate: 0.000894, scenario: 0, slope: -8.582500440584879e-05, fluctuations: 0.0\n",
      "step: 63200 loss: 0.644561 time elapsed: 83.5520 learning rate: 0.000894, scenario: 0, slope: -8.408372414786339e-05, fluctuations: 0.0\n",
      "step: 63210 loss: 0.643813 time elapsed: 83.5665 learning rate: 0.000894, scenario: 0, slope: -8.206682293988817e-05, fluctuations: 0.0\n",
      "step: 63220 loss: 0.643079 time elapsed: 83.5805 learning rate: 0.000894, scenario: 0, slope: -8.033182759405858e-05, fluctuations: 0.0\n",
      "step: 63230 loss: 0.642358 time elapsed: 83.5939 learning rate: 0.000894, scenario: 0, slope: -7.868391498332769e-05, fluctuations: 0.0\n",
      "step: 63240 loss: 0.641649 time elapsed: 83.6063 learning rate: 0.000894, scenario: 0, slope: -7.711722388728637e-05, fluctuations: 0.0\n",
      "step: 63250 loss: 0.640952 time elapsed: 83.6187 learning rate: 0.000894, scenario: 0, slope: -7.562647436961489e-05, fluctuations: 0.0\n",
      "step: 63260 loss: 0.640267 time elapsed: 83.6309 learning rate: 0.000894, scenario: 0, slope: -7.420688847834703e-05, fluctuations: 0.0\n",
      "step: 63270 loss: 0.639593 time elapsed: 83.6431 learning rate: 0.000894, scenario: 0, slope: -7.285412403914505e-05, fluctuations: 0.0\n",
      "step: 63280 loss: 0.638929 time elapsed: 83.6555 learning rate: 0.000894, scenario: 0, slope: -7.156421918486307e-05, fluctuations: 0.0\n",
      "step: 63290 loss: 0.638275 time elapsed: 83.6678 learning rate: 0.000894, scenario: 0, slope: -7.033354570178827e-05, fluctuations: 0.0\n",
      "step: 63300 loss: 0.637631 time elapsed: 83.6799 learning rate: 0.000894, scenario: 0, slope: -6.927382134300629e-05, fluctuations: 0.0\n",
      "step: 63310 loss: 0.636996 time elapsed: 83.6939 learning rate: 0.000894, scenario: 0, slope: -6.803681781744523e-05, fluctuations: 0.0\n",
      "step: 63320 loss: 0.636369 time elapsed: 83.7082 learning rate: 0.000894, scenario: 0, slope: -6.696484945712749e-05, fluctuations: 0.0\n",
      "step: 63330 loss: 0.635751 time elapsed: 83.7221 learning rate: 0.000894, scenario: 0, slope: -6.594023164019881e-05, fluctuations: 0.0\n",
      "step: 63340 loss: 0.635141 time elapsed: 83.7358 learning rate: 0.000894, scenario: 0, slope: -6.49605183481905e-05, fluctuations: 0.0\n",
      "step: 63350 loss: 0.634539 time elapsed: 83.7496 learning rate: 0.000903, scenario: 1, slope: -6.402343224710795e-05, fluctuations: 0.0\n",
      "step: 63360 loss: 0.633916 time elapsed: 83.7635 learning rate: 0.000997, scenario: 1, slope: -6.318377438949824e-05, fluctuations: 0.0\n",
      "step: 63370 loss: 0.633238 time elapsed: 83.7770 learning rate: 0.001102, scenario: 1, slope: -6.271068620897057e-05, fluctuations: 0.0\n",
      "step: 63380 loss: 0.632499 time elapsed: 83.7901 learning rate: 0.001217, scenario: 1, slope: -6.288148493746403e-05, fluctuations: 0.0\n",
      "step: 63390 loss: 0.631695 time elapsed: 83.8026 learning rate: 0.001331, scenario: 0, slope: -6.391636916059824e-05, fluctuations: 0.0\n",
      "step: 63400 loss: 0.630861 time elapsed: 83.8151 learning rate: 0.001331, scenario: 0, slope: -6.566103794074794e-05, fluctuations: 0.0\n",
      "step: 63410 loss: 0.630041 time elapsed: 83.8280 learning rate: 0.001331, scenario: 0, slope: -6.853227354565504e-05, fluctuations: 0.0\n",
      "step: 63420 loss: 0.629235 time elapsed: 83.8405 learning rate: 0.001331, scenario: 0, slope: -7.147131106184801e-05, fluctuations: 0.0\n",
      "step: 63430 loss: 0.628441 time elapsed: 83.8531 learning rate: 0.001331, scenario: 0, slope: -7.436565245972896e-05, fluctuations: 0.0\n",
      "step: 63440 loss: 0.627659 time elapsed: 83.8653 learning rate: 0.001331, scenario: 0, slope: -7.68812120598379e-05, fluctuations: 0.0\n",
      "step: 63450 loss: 0.626889 time elapsed: 83.8773 learning rate: 0.001331, scenario: 0, slope: -7.869516218173095e-05, fluctuations: 0.0\n",
      "step: 63460 loss: 0.626130 time elapsed: 83.8898 learning rate: 0.001331, scenario: 0, slope: -7.95580780767261e-05, fluctuations: 0.0\n",
      "step: 63470 loss: 0.625380 time elapsed: 83.9034 learning rate: 0.001331, scenario: 0, slope: -7.951812844203784e-05, fluctuations: 0.0\n",
      "step: 63480 loss: 0.624640 time elapsed: 83.9173 learning rate: 0.001331, scenario: 0, slope: -7.878699230699898e-05, fluctuations: 0.0\n",
      "step: 63490 loss: 0.623910 time elapsed: 83.9311 learning rate: 0.001331, scenario: 0, slope: -7.768908713651627e-05, fluctuations: 0.0\n",
      "step: 63500 loss: 0.623187 time elapsed: 83.9447 learning rate: 0.001331, scenario: 0, slope: -7.668646392150933e-05, fluctuations: 0.0\n",
      "step: 63510 loss: 0.622473 time elapsed: 83.9588 learning rate: 0.001331, scenario: 0, slope: -7.553073644431413e-05, fluctuations: 0.0\n",
      "step: 63520 loss: 0.621766 time elapsed: 83.9726 learning rate: 0.001331, scenario: 0, slope: -7.454335110908254e-05, fluctuations: 0.0\n",
      "step: 63530 loss: 0.621066 time elapsed: 83.9891 learning rate: 0.001331, scenario: 0, slope: -7.361258595759662e-05, fluctuations: 0.0\n",
      "step: 63540 loss: 0.620373 time elapsed: 84.0024 learning rate: 0.001331, scenario: 0, slope: -7.27349507010114e-05, fluctuations: 0.0\n",
      "step: 63550 loss: 0.619687 time elapsed: 84.0150 learning rate: 0.001331, scenario: 0, slope: -7.190713633807938e-05, fluctuations: 0.0\n",
      "step: 63560 loss: 0.619006 time elapsed: 84.0274 learning rate: 0.001331, scenario: 0, slope: -7.11260494094592e-05, fluctuations: 0.0\n",
      "step: 63570 loss: 0.618331 time elapsed: 84.0397 learning rate: 0.001331, scenario: 0, slope: -7.038881077610399e-05, fluctuations: 0.0\n",
      "step: 63580 loss: 0.617661 time elapsed: 84.0521 learning rate: 0.001331, scenario: 0, slope: -6.969274338302962e-05, fluctuations: 0.0\n",
      "step: 63590 loss: 0.616997 time elapsed: 84.0646 learning rate: 0.001331, scenario: 0, slope: -6.903535739913805e-05, fluctuations: 0.0\n",
      "step: 63600 loss: 0.616337 time elapsed: 84.0768 learning rate: 0.001331, scenario: 0, slope: -6.847486402321265e-05, fluctuations: 0.0\n",
      "step: 63610 loss: 0.615682 time elapsed: 84.0896 learning rate: 0.001331, scenario: 0, slope: -6.782751925233014e-05, fluctuations: 0.0\n",
      "step: 63620 loss: 0.615031 time elapsed: 84.1022 learning rate: 0.001331, scenario: 0, slope: -6.727289642829825e-05, fluctuations: 0.0\n",
      "step: 63630 loss: 0.614384 time elapsed: 84.1159 learning rate: 0.001331, scenario: 0, slope: -6.674858980069131e-05, fluctuations: 0.0\n",
      "step: 63640 loss: 0.613741 time elapsed: 84.1295 learning rate: 0.001331, scenario: 0, slope: -6.625284673469858e-05, fluctuations: 0.0\n",
      "step: 63650 loss: 0.613102 time elapsed: 84.1433 learning rate: 0.001331, scenario: 0, slope: -6.578402982014419e-05, fluctuations: 0.0\n",
      "step: 63660 loss: 0.612466 time elapsed: 84.1570 learning rate: 0.001331, scenario: 0, slope: -6.534060832586965e-05, fluctuations: 0.0\n",
      "step: 63670 loss: 0.611833 time elapsed: 84.1708 learning rate: 0.001331, scenario: 0, slope: -6.492115040185381e-05, fluctuations: 0.0\n",
      "step: 63680 loss: 0.611203 time elapsed: 84.1843 learning rate: 0.001331, scenario: 0, slope: -6.45243159491515e-05, fluctuations: 0.0\n",
      "step: 63690 loss: 0.610577 time elapsed: 84.1982 learning rate: 0.001331, scenario: 0, slope: -6.414885008774723e-05, fluctuations: 0.0\n",
      "step: 63700 loss: 0.609953 time elapsed: 84.2108 learning rate: 0.001331, scenario: 0, slope: -6.382822773079462e-05, fluctuations: 0.0\n",
      "step: 63710 loss: 0.609332 time elapsed: 84.2235 learning rate: 0.001331, scenario: 0, slope: -6.345739522101421e-05, fluctuations: 0.0\n",
      "step: 63720 loss: 0.608713 time elapsed: 84.2357 learning rate: 0.001331, scenario: 0, slope: -6.313927095437564e-05, fluctuations: 0.0\n",
      "step: 63730 loss: 0.608096 time elapsed: 84.2479 learning rate: 0.001331, scenario: 0, slope: -6.28382349953016e-05, fluctuations: 0.0\n",
      "step: 63740 loss: 0.607482 time elapsed: 84.2600 learning rate: 0.001331, scenario: 0, slope: -6.25533776009505e-05, fluctuations: 0.0\n",
      "step: 63750 loss: 0.606870 time elapsed: 84.2721 learning rate: 0.001331, scenario: 0, slope: -6.228384464971347e-05, fluctuations: 0.0\n",
      "step: 63760 loss: 0.606259 time elapsed: 84.2843 learning rate: 0.001331, scenario: 0, slope: -6.202883393569736e-05, fluctuations: 0.0\n",
      "step: 63770 loss: 0.605651 time elapsed: 84.2971 learning rate: 0.001331, scenario: 0, slope: -6.178759173327898e-05, fluctuations: 0.0\n",
      "step: 63780 loss: 0.605044 time elapsed: 84.3097 learning rate: 0.001331, scenario: 0, slope: -6.15594096090061e-05, fluctuations: 0.0\n",
      "step: 63790 loss: 0.604439 time elapsed: 84.3227 learning rate: 0.001331, scenario: 0, slope: -6.134362146056634e-05, fluctuations: 0.0\n",
      "step: 63800 loss: 0.603836 time elapsed: 84.3366 learning rate: 0.001331, scenario: 0, slope: -6.115949037801505e-05, fluctuations: 0.0\n",
      "step: 63810 loss: 0.603234 time elapsed: 84.3507 learning rate: 0.001331, scenario: 0, slope: -6.094675801093229e-05, fluctuations: 0.0\n",
      "step: 63820 loss: 0.602616 time elapsed: 84.3642 learning rate: 0.001456, scenario: 1, slope: -6.0794004524869216e-05, fluctuations: 0.0\n",
      "step: 63830 loss: 0.601940 time elapsed: 84.3779 learning rate: 0.001545, scenario: 0, slope: -6.092064006922136e-05, fluctuations: 0.0\n",
      "step: 63840 loss: 0.601247 time elapsed: 84.3917 learning rate: 0.001545, scenario: 0, slope: -6.148881540808551e-05, fluctuations: 0.0\n",
      "step: 63850 loss: 0.614252 time elapsed: 84.4058 learning rate: 0.001576, scenario: 1, slope: -5.193095418736191e-05, fluctuations: 0.0\n",
      "step: 63860 loss: 171.856444 time elapsed: 84.4187 learning rate: 0.001462, scenario: -1, slope: 0.5003763896831841, fluctuations: 0.01\n",
      "step: 63870 loss: 78.653181 time elapsed: 84.4312 learning rate: 0.001322, scenario: -1, slope: 0.6459462520940665, fluctuations: 0.05\n",
      "step: 63880 loss: 24.911399 time elapsed: 84.4436 learning rate: 0.001196, scenario: -1, slope: 0.5701606223075173, fluctuations: 0.1\n",
      "step: 63890 loss: 3.408451 time elapsed: 84.4557 learning rate: 0.001081, scenario: -1, slope: 0.46897920262963977, fluctuations: 0.15\n",
      "step: 63900 loss: 2.468278 time elapsed: 84.4679 learning rate: 0.000988, scenario: -1, slope: 0.3344080608310855, fluctuations: 0.19\n",
      "step: 63910 loss: 1.351206 time elapsed: 84.4809 learning rate: 0.000893, scenario: -1, slope: 0.1261556169167432, fluctuations: 0.23\n",
      "step: 63920 loss: 0.868766 time elapsed: 84.4934 learning rate: 0.000841, scenario: 0, slope: -0.060611268248276284, fluctuations: 0.27\n",
      "step: 63930 loss: 0.685319 time elapsed: 84.5057 learning rate: 0.000841, scenario: 0, slope: -0.258741905266941, fluctuations: 0.31\n",
      "step: 63940 loss: 0.645496 time elapsed: 84.5178 learning rate: 0.000841, scenario: 0, slope: -0.48977236637333815, fluctuations: 0.35\n",
      "step: 63950 loss: 0.639922 time elapsed: 84.5315 learning rate: 0.000841, scenario: 0, slope: -0.8076695303486985, fluctuations: 0.39\n",
      "step: 63960 loss: 0.636881 time elapsed: 84.5452 learning rate: 0.000841, scenario: 0, slope: -0.4978423677867143, fluctuations: 0.41\n",
      "step: 63970 loss: 0.634221 time elapsed: 84.5588 learning rate: 0.000841, scenario: 0, slope: -0.06954053572467181, fluctuations: 0.41\n",
      "step: 63980 loss: 0.631825 time elapsed: 84.5722 learning rate: 0.000841, scenario: 0, slope: -0.04022697787463416, fluctuations: 0.39\n",
      "step: 63990 loss: 0.629728 time elapsed: 84.5858 learning rate: 0.000841, scenario: 0, slope: -0.016947389455825298, fluctuations: 0.34\n",
      "step: 64000 loss: 0.627927 time elapsed: 84.5994 learning rate: 0.000841, scenario: 0, slope: -0.006157424076914035, fluctuations: 0.3\n",
      "step: 64010 loss: 0.626340 time elapsed: 84.6150 learning rate: 0.000841, scenario: 0, slope: -0.0015714684453732156, fluctuations: 0.26\n",
      "step: 64020 loss: 0.624909 time elapsed: 84.6283 learning rate: 0.000841, scenario: 0, slope: -0.000685233144015756, fluctuations: 0.22\n",
      "step: 64030 loss: 0.623604 time elapsed: 84.6413 learning rate: 0.000841, scenario: 0, slope: -0.0003848019313136579, fluctuations: 0.18\n",
      "step: 64040 loss: 0.622403 time elapsed: 84.6538 learning rate: 0.000841, scenario: 0, slope: -0.000253311101092063, fluctuations: 0.14\n",
      "step: 64050 loss: 0.621292 time elapsed: 84.6662 learning rate: 0.000841, scenario: 0, slope: -0.00018961101129094578, fluctuations: 0.1\n",
      "step: 64060 loss: 0.620259 time elapsed: 84.6785 learning rate: 0.000841, scenario: 0, slope: -0.00015874429481056387, fluctuations: 0.06\n",
      "step: 64070 loss: 0.619295 time elapsed: 84.6907 learning rate: 0.000841, scenario: 0, slope: -0.00014028126000782313, fluctuations: 0.03\n",
      "step: 64080 loss: 0.618393 time elapsed: 84.7033 learning rate: 0.000841, scenario: 0, slope: -0.00012896832630913504, fluctuations: 0.0\n",
      "step: 64090 loss: 0.617545 time elapsed: 84.7161 learning rate: 0.000841, scenario: 0, slope: -0.00011820399486584738, fluctuations: 0.0\n",
      "step: 64100 loss: 0.616745 time elapsed: 84.7283 learning rate: 0.000841, scenario: 0, slope: -0.00011008569520437001, fluctuations: 0.0\n",
      "step: 64110 loss: 0.615988 time elapsed: 84.7425 learning rate: 0.000841, scenario: 0, slope: -0.00010150597782006111, fluctuations: 0.0\n",
      "step: 64120 loss: 0.615270 time elapsed: 84.7563 learning rate: 0.000841, scenario: 0, slope: -9.473881285941904e-05, fluctuations: 0.0\n",
      "step: 64130 loss: 0.614587 time elapsed: 84.7699 learning rate: 0.000841, scenario: 0, slope: -8.876585083376661e-05, fluctuations: 0.0\n",
      "step: 64140 loss: 0.613934 time elapsed: 84.7835 learning rate: 0.000841, scenario: 0, slope: -8.347033958619061e-05, fluctuations: 0.0\n",
      "step: 64150 loss: 0.613308 time elapsed: 84.7975 learning rate: 0.000841, scenario: 0, slope: -7.876187415991896e-05, fluctuations: 0.0\n",
      "step: 64160 loss: 0.612708 time elapsed: 84.8120 learning rate: 0.000841, scenario: 0, slope: -7.456665933373707e-05, fluctuations: 0.0\n",
      "step: 64170 loss: 0.612129 time elapsed: 84.8262 learning rate: 0.000841, scenario: 0, slope: -7.082259275067394e-05, fluctuations: 0.0\n",
      "step: 64180 loss: 0.611571 time elapsed: 84.8393 learning rate: 0.000841, scenario: 0, slope: -6.747650322481219e-05, fluctuations: 0.0\n",
      "step: 64190 loss: 0.611031 time elapsed: 84.8521 learning rate: 0.000841, scenario: 0, slope: -6.448242434158442e-05, fluctuations: 0.0\n",
      "step: 64200 loss: 0.610506 time elapsed: 84.8645 learning rate: 0.000841, scenario: 0, slope: -6.205555444893806e-05, fluctuations: 0.0\n",
      "step: 64210 loss: 0.609978 time elapsed: 84.8772 learning rate: 0.000929, scenario: 1, slope: -5.943075127717746e-05, fluctuations: 0.0\n",
      "step: 64220 loss: 0.609411 time elapsed: 84.8894 learning rate: 0.001026, scenario: 1, slope: -5.755759172047425e-05, fluctuations: 0.0\n",
      "step: 64230 loss: 0.608802 time elapsed: 84.9016 learning rate: 0.001134, scenario: 1, slope: -5.6384178046427076e-05, fluctuations: 0.0\n",
      "step: 64240 loss: 0.608149 time elapsed: 84.9139 learning rate: 0.001252, scenario: 1, slope: -5.606097407139653e-05, fluctuations: 0.0\n",
      "step: 64250 loss: 0.607449 time elapsed: 84.9260 learning rate: 0.001383, scenario: 1, slope: -5.668520216980672e-05, fluctuations: 0.0\n",
      "step: 64260 loss: 0.606697 time elapsed: 84.9385 learning rate: 0.001528, scenario: 1, slope: -5.830079883413019e-05, fluctuations: 0.0\n",
      "step: 64270 loss: 0.605892 time elapsed: 84.9520 learning rate: 0.001688, scenario: 1, slope: -6.0898434594488244e-05, fluctuations: 0.0\n",
      "step: 64280 loss: 0.605058 time elapsed: 84.9660 learning rate: 0.001688, scenario: 0, slope: -6.435743177191607e-05, fluctuations: 0.0\n",
      "step: 64290 loss: 0.604245 time elapsed: 84.9798 learning rate: 0.001688, scenario: 0, slope: -6.82100739284054e-05, fluctuations: 0.0\n",
      "step: 64300 loss: 0.603450 time elapsed: 84.9934 learning rate: 0.001688, scenario: 0, slope: -7.15594960802654e-05, fluctuations: 0.0\n",
      "step: 64310 loss: 0.602672 time elapsed: 85.0076 learning rate: 0.001688, scenario: 0, slope: -7.496942991926381e-05, fluctuations: 0.0\n",
      "step: 64320 loss: 0.601907 time elapsed: 85.0215 learning rate: 0.001688, scenario: 0, slope: -7.717624963968423e-05, fluctuations: 0.0\n",
      "step: 64330 loss: 0.601153 time elapsed: 85.0358 learning rate: 0.001688, scenario: 0, slope: -7.846244095464031e-05, fluctuations: 0.0\n",
      "step: 64340 loss: 0.600410 time elapsed: 85.0485 learning rate: 0.001688, scenario: 0, slope: -7.885296206808315e-05, fluctuations: 0.0\n",
      "step: 64350 loss: 0.599675 time elapsed: 85.0609 learning rate: 0.001688, scenario: 0, slope: -7.846355253449237e-05, fluctuations: 0.0\n",
      "step: 64360 loss: 0.598948 time elapsed: 85.0732 learning rate: 0.001688, scenario: 0, slope: -7.750315182503374e-05, fluctuations: 0.0\n",
      "step: 64370 loss: 0.598228 time elapsed: 85.0858 learning rate: 0.001688, scenario: 0, slope: -7.627801298198024e-05, fluctuations: 0.0\n",
      "step: 64380 loss: 0.597513 time elapsed: 85.0981 learning rate: 0.001688, scenario: 0, slope: -7.513401215779132e-05, fluctuations: 0.0\n",
      "step: 64390 loss: 0.596804 time elapsed: 85.1101 learning rate: 0.001688, scenario: 0, slope: -7.414598797717814e-05, fluctuations: 0.0\n",
      "step: 64400 loss: 0.596100 time elapsed: 85.1222 learning rate: 0.001688, scenario: 0, slope: -7.336818679761426e-05, fluctuations: 0.0\n",
      "step: 64410 loss: 0.595400 time elapsed: 85.1352 learning rate: 0.001688, scenario: 0, slope: -7.253639077921995e-05, fluctuations: 0.0\n",
      "step: 64420 loss: 0.594704 time elapsed: 85.1474 learning rate: 0.001688, scenario: 0, slope: -7.187460519357521e-05, fluctuations: 0.0\n",
      "step: 64430 loss: 0.594012 time elapsed: 85.1606 learning rate: 0.001688, scenario: 0, slope: -7.12875771100438e-05, fluctuations: 0.0\n",
      "step: 64440 loss: 0.593322 time elapsed: 85.1757 learning rate: 0.001688, scenario: 0, slope: -7.076355858038147e-05, fluctuations: 0.0\n",
      "step: 64450 loss: 0.592636 time elapsed: 85.1895 learning rate: 0.001688, scenario: 0, slope: -7.029304898818475e-05, fluctuations: 0.0\n",
      "step: 64460 loss: 0.591953 time elapsed: 85.2031 learning rate: 0.001688, scenario: 0, slope: -6.986833041811002e-05, fluctuations: 0.0\n",
      "step: 64470 loss: 0.591272 time elapsed: 85.2168 learning rate: 0.001688, scenario: 0, slope: -6.948309942756435e-05, fluctuations: 0.0\n",
      "step: 64480 loss: 0.590594 time elapsed: 85.2305 learning rate: 0.001688, scenario: 0, slope: -6.913217759637867e-05, fluctuations: 0.0\n",
      "step: 64490 loss: 0.589917 time elapsed: 85.2447 learning rate: 0.001688, scenario: 0, slope: -6.881128398287011e-05, fluctuations: 0.0\n",
      "step: 64500 loss: 0.589244 time elapsed: 85.2572 learning rate: 0.001688, scenario: 0, slope: -6.854494549453373e-05, fluctuations: 0.0\n",
      "step: 64510 loss: 0.589627 time elapsed: 85.2704 learning rate: 0.001688, scenario: 0, slope: -6.711502297723915e-05, fluctuations: 0.0\n",
      "step: 64520 loss: 8.541008 time elapsed: 85.2829 learning rate: 0.001662, scenario: -1, slope: 0.007513449122918782, fluctuations: 0.0\n",
      "step: 64530 loss: 29.744531 time elapsed: 85.2953 learning rate: 0.001503, scenario: -1, slope: 0.18313248473578086, fluctuations: 0.02\n",
      "step: 64540 loss: 7.179023 time elapsed: 85.3080 learning rate: 0.001360, scenario: -1, slope: 0.18345319402875776, fluctuations: 0.05\n",
      "step: 64550 loss: 3.214307 time elapsed: 85.3203 learning rate: 0.001230, scenario: -1, slope: 0.151976471768651, fluctuations: 0.08\n",
      "step: 64560 loss: 1.544988 time elapsed: 85.3327 learning rate: 0.001112, scenario: -1, slope: 0.11236511004054964, fluctuations: 0.12\n",
      "step: 64570 loss: 0.610482 time elapsed: 85.3451 learning rate: 0.001006, scenario: -1, slope: 0.06966442413184963, fluctuations: 0.17\n",
      "step: 64580 loss: 0.636406 time elapsed: 85.3575 learning rate: 0.000909, scenario: -1, slope: 0.01771185423401273, fluctuations: 0.22\n",
      "step: 64590 loss: 0.601179 time elapsed: 85.3708 learning rate: 0.000891, scenario: 0, slope: -0.0420933608449976, fluctuations: 0.27\n",
      "step: 64600 loss: 0.593713 time elapsed: 85.3848 learning rate: 0.000891, scenario: 0, slope: -0.10200589139052765, fluctuations: 0.31\n",
      "step: 64610 loss: 0.592305 time elapsed: 85.3990 learning rate: 0.000891, scenario: 0, slope: -0.18780891172213937, fluctuations: 0.37\n",
      "step: 64620 loss: 0.591721 time elapsed: 85.4127 learning rate: 0.000891, scenario: 0, slope: -0.2797844286048911, fluctuations: 0.42\n",
      "step: 64630 loss: 0.591216 time elapsed: 85.4263 learning rate: 0.000891, scenario: 0, slope: -0.04376462396125643, fluctuations: 0.45\n",
      "step: 64640 loss: 0.590734 time elapsed: 85.4399 learning rate: 0.000891, scenario: 0, slope: -0.010851775508169554, fluctuations: 0.47\n",
      "step: 64650 loss: 0.590273 time elapsed: 85.4547 learning rate: 0.000891, scenario: 0, slope: -0.0040655387533352575, fluctuations: 0.48\n",
      "step: 64660 loss: 0.589830 time elapsed: 85.4684 learning rate: 0.000891, scenario: 0, slope: -0.0010024007376439803, fluctuations: 0.45\n",
      "step: 64670 loss: 0.589400 time elapsed: 85.4811 learning rate: 0.000891, scenario: 0, slope: -0.0005674417384432338, fluctuations: 0.4\n",
      "step: 64680 loss: 0.588979 time elapsed: 85.4936 learning rate: 0.000891, scenario: 0, slope: -0.00018950309782214043, fluctuations: 0.35\n",
      "step: 64690 loss: 0.588565 time elapsed: 85.5061 learning rate: 0.000891, scenario: 0, slope: -8.296926705358267e-05, fluctuations: 0.3\n",
      "step: 64700 loss: 0.588162 time elapsed: 85.5186 learning rate: 0.000848, scenario: -1, slope: -4.78296204263732e-05, fluctuations: 0.26\n",
      "step: 64710 loss: 0.587792 time elapsed: 85.5315 learning rate: 0.000767, scenario: -1, slope: -4.746353324198619e-05, fluctuations: 0.2\n",
      "step: 64720 loss: 0.587460 time elapsed: 85.5437 learning rate: 0.000693, scenario: -1, slope: -4.3521194746746174e-05, fluctuations: 0.15\n",
      "step: 64730 loss: 0.587161 time elapsed: 85.5558 learning rate: 0.000627, scenario: -1, slope: -4.099510312358068e-05, fluctuations: 0.1\n",
      "step: 64740 loss: 0.586893 time elapsed: 85.5680 learning rate: 0.000567, scenario: -1, slope: -3.888880517562489e-05, fluctuations: 0.05\n",
      "step: 64750 loss: 0.586651 time elapsed: 85.5822 learning rate: 0.000521, scenario: 1, slope: -3.6911119628557765e-05, fluctuations: 0.0\n",
      "step: 64760 loss: 0.586413 time elapsed: 85.5964 learning rate: 0.000575, scenario: 1, slope: -3.440283862800042e-05, fluctuations: 0.0\n",
      "step: 64770 loss: 0.586150 time elapsed: 85.6102 learning rate: 0.000635, scenario: 1, slope: -3.1979114165111406e-05, fluctuations: 0.0\n",
      "step: 64780 loss: 0.585861 time elapsed: 85.6240 learning rate: 0.000702, scenario: 1, slope: -2.9894695315009242e-05, fluctuations: 0.0\n",
      "step: 64790 loss: 0.585542 time elapsed: 85.6375 learning rate: 0.000775, scenario: 1, slope: -2.841378992101958e-05, fluctuations: 0.0\n",
      "step: 64800 loss: 0.585191 time elapsed: 85.6509 learning rate: 0.000848, scenario: 1, slope: -2.780218898366792e-05, fluctuations: 0.0\n",
      "step: 64810 loss: 0.584808 time elapsed: 85.6661 learning rate: 0.000937, scenario: 1, slope: -2.8110534240639293e-05, fluctuations: 0.0\n",
      "step: 64820 loss: 0.584386 time elapsed: 85.6788 learning rate: 0.001034, scenario: 1, slope: -2.936468940812273e-05, fluctuations: 0.0\n",
      "step: 64830 loss: 0.583922 time elapsed: 85.6915 learning rate: 0.001143, scenario: 1, slope: -3.1462401146861156e-05, fluctuations: 0.0\n",
      "step: 64840 loss: 0.583411 time elapsed: 85.7039 learning rate: 0.001262, scenario: 1, slope: -3.4279155573528555e-05, fluctuations: 0.0\n",
      "step: 64850 loss: 0.582849 time elapsed: 85.7164 learning rate: 0.001394, scenario: 1, slope: -3.7646096548027465e-05, fluctuations: 0.0\n",
      "step: 64860 loss: 0.582232 time elapsed: 85.7287 learning rate: 0.001540, scenario: 1, slope: -4.1391937206993196e-05, fluctuations: 0.0\n",
      "step: 64870 loss: 0.581553 time elapsed: 85.7409 learning rate: 0.001701, scenario: 1, slope: -4.5505153478826306e-05, fluctuations: 0.0\n",
      "step: 64880 loss: 0.580807 time elapsed: 85.7532 learning rate: 0.001879, scenario: 1, slope: -5.00241009067742e-05, fluctuations: 0.0\n",
      "step: 64890 loss: 3.384869 time elapsed: 85.7655 learning rate: 0.002005, scenario: -1, slope: 0.0018786600765135677, fluctuations: 0.0\n",
      "step: 64900 loss: 30.160226 time elapsed: 85.7777 learning rate: 0.001831, scenario: -1, slope: 2.298597257753728, fluctuations: 0.02\n",
      "step: 64910 loss: 319.339313 time elapsed: 85.7914 learning rate: 0.001656, scenario: -1, slope: 2.0361593502106397, fluctuations: 0.07\n",
      "step: 64920 loss: 89.329406 time elapsed: 85.8053 learning rate: 0.001498, scenario: -1, slope: 1.7363032800190632, fluctuations: 0.11\n",
      "step: 64930 loss: 33.781841 time elapsed: 85.8195 learning rate: 0.001355, scenario: -1, slope: 1.3182609241954832, fluctuations: 0.15\n",
      "step: 64940 loss: 2.131993 time elapsed: 85.8334 learning rate: 0.001225, scenario: -1, slope: 0.8175941784336155, fluctuations: 0.19\n",
      "step: 64950 loss: 4.548964 time elapsed: 85.8474 learning rate: 0.001108, scenario: -1, slope: 0.23299654960560381, fluctuations: 0.22\n",
      "step: 64960 loss: 2.036441 time elapsed: 85.8612 learning rate: 0.001064, scenario: 0, slope: -0.3314568533518358, fluctuations: 0.26\n",
      "step: 64970 loss: 1.154490 time elapsed: 85.8754 learning rate: 0.001064, scenario: 0, slope: -0.9773713916353057, fluctuations: 0.29\n",
      "step: 64980 loss: 1.044869 time elapsed: 85.8880 learning rate: 0.001064, scenario: 0, slope: -1.7430612763473607, fluctuations: 0.32\n",
      "step: 64990 loss: 0.998461 time elapsed: 85.9007 learning rate: 0.001064, scenario: 0, slope: -2.8204255540570244, fluctuations: 0.35\n",
      "step: 65000 loss: 0.937087 time elapsed: 85.9128 learning rate: 0.001064, scenario: 0, slope: -0.7651556019235498, fluctuations: 0.35\n",
      "step: 65010 loss: 0.899732 time elapsed: 85.9258 learning rate: 0.001064, scenario: 0, slope: -0.3155945872868305, fluctuations: 0.33\n",
      "step: 65020 loss: 0.878019 time elapsed: 85.9403 learning rate: 0.001064, scenario: 0, slope: -0.11234302850446651, fluctuations: 0.29\n",
      "step: 65030 loss: 0.860340 time elapsed: 85.9537 learning rate: 0.001064, scenario: 0, slope: -0.049635433307887535, fluctuations: 0.25\n",
      "step: 65040 loss: 0.845292 time elapsed: 85.9666 learning rate: 0.001064, scenario: 0, slope: -0.025011387888689084, fluctuations: 0.21\n",
      "step: 65050 loss: 0.832616 time elapsed: 85.9797 learning rate: 0.001064, scenario: 0, slope: -0.009171974863108326, fluctuations: 0.18\n",
      "step: 65060 loss: 0.821584 time elapsed: 85.9939 learning rate: 0.001064, scenario: 0, slope: -0.004574773001741237, fluctuations: 0.15\n",
      "step: 65070 loss: 0.811816 time elapsed: 86.0089 learning rate: 0.001064, scenario: 0, slope: -0.002776159282585881, fluctuations: 0.12\n",
      "step: 65080 loss: 0.803100 time elapsed: 86.0240 learning rate: 0.001064, scenario: 0, slope: -0.0020479914231075064, fluctuations: 0.08\n",
      "step: 65090 loss: 0.795258 time elapsed: 86.0386 learning rate: 0.001064, scenario: 0, slope: -0.0015478914859376807, fluctuations: 0.05\n",
      "step: 65100 loss: 0.788155 time elapsed: 86.0535 learning rate: 0.001064, scenario: 0, slope: -0.0013124298565431998, fluctuations: 0.02\n",
      "step: 65110 loss: 0.781686 time elapsed: 86.0685 learning rate: 0.001064, scenario: 0, slope: -0.0011145520859007284, fluctuations: 0.0\n",
      "step: 65120 loss: 0.775761 time elapsed: 86.0828 learning rate: 0.001064, scenario: 0, slope: -0.0009724092619885459, fluctuations: 0.0\n",
      "step: 65130 loss: 0.770308 time elapsed: 86.0965 learning rate: 0.001064, scenario: 0, slope: -0.0008621853747018342, fluctuations: 0.0\n",
      "step: 65140 loss: 0.765264 time elapsed: 86.1098 learning rate: 0.001064, scenario: 0, slope: -0.0007731389592365591, fluctuations: 0.0\n",
      "step: 65150 loss: 0.760578 time elapsed: 86.1224 learning rate: 0.001064, scenario: 0, slope: -0.0006990318094282666, fluctuations: 0.0\n",
      "step: 65160 loss: 0.756207 time elapsed: 86.1348 learning rate: 0.001064, scenario: 0, slope: -0.0006362888088996712, fluctuations: 0.0\n",
      "step: 65170 loss: 0.752114 time elapsed: 86.1470 learning rate: 0.001064, scenario: 0, slope: -0.0005825764787079927, fluctuations: 0.0\n",
      "step: 65180 loss: 0.748267 time elapsed: 86.1594 learning rate: 0.001064, scenario: 0, slope: -0.0005362158573920192, fluctuations: 0.0\n",
      "step: 65190 loss: 0.744641 time elapsed: 86.1718 learning rate: 0.001064, scenario: 0, slope: -0.0004959432452631914, fluctuations: 0.0\n",
      "step: 65200 loss: 0.741211 time elapsed: 86.1840 learning rate: 0.001064, scenario: 0, slope: -0.00046407611602502716, fluctuations: 0.0\n",
      "step: 65210 loss: 0.737959 time elapsed: 86.1971 learning rate: 0.001064, scenario: 0, slope: -0.00042986892961706006, fluctuations: 0.0\n",
      "step: 65220 loss: 0.734868 time elapsed: 86.2107 learning rate: 0.001064, scenario: 0, slope: -0.00040259961677508615, fluctuations: 0.0\n",
      "step: 65230 loss: 0.731921 time elapsed: 86.2245 learning rate: 0.001064, scenario: 0, slope: -0.00037840926477488324, fluctuations: 0.0\n",
      "step: 65240 loss: 0.729107 time elapsed: 86.2381 learning rate: 0.001064, scenario: 0, slope: -0.00035684634596524535, fluctuations: 0.0\n",
      "step: 65250 loss: 0.726413 time elapsed: 86.2519 learning rate: 0.001064, scenario: 0, slope: -0.00033753653473396, fluctuations: 0.0\n",
      "step: 65260 loss: 0.723831 time elapsed: 86.2655 learning rate: 0.001064, scenario: 0, slope: -0.00032016855937784424, fluctuations: 0.0\n",
      "step: 65270 loss: 0.721350 time elapsed: 86.2791 learning rate: 0.001064, scenario: 0, slope: -0.0003044824822652152, fluctuations: 0.0\n",
      "step: 65280 loss: 0.718962 time elapsed: 86.2942 learning rate: 0.001064, scenario: 0, slope: -0.00029026012716133336, fluctuations: 0.0\n",
      "step: 65290 loss: 0.716662 time elapsed: 86.3075 learning rate: 0.001064, scenario: 0, slope: -0.00027731736819641516, fluctuations: 0.0\n",
      "step: 65300 loss: 0.714441 time elapsed: 86.3205 learning rate: 0.001064, scenario: 0, slope: -0.00026663330567438094, fluctuations: 0.0\n",
      "step: 65310 loss: 0.712296 time elapsed: 86.3337 learning rate: 0.001064, scenario: 0, slope: -0.0002546685827214585, fluctuations: 0.0\n",
      "step: 65320 loss: 0.710220 time elapsed: 86.3461 learning rate: 0.001064, scenario: 0, slope: -0.0002447149448789536, fluctuations: 0.0\n",
      "step: 65330 loss: 0.708208 time elapsed: 86.3586 learning rate: 0.001064, scenario: 0, slope: -0.0002355385676565709, fluctuations: 0.0\n",
      "step: 65340 loss: 0.706258 time elapsed: 86.3708 learning rate: 0.001064, scenario: 0, slope: -0.0002270542513221256, fluctuations: 0.0\n",
      "step: 65350 loss: 0.704365 time elapsed: 86.3831 learning rate: 0.001064, scenario: 0, slope: -0.00021918799244945115, fluctuations: 0.0\n",
      "step: 65360 loss: 0.702525 time elapsed: 86.3958 learning rate: 0.001064, scenario: 0, slope: -0.00021187528389368193, fluctuations: 0.0\n",
      "step: 65370 loss: 0.700736 time elapsed: 86.4098 learning rate: 0.001064, scenario: 0, slope: -0.0002050597142014977, fluctuations: 0.0\n",
      "step: 65380 loss: 0.698995 time elapsed: 86.4238 learning rate: 0.001064, scenario: 0, slope: -0.0001986918058284167, fluctuations: 0.0\n",
      "step: 65390 loss: 0.697299 time elapsed: 86.4374 learning rate: 0.001064, scenario: 0, slope: -0.00019272804521476363, fluctuations: 0.0\n",
      "step: 65400 loss: 0.695646 time elapsed: 86.4509 learning rate: 0.001064, scenario: 0, slope: -0.00018767440403662748, fluctuations: 0.0\n",
      "step: 65410 loss: 0.694034 time elapsed: 86.4648 learning rate: 0.001064, scenario: 0, slope: -0.0001818639722566511, fluctuations: 0.0\n",
      "step: 65420 loss: 0.692460 time elapsed: 86.4786 learning rate: 0.001064, scenario: 0, slope: -0.00017689973279770365, fluctuations: 0.0\n",
      "step: 65430 loss: 0.690923 time elapsed: 86.4923 learning rate: 0.001064, scenario: 0, slope: -0.00017221070593877215, fluctuations: 0.0\n",
      "step: 65440 loss: 0.689421 time elapsed: 86.5052 learning rate: 0.001064, scenario: 0, slope: -0.00016777320298583227, fluctuations: 0.0\n",
      "step: 65450 loss: 0.687953 time elapsed: 86.5178 learning rate: 0.001064, scenario: 0, slope: -0.0001635661251793119, fluctuations: 0.0\n",
      "step: 65460 loss: 0.686516 time elapsed: 86.5302 learning rate: 0.001064, scenario: 0, slope: -0.00015957064910214014, fluctuations: 0.0\n",
      "step: 65470 loss: 0.685111 time elapsed: 86.5430 learning rate: 0.001064, scenario: 0, slope: -0.00015576995491589033, fluctuations: 0.0\n",
      "step: 65480 loss: 0.683735 time elapsed: 86.5559 learning rate: 0.001064, scenario: 0, slope: -0.00015214899091784063, fluctuations: 0.0\n",
      "step: 65490 loss: 0.682387 time elapsed: 86.5695 learning rate: 0.001064, scenario: 0, slope: -0.0001486942689865087, fluctuations: 0.0\n",
      "step: 65500 loss: 0.681067 time elapsed: 86.5823 learning rate: 0.001064, scenario: 0, slope: -0.0001457171272229743, fluctuations: 0.0\n",
      "step: 65510 loss: 0.679772 time elapsed: 86.5960 learning rate: 0.001064, scenario: 0, slope: -0.00014223636986357397, fluctuations: 0.0\n",
      "step: 65520 loss: 0.678503 time elapsed: 86.6098 learning rate: 0.001064, scenario: 0, slope: -0.0001392125394435627, fluctuations: 0.0\n",
      "step: 65530 loss: 0.677258 time elapsed: 86.6247 learning rate: 0.001064, scenario: 0, slope: -0.00013631338806407737, fluctuations: 0.0\n",
      "step: 65540 loss: 0.676036 time elapsed: 86.6400 learning rate: 0.001064, scenario: 0, slope: -0.00013353097580773757, fluctuations: 0.0\n",
      "step: 65550 loss: 0.674836 time elapsed: 86.6548 learning rate: 0.001064, scenario: 0, slope: -0.00013085813608397766, fluctuations: 0.0\n",
      "step: 65560 loss: 0.673659 time elapsed: 86.6695 learning rate: 0.001064, scenario: 0, slope: -0.0001282883922979001, fluctuations: 0.0\n",
      "step: 65570 loss: 0.672502 time elapsed: 86.6843 learning rate: 0.001064, scenario: 0, slope: -0.00012581588355182335, fluctuations: 0.0\n",
      "step: 65580 loss: 0.671365 time elapsed: 86.6982 learning rate: 0.001064, scenario: 0, slope: -0.00012343529818215768, fluctuations: 0.0\n",
      "step: 65590 loss: 0.670248 time elapsed: 86.7125 learning rate: 0.001064, scenario: 0, slope: -0.00012114181412892833, fluctuations: 0.0\n",
      "step: 65600 loss: 0.669150 time elapsed: 86.7266 learning rate: 0.001064, scenario: 0, slope: -0.00011914851678725866, fluctuations: 0.0\n",
      "step: 65610 loss: 0.668069 time elapsed: 86.7402 learning rate: 0.001064, scenario: 0, slope: -0.00011679899324188857, fluctuations: 0.0\n",
      "step: 65620 loss: 0.667007 time elapsed: 86.7526 learning rate: 0.001064, scenario: 0, slope: -0.00011474200352895598, fluctuations: 0.0\n",
      "step: 65630 loss: 0.665961 time elapsed: 86.7648 learning rate: 0.001064, scenario: 0, slope: -0.00011275672642573028, fluctuations: 0.0\n",
      "step: 65640 loss: 0.664931 time elapsed: 86.7768 learning rate: 0.001064, scenario: 0, slope: -0.0001108400814087348, fluctuations: 0.0\n",
      "step: 65650 loss: 0.663918 time elapsed: 86.7892 learning rate: 0.001064, scenario: 0, slope: -0.00010898922524212361, fluctuations: 0.0\n",
      "step: 65660 loss: 0.662920 time elapsed: 86.8016 learning rate: 0.001064, scenario: 0, slope: -0.00010720152333560106, fluctuations: 0.0\n",
      "step: 65670 loss: 0.661936 time elapsed: 86.8140 learning rate: 0.001064, scenario: 0, slope: -0.00010547452415455562, fluctuations: 0.0\n",
      "step: 65680 loss: 0.660967 time elapsed: 86.8271 learning rate: 0.001064, scenario: 0, slope: -0.00010380593648289043, fluctuations: 0.0\n",
      "step: 65690 loss: 0.660012 time elapsed: 86.8413 learning rate: 0.001064, scenario: 0, slope: -0.00010219360935348362, fluctuations: 0.0\n",
      "step: 65700 loss: 0.659070 time elapsed: 86.8549 learning rate: 0.001064, scenario: 0, slope: -0.00010078893903325483, fluctuations: 0.0\n",
      "step: 65710 loss: 0.658141 time elapsed: 86.8689 learning rate: 0.001064, scenario: 0, slope: -9.912973096421549e-05, fluctuations: 0.0\n",
      "step: 65720 loss: 0.657224 time elapsed: 86.8827 learning rate: 0.001064, scenario: 0, slope: -9.767443226555695e-05, fluctuations: 0.0\n",
      "step: 65730 loss: 0.656319 time elapsed: 86.8965 learning rate: 0.001064, scenario: 0, slope: -9.626787498614713e-05, fluctuations: 0.0\n",
      "step: 65740 loss: 0.655426 time elapsed: 86.9103 learning rate: 0.001064, scenario: 0, slope: -9.490838955682593e-05, fluctuations: 0.0\n",
      "step: 65750 loss: 0.654545 time elapsed: 86.9241 learning rate: 0.001064, scenario: 0, slope: -9.359437247518711e-05, fluctuations: 0.0\n",
      "step: 65760 loss: 0.653674 time elapsed: 86.9364 learning rate: 0.001064, scenario: 0, slope: -9.232427996142213e-05, fluctuations: 0.0\n",
      "step: 65770 loss: 0.652814 time elapsed: 86.9488 learning rate: 0.001064, scenario: 0, slope: -9.109662283700082e-05, fluctuations: 0.0\n",
      "step: 65780 loss: 0.651964 time elapsed: 86.9614 learning rate: 0.001064, scenario: 0, slope: -8.990996244541659e-05, fluctuations: 0.0\n",
      "step: 65790 loss: 0.651123 time elapsed: 86.9737 learning rate: 0.001064, scenario: 0, slope: -8.876290744206127e-05, fluctuations: 0.0\n",
      "step: 65800 loss: 0.650293 time elapsed: 86.9859 learning rate: 0.001064, scenario: 0, slope: -8.776330674311725e-05, fluctuations: 0.0\n",
      "step: 65810 loss: 0.649471 time elapsed: 86.9990 learning rate: 0.001064, scenario: 0, slope: -8.658227034307294e-05, fluctuations: 0.0\n",
      "step: 65820 loss: 0.648658 time elapsed: 87.0114 learning rate: 0.001064, scenario: 0, slope: -8.554612230513455e-05, fluctuations: 0.0\n",
      "step: 65830 loss: 0.647854 time elapsed: 87.0239 learning rate: 0.001064, scenario: 0, slope: -8.454444509027865e-05, fluctuations: 0.0\n",
      "step: 65840 loss: 0.647059 time elapsed: 87.0385 learning rate: 0.001064, scenario: 0, slope: -8.357605585237511e-05, fluctuations: 0.0\n",
      "step: 65850 loss: 0.646271 time elapsed: 87.0537 learning rate: 0.001064, scenario: 0, slope: -8.263981018824786e-05, fluctuations: 0.0\n",
      "step: 65860 loss: 0.645491 time elapsed: 87.0686 learning rate: 0.001064, scenario: 0, slope: -8.173460142458774e-05, fluctuations: 0.0\n",
      "step: 65870 loss: 0.644719 time elapsed: 87.0834 learning rate: 0.001064, scenario: 0, slope: -8.085935994550884e-05, fluctuations: 0.0\n",
      "step: 65880 loss: 0.643953 time elapsed: 87.0976 learning rate: 0.001064, scenario: 0, slope: -8.001305252486789e-05, fluctuations: 0.0\n",
      "step: 65890 loss: 0.643195 time elapsed: 87.1121 learning rate: 0.001064, scenario: 0, slope: -7.919468164006228e-05, fluctuations: 0.0\n",
      "step: 65900 loss: 0.642444 time elapsed: 87.1267 learning rate: 0.001064, scenario: 0, slope: -7.848123733249999e-05, fluctuations: 0.0\n",
      "step: 65910 loss: 0.641699 time elapsed: 87.1430 learning rate: 0.001064, scenario: 0, slope: -7.763793354216721e-05, fluctuations: 0.0\n",
      "step: 65920 loss: 0.640961 time elapsed: 87.1568 learning rate: 0.001064, scenario: 0, slope: -7.689773310888658e-05, fluctuations: 0.0\n",
      "step: 65930 loss: 0.640229 time elapsed: 87.1712 learning rate: 0.001064, scenario: 0, slope: -7.618182111757059e-05, fluctuations: 0.0\n",
      "step: 65940 loss: 0.639503 time elapsed: 87.1844 learning rate: 0.001064, scenario: 0, slope: -7.548936691626882e-05, fluctuations: 0.0\n",
      "step: 65950 loss: 0.638782 time elapsed: 87.1972 learning rate: 0.001064, scenario: 0, slope: -7.48195706191291e-05, fluctuations: 0.0\n",
      "step: 65960 loss: 0.638067 time elapsed: 87.2097 learning rate: 0.001064, scenario: 0, slope: -7.417166216597031e-05, fluctuations: 0.0\n",
      "step: 65970 loss: 0.637358 time elapsed: 87.2221 learning rate: 0.001064, scenario: 0, slope: -7.354490036601784e-05, fluctuations: 0.0\n",
      "step: 65980 loss: 0.636653 time elapsed: 87.2344 learning rate: 0.001064, scenario: 0, slope: -7.293857193218655e-05, fluctuations: 0.0\n",
      "step: 65990 loss: 0.635954 time elapsed: 87.2475 learning rate: 0.001064, scenario: 0, slope: -7.235199051382256e-05, fluctuations: 0.0\n",
      "step: 66000 loss: 0.635260 time elapsed: 87.2612 learning rate: 0.001064, scenario: 0, slope: -7.1840404612381e-05, fluctuations: 0.0\n",
      "step: 66010 loss: 0.634570 time elapsed: 87.2755 learning rate: 0.001064, scenario: 0, slope: -7.123545223389891e-05, fluctuations: 0.0\n",
      "step: 66020 loss: 0.633885 time elapsed: 87.2891 learning rate: 0.001064, scenario: 0, slope: -7.070424873819902e-05, fluctuations: 0.0\n",
      "step: 66030 loss: 0.633204 time elapsed: 87.3026 learning rate: 0.001064, scenario: 0, slope: -7.019029713003041e-05, fluctuations: 0.0\n",
      "step: 66040 loss: 0.632528 time elapsed: 87.3164 learning rate: 0.001064, scenario: 0, slope: -6.969303155385347e-05, fluctuations: 0.0\n",
      "step: 66050 loss: 0.631856 time elapsed: 87.3305 learning rate: 0.001064, scenario: 0, slope: -6.921190753946629e-05, fluctuations: 0.0\n",
      "step: 66060 loss: 0.631188 time elapsed: 87.3439 learning rate: 0.001064, scenario: 0, slope: -6.874640115188138e-05, fluctuations: 0.0\n",
      "step: 66070 loss: 0.630523 time elapsed: 87.3565 learning rate: 0.001064, scenario: 0, slope: -6.829600816827587e-05, fluctuations: 0.0\n",
      "step: 66080 loss: 0.629863 time elapsed: 87.3690 learning rate: 0.001064, scenario: 0, slope: -6.786024328249761e-05, fluctuations: 0.0\n",
      "step: 66090 loss: 0.629206 time elapsed: 87.3814 learning rate: 0.001064, scenario: 0, slope: -6.743863933725217e-05, fluctuations: 0.0\n",
      "step: 66100 loss: 0.628552 time elapsed: 87.3934 learning rate: 0.001064, scenario: 0, slope: -6.707093132595633e-05, fluctuations: 0.0\n",
      "step: 66110 loss: 0.627902 time elapsed: 87.4065 learning rate: 0.001064, scenario: 0, slope: -6.663613197515582e-05, fluctuations: 0.0\n",
      "step: 66120 loss: 0.627256 time elapsed: 87.4190 learning rate: 0.001064, scenario: 0, slope: -6.625437847244966e-05, fluctuations: 0.0\n",
      "step: 66130 loss: 0.626612 time elapsed: 87.4315 learning rate: 0.001064, scenario: 0, slope: -6.588508439721578e-05, fluctuations: 0.0\n",
      "step: 66140 loss: 0.625972 time elapsed: 87.4440 learning rate: 0.001064, scenario: 0, slope: -6.552786279602623e-05, fluctuations: 0.0\n",
      "step: 66150 loss: 0.625334 time elapsed: 87.4585 learning rate: 0.001064, scenario: 0, slope: -6.518234083623971e-05, fluctuations: 0.0\n",
      "step: 66160 loss: 0.624699 time elapsed: 87.4737 learning rate: 0.001064, scenario: 0, slope: -6.484815922539594e-05, fluctuations: 0.0\n",
      "step: 66170 loss: 0.624068 time elapsed: 87.4885 learning rate: 0.001064, scenario: 0, slope: -6.452497165504453e-05, fluctuations: 0.0\n",
      "step: 66180 loss: 0.623438 time elapsed: 87.5031 learning rate: 0.001064, scenario: 0, slope: -6.421244426743618e-05, fluctuations: 0.0\n",
      "step: 66190 loss: 0.622812 time elapsed: 87.5182 learning rate: 0.001064, scenario: 0, slope: -6.391025514435052e-05, fluctuations: 0.0\n",
      "step: 66200 loss: 0.622188 time elapsed: 87.5323 learning rate: 0.001064, scenario: 0, slope: -6.364686731878887e-05, fluctuations: 0.0\n",
      "step: 66210 loss: 0.621566 time elapsed: 87.5492 learning rate: 0.001064, scenario: 0, slope: -6.333566079989846e-05, fluctuations: 0.0\n",
      "step: 66220 loss: 0.620947 time elapsed: 87.5626 learning rate: 0.001064, scenario: 0, slope: -6.306266713593387e-05, fluctuations: 0.0\n",
      "step: 66230 loss: 0.620329 time elapsed: 87.5760 learning rate: 0.001064, scenario: 0, slope: -6.279883397186327e-05, fluctuations: 0.0\n",
      "step: 66240 loss: 0.619714 time elapsed: 87.5889 learning rate: 0.001097, scenario: 1, slope: -6.254425679545154e-05, fluctuations: 0.0\n",
      "step: 66250 loss: 0.619060 time elapsed: 87.6014 learning rate: 0.001211, scenario: 1, slope: -6.239884344316879e-05, fluctuations: 0.0\n",
      "step: 66260 loss: 0.618345 time elapsed: 87.6137 learning rate: 0.001273, scenario: 0, slope: -6.266276351908543e-05, fluctuations: 0.0\n",
      "step: 66270 loss: 0.617618 time elapsed: 87.6262 learning rate: 0.001273, scenario: 0, slope: -6.34481004719208e-05, fluctuations: 0.0\n",
      "step: 66280 loss: 0.616895 time elapsed: 87.6386 learning rate: 0.001273, scenario: 0, slope: -6.462042237953617e-05, fluctuations: 0.0\n",
      "step: 66290 loss: 0.616175 time elapsed: 87.6512 learning rate: 0.001273, scenario: 0, slope: -6.603135464275587e-05, fluctuations: 0.0\n",
      "step: 66300 loss: 0.615458 time elapsed: 87.6645 learning rate: 0.001273, scenario: 0, slope: -6.738407253972534e-05, fluctuations: 0.0\n",
      "step: 66310 loss: 0.614744 time elapsed: 87.6790 learning rate: 0.001273, scenario: 0, slope: -6.898491010572439e-05, fluctuations: 0.0\n",
      "step: 66320 loss: 0.614033 time elapsed: 87.6929 learning rate: 0.001273, scenario: 0, slope: -7.024021510229339e-05, fluctuations: 0.0\n",
      "step: 66330 loss: 0.613324 time elapsed: 87.7064 learning rate: 0.001273, scenario: 0, slope: -7.115940188471954e-05, fluctuations: 0.0\n",
      "step: 66340 loss: 0.612619 time elapsed: 87.7200 learning rate: 0.001273, scenario: 0, slope: -7.160369988018347e-05, fluctuations: 0.0\n",
      "step: 66350 loss: 0.611915 time elapsed: 87.7338 learning rate: 0.001273, scenario: 0, slope: -7.154865544752435e-05, fluctuations: 0.0\n",
      "step: 66360 loss: 0.611215 time elapsed: 87.7472 learning rate: 0.001273, scenario: 0, slope: -7.126867281361176e-05, fluctuations: 0.0\n",
      "step: 66370 loss: 0.610517 time elapsed: 87.7614 learning rate: 0.001273, scenario: 0, slope: -7.098431627328218e-05, fluctuations: 0.0\n",
      "step: 66380 loss: 0.609821 time elapsed: 87.7743 learning rate: 0.001273, scenario: 0, slope: -7.071041398732645e-05, fluctuations: 0.0\n",
      "step: 66390 loss: 0.609127 time elapsed: 87.7869 learning rate: 0.001273, scenario: 0, slope: -7.04468054234505e-05, fluctuations: 0.0\n",
      "step: 66400 loss: 0.608435 time elapsed: 87.7991 learning rate: 0.001273, scenario: 0, slope: -7.021812167400594e-05, fluctuations: 0.0\n",
      "step: 66410 loss: 0.607746 time elapsed: 87.8121 learning rate: 0.001273, scenario: 0, slope: -6.994928168109034e-05, fluctuations: 0.0\n",
      "step: 66420 loss: 0.607058 time elapsed: 87.8245 learning rate: 0.001273, scenario: 0, slope: -6.971470741625308e-05, fluctuations: 0.0\n",
      "step: 66430 loss: 0.606382 time elapsed: 87.8371 learning rate: 0.001273, scenario: 0, slope: -6.947858633494402e-05, fluctuations: 0.0\n",
      "step: 66440 loss: 0.626034 time elapsed: 87.8496 learning rate: 0.001299, scenario: 1, slope: -4.7420857458983337e-05, fluctuations: 0.0\n",
      "step: 66450 loss: 31.147296 time elapsed: 87.8619 learning rate: 0.001204, scenario: -1, slope: 0.04393463675197804, fluctuations: 0.01\n",
      "step: 66460 loss: 2.512813 time elapsed: 87.8761 learning rate: 0.001089, scenario: -1, slope: 0.05924540234913922, fluctuations: 0.03\n",
      "step: 66470 loss: 2.461814 time elapsed: 87.8912 learning rate: 0.000985, scenario: -1, slope: 0.05395588175618096, fluctuations: 0.06\n",
      "step: 66480 loss: 1.213096 time elapsed: 87.9061 learning rate: 0.000891, scenario: -1, slope: 0.04276836731909362, fluctuations: 0.1\n",
      "step: 66490 loss: 0.632667 time elapsed: 87.9208 learning rate: 0.000806, scenario: -1, slope: 0.030481724071940046, fluctuations: 0.15\n",
      "step: 66500 loss: 0.660405 time elapsed: 87.9352 learning rate: 0.000736, scenario: -1, slope: 0.01586422737100219, fluctuations: 0.19\n",
      "step: 66510 loss: 0.622721 time elapsed: 87.9501 learning rate: 0.000686, scenario: 0, slope: -0.004264229876126102, fluctuations: 0.24\n",
      "step: 66520 loss: 0.609026 time elapsed: 87.9639 learning rate: 0.000686, scenario: 0, slope: -0.02400399640172856, fluctuations: 0.29\n",
      "step: 66530 loss: 0.604583 time elapsed: 87.9773 learning rate: 0.000686, scenario: 0, slope: -0.04645847542118685, fluctuations: 0.34\n",
      "step: 66540 loss: 0.603121 time elapsed: 87.9907 learning rate: 0.000686, scenario: 0, slope: -0.07109961949247572, fluctuations: 0.37\n",
      "step: 66550 loss: 0.602463 time elapsed: 88.0033 learning rate: 0.000686, scenario: 0, slope: -0.03627333915083706, fluctuations: 0.41\n",
      "step: 66560 loss: 0.602021 time elapsed: 88.0161 learning rate: 0.000686, scenario: 0, slope: -0.012624758910304207, fluctuations: 0.43\n",
      "step: 66570 loss: 0.601632 time elapsed: 88.0287 learning rate: 0.000686, scenario: 0, slope: -0.0018661066936763468, fluctuations: 0.41\n",
      "step: 66580 loss: 0.601258 time elapsed: 88.0412 learning rate: 0.000686, scenario: 0, slope: -0.0005243404028819668, fluctuations: 0.37\n",
      "step: 66590 loss: 0.600891 time elapsed: 88.0535 learning rate: 0.000686, scenario: 0, slope: -0.00031964907680920925, fluctuations: 0.32\n",
      "step: 66600 loss: 0.600526 time elapsed: 88.0656 learning rate: 0.000679, scenario: 0, slope: -0.000104427824653668, fluctuations: 0.28\n",
      "step: 66610 loss: 0.600171 time elapsed: 88.0788 learning rate: 0.000646, scenario: -1, slope: -4.99396429978438e-05, fluctuations: 0.23\n",
      "step: 66620 loss: 0.599839 time elapsed: 88.0929 learning rate: 0.000590, scenario: -1, slope: -4.5638140964921525e-05, fluctuations: 0.18\n",
      "step: 66630 loss: 0.599538 time elapsed: 88.1069 learning rate: 0.000534, scenario: -1, slope: -4.0634346282178703e-05, fluctuations: 0.13\n",
      "step: 66640 loss: 0.599266 time elapsed: 88.1205 learning rate: 0.000483, scenario: -1, slope: -3.651491993975452e-05, fluctuations: 0.1\n",
      "step: 66650 loss: 0.599020 time elapsed: 88.1343 learning rate: 0.000436, scenario: -1, slope: -3.445411374489583e-05, fluctuations: 0.05\n",
      "step: 66660 loss: 0.598797 time elapsed: 88.1480 learning rate: 0.000401, scenario: 1, slope: -3.2829311577429595e-05, fluctuations: 0.0\n",
      "step: 66670 loss: 0.598577 time elapsed: 88.1616 learning rate: 0.000443, scenario: 1, slope: -3.0816322276008923e-05, fluctuations: 0.0\n",
      "step: 66680 loss: 0.598334 time elapsed: 88.1751 learning rate: 0.000489, scenario: 1, slope: -2.885771705424121e-05, fluctuations: 0.0\n",
      "step: 66690 loss: 0.598065 time elapsed: 88.1880 learning rate: 0.000540, scenario: 1, slope: -2.7173664550228542e-05, fluctuations: 0.0\n",
      "step: 66700 loss: 0.597768 time elapsed: 88.2005 learning rate: 0.000591, scenario: 1, slope: -2.6096223827940166e-05, fluctuations: 0.0\n",
      "step: 66710 loss: 0.597442 time elapsed: 88.2135 learning rate: 0.000652, scenario: 1, slope: -2.5576676438310486e-05, fluctuations: 0.0\n",
      "step: 66720 loss: 0.597082 time elapsed: 88.2261 learning rate: 0.000721, scenario: 1, slope: -2.598797019770969e-05, fluctuations: 0.0\n",
      "step: 66730 loss: 0.596685 time elapsed: 88.2388 learning rate: 0.000796, scenario: 1, slope: -2.7252645828608643e-05, fluctuations: 0.0\n",
      "step: 66740 loss: 0.596246 time elapsed: 88.2516 learning rate: 0.000879, scenario: 1, slope: -2.9307582624221477e-05, fluctuations: 0.0\n",
      "step: 66750 loss: 0.595761 time elapsed: 88.2641 learning rate: 0.000971, scenario: 1, slope: -3.204767216067821e-05, fluctuations: 0.0\n",
      "step: 66760 loss: 0.595225 time elapsed: 88.2766 learning rate: 0.001073, scenario: 1, slope: -3.5324578751162665e-05, fluctuations: 0.0\n",
      "step: 66770 loss: 0.594635 time elapsed: 88.2904 learning rate: 0.001185, scenario: 1, slope: -3.898556324281241e-05, fluctuations: 0.0\n",
      "step: 66780 loss: 0.593983 time elapsed: 88.3046 learning rate: 0.001309, scenario: 1, slope: -4.3024026806814045e-05, fluctuations: 0.0\n",
      "step: 66790 loss: 0.593265 time elapsed: 88.3186 learning rate: 0.001446, scenario: 1, slope: -4.747930557495559e-05, fluctuations: 0.0\n",
      "step: 66800 loss: 0.742872 time elapsed: 88.3325 learning rate: 0.001582, scenario: 1, slope: -3.6480052155990115e-05, fluctuations: 0.0\n",
      "step: 66810 loss: 390.322333 time elapsed: 88.3469 learning rate: 0.001438, scenario: -1, slope: 0.8939906106420475, fluctuations: 0.02\n",
      "step: 66820 loss: 77.524863 time elapsed: 88.3609 learning rate: 0.001300, scenario: -1, slope: 0.898562139076053, fluctuations: 0.07\n",
      "step: 66830 loss: 35.330812 time elapsed: 88.3757 learning rate: 0.001176, scenario: -1, slope: 0.8715510489509801, fluctuations: 0.11\n",
      "step: 66840 loss: 15.027663 time elapsed: 88.3888 learning rate: 0.001064, scenario: -1, slope: 0.6797352583193911, fluctuations: 0.15\n",
      "step: 66850 loss: 3.261834 time elapsed: 88.4014 learning rate: 0.000962, scenario: -1, slope: 0.43647913545333206, fluctuations: 0.19\n",
      "step: 66860 loss: 1.497992 time elapsed: 88.4139 learning rate: 0.000870, scenario: -1, slope: 0.17715681397074803, fluctuations: 0.23\n",
      "step: 66870 loss: 0.789374 time elapsed: 88.4267 learning rate: 0.000819, scenario: 0, slope: -0.11593912777462598, fluctuations: 0.26\n",
      "step: 66880 loss: 0.855654 time elapsed: 88.4398 learning rate: 0.000819, scenario: 0, slope: -0.4187180582776112, fluctuations: 0.29\n",
      "step: 66890 loss: 0.717816 time elapsed: 88.4533 learning rate: 0.000819, scenario: 0, slope: -0.7813024987637525, fluctuations: 0.33\n",
      "step: 66900 loss: 0.684917 time elapsed: 88.4663 learning rate: 0.000819, scenario: 0, slope: -1.2334466263350188, fluctuations: 0.36\n",
      "step: 66910 loss: 0.678403 time elapsed: 88.4800 learning rate: 0.000819, scenario: 0, slope: -0.5202463124877497, fluctuations: 0.36\n",
      "step: 66920 loss: 0.668174 time elapsed: 88.4928 learning rate: 0.000819, scenario: 0, slope: -0.23500671786726368, fluctuations: 0.35\n",
      "step: 66930 loss: 0.662719 time elapsed: 88.5072 learning rate: 0.000819, scenario: 0, slope: -0.059189110186069566, fluctuations: 0.33\n",
      "step: 66940 loss: 0.658764 time elapsed: 88.5214 learning rate: 0.000819, scenario: 0, slope: -0.019950365406867447, fluctuations: 0.29\n",
      "step: 66950 loss: 0.655121 time elapsed: 88.5355 learning rate: 0.000819, scenario: 0, slope: -0.00854000461073005, fluctuations: 0.25\n",
      "step: 66960 loss: 0.652038 time elapsed: 88.5492 learning rate: 0.000819, scenario: 0, slope: -0.002633221309801824, fluctuations: 0.22\n",
      "step: 66970 loss: 0.649249 time elapsed: 88.5629 learning rate: 0.000819, scenario: 0, slope: -0.0014633380444835438, fluctuations: 0.18\n",
      "step: 66980 loss: 0.646713 time elapsed: 88.5764 learning rate: 0.000819, scenario: 0, slope: -0.0006955607732259602, fluctuations: 0.15\n",
      "step: 66990 loss: 0.644398 time elapsed: 88.5898 learning rate: 0.000819, scenario: 0, slope: -0.0004755013944008237, fluctuations: 0.12\n",
      "step: 67000 loss: 0.642267 time elapsed: 88.6028 learning rate: 0.000819, scenario: 0, slope: -0.00037269549802242704, fluctuations: 0.09\n",
      "step: 67010 loss: 0.640300 time elapsed: 88.6163 learning rate: 0.000819, scenario: 0, slope: -0.0003100562209077386, fluctuations: 0.05\n",
      "step: 67020 loss: 0.638478 time elapsed: 88.6291 learning rate: 0.000819, scenario: 0, slope: -0.0002755442401696353, fluctuations: 0.02\n",
      "step: 67030 loss: 0.636786 time elapsed: 88.6420 learning rate: 0.000819, scenario: 0, slope: -0.00025078890198956875, fluctuations: 0.0\n",
      "step: 67040 loss: 0.635209 time elapsed: 88.6546 learning rate: 0.000819, scenario: 0, slope: -0.00022802692610699487, fluctuations: 0.0\n",
      "step: 67050 loss: 0.633736 time elapsed: 88.6674 learning rate: 0.000819, scenario: 0, slope: -0.00020914524798174009, fluctuations: 0.0\n",
      "step: 67060 loss: 0.632358 time elapsed: 88.6801 learning rate: 0.000819, scenario: 0, slope: -0.00019285500551731203, fluctuations: 0.0\n",
      "step: 67070 loss: 0.631065 time elapsed: 88.6929 learning rate: 0.000819, scenario: 0, slope: -0.0001785479861084572, fluctuations: 0.0\n",
      "step: 67080 loss: 0.629849 time elapsed: 88.7073 learning rate: 0.000819, scenario: 0, slope: -0.00016585540268979524, fluctuations: 0.0\n",
      "step: 67090 loss: 0.628703 time elapsed: 88.7214 learning rate: 0.000819, scenario: 0, slope: -0.00015451122680393715, fluctuations: 0.0\n",
      "step: 67100 loss: 0.627620 time elapsed: 88.7363 learning rate: 0.000819, scenario: 0, slope: -0.00014529729397839926, fluctuations: 0.0\n",
      "step: 67110 loss: 0.626595 time elapsed: 88.7507 learning rate: 0.000819, scenario: 0, slope: -0.00013515086131590806, fluctuations: 0.0\n",
      "step: 67120 loss: 0.625622 time elapsed: 88.7648 learning rate: 0.000819, scenario: 0, slope: -0.00012686338343409698, fluctuations: 0.0\n",
      "step: 67130 loss: 0.624697 time elapsed: 88.7787 learning rate: 0.000819, scenario: 0, slope: -0.00011936338657769725, fluctuations: 0.0\n",
      "step: 67140 loss: 0.623816 time elapsed: 88.7926 learning rate: 0.000819, scenario: 0, slope: -0.0001125653954447503, fluctuations: 0.0\n",
      "step: 67150 loss: 0.622974 time elapsed: 88.8058 learning rate: 0.000819, scenario: 0, slope: -0.00010639579114329482, fluctuations: 0.0\n",
      "step: 67160 loss: 0.622168 time elapsed: 88.8190 learning rate: 0.000819, scenario: 0, slope: -0.00010079037943677837, fluctuations: 0.0\n",
      "step: 67170 loss: 0.621395 time elapsed: 88.8322 learning rate: 0.000819, scenario: 0, slope: -9.569270892449829e-05, fluctuations: 0.0\n",
      "step: 67180 loss: 0.620653 time elapsed: 88.8450 learning rate: 0.000819, scenario: 0, slope: -9.105279857215845e-05, fluctuations: 0.0\n",
      "step: 67190 loss: 0.619938 time elapsed: 88.8576 learning rate: 0.000819, scenario: 0, slope: -8.682617932040787e-05, fluctuations: 0.0\n",
      "step: 67200 loss: 0.619248 time elapsed: 88.8699 learning rate: 0.000819, scenario: 0, slope: -8.334267519653546e-05, fluctuations: 0.0\n",
      "step: 67210 loss: 0.618581 time elapsed: 88.8828 learning rate: 0.000819, scenario: 0, slope: -7.945811292618415e-05, fluctuations: 0.0\n",
      "step: 67220 loss: 0.617936 time elapsed: 88.8952 learning rate: 0.000819, scenario: 0, slope: -7.624918787333652e-05, fluctuations: 0.0\n",
      "step: 67230 loss: 0.617310 time elapsed: 88.9078 learning rate: 0.000819, scenario: 0, slope: -7.331767544067098e-05, fluctuations: 0.0\n",
      "step: 67240 loss: 0.616701 time elapsed: 88.9218 learning rate: 0.000819, scenario: 0, slope: -7.063775402776886e-05, fluctuations: 0.0\n",
      "step: 67250 loss: 0.616109 time elapsed: 88.9354 learning rate: 0.000819, scenario: 0, slope: -6.818615618777727e-05, fluctuations: 0.0\n",
      "step: 67260 loss: 0.615532 time elapsed: 88.9496 learning rate: 0.000819, scenario: 0, slope: -6.594189593637074e-05, fluctuations: 0.0\n",
      "step: 67270 loss: 0.614968 time elapsed: 88.9636 learning rate: 0.000819, scenario: 0, slope: -6.3886028967624e-05, fluctuations: 0.0\n",
      "step: 67280 loss: 0.614418 time elapsed: 88.9776 learning rate: 0.000827, scenario: 1, slope: -6.200144081906982e-05, fluctuations: 0.0\n",
      "step: 67290 loss: 0.613854 time elapsed: 88.9918 learning rate: 0.000914, scenario: 1, slope: -6.032413350033126e-05, fluctuations: 0.0\n",
      "step: 67300 loss: 0.613245 time elapsed: 89.0057 learning rate: 0.000999, scenario: 1, slope: -5.9180371308943776e-05, fluctuations: 0.0\n",
      "step: 67310 loss: 0.612593 time elapsed: 89.0199 learning rate: 0.001104, scenario: 1, slope: -5.849288321650472e-05, fluctuations: 0.0\n",
      "step: 67320 loss: 0.611890 time elapsed: 89.0330 learning rate: 0.001219, scenario: 1, slope: -5.871379729596613e-05, fluctuations: 0.0\n",
      "step: 67330 loss: 0.611133 time elapsed: 89.0458 learning rate: 0.001347, scenario: 1, slope: -5.9870452070506966e-05, fluctuations: 0.0\n",
      "step: 67340 loss: 0.610317 time elapsed: 89.0587 learning rate: 0.001459, scenario: 0, slope: -6.202921334494657e-05, fluctuations: 0.0\n",
      "step: 67350 loss: 0.609486 time elapsed: 89.0713 learning rate: 0.001459, scenario: 0, slope: -6.509090903440968e-05, fluctuations: 0.0\n",
      "step: 67360 loss: 0.608675 time elapsed: 89.0850 learning rate: 0.001459, scenario: 0, slope: -6.861234603107849e-05, fluctuations: 0.0\n",
      "step: 67370 loss: 0.607883 time elapsed: 89.1012 learning rate: 0.001459, scenario: 0, slope: -7.211335775023485e-05, fluctuations: 0.0\n",
      "step: 67380 loss: 0.607106 time elapsed: 89.1145 learning rate: 0.001459, scenario: 0, slope: -7.51379482825188e-05, fluctuations: 0.0\n",
      "step: 67390 loss: 0.606344 time elapsed: 89.1279 learning rate: 0.001459, scenario: 0, slope: -7.730843548952187e-05, fluctuations: 0.0\n",
      "step: 67400 loss: 0.605595 time elapsed: 89.1420 learning rate: 0.001459, scenario: 0, slope: -7.844429926380916e-05, fluctuations: 0.0\n",
      "step: 67410 loss: 0.604857 time elapsed: 89.1566 learning rate: 0.001459, scenario: 0, slope: -7.881016203662198e-05, fluctuations: 0.0\n",
      "step: 67420 loss: 0.604130 time elapsed: 89.1705 learning rate: 0.001459, scenario: 0, slope: -7.82839532407049e-05, fluctuations: 0.0\n",
      "step: 67430 loss: 0.603412 time elapsed: 89.1845 learning rate: 0.001459, scenario: 0, slope: -7.717861302388278e-05, fluctuations: 0.0\n",
      "step: 67440 loss: 0.602704 time elapsed: 89.1984 learning rate: 0.001459, scenario: 0, slope: -7.583107843551008e-05, fluctuations: 0.0\n",
      "step: 67450 loss: 0.602003 time elapsed: 89.2127 learning rate: 0.001459, scenario: 0, slope: -7.456776200595385e-05, fluctuations: 0.0\n",
      "step: 67460 loss: 0.601309 time elapsed: 89.2263 learning rate: 0.001459, scenario: 0, slope: -7.342950184535647e-05, fluctuations: 0.0\n",
      "step: 67470 loss: 0.600622 time elapsed: 89.2394 learning rate: 0.001459, scenario: 0, slope: -7.24001476089974e-05, fluctuations: 0.0\n",
      "step: 67480 loss: 0.599942 time elapsed: 89.2521 learning rate: 0.001459, scenario: 0, slope: -7.146532391075161e-05, fluctuations: 0.0\n",
      "step: 67490 loss: 0.599267 time elapsed: 89.2646 learning rate: 0.001459, scenario: 0, slope: -7.061267397765965e-05, fluctuations: 0.0\n",
      "step: 67500 loss: 0.598597 time elapsed: 89.2769 learning rate: 0.001459, scenario: 0, slope: -6.990684012247829e-05, fluctuations: 0.0\n",
      "step: 67510 loss: 0.597933 time elapsed: 89.2897 learning rate: 0.001459, scenario: 0, slope: -6.91135315840968e-05, fluctuations: 0.0\n",
      "step: 67520 loss: 0.597274 time elapsed: 89.3021 learning rate: 0.001459, scenario: 0, slope: -6.845063350757909e-05, fluctuations: 0.0\n",
      "step: 67530 loss: 0.596619 time elapsed: 89.3143 learning rate: 0.001459, scenario: 0, slope: -6.783662544110728e-05, fluctuations: 0.0\n",
      "step: 67540 loss: 0.595968 time elapsed: 89.3266 learning rate: 0.001459, scenario: 0, slope: -6.726607336763055e-05, fluctuations: 0.0\n",
      "step: 67550 loss: 0.595321 time elapsed: 89.3403 learning rate: 0.001459, scenario: 0, slope: -6.673433688956352e-05, fluctuations: 0.0\n",
      "step: 67560 loss: 0.594678 time elapsed: 89.3543 learning rate: 0.001459, scenario: 0, slope: -6.623743906631612e-05, fluctuations: 0.0\n",
      "step: 67570 loss: 0.594038 time elapsed: 89.3713 learning rate: 0.001459, scenario: 0, slope: -6.577195855991335e-05, fluctuations: 0.0\n",
      "step: 67580 loss: 0.593402 time elapsed: 89.3857 learning rate: 0.001459, scenario: 0, slope: -6.533494025870611e-05, fluctuations: 0.0\n",
      "step: 67590 loss: 0.592769 time elapsed: 89.3993 learning rate: 0.001459, scenario: 0, slope: -6.492382115806574e-05, fluctuations: 0.0\n",
      "step: 67600 loss: 0.592138 time elapsed: 89.4126 learning rate: 0.001459, scenario: 0, slope: -6.457410670009141e-05, fluctuations: 0.0\n",
      "step: 67610 loss: 0.591511 time elapsed: 89.4270 learning rate: 0.001459, scenario: 0, slope: -6.417063025792025e-05, fluctuations: 0.0\n",
      "step: 67620 loss: 0.590886 time elapsed: 89.4400 learning rate: 0.001459, scenario: 0, slope: -6.382488929682682e-05, fluctuations: 0.0\n",
      "step: 67630 loss: 0.590264 time elapsed: 89.4525 learning rate: 0.001459, scenario: 0, slope: -6.349763115221527e-05, fluctuations: 0.0\n",
      "step: 67640 loss: 0.589645 time elapsed: 89.4652 learning rate: 0.001459, scenario: 0, slope: -6.318751276945235e-05, fluctuations: 0.0\n",
      "step: 67650 loss: 0.589028 time elapsed: 89.4774 learning rate: 0.001459, scenario: 0, slope: -6.289333805405362e-05, fluctuations: 0.0\n",
      "step: 67660 loss: 0.588413 time elapsed: 89.4895 learning rate: 0.001459, scenario: 0, slope: -6.261403711353164e-05, fluctuations: 0.0\n",
      "step: 67670 loss: 0.587800 time elapsed: 89.5019 learning rate: 0.001459, scenario: 0, slope: -6.234864883197023e-05, fluctuations: 0.0\n",
      "step: 67680 loss: 0.587189 time elapsed: 89.5140 learning rate: 0.001459, scenario: 0, slope: -6.209630620989191e-05, fluctuations: 0.0\n",
      "step: 67690 loss: 0.586580 time elapsed: 89.5264 learning rate: 0.001459, scenario: 0, slope: -6.185622400235899e-05, fluctuations: 0.0\n",
      "step: 67700 loss: 0.585973 time elapsed: 89.5384 learning rate: 0.001459, scenario: 0, slope: -6.16500412713857e-05, fluctuations: 0.0\n",
      "step: 67710 loss: 0.585367 time elapsed: 89.5529 learning rate: 0.001459, scenario: 0, slope: -6.141004751242479e-05, fluctuations: 0.0\n",
      "step: 67720 loss: 0.584764 time elapsed: 89.5667 learning rate: 0.001459, scenario: 0, slope: -6.120270515291005e-05, fluctuations: 0.0\n",
      "step: 67730 loss: 0.584161 time elapsed: 89.5805 learning rate: 0.001459, scenario: 0, slope: -6.10051131001536e-05, fluctuations: 0.0\n",
      "step: 67740 loss: 0.583561 time elapsed: 89.5942 learning rate: 0.001459, scenario: 0, slope: -6.081676626117656e-05, fluctuations: 0.0\n",
      "step: 67750 loss: 0.582962 time elapsed: 89.6078 learning rate: 0.001459, scenario: 0, slope: -6.0637197822414024e-05, fluctuations: 0.0\n",
      "step: 67760 loss: 0.582364 time elapsed: 89.6216 learning rate: 0.001459, scenario: 0, slope: -6.0465975185284715e-05, fluctuations: 0.0\n",
      "step: 67770 loss: 0.581767 time elapsed: 89.6361 learning rate: 0.001459, scenario: 0, slope: -6.0302696451114295e-05, fluctuations: 0.0\n",
      "step: 67780 loss: 0.581172 time elapsed: 89.6521 learning rate: 0.001459, scenario: 0, slope: -6.0146987369929235e-05, fluctuations: 0.0\n",
      "step: 67790 loss: 0.580578 time elapsed: 89.6654 learning rate: 0.001459, scenario: 0, slope: -5.9998498681517356e-05, fluctuations: 0.0\n",
      "step: 67800 loss: 0.579985 time elapsed: 89.6797 learning rate: 0.001459, scenario: 0, slope: -5.9870761930903395e-05, fluctuations: 0.0\n",
      "step: 67810 loss: 0.579393 time elapsed: 89.6947 learning rate: 0.001459, scenario: 0, slope: -5.972189671627127e-05, fluctuations: 0.0\n",
      "step: 67820 loss: 0.578803 time elapsed: 89.7090 learning rate: 0.001459, scenario: 0, slope: -5.959319030570997e-05, fluctuations: 0.0\n",
      "step: 67830 loss: 0.578213 time elapsed: 89.7236 learning rate: 0.001459, scenario: 0, slope: -5.9470514624467625e-05, fluctuations: 0.0\n",
      "step: 67840 loss: 0.577624 time elapsed: 89.7377 learning rate: 0.001459, scenario: 0, slope: -5.935361554774847e-05, fluctuations: 0.0\n",
      "step: 67850 loss: 0.577036 time elapsed: 89.7518 learning rate: 0.001459, scenario: 0, slope: -5.924225349536874e-05, fluctuations: 0.0\n",
      "step: 67860 loss: 0.576449 time elapsed: 89.7666 learning rate: 0.001459, scenario: 0, slope: -5.913620230162824e-05, fluctuations: 0.0\n",
      "step: 67870 loss: 0.575863 time elapsed: 89.7810 learning rate: 0.001459, scenario: 0, slope: -5.90352481567231e-05, fluctuations: 0.0\n",
      "step: 67880 loss: 0.575277 time elapsed: 89.7949 learning rate: 0.001459, scenario: 0, slope: -5.893915302048244e-05, fluctuations: 0.0\n",
      "step: 67890 loss: 0.574737 time elapsed: 89.8087 learning rate: 0.001459, scenario: 0, slope: -5.879783623921212e-05, fluctuations: 0.0\n",
      "step: 67900 loss: 0.832844 time elapsed: 89.8225 learning rate: 0.001525, scenario: -1, slope: 1.9716567494915686e-05, fluctuations: 0.0\n",
      "step: 67910 loss: 25.094613 time elapsed: 89.8370 learning rate: 0.001380, scenario: -1, slope: 0.2099772983512556, fluctuations: 0.01\n",
      "step: 67920 loss: 26.481616 time elapsed: 89.8519 learning rate: 0.001248, scenario: -1, slope: 0.25783131263338394, fluctuations: 0.04\n",
      "step: 67930 loss: 8.106546 time elapsed: 89.8678 learning rate: 0.001128, scenario: -1, slope: 0.22600369523479674, fluctuations: 0.08\n",
      "step: 67940 loss: 0.926384 time elapsed: 89.8818 learning rate: 0.001020, scenario: -1, slope: 0.1834695102886408, fluctuations: 0.13\n",
      "step: 67950 loss: 1.032313 time elapsed: 89.8961 learning rate: 0.000923, scenario: -1, slope: 0.10772213060319193, fluctuations: 0.16\n",
      "step: 67960 loss: 0.679013 time elapsed: 89.9100 learning rate: 0.000835, scenario: -1, slope: 0.04390178981239028, fluctuations: 0.21\n",
      "step: 67970 loss: 0.614348 time elapsed: 89.9242 learning rate: 0.000786, scenario: 0, slope: -0.030954374640541335, fluctuations: 0.26\n",
      "step: 67980 loss: 0.605149 time elapsed: 89.9380 learning rate: 0.000786, scenario: 0, slope: -0.11774502223885652, fluctuations: 0.3\n",
      "step: 67990 loss: 0.585068 time elapsed: 89.9516 learning rate: 0.000786, scenario: 0, slope: -0.21692243146482165, fluctuations: 0.35\n",
      "step: 68000 loss: 0.584920 time elapsed: 89.9652 learning rate: 0.000786, scenario: 0, slope: -0.32979089786406707, fluctuations: 0.39\n",
      "step: 68010 loss: 0.582209 time elapsed: 89.9798 learning rate: 0.000786, scenario: 0, slope: -0.18738435650916535, fluctuations: 0.42\n",
      "step: 68020 loss: 0.581252 time elapsed: 89.9939 learning rate: 0.000786, scenario: 0, slope: -0.03161352441200433, fluctuations: 0.44\n",
      "step: 68030 loss: 0.580739 time elapsed: 90.0077 learning rate: 0.000786, scenario: 0, slope: -0.009087021303880118, fluctuations: 0.43\n",
      "step: 68040 loss: 0.580140 time elapsed: 90.0212 learning rate: 0.000786, scenario: 0, slope: -0.005126524743862449, fluctuations: 0.4\n",
      "step: 68050 loss: 0.579678 time elapsed: 90.0348 learning rate: 0.000786, scenario: 0, slope: -0.001276618025770096, fluctuations: 0.37\n",
      "step: 68060 loss: 0.579239 time elapsed: 90.0485 learning rate: 0.000786, scenario: 0, slope: -0.00042668130679205806, fluctuations: 0.32\n",
      "step: 68070 loss: 0.578809 time elapsed: 90.0624 learning rate: 0.000786, scenario: 0, slope: -0.0001989587149310623, fluctuations: 0.27\n",
      "step: 68080 loss: 0.578396 time elapsed: 90.0758 learning rate: 0.000786, scenario: 0, slope: -9.285920069617608e-05, fluctuations: 0.23\n",
      "step: 68090 loss: 0.577994 time elapsed: 90.0896 learning rate: 0.000786, scenario: 0, slope: -6.561151144332005e-05, fluctuations: 0.18\n",
      "step: 68100 loss: 0.577607 time elapsed: 90.1036 learning rate: 0.000732, scenario: -1, slope: -5.143963515343789e-05, fluctuations: 0.14\n",
      "step: 68110 loss: 0.577259 time elapsed: 90.1171 learning rate: 0.000662, scenario: -1, slope: -4.5023737896412544e-05, fluctuations: 0.09\n",
      "step: 68120 loss: 0.576948 time elapsed: 90.1294 learning rate: 0.000599, scenario: -1, slope: -4.237190080839856e-05, fluctuations: 0.04\n",
      "step: 68130 loss: 0.576670 time elapsed: 90.1417 learning rate: 0.000542, scenario: -1, slope: -3.9948578266928764e-05, fluctuations: 0.01\n",
      "step: 68140 loss: 0.576402 time elapsed: 90.1540 learning rate: 0.000596, scenario: 1, slope: -3.773571174290385e-05, fluctuations: 0.0\n",
      "step: 68150 loss: 0.576110 time elapsed: 90.1665 learning rate: 0.000658, scenario: 1, slope: -3.552711193876305e-05, fluctuations: 0.0\n",
      "step: 68160 loss: 0.575790 time elapsed: 90.1791 learning rate: 0.000727, scenario: 1, slope: -3.362336505906241e-05, fluctuations: 0.0\n",
      "step: 68170 loss: 0.575439 time elapsed: 90.1933 learning rate: 0.000803, scenario: 1, slope: -3.221226537688989e-05, fluctuations: 0.0\n",
      "step: 68180 loss: 0.575055 time elapsed: 90.2078 learning rate: 0.000887, scenario: 1, slope: -3.148026347426843e-05, fluctuations: 0.0\n",
      "step: 68190 loss: 0.574634 time elapsed: 90.2216 learning rate: 0.000979, scenario: 1, slope: -3.160199485206069e-05, fluctuations: 0.0\n",
      "step: 68200 loss: 0.574174 time elapsed: 90.2354 learning rate: 0.001071, scenario: 1, slope: -3.255731218597334e-05, fluctuations: 0.0\n",
      "step: 68210 loss: 0.573674 time elapsed: 90.2498 learning rate: 0.001183, scenario: 1, slope: -3.4775918797528055e-05, fluctuations: 0.0\n",
      "step: 68220 loss: 0.573128 time elapsed: 90.2635 learning rate: 0.001307, scenario: 1, slope: -3.7629043844006064e-05, fluctuations: 0.0\n",
      "step: 68230 loss: 0.572531 time elapsed: 90.2780 learning rate: 0.001444, scenario: 1, slope: -4.10723697372429e-05, fluctuations: 0.0\n",
      "step: 68240 loss: 0.571878 time elapsed: 90.2913 learning rate: 0.001595, scenario: 1, slope: -4.489390620012596e-05, fluctuations: 0.0\n",
      "step: 68250 loss: 0.571164 time elapsed: 90.3040 learning rate: 0.001762, scenario: 1, slope: -4.9057634943442206e-05, fluctuations: 0.0\n",
      "step: 68260 loss: 0.570393 time elapsed: 90.3167 learning rate: 0.001946, scenario: 1, slope: -5.359386669956293e-05, fluctuations: 0.0\n",
      "step: 68270 loss: 1062.758780 time elapsed: 90.3291 learning rate: 0.001955, scenario: -1, slope: 0.7636170413817528, fluctuations: 0.0\n",
      "step: 68280 loss: 362.529594 time elapsed: 90.3417 learning rate: 0.001768, scenario: -1, slope: 2.4230283091261877, fluctuations: 0.04\n",
      "step: 68290 loss: 173.453768 time elapsed: 90.3552 learning rate: 0.001599, scenario: -1, slope: 2.499247996934136, fluctuations: 0.08\n",
      "step: 68300 loss: 56.546405 time elapsed: 90.3678 learning rate: 0.001461, scenario: -1, slope: 2.1284712401315202, fluctuations: 0.12\n",
      "step: 68310 loss: 17.768072 time elapsed: 90.3812 learning rate: 0.001321, scenario: -1, slope: 1.4897585137799145, fluctuations: 0.16\n",
      "step: 68320 loss: 2.466499 time elapsed: 90.3936 learning rate: 0.001195, scenario: -1, slope: 0.8344600076257942, fluctuations: 0.2\n",
      "step: 68330 loss: 2.297047 time elapsed: 90.4078 learning rate: 0.001080, scenario: -1, slope: 0.07679180081870719, fluctuations: 0.23\n",
      "step: 68340 loss: 1.699188 time elapsed: 90.4217 learning rate: 0.001080, scenario: 0, slope: -0.6883351793313027, fluctuations: 0.26\n",
      "step: 68350 loss: 1.050336 time elapsed: 90.4358 learning rate: 0.001080, scenario: 0, slope: -1.5284466110727253, fluctuations: 0.3\n",
      "step: 68360 loss: 0.890021 time elapsed: 90.4498 learning rate: 0.001080, scenario: 0, slope: -2.586663354776019, fluctuations: 0.33\n",
      "step: 68370 loss: 0.874114 time elapsed: 90.4639 learning rate: 0.001080, scenario: 0, slope: -5.052786815363338, fluctuations: 0.35\n",
      "step: 68380 loss: 0.834730 time elapsed: 90.4779 learning rate: 0.001080, scenario: 0, slope: -0.8155233363165596, fluctuations: 0.35\n",
      "step: 68390 loss: 0.807573 time elapsed: 90.4925 learning rate: 0.001080, scenario: 0, slope: -0.21567844347102455, fluctuations: 0.34\n",
      "step: 68400 loss: 0.793916 time elapsed: 90.5055 learning rate: 0.001080, scenario: 0, slope: -0.1276284312746797, fluctuations: 0.32\n",
      "step: 68410 loss: 0.781456 time elapsed: 90.5189 learning rate: 0.001080, scenario: 0, slope: -0.03523095967087082, fluctuations: 0.28\n",
      "step: 68420 loss: 0.770808 time elapsed: 90.5316 learning rate: 0.001080, scenario: 0, slope: -0.013379071834936678, fluctuations: 0.25\n",
      "step: 68430 loss: 0.761917 time elapsed: 90.5443 learning rate: 0.001080, scenario: 0, slope: -0.006567954335122341, fluctuations: 0.21\n",
      "step: 68440 loss: 0.754004 time elapsed: 90.5571 learning rate: 0.001080, scenario: 0, slope: -0.00303909709552246, fluctuations: 0.18\n",
      "step: 68450 loss: 0.746965 time elapsed: 90.5697 learning rate: 0.001080, scenario: 0, slope: -0.001874195961572259, fluctuations: 0.15\n",
      "step: 68460 loss: 0.740659 time elapsed: 90.5819 learning rate: 0.001080, scenario: 0, slope: -0.0014210711789139853, fluctuations: 0.11\n",
      "step: 68470 loss: 0.734956 time elapsed: 90.5942 learning rate: 0.001080, scenario: 0, slope: -0.0010788888988886168, fluctuations: 0.08\n",
      "step: 68480 loss: 0.729762 time elapsed: 90.6070 learning rate: 0.001080, scenario: 0, slope: -0.0008923451346308905, fluctuations: 0.05\n",
      "step: 68490 loss: 0.724999 time elapsed: 90.6210 learning rate: 0.001080, scenario: 0, slope: -0.0007879821174020962, fluctuations: 0.01\n",
      "step: 68500 loss: 0.720604 time elapsed: 90.6350 learning rate: 0.001080, scenario: 0, slope: -0.0007063835118236777, fluctuations: 0.0\n",
      "step: 68510 loss: 0.716528 time elapsed: 90.6493 learning rate: 0.001080, scenario: 0, slope: -0.0006235928009492167, fluctuations: 0.0\n",
      "step: 68520 loss: 0.712730 time elapsed: 90.6628 learning rate: 0.001080, scenario: 0, slope: -0.0005631028138656465, fluctuations: 0.0\n",
      "step: 68530 loss: 0.709179 time elapsed: 90.6769 learning rate: 0.001080, scenario: 0, slope: -0.0005124136036222846, fluctuations: 0.0\n",
      "step: 68540 loss: 0.705848 time elapsed: 90.6907 learning rate: 0.001080, scenario: 0, slope: -0.000469462670125208, fluctuations: 0.0\n",
      "step: 68550 loss: 0.702712 time elapsed: 90.7044 learning rate: 0.001080, scenario: 0, slope: -0.0004327170665232551, fluctuations: 0.0\n",
      "step: 68560 loss: 0.699753 time elapsed: 90.7175 learning rate: 0.001080, scenario: 0, slope: -0.00040098887529287195, fluctuations: 0.0\n",
      "step: 68570 loss: 0.696954 time elapsed: 90.7299 learning rate: 0.001080, scenario: 0, slope: -0.00037333850539642927, fluctuations: 0.0\n",
      "step: 68580 loss: 0.694300 time elapsed: 90.7428 learning rate: 0.001080, scenario: 0, slope: -0.0003490155617263237, fluctuations: 0.0\n",
      "step: 68590 loss: 0.691777 time elapsed: 90.7555 learning rate: 0.001080, scenario: 0, slope: -0.0003274413388404406, fluctuations: 0.0\n",
      "step: 68600 loss: 0.689375 time elapsed: 90.7681 learning rate: 0.001080, scenario: 0, slope: -0.00031000575151091326, fluctuations: 0.0\n",
      "step: 68610 loss: 0.687082 time elapsed: 90.7814 learning rate: 0.001080, scenario: 0, slope: -0.00029086253927929366, fluctuations: 0.0\n",
      "step: 68620 loss: 0.684891 time elapsed: 90.7939 learning rate: 0.001080, scenario: 0, slope: -0.0002752389539788264, fluctuations: 0.0\n",
      "step: 68630 loss: 0.682792 time elapsed: 90.8063 learning rate: 0.001080, scenario: 0, slope: -0.00026108004981381417, fluctuations: 0.0\n",
      "step: 68640 loss: 0.680779 time elapsed: 90.8187 learning rate: 0.001080, scenario: 0, slope: -0.0002482040651712642, fluctuations: 0.0\n",
      "step: 68650 loss: 0.678844 time elapsed: 90.8322 learning rate: 0.001080, scenario: 0, slope: -0.00023645948841912026, fluctuations: 0.0\n",
      "step: 68660 loss: 0.676982 time elapsed: 90.8463 learning rate: 0.001080, scenario: 0, slope: -0.00022571837905604282, fluctuations: 0.0\n",
      "step: 68670 loss: 0.675187 time elapsed: 90.8602 learning rate: 0.001080, scenario: 0, slope: -0.000215871482039732, fluctuations: 0.0\n",
      "step: 68680 loss: 0.673455 time elapsed: 90.8739 learning rate: 0.001080, scenario: 0, slope: -0.00020682460348088504, fluctuations: 0.0\n",
      "step: 68690 loss: 0.671780 time elapsed: 90.8873 learning rate: 0.001080, scenario: 0, slope: -0.0001984958750123846, fluctuations: 0.0\n",
      "step: 68700 loss: 0.670160 time elapsed: 90.9009 learning rate: 0.001080, scenario: 0, slope: -0.00019155464012445098, fluctuations: 0.0\n",
      "step: 68710 loss: 0.668589 time elapsed: 90.9149 learning rate: 0.001080, scenario: 0, slope: -0.00018371487576673476, fluctuations: 0.0\n",
      "step: 68720 loss: 0.667066 time elapsed: 90.9288 learning rate: 0.001080, scenario: 0, slope: -0.0001771437685339683, fluctuations: 0.0\n",
      "step: 68730 loss: 0.665586 time elapsed: 90.9419 learning rate: 0.001080, scenario: 0, slope: -0.00017105079407539359, fluctuations: 0.0\n",
      "step: 68740 loss: 0.664147 time elapsed: 90.9545 learning rate: 0.001080, scenario: 0, slope: -0.0001653918048995984, fluctuations: 0.0\n",
      "step: 68750 loss: 0.662746 time elapsed: 90.9670 learning rate: 0.001080, scenario: 0, slope: -0.0001601273422909671, fluctuations: 0.0\n",
      "step: 68760 loss: 0.661382 time elapsed: 90.9792 learning rate: 0.001080, scenario: 0, slope: -0.00015522205272431493, fluctuations: 0.0\n",
      "step: 68770 loss: 0.660051 time elapsed: 90.9913 learning rate: 0.001080, scenario: 0, slope: -0.0001506441965847873, fluctuations: 0.0\n",
      "step: 68780 loss: 0.658752 time elapsed: 91.0033 learning rate: 0.001080, scenario: 0, slope: -0.0001463652306012033, fluctuations: 0.0\n",
      "step: 68790 loss: 0.657483 time elapsed: 91.0154 learning rate: 0.001080, scenario: 0, slope: -0.0001423594497936801, fluctuations: 0.0\n",
      "step: 68800 loss: 0.656243 time elapsed: 91.0277 learning rate: 0.001080, scenario: 0, slope: -0.00013896861747871294, fluctuations: 0.0\n",
      "step: 68810 loss: 0.655029 time elapsed: 91.0416 learning rate: 0.001080, scenario: 0, slope: -0.00013507699837513493, fluctuations: 0.0\n",
      "step: 68820 loss: 0.653841 time elapsed: 91.0559 learning rate: 0.001080, scenario: 0, slope: -0.00013176051744086766, fluctuations: 0.0\n",
      "step: 68830 loss: 0.652677 time elapsed: 91.0699 learning rate: 0.001080, scenario: 0, slope: -0.00012863715694689105, fluctuations: 0.0\n",
      "step: 68840 loss: 0.651536 time elapsed: 91.0846 learning rate: 0.001080, scenario: 0, slope: -0.0001256914697413206, fluctuations: 0.0\n",
      "step: 68850 loss: 0.650417 time elapsed: 91.0988 learning rate: 0.001080, scenario: 0, slope: -0.00012290947595714656, fluctuations: 0.0\n",
      "step: 68860 loss: 0.649318 time elapsed: 91.1136 learning rate: 0.001080, scenario: 0, slope: -0.00012027851680133567, fluctuations: 0.0\n",
      "step: 68870 loss: 0.648239 time elapsed: 91.1279 learning rate: 0.001080, scenario: 0, slope: -0.00011778712362194402, fluctuations: 0.0\n",
      "step: 68880 loss: 0.647178 time elapsed: 91.1440 learning rate: 0.001080, scenario: 0, slope: -0.00011542490032908167, fluctuations: 0.0\n",
      "step: 68890 loss: 0.646136 time elapsed: 91.1600 learning rate: 0.001080, scenario: 0, slope: -0.00011318241755569938, fluctuations: 0.0\n",
      "step: 68900 loss: 0.645110 time elapsed: 91.1744 learning rate: 0.001080, scenario: 0, slope: -0.00011125947134172886, fluctuations: 0.0\n",
      "step: 68910 loss: 0.644101 time elapsed: 91.1902 learning rate: 0.001080, scenario: 0, slope: -0.00010902322616553373, fluctuations: 0.0\n",
      "step: 68920 loss: 0.643108 time elapsed: 91.2057 learning rate: 0.001080, scenario: 0, slope: -0.00010709167840983837, fluctuations: 0.0\n",
      "step: 68930 loss: 0.642129 time elapsed: 91.2198 learning rate: 0.001080, scenario: 0, slope: -0.00010525004431572597, fluctuations: 0.0\n",
      "step: 68940 loss: 0.641165 time elapsed: 91.2339 learning rate: 0.001080, scenario: 0, slope: -0.00010349246682601848, fluctuations: 0.0\n",
      "step: 68950 loss: 0.640215 time elapsed: 91.2481 learning rate: 0.001080, scenario: 0, slope: -0.00010181360364148445, fluctuations: 0.0\n",
      "step: 68960 loss: 0.639277 time elapsed: 91.2630 learning rate: 0.001080, scenario: 0, slope: -0.00010020857499004138, fluctuations: 0.0\n",
      "step: 68970 loss: 0.638353 time elapsed: 91.2781 learning rate: 0.001080, scenario: 0, slope: -9.867291650590737e-05, fluctuations: 0.0\n",
      "step: 68980 loss: 0.637440 time elapsed: 91.2928 learning rate: 0.001080, scenario: 0, slope: -9.720253681710372e-05, fluctuations: 0.0\n",
      "step: 68990 loss: 0.636540 time elapsed: 91.3076 learning rate: 0.001080, scenario: 0, slope: -9.579367947772134e-05, fluctuations: 0.0\n",
      "step: 69000 loss: 0.635650 time elapsed: 91.3223 learning rate: 0.001080, scenario: 0, slope: -9.45754477292149e-05, fluctuations: 0.0\n",
      "step: 69010 loss: 0.634771 time elapsed: 91.3375 learning rate: 0.001080, scenario: 0, slope: -9.314698003960777e-05, fluctuations: 0.0\n",
      "step: 69020 loss: 0.633903 time elapsed: 91.3521 learning rate: 0.001080, scenario: 0, slope: -9.190301131526733e-05, fluctuations: 0.0\n",
      "step: 69030 loss: 0.633045 time elapsed: 91.3663 learning rate: 0.001080, scenario: 0, slope: -9.070826082010209e-05, fluctuations: 0.0\n",
      "step: 69040 loss: 0.632196 time elapsed: 91.3800 learning rate: 0.001080, scenario: 0, slope: -8.95602051729443e-05, fluctuations: 0.0\n",
      "step: 69050 loss: 0.631357 time elapsed: 91.3945 learning rate: 0.001080, scenario: 0, slope: -8.845650093276042e-05, fluctuations: 0.0\n",
      "step: 69060 loss: 0.630526 time elapsed: 91.4089 learning rate: 0.001080, scenario: 0, slope: -8.739496822238826e-05, fluctuations: 0.0\n",
      "step: 69070 loss: 0.629704 time elapsed: 91.4233 learning rate: 0.001080, scenario: 0, slope: -8.637357629609851e-05, fluctuations: 0.0\n",
      "step: 69080 loss: 0.628890 time elapsed: 91.4374 learning rate: 0.001080, scenario: 0, slope: -8.539043079124458e-05, fluctuations: 0.0\n",
      "step: 69090 loss: 0.628085 time elapsed: 91.4520 learning rate: 0.001080, scenario: 0, slope: -8.444376242048702e-05, fluctuations: 0.0\n",
      "step: 69100 loss: 0.627287 time elapsed: 91.4671 learning rate: 0.001080, scenario: 0, slope: -8.362157933087894e-05, fluctuations: 0.0\n",
      "step: 69110 loss: 0.626496 time elapsed: 91.4829 learning rate: 0.001080, scenario: 0, slope: -8.2653345807279e-05, fluctuations: 0.0\n",
      "step: 69120 loss: 0.625713 time elapsed: 91.4976 learning rate: 0.001080, scenario: 0, slope: -8.180659852587784e-05, fluctuations: 0.0\n",
      "step: 69130 loss: 0.624937 time elapsed: 91.5123 learning rate: 0.001080, scenario: 0, slope: -8.099031459069378e-05, fluctuations: 0.0\n",
      "step: 69140 loss: 0.624167 time elapsed: 91.5271 learning rate: 0.001080, scenario: 0, slope: -8.020321686279717e-05, fluctuations: 0.0\n",
      "step: 69150 loss: 0.623404 time elapsed: 91.5417 learning rate: 0.001080, scenario: 0, slope: -7.94441051164526e-05, fluctuations: 0.0\n",
      "step: 69160 loss: 0.622647 time elapsed: 91.5563 learning rate: 0.001080, scenario: 0, slope: -7.871185008182402e-05, fluctuations: 0.0\n",
      "step: 69170 loss: 0.621896 time elapsed: 91.5705 learning rate: 0.001080, scenario: 0, slope: -7.800538787924251e-05, fluctuations: 0.0\n",
      "step: 69180 loss: 0.621151 time elapsed: 91.5847 learning rate: 0.001080, scenario: 0, slope: -7.732371481430267e-05, fluctuations: 0.0\n",
      "step: 69190 loss: 0.620411 time elapsed: 91.5982 learning rate: 0.001080, scenario: 0, slope: -7.66658825144504e-05, fluctuations: 0.0\n",
      "step: 69200 loss: 0.619677 time elapsed: 91.6121 learning rate: 0.001080, scenario: 0, slope: -7.609347446621778e-05, fluctuations: 0.0\n",
      "step: 69210 loss: 0.618948 time elapsed: 91.6267 learning rate: 0.001080, scenario: 0, slope: -7.541819646568606e-05, fluctuations: 0.0\n",
      "step: 69220 loss: 0.618224 time elapsed: 91.6409 learning rate: 0.001080, scenario: 0, slope: -7.482668342537065e-05, fluctuations: 0.0\n",
      "step: 69230 loss: 0.617505 time elapsed: 91.6570 learning rate: 0.001080, scenario: 0, slope: -7.42556851253656e-05, fluctuations: 0.0\n",
      "step: 69240 loss: 0.616791 time elapsed: 91.6730 learning rate: 0.001080, scenario: 0, slope: -7.370446831513115e-05, fluctuations: 0.0\n",
      "step: 69250 loss: 0.616081 time elapsed: 91.6880 learning rate: 0.001080, scenario: 0, slope: -7.31723327118658e-05, fluctuations: 0.0\n",
      "step: 69260 loss: 0.615376 time elapsed: 91.7029 learning rate: 0.001080, scenario: 0, slope: -7.265860836911497e-05, fluctuations: 0.0\n",
      "step: 69270 loss: 0.614674 time elapsed: 91.7172 learning rate: 0.001080, scenario: 0, slope: -7.216265333206732e-05, fluctuations: 0.0\n",
      "step: 69280 loss: 0.613977 time elapsed: 91.7313 learning rate: 0.001080, scenario: 0, slope: -7.168385156317817e-05, fluctuations: 0.0\n",
      "step: 69290 loss: 0.613284 time elapsed: 91.7453 learning rate: 0.001080, scenario: 0, slope: -7.122161111874307e-05, fluctuations: 0.0\n",
      "step: 69300 loss: 0.612595 time elapsed: 91.7600 learning rate: 0.001080, scenario: 0, slope: -7.081928356396514e-05, fluctuations: 0.0\n",
      "step: 69310 loss: 0.611909 time elapsed: 91.7752 learning rate: 0.001080, scenario: 0, slope: -7.034455754176547e-05, fluctuations: 0.0\n",
      "step: 69320 loss: 0.611227 time elapsed: 91.7895 learning rate: 0.001080, scenario: 0, slope: -6.992866765777951e-05, fluctuations: 0.0\n",
      "step: 69330 loss: 0.610548 time elapsed: 91.8042 learning rate: 0.001080, scenario: 0, slope: -6.95271833484533e-05, fluctuations: 0.0\n",
      "step: 69340 loss: 0.609873 time elapsed: 91.8185 learning rate: 0.001080, scenario: 0, slope: -6.913961302034941e-05, fluctuations: 0.0\n",
      "step: 69350 loss: 0.609200 time elapsed: 91.8317 learning rate: 0.001080, scenario: 0, slope: -6.876548225247535e-05, fluctuations: 0.0\n",
      "step: 69360 loss: 0.608531 time elapsed: 91.8458 learning rate: 0.001080, scenario: 0, slope: -6.840433310504908e-05, fluctuations: 0.0\n",
      "step: 69370 loss: 0.607865 time elapsed: 91.8607 learning rate: 0.001080, scenario: 0, slope: -6.805572350794466e-05, fluctuations: 0.0\n",
      "step: 69380 loss: 0.607202 time elapsed: 91.8761 learning rate: 0.001080, scenario: 0, slope: -6.771922671242724e-05, fluctuations: 0.0\n",
      "step: 69390 loss: 0.606541 time elapsed: 91.8911 learning rate: 0.001080, scenario: 0, slope: -6.73944307935974e-05, fluctuations: 0.0\n",
      "step: 69400 loss: 0.605883 time elapsed: 91.9060 learning rate: 0.001080, scenario: 0, slope: -6.71117898429239e-05, fluctuations: 0.0\n",
      "step: 69410 loss: 0.605228 time elapsed: 91.9217 learning rate: 0.001080, scenario: 0, slope: -6.677836528020181e-05, fluctuations: 0.0\n",
      "step: 69420 loss: 0.604575 time elapsed: 91.9361 learning rate: 0.001080, scenario: 0, slope: -6.648634196100542e-05, fluctuations: 0.0\n",
      "step: 69430 loss: 0.603925 time elapsed: 91.9507 learning rate: 0.001080, scenario: 0, slope: -6.62045112668714e-05, fluctuations: 0.0\n",
      "step: 69440 loss: 0.603277 time elapsed: 91.9650 learning rate: 0.001080, scenario: 0, slope: -6.593252898018805e-05, fluctuations: 0.0\n",
      "step: 69450 loss: 0.602631 time elapsed: 91.9787 learning rate: 0.001080, scenario: 0, slope: -6.567006325609958e-05, fluctuations: 0.0\n",
      "step: 69460 loss: 0.601987 time elapsed: 91.9931 learning rate: 0.001080, scenario: 0, slope: -6.541679425099844e-05, fluctuations: 0.0\n",
      "step: 69470 loss: 0.601345 time elapsed: 92.0074 learning rate: 0.001080, scenario: 0, slope: -6.517241375373449e-05, fluctuations: 0.0\n",
      "step: 69480 loss: 0.600706 time elapsed: 92.0232 learning rate: 0.001080, scenario: 0, slope: -6.493662481911934e-05, fluctuations: 0.0\n",
      "step: 69490 loss: 0.600068 time elapsed: 92.0390 learning rate: 0.001080, scenario: 0, slope: -6.470914140387508e-05, fluctuations: 0.0\n",
      "step: 69500 loss: 0.599432 time elapsed: 92.0533 learning rate: 0.001080, scenario: 0, slope: -6.451127963351516e-05, fluctuations: 0.0\n",
      "step: 69510 loss: 0.598798 time elapsed: 92.0688 learning rate: 0.001080, scenario: 0, slope: -6.427799929758398e-05, fluctuations: 0.0\n",
      "step: 69520 loss: 0.598165 time elapsed: 92.0864 learning rate: 0.001080, scenario: 0, slope: -6.407381978473236e-05, fluctuations: 0.0\n",
      "step: 69530 loss: 0.597535 time elapsed: 92.1037 learning rate: 0.001080, scenario: 0, slope: -6.387690344032519e-05, fluctuations: 0.0\n",
      "step: 69540 loss: 0.596906 time elapsed: 92.1202 learning rate: 0.001080, scenario: 0, slope: -6.368701336517532e-05, fluctuations: 0.0\n",
      "step: 69550 loss: 0.596278 time elapsed: 92.1374 learning rate: 0.001080, scenario: 0, slope: -6.35039214458381e-05, fluctuations: 0.0\n",
      "step: 69560 loss: 0.595652 time elapsed: 92.1543 learning rate: 0.001080, scenario: 0, slope: -6.332740802093973e-05, fluctuations: 0.0\n",
      "step: 69570 loss: 0.595028 time elapsed: 92.1708 learning rate: 0.001080, scenario: 0, slope: -6.315726155458186e-05, fluctuations: 0.0\n",
      "step: 69580 loss: 0.594404 time elapsed: 92.1869 learning rate: 0.001080, scenario: 0, slope: -6.299327831793856e-05, fluctuations: 0.0\n",
      "step: 69590 loss: 0.593783 time elapsed: 92.2030 learning rate: 0.001080, scenario: 0, slope: -6.283526207983685e-05, fluctuations: 0.0\n",
      "step: 69600 loss: 0.593162 time elapsed: 92.2187 learning rate: 0.001080, scenario: 0, slope: -6.269799286856909e-05, fluctuations: 0.0\n",
      "step: 69610 loss: 0.592543 time elapsed: 92.2352 learning rate: 0.001080, scenario: 0, slope: -6.253638136838377e-05, fluctuations: 0.0\n",
      "step: 69620 loss: 0.591924 time elapsed: 92.2507 learning rate: 0.001080, scenario: 0, slope: -6.239515926275175e-05, fluctuations: 0.0\n",
      "step: 69630 loss: 0.591307 time elapsed: 92.2642 learning rate: 0.001080, scenario: 0, slope: -6.225918833831717e-05, fluctuations: 0.0\n",
      "step: 69640 loss: 0.590691 time elapsed: 92.2809 learning rate: 0.001080, scenario: 0, slope: -6.212830553550129e-05, fluctuations: 0.0\n",
      "step: 69650 loss: 0.590077 time elapsed: 92.2981 learning rate: 0.001080, scenario: 0, slope: -6.200235363447134e-05, fluctuations: 0.0\n",
      "step: 69660 loss: 0.589463 time elapsed: 92.3140 learning rate: 0.001080, scenario: 0, slope: -6.188118101333225e-05, fluctuations: 0.0\n",
      "step: 69670 loss: 0.588850 time elapsed: 92.3296 learning rate: 0.001080, scenario: 0, slope: -6.176464141639877e-05, fluctuations: 0.0\n",
      "step: 69680 loss: 0.588238 time elapsed: 92.3458 learning rate: 0.001080, scenario: 0, slope: -6.165259373153629e-05, fluctuations: 0.0\n",
      "step: 69690 loss: 0.587627 time elapsed: 92.3610 learning rate: 0.001080, scenario: 0, slope: -6.154490177759843e-05, fluctuations: 0.0\n",
      "step: 69700 loss: 0.587017 time elapsed: 92.3763 learning rate: 0.001080, scenario: 0, slope: -6.145159442772138e-05, fluctuations: 0.0\n",
      "step: 69710 loss: 0.586407 time elapsed: 92.3915 learning rate: 0.001080, scenario: 0, slope: -6.134206377880684e-05, fluctuations: 0.0\n",
      "step: 69720 loss: 0.585799 time elapsed: 92.4085 learning rate: 0.001080, scenario: 0, slope: -6.124666823710798e-05, fluctuations: 0.0\n",
      "step: 69730 loss: 0.585191 time elapsed: 92.4245 learning rate: 0.001080, scenario: 0, slope: -6.115512906880308e-05, fluctuations: 0.0\n",
      "step: 69740 loss: 0.584584 time elapsed: 92.4394 learning rate: 0.001080, scenario: 0, slope: -6.106733186629161e-05, fluctuations: 0.0\n",
      "step: 69750 loss: 0.583977 time elapsed: 92.4549 learning rate: 0.001080, scenario: 0, slope: -6.098316605903091e-05, fluctuations: 0.0\n",
      "step: 69760 loss: 0.583371 time elapsed: 92.4704 learning rate: 0.001080, scenario: 0, slope: -6.090252475923597e-05, fluctuations: 0.0\n",
      "step: 69770 loss: 0.582766 time elapsed: 92.4864 learning rate: 0.001080, scenario: 0, slope: -6.082530461463141e-05, fluctuations: 0.0\n",
      "step: 69780 loss: 0.582162 time elapsed: 92.5031 learning rate: 0.001080, scenario: 0, slope: -6.075140566795234e-05, fluctuations: 0.0\n",
      "step: 69790 loss: 0.581558 time elapsed: 92.5184 learning rate: 0.001080, scenario: 0, slope: -6.068073122323408e-05, fluctuations: 0.0\n",
      "step: 69800 loss: 0.580954 time elapsed: 92.5332 learning rate: 0.001080, scenario: 0, slope: -6.061980378014522e-05, fluctuations: 0.0\n",
      "step: 69810 loss: 0.580352 time elapsed: 92.5483 learning rate: 0.001080, scenario: 0, slope: -6.054868460245254e-05, fluctuations: 0.0\n",
      "step: 69820 loss: 0.579749 time elapsed: 92.5642 learning rate: 0.001080, scenario: 0, slope: -6.048713422189341e-05, fluctuations: 0.0\n",
      "step: 69830 loss: 0.579147 time elapsed: 92.5807 learning rate: 0.001080, scenario: 0, slope: -6.042845170795418e-05, fluctuations: 0.0\n",
      "step: 69840 loss: 0.578546 time elapsed: 92.5952 learning rate: 0.001080, scenario: 0, slope: -6.037255487227413e-05, fluctuations: 0.0\n",
      "step: 69850 loss: 0.577945 time elapsed: 92.6095 learning rate: 0.001080, scenario: 0, slope: -6.031936410632249e-05, fluctuations: 0.0\n",
      "step: 69860 loss: 0.577344 time elapsed: 92.6239 learning rate: 0.001080, scenario: 0, slope: -6.026880228557676e-05, fluctuations: 0.0\n",
      "step: 69870 loss: 0.576744 time elapsed: 92.6378 learning rate: 0.001080, scenario: 0, slope: -6.022079467808793e-05, fluctuations: 0.0\n",
      "step: 69880 loss: 0.576144 time elapsed: 92.6518 learning rate: 0.001080, scenario: 0, slope: -6.01752688573057e-05, fluctuations: 0.0\n",
      "step: 69890 loss: 0.575544 time elapsed: 92.6651 learning rate: 0.001080, scenario: 0, slope: -6.0132154619040186e-05, fluctuations: 0.0\n",
      "step: 69900 loss: 0.574945 time elapsed: 92.6789 learning rate: 0.001080, scenario: 0, slope: -6.009535741178614e-05, fluctuations: 0.0\n",
      "step: 69910 loss: 0.574346 time elapsed: 92.6947 learning rate: 0.001080, scenario: 0, slope: -6.005289071123481e-05, fluctuations: 0.0\n",
      "step: 69920 loss: 0.573747 time elapsed: 92.7111 learning rate: 0.001080, scenario: 0, slope: -6.001661104751691e-05, fluctuations: 0.0\n",
      "step: 69930 loss: 0.573148 time elapsed: 92.7272 learning rate: 0.001080, scenario: 0, slope: -5.9982482836342796e-05, fluctuations: 0.0\n",
      "step: 69940 loss: 0.572550 time elapsed: 92.7422 learning rate: 0.001080, scenario: 0, slope: -5.995044586256078e-05, fluctuations: 0.0\n",
      "step: 69950 loss: 0.571952 time elapsed: 92.7573 learning rate: 0.001080, scenario: 0, slope: -5.992044170711593e-05, fluctuations: 0.0\n",
      "step: 69960 loss: 0.571354 time elapsed: 92.7730 learning rate: 0.001080, scenario: 0, slope: -5.9892413686500564e-05, fluctuations: 0.0\n",
      "step: 69970 loss: 0.570757 time elapsed: 92.7883 learning rate: 0.001080, scenario: 0, slope: -5.986630679471942e-05, fluctuations: 0.0\n",
      "step: 69980 loss: 0.570159 time elapsed: 92.8022 learning rate: 0.001080, scenario: 0, slope: -5.984206764800798e-05, fluctuations: 0.0\n",
      "step: 69990 loss: 0.569562 time elapsed: 92.8151 learning rate: 0.001080, scenario: 0, slope: -5.9819644431715786e-05, fluctuations: 0.0\n",
      "step: 70000 loss: 0.568964 time elapsed: 92.8287 learning rate: 0.001080, scenario: 0, slope: -5.980097455669214e-05, fluctuations: 0.0\n",
      "step: 70010 loss: 0.568367 time elapsed: 92.8435 learning rate: 0.001080, scenario: 0, slope: -5.978004606797623e-05, fluctuations: 0.0\n",
      "step: 70020 loss: 0.567770 time elapsed: 92.8578 learning rate: 0.001080, scenario: 0, slope: -5.9762770602575865e-05, fluctuations: 0.0\n",
      "step: 70030 loss: 0.567179 time elapsed: 92.8729 learning rate: 0.001080, scenario: 0, slope: -5.974119550433149e-05, fluctuations: 0.0\n",
      "step: 70040 loss: 0.580241 time elapsed: 92.8876 learning rate: 0.001113, scenario: 1, slope: -4.5423571107699295e-05, fluctuations: 0.0\n",
      "step: 70050 loss: 29.564245 time elapsed: 92.9025 learning rate: 0.001032, scenario: -1, slope: 0.04758066148137093, fluctuations: 0.01\n",
      "step: 70060 loss: 8.548635 time elapsed: 92.9176 learning rate: 0.000934, scenario: -1, slope: 0.06508818992836961, fluctuations: 0.03\n",
      "step: 70070 loss: 1.265394 time elapsed: 92.9327 learning rate: 0.000844, scenario: -1, slope: 0.05693333121249393, fluctuations: 0.07\n",
      "step: 70080 loss: 1.106992 time elapsed: 92.9467 learning rate: 0.000764, scenario: -1, slope: 0.045766943754737804, fluctuations: 0.11\n",
      "step: 70090 loss: 0.598282 time elapsed: 92.9615 learning rate: 0.000691, scenario: -1, slope: 0.032707552955418134, fluctuations: 0.16\n",
      "step: 70100 loss: 0.600402 time elapsed: 92.9756 learning rate: 0.000631, scenario: -1, slope: 0.016950540048521548, fluctuations: 0.2\n",
      "step: 70110 loss: 0.568227 time elapsed: 92.9902 learning rate: 0.000594, scenario: 0, slope: -0.005089901136679579, fluctuations: 0.24\n",
      "step: 70120 loss: 0.566278 time elapsed: 93.0063 learning rate: 0.000594, scenario: 0, slope: -0.025328868390870136, fluctuations: 0.29\n",
      "step: 70130 loss: 0.566934 time elapsed: 93.0202 learning rate: 0.000594, scenario: 0, slope: -0.04840428937608972, fluctuations: 0.33\n",
      "step: 70140 loss: 0.565754 time elapsed: 93.0345 learning rate: 0.000594, scenario: 0, slope: -0.07780261031681886, fluctuations: 0.38\n",
      "step: 70150 loss: 0.564730 time elapsed: 93.0479 learning rate: 0.000594, scenario: 0, slope: -0.035516830236109916, fluctuations: 0.42\n",
      "step: 70160 loss: 0.564176 time elapsed: 93.0615 learning rate: 0.000594, scenario: 0, slope: -0.008254842152094112, fluctuations: 0.42\n",
      "step: 70170 loss: 0.563818 time elapsed: 93.0753 learning rate: 0.000594, scenario: 0, slope: -0.0034810464208772803, fluctuations: 0.42\n",
      "step: 70180 loss: 0.563504 time elapsed: 93.0891 learning rate: 0.000594, scenario: 0, slope: -0.0007976807702780884, fluctuations: 0.39\n",
      "step: 70190 loss: 0.563195 time elapsed: 93.1019 learning rate: 0.000594, scenario: 0, slope: -0.00045072441081713666, fluctuations: 0.34\n",
      "step: 70200 loss: 0.562885 time elapsed: 93.1152 learning rate: 0.000594, scenario: 0, slope: -0.00015367364862316004, fluctuations: 0.31\n",
      "step: 70210 loss: 0.562577 time elapsed: 93.1299 learning rate: 0.000576, scenario: -1, slope: -5.7896122657105483e-05, fluctuations: 0.26\n",
      "step: 70220 loss: 0.562288 time elapsed: 93.1442 learning rate: 0.000521, scenario: -1, slope: -4.471010472680368e-05, fluctuations: 0.21\n",
      "step: 70230 loss: 0.562026 time elapsed: 93.1579 learning rate: 0.000471, scenario: -1, slope: -3.259810367422901e-05, fluctuations: 0.17\n",
      "step: 70240 loss: 0.561790 time elapsed: 93.1719 learning rate: 0.000426, scenario: -1, slope: -3.072527936622928e-05, fluctuations: 0.12\n",
      "step: 70250 loss: 0.561576 time elapsed: 93.1864 learning rate: 0.000386, scenario: -1, slope: -2.9749534956017702e-05, fluctuations: 0.07\n",
      "step: 70260 loss: 0.561382 time elapsed: 93.2016 learning rate: 0.000349, scenario: -1, slope: -2.8063786350426938e-05, fluctuations: 0.05\n",
      "step: 70270 loss: 0.561206 time elapsed: 93.2152 learning rate: 0.000320, scenario: 1, slope: -2.6645647629222894e-05, fluctuations: 0.0\n",
      "step: 70280 loss: 0.561032 time elapsed: 93.2288 learning rate: 0.000354, scenario: 1, slope: -2.481846183970641e-05, fluctuations: 0.0\n",
      "step: 70290 loss: 0.560840 time elapsed: 93.2433 learning rate: 0.000391, scenario: 1, slope: -2.303978896954578e-05, fluctuations: 0.0\n",
      "step: 70300 loss: 0.560627 time elapsed: 93.2575 learning rate: 0.000427, scenario: 1, slope: -2.1666257351874795e-05, fluctuations: 0.0\n",
      "step: 70310 loss: 0.560393 time elapsed: 93.2719 learning rate: 0.000472, scenario: 1, slope: -2.052754394947543e-05, fluctuations: 0.0\n",
      "step: 70320 loss: 0.560135 time elapsed: 93.2861 learning rate: 0.000521, scenario: 1, slope: -2.0180754908013772e-05, fluctuations: 0.0\n",
      "step: 70330 loss: 0.559848 time elapsed: 93.2997 learning rate: 0.000576, scenario: 1, slope: -2.052708173688641e-05, fluctuations: 0.0\n",
      "step: 70340 loss: 0.559532 time elapsed: 93.3130 learning rate: 0.000636, scenario: 1, slope: -2.155156284760445e-05, fluctuations: 0.0\n",
      "step: 70350 loss: 0.559182 time elapsed: 93.3280 learning rate: 0.000703, scenario: 1, slope: -2.320725066773932e-05, fluctuations: 0.0\n",
      "step: 70360 loss: 0.558794 time elapsed: 93.3428 learning rate: 0.000776, scenario: 1, slope: -2.541395663053394e-05, fluctuations: 0.0\n",
      "step: 70370 loss: 0.558366 time elapsed: 93.3571 learning rate: 0.000857, scenario: 1, slope: -2.8057046478196643e-05, fluctuations: 0.0\n",
      "step: 70380 loss: 0.557893 time elapsed: 93.3745 learning rate: 0.000947, scenario: 1, slope: -3.101818132862632e-05, fluctuations: 0.0\n",
      "step: 70390 loss: 0.557370 time elapsed: 93.3912 learning rate: 0.001046, scenario: 1, slope: -3.429446130445761e-05, fluctuations: 0.0\n",
      "step: 70400 loss: 0.556792 time elapsed: 93.4073 learning rate: 0.001144, scenario: 1, slope: -3.754035197156346e-05, fluctuations: 0.0\n",
      "step: 70410 loss: 0.556160 time elapsed: 93.4239 learning rate: 0.001264, scenario: 1, slope: -4.1907736343775115e-05, fluctuations: 0.0\n",
      "step: 70420 loss: 0.558995 time elapsed: 93.4381 learning rate: 0.001396, scenario: 1, slope: -4.3774800663102385e-05, fluctuations: 0.0\n",
      "step: 70430 loss: 461.881910 time elapsed: 93.4516 learning rate: 0.001295, scenario: -1, slope: 0.7581704432659095, fluctuations: 0.01\n",
      "step: 70440 loss: 130.509585 time elapsed: 93.4654 learning rate: 0.001171, scenario: -1, slope: 0.6241682031160354, fluctuations: 0.06\n",
      "step: 70450 loss: 14.415302 time elapsed: 93.4788 learning rate: 0.001059, scenario: -1, slope: 0.570932895504706, fluctuations: 0.11\n",
      "step: 70460 loss: 2.583269 time elapsed: 93.4918 learning rate: 0.000958, scenario: -1, slope: 0.468385709730003, fluctuations: 0.15\n",
      "step: 70470 loss: 3.316178 time elapsed: 93.5061 learning rate: 0.000866, scenario: -1, slope: 0.31851673254789525, fluctuations: 0.19\n",
      "step: 70480 loss: 1.807420 time elapsed: 93.5201 learning rate: 0.000783, scenario: -1, slope: 0.12756815925305803, fluctuations: 0.22\n",
      "step: 70490 loss: 0.870924 time elapsed: 93.5346 learning rate: 0.000730, scenario: 0, slope: -0.05051338960312175, fluctuations: 0.26\n",
      "step: 70500 loss: 0.668912 time elapsed: 93.5498 learning rate: 0.000730, scenario: 0, slope: -0.22805382999256238, fluctuations: 0.29\n",
      "step: 70510 loss: 0.678892 time elapsed: 93.5648 learning rate: 0.000730, scenario: 0, slope: -0.4745133892727656, fluctuations: 0.32\n",
      "step: 70520 loss: 0.617211 time elapsed: 93.5795 learning rate: 0.000730, scenario: 0, slope: -0.7928559987355995, fluctuations: 0.36\n",
      "step: 70530 loss: 0.614715 time elapsed: 93.5940 learning rate: 0.000730, scenario: 0, slope: -0.254868624837265, fluctuations: 0.37\n",
      "step: 70540 loss: 0.607486 time elapsed: 93.6113 learning rate: 0.000730, scenario: 0, slope: -0.1698162841408306, fluctuations: 0.36\n",
      "step: 70550 loss: 0.603290 time elapsed: 93.6260 learning rate: 0.000730, scenario: 0, slope: -0.05581248861544373, fluctuations: 0.35\n",
      "step: 70560 loss: 0.600856 time elapsed: 93.6407 learning rate: 0.000730, scenario: 0, slope: -0.025358784279684573, fluctuations: 0.32\n",
      "step: 70570 loss: 0.598409 time elapsed: 93.6550 learning rate: 0.000730, scenario: 0, slope: -0.005847075817688545, fluctuations: 0.29\n",
      "step: 70580 loss: 0.596552 time elapsed: 93.6690 learning rate: 0.000730, scenario: 0, slope: -0.0026336614362817553, fluctuations: 0.25\n",
      "step: 70590 loss: 0.594830 time elapsed: 93.6823 learning rate: 0.000730, scenario: 0, slope: -0.0009192791039243363, fluctuations: 0.22\n",
      "step: 70600 loss: 0.593315 time elapsed: 93.6961 learning rate: 0.000730, scenario: 0, slope: -0.0005282535324286708, fluctuations: 0.19\n",
      "step: 70610 loss: 0.591934 time elapsed: 93.7113 learning rate: 0.000730, scenario: 0, slope: -0.0003285732767852002, fluctuations: 0.15\n",
      "step: 70620 loss: 0.590676 time elapsed: 93.7256 learning rate: 0.000730, scenario: 0, slope: -0.00024261516796865586, fluctuations: 0.12\n",
      "step: 70630 loss: 0.589518 time elapsed: 93.7414 learning rate: 0.000730, scenario: 0, slope: -0.00019729078531923038, fluctuations: 0.08\n",
      "step: 70640 loss: 0.588447 time elapsed: 93.7569 learning rate: 0.000730, scenario: 0, slope: -0.00016750329596818957, fluctuations: 0.05\n",
      "step: 70650 loss: 0.587453 time elapsed: 93.7726 learning rate: 0.000730, scenario: 0, slope: -0.00015148260782836973, fluctuations: 0.01\n",
      "step: 70660 loss: 0.586525 time elapsed: 93.7877 learning rate: 0.000730, scenario: 0, slope: -0.0001366526109361063, fluctuations: 0.0\n",
      "step: 70670 loss: 0.585656 time elapsed: 93.8023 learning rate: 0.000730, scenario: 0, slope: -0.00012434888731494866, fluctuations: 0.0\n",
      "step: 70680 loss: 0.584838 time elapsed: 93.8171 learning rate: 0.000730, scenario: 0, slope: -0.00011407929990267388, fluctuations: 0.0\n",
      "step: 70690 loss: 0.584066 time elapsed: 93.8313 learning rate: 0.000730, scenario: 0, slope: -0.00010537816849343671, fluctuations: 0.0\n",
      "step: 70700 loss: 0.583334 time elapsed: 93.8468 learning rate: 0.000730, scenario: 0, slope: -9.857571889497004e-05, fluctuations: 0.0\n",
      "step: 70710 loss: 0.582639 time elapsed: 93.8613 learning rate: 0.000730, scenario: 0, slope: -9.133810407436452e-05, fluctuations: 0.0\n",
      "step: 70720 loss: 0.581976 time elapsed: 93.8754 learning rate: 0.000730, scenario: 0, slope: -8.561222741403551e-05, fluctuations: 0.0\n",
      "step: 70730 loss: 0.581342 time elapsed: 93.8893 learning rate: 0.000730, scenario: 0, slope: -8.056798494229187e-05, fluctuations: 0.0\n",
      "step: 70740 loss: 0.580733 time elapsed: 93.9032 learning rate: 0.000730, scenario: 0, slope: -7.6105508038627e-05, fluctuations: 0.0\n",
      "step: 70750 loss: 0.580149 time elapsed: 93.9169 learning rate: 0.000730, scenario: 0, slope: -7.2143908649889e-05, fluctuations: 0.0\n",
      "step: 70760 loss: 0.579585 time elapsed: 93.9302 learning rate: 0.000730, scenario: 0, slope: -6.861629075531755e-05, fluctuations: 0.0\n",
      "step: 70770 loss: 0.579040 time elapsed: 93.9448 learning rate: 0.000730, scenario: 0, slope: -6.546664870900841e-05, fluctuations: 0.0\n",
      "step: 70780 loss: 0.578512 time elapsed: 93.9603 learning rate: 0.000730, scenario: 0, slope: -6.26475736720375e-05, fluctuations: 0.0\n",
      "step: 70790 loss: 0.578000 time elapsed: 93.9755 learning rate: 0.000730, scenario: 0, slope: -6.011862559589798e-05, fluctuations: 0.0\n",
      "step: 70800 loss: 0.577501 time elapsed: 93.9904 learning rate: 0.000745, scenario: 1, slope: -5.8061829226514425e-05, fluctuations: 0.0\n",
      "step: 70810 loss: 0.576988 time elapsed: 94.0058 learning rate: 0.000823, scenario: 1, slope: -5.586407582732836e-05, fluctuations: 0.0\n",
      "step: 70820 loss: 0.576437 time elapsed: 94.0202 learning rate: 0.000909, scenario: 1, slope: -5.436907849488778e-05, fluctuations: 0.0\n",
      "step: 70830 loss: 0.575845 time elapsed: 94.0351 learning rate: 0.001004, scenario: 1, slope: -5.354858671918068e-05, fluctuations: 0.0\n",
      "step: 70840 loss: 0.575209 time elapsed: 94.0539 learning rate: 0.001109, scenario: 1, slope: -5.354012877138429e-05, fluctuations: 0.0\n",
      "step: 70850 loss: 0.574527 time elapsed: 94.0686 learning rate: 0.001225, scenario: 1, slope: -5.4429533946956673e-05, fluctuations: 0.0\n",
      "step: 70860 loss: 0.573795 time elapsed: 94.0814 learning rate: 0.001353, scenario: 1, slope: -5.625016851621369e-05, fluctuations: 0.0\n",
      "step: 70870 loss: 0.573014 time elapsed: 94.0952 learning rate: 0.001436, scenario: 0, slope: -5.898033861084725e-05, fluctuations: 0.0\n",
      "step: 70880 loss: 0.572235 time elapsed: 94.1089 learning rate: 0.001436, scenario: 0, slope: -6.238900829426935e-05, fluctuations: 0.0\n",
      "step: 70890 loss: 0.571477 time elapsed: 94.1234 learning rate: 0.001436, scenario: 0, slope: -6.596938752593517e-05, fluctuations: 0.0\n",
      "step: 70900 loss: 0.570738 time elapsed: 94.1370 learning rate: 0.001436, scenario: 0, slope: -6.892245307914251e-05, fluctuations: 0.0\n",
      "step: 70910 loss: 0.570015 time elapsed: 94.1512 learning rate: 0.001436, scenario: 0, slope: -7.173825948766298e-05, fluctuations: 0.0\n",
      "step: 70920 loss: 0.569307 time elapsed: 94.1658 learning rate: 0.001436, scenario: 0, slope: -7.337282520170375e-05, fluctuations: 0.0\n",
      "step: 70930 loss: 0.568611 time elapsed: 94.1796 learning rate: 0.001436, scenario: 0, slope: -7.40967090881466e-05, fluctuations: 0.0\n",
      "step: 70940 loss: 0.567927 time elapsed: 94.1929 learning rate: 0.001436, scenario: 0, slope: -7.397239517836382e-05, fluctuations: 0.0\n",
      "step: 70950 loss: 0.567254 time elapsed: 94.2064 learning rate: 0.001436, scenario: 0, slope: -7.315146293862516e-05, fluctuations: 0.0\n",
      "step: 70960 loss: 0.566589 time elapsed: 94.2199 learning rate: 0.001436, scenario: 0, slope: -7.187638967848841e-05, fluctuations: 0.0\n",
      "step: 70970 loss: 0.565933 time elapsed: 94.2337 learning rate: 0.001436, scenario: 0, slope: -7.048155398906246e-05, fluctuations: 0.0\n",
      "step: 70980 loss: 0.565284 time elapsed: 94.2485 learning rate: 0.001436, scenario: 0, slope: -6.92214491847969e-05, fluctuations: 0.0\n",
      "step: 70990 loss: 0.564642 time elapsed: 94.2617 learning rate: 0.001436, scenario: 0, slope: -6.809960357762438e-05, fluctuations: 0.0\n",
      "step: 71000 loss: 0.564007 time elapsed: 94.2742 learning rate: 0.001436, scenario: 0, slope: -6.719157919995335e-05, fluctuations: 0.0\n",
      "step: 71010 loss: 0.563377 time elapsed: 94.2877 learning rate: 0.001436, scenario: 0, slope: -6.619378465113318e-05, fluctuations: 0.0\n",
      "step: 71020 loss: 0.562753 time elapsed: 94.3009 learning rate: 0.001436, scenario: 0, slope: -6.537871611369635e-05, fluctuations: 0.0\n",
      "step: 71030 loss: 0.562133 time elapsed: 94.3138 learning rate: 0.001436, scenario: 0, slope: -6.463911650251674e-05, fluctuations: 0.0\n",
      "step: 71040 loss: 0.561518 time elapsed: 94.3265 learning rate: 0.001436, scenario: 0, slope: -6.39652447682067e-05, fluctuations: 0.0\n",
      "step: 71050 loss: 0.560907 time elapsed: 94.3390 learning rate: 0.001436, scenario: 0, slope: -6.334896091968602e-05, fluctuations: 0.0\n",
      "step: 71060 loss: 0.560300 time elapsed: 94.3518 learning rate: 0.001436, scenario: 0, slope: -6.278342227976566e-05, fluctuations: 0.0\n",
      "step: 71070 loss: 0.559697 time elapsed: 94.3656 learning rate: 0.001436, scenario: 0, slope: -6.226284115860355e-05, fluctuations: 0.0\n",
      "step: 71080 loss: 0.559098 time elapsed: 94.3800 learning rate: 0.001436, scenario: 0, slope: -6.178229228536224e-05, fluctuations: 0.0\n",
      "step: 71090 loss: 0.558501 time elapsed: 94.3943 learning rate: 0.001436, scenario: 0, slope: -6.133755948475842e-05, fluctuations: 0.0\n",
      "step: 71100 loss: 0.557907 time elapsed: 94.4080 learning rate: 0.001436, scenario: 0, slope: -6.096491229618581e-05, fluctuations: 0.0\n",
      "step: 71110 loss: 0.557317 time elapsed: 94.4225 learning rate: 0.001436, scenario: 0, slope: -6.054151124463577e-05, fluctuations: 0.0\n",
      "step: 71120 loss: 0.556728 time elapsed: 94.4365 learning rate: 0.001436, scenario: 0, slope: -6.018432046133977e-05, fluctuations: 0.0\n",
      "step: 71130 loss: 0.556143 time elapsed: 94.4506 learning rate: 0.001436, scenario: 0, slope: -5.9851050584745174e-05, fluctuations: 0.0\n",
      "step: 71140 loss: 0.555559 time elapsed: 94.4641 learning rate: 0.001436, scenario: 0, slope: -5.953960195685571e-05, fluctuations: 0.0\n",
      "step: 71150 loss: 0.554978 time elapsed: 94.4770 learning rate: 0.001436, scenario: 0, slope: -5.924812186909778e-05, fluctuations: 0.0\n",
      "step: 71160 loss: 0.554399 time elapsed: 94.4894 learning rate: 0.001436, scenario: 0, slope: -5.8974968609278954e-05, fluctuations: 0.0\n",
      "step: 71170 loss: 0.553822 time elapsed: 94.5035 learning rate: 0.001436, scenario: 0, slope: -5.871868161221817e-05, fluctuations: 0.0\n",
      "step: 71180 loss: 0.553246 time elapsed: 94.5186 learning rate: 0.001436, scenario: 0, slope: -5.847795655648264e-05, fluctuations: 0.0\n",
      "step: 71190 loss: 0.552672 time elapsed: 94.5334 learning rate: 0.001436, scenario: 0, slope: -5.825162448882367e-05, fluctuations: 0.0\n",
      "step: 71200 loss: 0.552100 time elapsed: 94.5458 learning rate: 0.001436, scenario: 0, slope: -5.805936065738977e-05, fluctuations: 0.0\n",
      "step: 71210 loss: 0.551530 time elapsed: 94.5594 learning rate: 0.001436, scenario: 0, slope: -5.78380375713629e-05, fluctuations: 0.0\n",
      "step: 71220 loss: 0.550961 time elapsed: 94.5731 learning rate: 0.001436, scenario: 0, slope: -5.7648976499380835e-05, fluctuations: 0.0\n",
      "step: 71230 loss: 0.550393 time elapsed: 94.5869 learning rate: 0.001436, scenario: 0, slope: -5.747067255031581e-05, fluctuations: 0.0\n",
      "step: 71240 loss: 0.549827 time elapsed: 94.6015 learning rate: 0.001436, scenario: 0, slope: -5.73024174997489e-05, fluctuations: 0.0\n",
      "step: 71250 loss: 0.549261 time elapsed: 94.6159 learning rate: 0.001436, scenario: 0, slope: -5.714356542537635e-05, fluctuations: 0.0\n",
      "step: 71260 loss: 0.548697 time elapsed: 94.6296 learning rate: 0.001436, scenario: 0, slope: -5.699352583784121e-05, fluctuations: 0.0\n",
      "step: 71270 loss: 0.548134 time elapsed: 94.6437 learning rate: 0.001436, scenario: 0, slope: -5.685175772444502e-05, fluctuations: 0.0\n",
      "step: 71280 loss: 0.547573 time elapsed: 94.6582 learning rate: 0.001436, scenario: 0, slope: -5.671776436388612e-05, fluctuations: 0.0\n",
      "step: 71290 loss: 0.547012 time elapsed: 94.6727 learning rate: 0.001436, scenario: 0, slope: -5.659108852802618e-05, fluctuations: 0.0\n",
      "step: 71300 loss: 0.546452 time elapsed: 94.6888 learning rate: 0.001436, scenario: 0, slope: -5.6482798637381554e-05, fluctuations: 0.0\n",
      "step: 71310 loss: 0.546824 time elapsed: 94.7044 learning rate: 0.001436, scenario: 0, slope: -5.53708509568669e-05, fluctuations: 0.0\n",
      "step: 71320 loss: 14.289532 time elapsed: 94.7187 learning rate: 0.001414, scenario: -1, slope: 0.012913836970616792, fluctuations: 0.0\n",
      "step: 71330 loss: 2.718403 time elapsed: 94.7336 learning rate: 0.001279, scenario: -1, slope: 0.14876607950549728, fluctuations: 0.02\n",
      "step: 71340 loss: 8.868283 time elapsed: 94.7465 learning rate: 0.001157, scenario: -1, slope: 0.15706693648647144, fluctuations: 0.05\n",
      "step: 71350 loss: 2.110287 time elapsed: 94.7598 learning rate: 0.001046, scenario: -1, slope: 0.13176968635461087, fluctuations: 0.09\n",
      "step: 71360 loss: 1.376922 time elapsed: 94.7734 learning rate: 0.000946, scenario: -1, slope: 0.0967454196173131, fluctuations: 0.13\n",
      "step: 71370 loss: 0.684822 time elapsed: 94.7889 learning rate: 0.000856, scenario: -1, slope: 0.05810102659353931, fluctuations: 0.18\n",
      "step: 71380 loss: 0.590870 time elapsed: 94.8042 learning rate: 0.000774, scenario: -1, slope: 0.012715064111881093, fluctuations: 0.23\n",
      "step: 71390 loss: 0.577605 time elapsed: 94.8191 learning rate: 0.000766, scenario: 0, slope: -0.041316479837766054, fluctuations: 0.27\n",
      "step: 71400 loss: 0.556685 time elapsed: 94.8330 learning rate: 0.000766, scenario: 0, slope: -0.09219866231907829, fluctuations: 0.32\n",
      "step: 71410 loss: 0.548862 time elapsed: 94.8474 learning rate: 0.000766, scenario: 0, slope: -0.16653777622564367, fluctuations: 0.37\n",
      "step: 71420 loss: 0.547468 time elapsed: 94.8613 learning rate: 0.000766, scenario: 0, slope: -0.2409918735261973, fluctuations: 0.42\n",
      "step: 71430 loss: 0.547190 time elapsed: 94.8756 learning rate: 0.000766, scenario: 0, slope: -0.0680495641575132, fluctuations: 0.44\n",
      "step: 71440 loss: 0.546853 time elapsed: 94.8892 learning rate: 0.000766, scenario: 0, slope: -0.0115159869424353, fluctuations: 0.44\n",
      "step: 71450 loss: 0.546475 time elapsed: 94.9022 learning rate: 0.000766, scenario: 0, slope: -0.003657097553968385, fluctuations: 0.43\n",
      "step: 71460 loss: 0.546115 time elapsed: 94.9149 learning rate: 0.000766, scenario: 0, slope: -0.0007530036872123417, fluctuations: 0.41\n",
      "step: 71470 loss: 0.545777 time elapsed: 94.9272 learning rate: 0.000766, scenario: 0, slope: -0.00048318447130062787, fluctuations: 0.36\n",
      "step: 71480 loss: 0.545451 time elapsed: 94.9394 learning rate: 0.000766, scenario: 0, slope: -0.00012024527830902119, fluctuations: 0.32\n",
      "step: 71490 loss: 0.545132 time elapsed: 94.9515 learning rate: 0.000743, scenario: -1, slope: -4.858495043744167e-05, fluctuations: 0.27\n",
      "step: 71500 loss: 0.544828 time elapsed: 94.9654 learning rate: 0.000707, scenario: -1, slope: -5.338577526587579e-05, fluctuations: 0.22\n",
      "step: 71510 loss: 0.544548 time elapsed: 94.9783 learning rate: 0.000639, scenario: -1, slope: -3.636447606569238e-05, fluctuations: 0.17\n",
      "step: 71520 loss: 0.544295 time elapsed: 94.9913 learning rate: 0.000578, scenario: -1, slope: -3.3440347715803536e-05, fluctuations: 0.12\n",
      "step: 71530 loss: 0.544066 time elapsed: 95.0055 learning rate: 0.000523, scenario: -1, slope: -3.1596550077321624e-05, fluctuations: 0.07\n",
      "step: 71540 loss: 0.543860 time elapsed: 95.0194 learning rate: 0.000473, scenario: -1, slope: -2.9603163626631007e-05, fluctuations: 0.05\n",
      "step: 71550 loss: 0.543674 time elapsed: 95.0334 learning rate: 0.000428, scenario: -1, slope: -2.8004718013640377e-05, fluctuations: 0.01\n",
      "step: 71560 loss: 0.543494 time elapsed: 95.0470 learning rate: 0.000470, scenario: 1, slope: -2.6214515338986894e-05, fluctuations: 0.0\n",
      "step: 71570 loss: 0.543295 time elapsed: 95.0607 learning rate: 0.000519, scenario: 1, slope: -2.437405084853987e-05, fluctuations: 0.0\n",
      "step: 71580 loss: 0.543075 time elapsed: 95.0744 learning rate: 0.000574, scenario: 1, slope: -2.2783047470931823e-05, fluctuations: 0.0\n",
      "step: 71590 loss: 0.542832 time elapsed: 95.0882 learning rate: 0.000634, scenario: 1, slope: -2.1664237171504483e-05, fluctuations: 0.0\n",
      "step: 71600 loss: 0.542565 time elapsed: 95.1019 learning rate: 0.000693, scenario: 1, slope: -2.1203647311786177e-05, fluctuations: 0.0\n",
      "step: 71610 loss: 0.542271 time elapsed: 95.1161 learning rate: 0.000765, scenario: 1, slope: -2.143097298859408e-05, fluctuations: 0.0\n",
      "step: 71620 loss: 0.541948 time elapsed: 95.1298 learning rate: 0.000846, scenario: 1, slope: -2.2381592278856532e-05, fluctuations: 0.0\n",
      "step: 71630 loss: 0.541592 time elapsed: 95.1447 learning rate: 0.000934, scenario: 1, slope: -2.3987233856178237e-05, fluctuations: 0.0\n",
      "step: 71640 loss: 0.541199 time elapsed: 95.1608 learning rate: 0.001032, scenario: 1, slope: -2.6161625319885074e-05, fluctuations: 0.0\n",
      "step: 71650 loss: 0.540765 time elapsed: 95.1746 learning rate: 0.001140, scenario: 1, slope: -2.8783458709783145e-05, fluctuations: 0.0\n",
      "step: 71660 loss: 0.540288 time elapsed: 95.1882 learning rate: 0.001259, scenario: 1, slope: -3.1719126005642314e-05, fluctuations: 0.0\n",
      "step: 71670 loss: 0.539761 time elapsed: 95.2030 learning rate: 0.001391, scenario: 1, slope: -3.4950775344489766e-05, fluctuations: 0.0\n",
      "step: 71680 loss: 0.539182 time elapsed: 95.2196 learning rate: 0.001536, scenario: 1, slope: -3.850969494880853e-05, fluctuations: 0.0\n",
      "step: 71690 loss: 0.538544 time elapsed: 95.2345 learning rate: 0.001697, scenario: 1, slope: -4.2430000053357206e-05, fluctuations: 0.0\n",
      "step: 71700 loss: 1.020851 time elapsed: 95.2486 learning rate: 0.001856, scenario: 1, slope: -2.1576450298547863e-06, fluctuations: 0.0\n",
      "step: 71710 loss: 366.228890 time elapsed: 95.2635 learning rate: 0.001687, scenario: -1, slope: 1.5179933500372704, fluctuations: 0.03\n",
      "step: 71720 loss: 129.813881 time elapsed: 95.2784 learning rate: 0.001526, scenario: -1, slope: 1.7644236659848485, fluctuations: 0.08\n",
      "step: 71730 loss: 23.389739 time elapsed: 95.2925 learning rate: 0.001380, scenario: -1, slope: 1.6303148444303817, fluctuations: 0.12\n",
      "step: 71740 loss: 19.901147 time elapsed: 95.3063 learning rate: 0.001248, scenario: -1, slope: 1.291687509697677, fluctuations: 0.16\n",
      "step: 71750 loss: 6.369650 time elapsed: 95.3192 learning rate: 0.001128, scenario: -1, slope: 0.7601773184103876, fluctuations: 0.19\n",
      "step: 71760 loss: 2.678716 time elapsed: 95.3319 learning rate: 0.001021, scenario: -1, slope: 0.2682623219165131, fluctuations: 0.23\n",
      "step: 71770 loss: 1.083875 time elapsed: 95.3443 learning rate: 0.000980, scenario: 0, slope: -0.29028268897928766, fluctuations: 0.26\n",
      "step: 71780 loss: 0.995804 time elapsed: 95.3568 learning rate: 0.000980, scenario: 0, slope: -0.8773632434939825, fluctuations: 0.29\n",
      "step: 71790 loss: 0.884735 time elapsed: 95.3690 learning rate: 0.000980, scenario: 0, slope: -1.5745318640366852, fluctuations: 0.32\n",
      "step: 71800 loss: 0.789786 time elapsed: 95.3815 learning rate: 0.000980, scenario: 0, slope: -2.4559544911697433, fluctuations: 0.35\n",
      "step: 71810 loss: 0.756450 time elapsed: 95.3943 learning rate: 0.000980, scenario: 0, slope: -1.1417901181826096, fluctuations: 0.35\n",
      "step: 71820 loss: 0.739456 time elapsed: 95.4075 learning rate: 0.000980, scenario: 0, slope: -0.2918390512459185, fluctuations: 0.33\n",
      "step: 71830 loss: 0.725397 time elapsed: 95.4215 learning rate: 0.000980, scenario: 0, slope: -0.1136202131827173, fluctuations: 0.29\n",
      "step: 71840 loss: 0.715063 time elapsed: 95.4354 learning rate: 0.000980, scenario: 0, slope: -0.03725312174700774, fluctuations: 0.25\n",
      "step: 71850 loss: 0.706673 time elapsed: 95.4488 learning rate: 0.000980, scenario: 0, slope: -0.01900668128530579, fluctuations: 0.21\n",
      "step: 71860 loss: 0.699359 time elapsed: 95.4625 learning rate: 0.000980, scenario: 0, slope: -0.006681879073242809, fluctuations: 0.18\n",
      "step: 71870 loss: 0.692975 time elapsed: 95.4761 learning rate: 0.000980, scenario: 0, slope: -0.0033075542295027344, fluctuations: 0.15\n",
      "step: 71880 loss: 0.687273 time elapsed: 95.4896 learning rate: 0.000980, scenario: 0, slope: -0.0019209734881966214, fluctuations: 0.11\n",
      "step: 71890 loss: 0.682122 time elapsed: 95.5031 learning rate: 0.000980, scenario: 0, slope: -0.0011935817979399487, fluctuations: 0.08\n",
      "step: 71900 loss: 0.677445 time elapsed: 95.5166 learning rate: 0.000980, scenario: 0, slope: -0.0009234801757691369, fluctuations: 0.05\n",
      "step: 71910 loss: 0.673168 time elapsed: 95.5296 learning rate: 0.000980, scenario: 0, slope: -0.0007539561907929734, fluctuations: 0.01\n",
      "step: 71920 loss: 0.669239 time elapsed: 95.5427 learning rate: 0.000980, scenario: 0, slope: -0.0006471059421562029, fluctuations: 0.0\n",
      "step: 71930 loss: 0.665611 time elapsed: 95.5565 learning rate: 0.000980, scenario: 0, slope: -0.000568776779465048, fluctuations: 0.0\n",
      "step: 71940 loss: 0.662249 time elapsed: 95.5705 learning rate: 0.000980, scenario: 0, slope: -0.0005091184516143509, fluctuations: 0.0\n",
      "step: 71950 loss: 0.659118 time elapsed: 95.5833 learning rate: 0.000980, scenario: 0, slope: -0.00046088689543452756, fluctuations: 0.0\n",
      "step: 71960 loss: 0.656193 time elapsed: 95.5960 learning rate: 0.000980, scenario: 0, slope: -0.00042060549519500463, fluctuations: 0.0\n",
      "step: 71970 loss: 0.653450 time elapsed: 95.6100 learning rate: 0.000980, scenario: 0, slope: -0.0003862089188740955, fluctuations: 0.0\n",
      "step: 71980 loss: 0.650869 time elapsed: 95.6250 learning rate: 0.000980, scenario: 0, slope: -0.0003564563619491167, fluctuations: 0.0\n",
      "step: 71990 loss: 0.648431 time elapsed: 95.6399 learning rate: 0.000980, scenario: 0, slope: -0.0003305050830352761, fluctuations: 0.0\n",
      "step: 72000 loss: 0.646124 time elapsed: 95.6543 learning rate: 0.000980, scenario: 0, slope: -0.00030987637918774647, fluctuations: 0.0\n",
      "step: 72010 loss: 0.643933 time elapsed: 95.6696 learning rate: 0.000980, scenario: 0, slope: -0.000287635965760268, fluctuations: 0.0\n",
      "step: 72020 loss: 0.641847 time elapsed: 95.6841 learning rate: 0.000980, scenario: 0, slope: -0.00026983872048567714, fluctuations: 0.0\n",
      "step: 72030 loss: 0.639858 time elapsed: 95.6984 learning rate: 0.000980, scenario: 0, slope: -0.0002540065125115955, fluctuations: 0.0\n",
      "step: 72040 loss: 0.637956 time elapsed: 95.7130 learning rate: 0.000980, scenario: 0, slope: -0.00023986311273380218, fluctuations: 0.0\n",
      "step: 72050 loss: 0.636134 time elapsed: 95.7263 learning rate: 0.000980, scenario: 0, slope: -0.0002271747999267853, fluctuations: 0.0\n",
      "step: 72060 loss: 0.634385 time elapsed: 95.7387 learning rate: 0.000980, scenario: 0, slope: -0.00021574401538042748, fluctuations: 0.0\n",
      "step: 72070 loss: 0.632704 time elapsed: 95.7510 learning rate: 0.000980, scenario: 0, slope: -0.00020540399751719834, fluctuations: 0.0\n",
      "step: 72080 loss: 0.631085 time elapsed: 95.7632 learning rate: 0.000980, scenario: 0, slope: -0.00019601409379720186, fluctuations: 0.0\n",
      "step: 72090 loss: 0.629524 time elapsed: 95.7770 learning rate: 0.000980, scenario: 0, slope: -0.0001874555661212783, fluctuations: 0.0\n",
      "step: 72100 loss: 0.628017 time elapsed: 95.7902 learning rate: 0.000980, scenario: 0, slope: -0.00018038031156795108, fluctuations: 0.0\n",
      "step: 72110 loss: 0.626559 time elapsed: 95.8039 learning rate: 0.000980, scenario: 0, slope: -0.0001724456022432315, fluctuations: 0.0\n",
      "step: 72120 loss: 0.625147 time elapsed: 95.8177 learning rate: 0.000980, scenario: 0, slope: -0.00016583565337649437, fluctuations: 0.0\n",
      "step: 72130 loss: 0.623779 time elapsed: 95.8325 learning rate: 0.000980, scenario: 0, slope: -0.00015973527100855283, fluctuations: 0.0\n",
      "step: 72140 loss: 0.622452 time elapsed: 95.8465 learning rate: 0.000980, scenario: 0, slope: -0.00015409021426365883, fluctuations: 0.0\n",
      "step: 72150 loss: 0.621162 time elapsed: 95.8605 learning rate: 0.000980, scenario: 0, slope: -0.00014885337581479325, fluctuations: 0.0\n",
      "step: 72160 loss: 0.619908 time elapsed: 95.8744 learning rate: 0.000980, scenario: 0, slope: -0.00014398365338424087, fluctuations: 0.0\n",
      "step: 72170 loss: 0.618688 time elapsed: 95.8879 learning rate: 0.000980, scenario: 0, slope: -0.00013944503480784392, fluctuations: 0.0\n",
      "step: 72180 loss: 0.617499 time elapsed: 95.9029 learning rate: 0.000980, scenario: 0, slope: -0.00013520585063489575, fluctuations: 0.0\n",
      "step: 72190 loss: 0.616340 time elapsed: 95.9202 learning rate: 0.000980, scenario: 0, slope: -0.00013123815837962722, fluctuations: 0.0\n",
      "step: 72200 loss: 0.615209 time elapsed: 95.9348 learning rate: 0.000980, scenario: 0, slope: -0.00012787886461434035, fluctuations: 0.0\n",
      "step: 72210 loss: 0.614106 time elapsed: 95.9492 learning rate: 0.000980, scenario: 0, slope: -0.00012402112764410282, fluctuations: 0.0\n",
      "step: 72220 loss: 0.613027 time elapsed: 95.9633 learning rate: 0.000980, scenario: 0, slope: -0.00012073033322462399, fluctuations: 0.0\n",
      "step: 72230 loss: 0.611973 time elapsed: 95.9762 learning rate: 0.000980, scenario: 0, slope: -0.00011762744988971687, fluctuations: 0.0\n",
      "step: 72240 loss: 0.610941 time elapsed: 95.9892 learning rate: 0.000980, scenario: 0, slope: -0.00011469693519455355, fluctuations: 0.0\n",
      "step: 72250 loss: 0.609932 time elapsed: 96.0020 learning rate: 0.000980, scenario: 0, slope: -0.00011192487664828007, fluctuations: 0.0\n",
      "step: 72260 loss: 0.608943 time elapsed: 96.0152 learning rate: 0.000980, scenario: 0, slope: -0.00010929879745263647, fluctuations: 0.0\n",
      "step: 72270 loss: 0.607974 time elapsed: 96.0289 learning rate: 0.000980, scenario: 0, slope: -0.00010680748839482097, fluctuations: 0.0\n",
      "step: 72280 loss: 0.607024 time elapsed: 96.0429 learning rate: 0.000980, scenario: 0, slope: -0.00010444086189085644, fluctuations: 0.0\n",
      "step: 72290 loss: 0.606092 time elapsed: 96.0575 learning rate: 0.000980, scenario: 0, slope: -0.00010218982487788551, fluctuations: 0.0\n",
      "step: 72300 loss: 0.605178 time elapsed: 96.0721 learning rate: 0.000980, scenario: 0, slope: -0.00010025591874016237, fluctuations: 0.0\n",
      "step: 72310 loss: 0.604280 time elapsed: 96.0872 learning rate: 0.000980, scenario: 0, slope: -9.80024674710296e-05, fluctuations: 0.0\n",
      "step: 72320 loss: 0.603398 time elapsed: 96.1017 learning rate: 0.000980, scenario: 0, slope: -9.605200163163863e-05, fluctuations: 0.0\n",
      "step: 72330 loss: 0.602531 time elapsed: 96.1161 learning rate: 0.000980, scenario: 0, slope: -9.418867397273686e-05, fluctuations: 0.0\n",
      "step: 72340 loss: 0.601679 time elapsed: 96.1323 learning rate: 0.000980, scenario: 0, slope: -9.240694782370773e-05, fluctuations: 0.0\n",
      "step: 72350 loss: 0.600841 time elapsed: 96.1464 learning rate: 0.000980, scenario: 0, slope: -9.070178756612237e-05, fluctuations: 0.0\n",
      "step: 72360 loss: 0.600017 time elapsed: 96.1597 learning rate: 0.000980, scenario: 0, slope: -8.906860668069199e-05, fluctuations: 0.0\n",
      "step: 72370 loss: 0.599205 time elapsed: 96.1726 learning rate: 0.000980, scenario: 0, slope: -8.750322157392413e-05, fluctuations: 0.0\n",
      "step: 72380 loss: 0.598406 time elapsed: 96.1854 learning rate: 0.000980, scenario: 0, slope: -8.600181044321774e-05, fluctuations: 0.0\n",
      "step: 72390 loss: 0.597620 time elapsed: 96.1985 learning rate: 0.000980, scenario: 0, slope: -8.45608765422642e-05, fluctuations: 0.0\n",
      "step: 72400 loss: 0.596844 time elapsed: 96.2111 learning rate: 0.000980, scenario: 0, slope: -8.331308993009781e-05, fluctuations: 0.0\n",
      "step: 72410 loss: 0.596080 time elapsed: 96.2245 learning rate: 0.000980, scenario: 0, slope: -8.184788479693875e-05, fluctuations: 0.0\n",
      "step: 72420 loss: 0.595326 time elapsed: 96.2388 learning rate: 0.000980, scenario: 0, slope: -8.057017925294129e-05, fluctuations: 0.0\n",
      "step: 72430 loss: 0.594583 time elapsed: 96.2538 learning rate: 0.000980, scenario: 0, slope: -7.934160507517993e-05, fluctuations: 0.0\n",
      "step: 72440 loss: 0.593850 time elapsed: 96.2689 learning rate: 0.000980, scenario: 0, slope: -7.815985927873104e-05, fluctuations: 0.0\n",
      "step: 72450 loss: 0.593126 time elapsed: 96.2837 learning rate: 0.000980, scenario: 0, slope: -7.702280997832375e-05, fluctuations: 0.0\n",
      "step: 72460 loss: 0.592412 time elapsed: 96.2983 learning rate: 0.000980, scenario: 0, slope: -7.59284787777838e-05, fluctuations: 0.0\n",
      "step: 72470 loss: 0.591706 time elapsed: 96.3133 learning rate: 0.000980, scenario: 0, slope: -7.487502489560696e-05, fluctuations: 0.0\n",
      "step: 72480 loss: 0.591009 time elapsed: 96.3277 learning rate: 0.000980, scenario: 0, slope: -7.38607308932517e-05, fluctuations: 0.0\n",
      "step: 72490 loss: 0.590320 time elapsed: 96.3445 learning rate: 0.000980, scenario: 0, slope: -7.288398988736358e-05, fluctuations: 0.0\n",
      "step: 72500 loss: 0.589639 time elapsed: 96.3583 learning rate: 0.000980, scenario: 0, slope: -7.203578276274495e-05, fluctuations: 0.0\n",
      "step: 72510 loss: 0.588966 time elapsed: 96.3718 learning rate: 0.000980, scenario: 0, slope: -7.10372248769616e-05, fluctuations: 0.0\n",
      "step: 72520 loss: 0.588300 time elapsed: 96.3845 learning rate: 0.000980, scenario: 0, slope: -7.016444334093975e-05, fluctuations: 0.0\n",
      "step: 72530 loss: 0.587642 time elapsed: 96.3971 learning rate: 0.000980, scenario: 0, slope: -6.932368277721285e-05, fluctuations: 0.0\n",
      "step: 72540 loss: 0.586990 time elapsed: 96.4100 learning rate: 0.000980, scenario: 0, slope: -6.851374144177867e-05, fluctuations: 0.0\n",
      "step: 72550 loss: 0.586344 time elapsed: 96.4225 learning rate: 0.000980, scenario: 0, slope: -6.773347642114835e-05, fluctuations: 0.0\n",
      "step: 72560 loss: 0.585705 time elapsed: 96.4345 learning rate: 0.000980, scenario: 0, slope: -6.698179821720414e-05, fluctuations: 0.0\n",
      "step: 72570 loss: 0.585072 time elapsed: 96.4481 learning rate: 0.000980, scenario: 0, slope: -6.625766601590181e-05, fluctuations: 0.0\n",
      "step: 72580 loss: 0.584445 time elapsed: 96.4619 learning rate: 0.000980, scenario: 0, slope: -6.556008357254857e-05, fluctuations: 0.0\n",
      "step: 72590 loss: 0.583824 time elapsed: 96.4757 learning rate: 0.000980, scenario: 0, slope: -6.48880956483358e-05, fluctuations: 0.0\n",
      "step: 72600 loss: 0.583208 time elapsed: 96.4890 learning rate: 0.000980, scenario: 0, slope: -6.430443093799681e-05, fluctuations: 0.0\n",
      "step: 72610 loss: 0.582598 time elapsed: 96.5033 learning rate: 0.000980, scenario: 0, slope: -6.361726943184776e-05, fluctuations: 0.0\n",
      "step: 72620 loss: 0.581992 time elapsed: 96.5171 learning rate: 0.000980, scenario: 0, slope: -6.301670014389641e-05, fluctuations: 0.0\n",
      "step: 72630 loss: 0.581391 time elapsed: 96.5306 learning rate: 0.000980, scenario: 0, slope: -6.243825917359758e-05, fluctuations: 0.0\n",
      "step: 72640 loss: 0.580795 time elapsed: 96.5451 learning rate: 0.000980, scenario: 0, slope: -6.188115803214436e-05, fluctuations: 0.0\n",
      "step: 72650 loss: 0.580204 time elapsed: 96.5583 learning rate: 0.000980, scenario: 0, slope: -6.134463620292957e-05, fluctuations: 0.0\n",
      "step: 72660 loss: 0.579617 time elapsed: 96.5710 learning rate: 0.000980, scenario: 0, slope: -6.08279598944475e-05, fluctuations: 0.0\n",
      "step: 72670 loss: 0.579034 time elapsed: 96.5836 learning rate: 0.000980, scenario: 0, slope: -6.0330420952565415e-05, fluctuations: 0.0\n",
      "step: 72680 loss: 0.578455 time elapsed: 96.5959 learning rate: 0.000980, scenario: 0, slope: -5.9851335903690246e-05, fluctuations: 0.0\n",
      "step: 72690 loss: 0.577880 time elapsed: 96.6082 learning rate: 0.000980, scenario: 0, slope: -5.939004510300211e-05, fluctuations: 0.0\n",
      "step: 72700 loss: 0.577309 time elapsed: 96.6203 learning rate: 0.000980, scenario: 0, slope: -5.898957086294187e-05, fluctuations: 0.0\n",
      "step: 72710 loss: 0.576741 time elapsed: 96.6332 learning rate: 0.000980, scenario: 0, slope: -5.8518322267993186e-05, fluctuations: 0.0\n",
      "step: 72720 loss: 0.576176 time elapsed: 96.6460 learning rate: 0.001010, scenario: 1, slope: -5.8107017668157035e-05, fluctuations: 0.0\n",
      "step: 72730 loss: 0.575578 time elapsed: 96.6598 learning rate: 0.001116, scenario: 1, slope: -5.7803069247604696e-05, fluctuations: 0.0\n",
      "step: 72740 loss: 0.574921 time elapsed: 96.6740 learning rate: 0.001232, scenario: 1, slope: -5.788371355727117e-05, fluctuations: 0.0\n",
      "step: 72750 loss: 0.574212 time elapsed: 96.6879 learning rate: 0.001270, scenario: 0, slope: -5.859527769463753e-05, fluctuations: 0.0\n",
      "step: 72760 loss: 0.573501 time elapsed: 96.7018 learning rate: 0.001270, scenario: 0, slope: -5.9917409617536405e-05, fluctuations: 0.0\n",
      "step: 72770 loss: 0.572797 time elapsed: 96.7157 learning rate: 0.001270, scenario: 0, slope: -6.164693474104531e-05, fluctuations: 0.0\n",
      "step: 72780 loss: 0.572097 time elapsed: 96.7295 learning rate: 0.001270, scenario: 0, slope: -6.357996715698762e-05, fluctuations: 0.0\n",
      "step: 72790 loss: 0.571403 time elapsed: 96.7431 learning rate: 0.001270, scenario: 0, slope: -6.55164821485745e-05, fluctuations: 0.0\n",
      "step: 72800 loss: 0.570714 time elapsed: 96.7564 learning rate: 0.001270, scenario: 0, slope: -6.710007327142867e-05, fluctuations: 0.0\n",
      "step: 72810 loss: 0.570029 time elapsed: 96.7697 learning rate: 0.001270, scenario: 0, slope: -6.861874876693916e-05, fluctuations: 0.0\n",
      "step: 72820 loss: 0.569349 time elapsed: 96.7824 learning rate: 0.001270, scenario: 0, slope: -6.94032062023857e-05, fluctuations: 0.0\n",
      "step: 72830 loss: 0.568673 time elapsed: 96.7952 learning rate: 0.001270, scenario: 0, slope: -6.95312553822942e-05, fluctuations: 0.0\n",
      "step: 72840 loss: 0.568002 time elapsed: 96.8080 learning rate: 0.001270, scenario: 0, slope: -6.919854047364088e-05, fluctuations: 0.0\n",
      "step: 72850 loss: 0.567334 time elapsed: 96.8204 learning rate: 0.001270, scenario: 0, slope: -6.871767390421437e-05, fluctuations: 0.0\n",
      "step: 72860 loss: 0.566670 time elapsed: 96.8329 learning rate: 0.001270, scenario: 0, slope: -6.825324990449795e-05, fluctuations: 0.0\n",
      "step: 72870 loss: 0.566010 time elapsed: 96.8452 learning rate: 0.001270, scenario: 0, slope: -6.780914315624469e-05, fluctuations: 0.0\n",
      "step: 72880 loss: 0.565354 time elapsed: 96.8578 learning rate: 0.001270, scenario: 0, slope: -6.738462673418318e-05, fluctuations: 0.0\n",
      "step: 72890 loss: 0.564700 time elapsed: 96.8718 learning rate: 0.001270, scenario: 0, slope: -6.697885354721287e-05, fluctuations: 0.0\n",
      "step: 72900 loss: 0.564050 time elapsed: 96.8856 learning rate: 0.001270, scenario: 0, slope: -6.662897465384087e-05, fluctuations: 0.0\n",
      "step: 72910 loss: 0.563403 time elapsed: 96.9001 learning rate: 0.001270, scenario: 0, slope: -6.622013766900036e-05, fluctuations: 0.0\n",
      "step: 72920 loss: 0.562758 time elapsed: 96.9137 learning rate: 0.001270, scenario: 0, slope: -6.586557462117033e-05, fluctuations: 0.0\n",
      "step: 72930 loss: 0.562117 time elapsed: 96.9280 learning rate: 0.001270, scenario: 0, slope: -6.552652727661169e-05, fluctuations: 0.0\n",
      "step: 72940 loss: 0.561478 time elapsed: 96.9424 learning rate: 0.001270, scenario: 0, slope: -6.520228465527189e-05, fluctuations: 0.0\n",
      "step: 72950 loss: 0.560841 time elapsed: 96.9564 learning rate: 0.001270, scenario: 0, slope: -6.489217350413107e-05, fluctuations: 0.0\n",
      "step: 72960 loss: 0.560207 time elapsed: 96.9711 learning rate: 0.001270, scenario: 0, slope: -6.45955562112721e-05, fluctuations: 0.0\n",
      "step: 72970 loss: 0.559576 time elapsed: 96.9836 learning rate: 0.001270, scenario: 0, slope: -6.431182872642969e-05, fluctuations: 0.0\n",
      "step: 72980 loss: 0.558946 time elapsed: 96.9963 learning rate: 0.001270, scenario: 0, slope: -6.404041856774279e-05, fluctuations: 0.0\n",
      "step: 72990 loss: 0.558319 time elapsed: 97.0090 learning rate: 0.001270, scenario: 0, slope: -6.37807829401157e-05, fluctuations: 0.0\n",
      "step: 73000 loss: 0.557694 time elapsed: 97.0220 learning rate: 0.001270, scenario: 0, slope: -6.35567520037518e-05, fluctuations: 0.0\n",
      "step: 73010 loss: 0.557070 time elapsed: 97.0356 learning rate: 0.001270, scenario: 0, slope: -6.329480203171491e-05, fluctuations: 0.0\n",
      "step: 73020 loss: 0.556449 time elapsed: 97.0486 learning rate: 0.001270, scenario: 0, slope: -6.306750422866734e-05, fluctuations: 0.0\n",
      "step: 73030 loss: 0.555829 time elapsed: 97.0614 learning rate: 0.001270, scenario: 0, slope: -6.285007290698783e-05, fluctuations: 0.0\n",
      "step: 73040 loss: 0.555211 time elapsed: 97.0749 learning rate: 0.001270, scenario: 0, slope: -6.26420893126484e-05, fluctuations: 0.0\n",
      "step: 73050 loss: 0.554595 time elapsed: 97.0894 learning rate: 0.001270, scenario: 0, slope: -6.244315531529925e-05, fluctuations: 0.0\n",
      "step: 73060 loss: 0.553980 time elapsed: 97.1035 learning rate: 0.001270, scenario: 0, slope: -6.225289221718301e-05, fluctuations: 0.0\n",
      "step: 73070 loss: 0.553366 time elapsed: 97.1173 learning rate: 0.001270, scenario: 0, slope: -6.207093963892657e-05, fluctuations: 0.0\n",
      "step: 73080 loss: 0.552754 time elapsed: 97.1311 learning rate: 0.001270, scenario: 0, slope: -6.1896954475896e-05, fluctuations: 0.0\n",
      "step: 73090 loss: 0.552144 time elapsed: 97.1450 learning rate: 0.001270, scenario: 0, slope: -6.173060992015645e-05, fluctuations: 0.0\n",
      "step: 73100 loss: 0.551534 time elapsed: 97.1586 learning rate: 0.001270, scenario: 0, slope: -6.15871739274439e-05, fluctuations: 0.0\n",
      "step: 73110 loss: 0.550929 time elapsed: 97.1730 learning rate: 0.001270, scenario: 0, slope: -6.141676157658222e-05, fluctuations: 0.0\n",
      "step: 73120 loss: 0.557006 time elapsed: 97.1873 learning rate: 0.001282, scenario: 1, slope: -5.4196421802009927e-05, fluctuations: 0.0\n",
      "step: 73130 loss: 32.092650 time elapsed: 97.2003 learning rate: 0.001213, scenario: -1, slope: 0.04040152244634589, fluctuations: 0.0\n",
      "step: 73140 loss: 4.202044 time elapsed: 97.2132 learning rate: 0.001097, scenario: -1, slope: 0.06218325080310516, fluctuations: 0.03\n",
      "step: 73150 loss: 0.826203 time elapsed: 97.2261 learning rate: 0.000992, scenario: -1, slope: 0.057351182063098576, fluctuations: 0.06\n",
      "step: 73160 loss: 0.561338 time elapsed: 97.2387 learning rate: 0.000898, scenario: -1, slope: 0.04714538066404462, fluctuations: 0.1\n",
      "step: 73170 loss: 0.659766 time elapsed: 97.2512 learning rate: 0.000812, scenario: -1, slope: 0.03260196675005841, fluctuations: 0.14\n",
      "step: 73180 loss: 0.557353 time elapsed: 97.2639 learning rate: 0.000734, scenario: -1, slope: 0.01717786137733914, fluctuations: 0.19\n",
      "step: 73190 loss: 0.553329 time elapsed: 97.2768 learning rate: 0.000677, scenario: 0, slope: -0.0012775855079227814, fluctuations: 0.24\n",
      "step: 73200 loss: 0.549026 time elapsed: 97.2911 learning rate: 0.000677, scenario: 0, slope: -0.020242362477869313, fluctuations: 0.28\n",
      "step: 73210 loss: 0.548529 time elapsed: 97.3061 learning rate: 0.000677, scenario: 0, slope: -0.04573867058150612, fluctuations: 0.34\n",
      "step: 73220 loss: 0.548253 time elapsed: 97.3200 learning rate: 0.000677, scenario: 0, slope: -0.07435597519376524, fluctuations: 0.39\n",
      "step: 73230 loss: 0.547885 time elapsed: 97.3339 learning rate: 0.000677, scenario: 0, slope: -0.07332069958857888, fluctuations: 0.43\n",
      "step: 73240 loss: 0.547528 time elapsed: 97.3477 learning rate: 0.000677, scenario: 0, slope: -0.009555329900341367, fluctuations: 0.46\n",
      "step: 73250 loss: 0.547186 time elapsed: 97.3613 learning rate: 0.000677, scenario: 0, slope: -0.004022014319571999, fluctuations: 0.43\n",
      "step: 73260 loss: 0.546859 time elapsed: 97.3751 learning rate: 0.000677, scenario: 0, slope: -0.001153886312275633, fluctuations: 0.39\n",
      "step: 73270 loss: 0.546539 time elapsed: 97.3891 learning rate: 0.000677, scenario: 0, slope: -0.00024310165817729453, fluctuations: 0.35\n",
      "step: 73280 loss: 0.546223 time elapsed: 97.4021 learning rate: 0.000677, scenario: 0, slope: -0.00012389750357644807, fluctuations: 0.3\n",
      "step: 73290 loss: 0.545909 time elapsed: 97.4143 learning rate: 0.000657, scenario: -1, slope: -5.4555445437171206e-05, fluctuations: 0.25\n",
      "step: 73300 loss: 0.545615 time elapsed: 97.4274 learning rate: 0.000600, scenario: -1, slope: -3.306645599111242e-05, fluctuations: 0.21\n",
      "step: 73310 loss: 0.545347 time elapsed: 97.4407 learning rate: 0.000543, scenario: -1, slope: -3.455467858048496e-05, fluctuations: 0.15\n",
      "step: 73320 loss: 0.545104 time elapsed: 97.4532 learning rate: 0.000491, scenario: -1, slope: -3.199867689322126e-05, fluctuations: 0.1\n",
      "step: 73330 loss: 0.544884 time elapsed: 97.4659 learning rate: 0.000444, scenario: -1, slope: -3.032968797224086e-05, fluctuations: 0.05\n",
      "step: 73340 loss: 0.544685 time elapsed: 97.4791 learning rate: 0.000408, scenario: 1, slope: -2.8939170270054358e-05, fluctuations: 0.0\n",
      "step: 73350 loss: 0.544489 time elapsed: 97.4934 learning rate: 0.000450, scenario: 1, slope: -2.7245799789927646e-05, fluctuations: 0.0\n",
      "step: 73360 loss: 0.544272 time elapsed: 97.5076 learning rate: 0.000498, scenario: 1, slope: -2.5608505593801878e-05, fluctuations: 0.0\n",
      "step: 73370 loss: 0.544031 time elapsed: 97.5214 learning rate: 0.000550, scenario: 1, slope: -2.4194134210158682e-05, fluctuations: 0.0\n",
      "step: 73380 loss: 0.543765 time elapsed: 97.5355 learning rate: 0.000607, scenario: 1, slope: -2.3205188128226765e-05, fluctuations: 0.0\n",
      "step: 73390 loss: 0.543471 time elapsed: 97.5492 learning rate: 0.000671, scenario: 1, slope: -2.284558319242462e-05, fluctuations: 0.0\n",
      "step: 73400 loss: 0.543146 time elapsed: 97.5634 learning rate: 0.000733, scenario: 1, slope: -2.3177097999165862e-05, fluctuations: 0.0\n",
      "step: 73410 loss: 0.542790 time elapsed: 97.5780 learning rate: 0.000810, scenario: 1, slope: -2.4418349279284683e-05, fluctuations: 0.0\n",
      "step: 73420 loss: 0.542397 time elapsed: 97.5918 learning rate: 0.000895, scenario: 1, slope: -2.6283861372818276e-05, fluctuations: 0.0\n",
      "step: 73430 loss: 0.541962 time elapsed: 97.6043 learning rate: 0.000989, scenario: 1, slope: -2.875610731084408e-05, fluctuations: 0.0\n",
      "step: 73440 loss: 0.541482 time elapsed: 97.6172 learning rate: 0.001092, scenario: 1, slope: -3.1703289009432296e-05, fluctuations: 0.0\n",
      "step: 73450 loss: 0.540952 time elapsed: 97.6297 learning rate: 0.001206, scenario: 1, slope: -3.4989604647341825e-05, fluctuations: 0.0\n",
      "step: 73460 loss: 0.540368 time elapsed: 97.6426 learning rate: 0.001332, scenario: 1, slope: -3.860975755045337e-05, fluctuations: 0.0\n",
      "step: 73470 loss: 0.539725 time elapsed: 97.6549 learning rate: 0.001472, scenario: 1, slope: -4.259802538858734e-05, fluctuations: 0.0\n",
      "step: 73480 loss: 132.790891 time elapsed: 97.6673 learning rate: 0.001509, scenario: -1, slope: 0.09257341958353307, fluctuations: 0.0\n",
      "step: 73490 loss: 76.360472 time elapsed: 97.6795 learning rate: 0.001364, scenario: -1, slope: 0.8891214486808581, fluctuations: 0.04\n",
      "step: 73500 loss: 64.396388 time elapsed: 97.6931 learning rate: 0.001246, scenario: -1, slope: 1.0018979403383874, fluctuations: 0.08\n",
      "step: 73510 loss: 22.466934 time elapsed: 97.7076 learning rate: 0.001127, scenario: -1, slope: 0.8663717933408357, fluctuations: 0.12\n",
      "step: 73520 loss: 7.653468 time elapsed: 97.7215 learning rate: 0.001019, scenario: -1, slope: 0.6250257732679745, fluctuations: 0.16\n",
      "step: 73530 loss: 2.672831 time elapsed: 97.7351 learning rate: 0.000922, scenario: -1, slope: 0.35285829922753376, fluctuations: 0.2\n",
      "step: 73540 loss: 0.942979 time elapsed: 97.7487 learning rate: 0.000834, scenario: -1, slope: 0.06340365519170137, fluctuations: 0.24\n",
      "step: 73550 loss: 0.821216 time elapsed: 97.7623 learning rate: 0.000825, scenario: 0, slope: -0.2571071231032288, fluctuations: 0.27\n",
      "step: 73560 loss: 0.709774 time elapsed: 97.7757 learning rate: 0.000825, scenario: 0, slope: -0.5979044549191409, fluctuations: 0.3\n",
      "step: 73570 loss: 0.621849 time elapsed: 97.7901 learning rate: 0.000825, scenario: 0, slope: -1.0278135140352394, fluctuations: 0.34\n",
      "step: 73580 loss: 0.622527 time elapsed: 97.8048 learning rate: 0.000825, scenario: 0, slope: -1.5032890599609696, fluctuations: 0.37\n",
      "step: 73590 loss: 0.604742 time elapsed: 97.8178 learning rate: 0.000825, scenario: 0, slope: -0.49659433648215495, fluctuations: 0.36\n",
      "step: 73600 loss: 0.601116 time elapsed: 97.8300 learning rate: 0.000825, scenario: 0, slope: -0.16924177925082923, fluctuations: 0.35\n",
      "step: 73610 loss: 0.596595 time elapsed: 97.8436 learning rate: 0.000825, scenario: 0, slope: -0.0290957012783562, fluctuations: 0.35\n",
      "step: 73620 loss: 0.593505 time elapsed: 97.8570 learning rate: 0.000825, scenario: 0, slope: -0.011562433259108117, fluctuations: 0.32\n",
      "step: 73630 loss: 0.590825 time elapsed: 97.8695 learning rate: 0.000825, scenario: 0, slope: -0.004780404932155615, fluctuations: 0.28\n",
      "step: 73640 loss: 0.588500 time elapsed: 97.8821 learning rate: 0.000825, scenario: 0, slope: -0.0017440885099760705, fluctuations: 0.25\n",
      "step: 73650 loss: 0.586436 time elapsed: 97.8945 learning rate: 0.000825, scenario: 0, slope: -0.0010150953354426293, fluctuations: 0.21\n",
      "step: 73660 loss: 0.584572 time elapsed: 97.9088 learning rate: 0.000825, scenario: 0, slope: -0.0005075011382381637, fluctuations: 0.18\n",
      "step: 73670 loss: 0.582879 time elapsed: 97.9233 learning rate: 0.000825, scenario: 0, slope: -0.00036588281941473994, fluctuations: 0.14\n",
      "step: 73680 loss: 0.581331 time elapsed: 97.9376 learning rate: 0.000825, scenario: 0, slope: -0.00027256290617655503, fluctuations: 0.11\n",
      "step: 73690 loss: 0.579907 time elapsed: 97.9516 learning rate: 0.000825, scenario: 0, slope: -0.0002302760963478791, fluctuations: 0.08\n",
      "step: 73700 loss: 0.578593 time elapsed: 97.9656 learning rate: 0.000825, scenario: 0, slope: -0.00020679534402035624, fluctuations: 0.04\n",
      "step: 73710 loss: 0.577374 time elapsed: 97.9799 learning rate: 0.000825, scenario: 0, slope: -0.00018362457988316264, fluctuations: 0.01\n",
      "step: 73720 loss: 0.576239 time elapsed: 97.9939 learning rate: 0.000825, scenario: 0, slope: -0.00016704001419586277, fluctuations: 0.0\n",
      "step: 73730 loss: 0.575179 time elapsed: 98.0098 learning rate: 0.000825, scenario: 0, slope: -0.00015235029137159568, fluctuations: 0.0\n",
      "step: 73740 loss: 0.574186 time elapsed: 98.0236 learning rate: 0.000825, scenario: 0, slope: -0.00013989686016874147, fluctuations: 0.0\n",
      "step: 73750 loss: 0.573252 time elapsed: 98.0365 learning rate: 0.000825, scenario: 0, slope: -0.0001291322015730946, fluctuations: 0.0\n",
      "step: 73760 loss: 0.572370 time elapsed: 98.0494 learning rate: 0.000825, scenario: 0, slope: -0.00011973818769274034, fluctuations: 0.0\n",
      "step: 73770 loss: 0.571536 time elapsed: 98.0620 learning rate: 0.000825, scenario: 0, slope: -0.00011147587127707154, fluctuations: 0.0\n",
      "step: 73780 loss: 0.570744 time elapsed: 98.0750 learning rate: 0.000825, scenario: 0, slope: -0.00010417209586756344, fluctuations: 0.0\n",
      "step: 73790 loss: 0.569991 time elapsed: 98.0874 learning rate: 0.000825, scenario: 0, slope: -9.768915197371477e-05, fluctuations: 0.0\n",
      "step: 73800 loss: 0.569272 time elapsed: 98.0999 learning rate: 0.000825, scenario: 0, slope: -9.246446039047232e-05, fluctuations: 0.0\n",
      "step: 73810 loss: 0.568584 time elapsed: 98.1143 learning rate: 0.000825, scenario: 0, slope: -8.67622473838516e-05, fluctuations: 0.0\n",
      "step: 73820 loss: 0.567924 time elapsed: 98.1283 learning rate: 0.000825, scenario: 0, slope: -8.214981444994294e-05, fluctuations: 0.0\n",
      "step: 73830 loss: 0.567289 time elapsed: 98.1424 learning rate: 0.000825, scenario: 0, slope: -7.801372756178662e-05, fluctuations: 0.0\n",
      "step: 73840 loss: 0.566677 time elapsed: 98.1559 learning rate: 0.000825, scenario: 0, slope: -7.42978651212213e-05, fluctuations: 0.0\n",
      "step: 73850 loss: 0.566086 time elapsed: 98.1694 learning rate: 0.000825, scenario: 0, slope: -7.095370733992125e-05, fluctuations: 0.0\n",
      "step: 73860 loss: 0.565513 time elapsed: 98.1830 learning rate: 0.000825, scenario: 0, slope: -6.793906943264103e-05, fluctuations: 0.0\n",
      "step: 73870 loss: 0.564958 time elapsed: 98.1970 learning rate: 0.000825, scenario: 0, slope: -6.521712054944886e-05, fluctuations: 0.0\n",
      "step: 73880 loss: 0.564419 time elapsed: 98.2108 learning rate: 0.000825, scenario: 0, slope: -6.275559630805093e-05, fluctuations: 0.0\n",
      "step: 73890 loss: 0.563893 time elapsed: 98.2249 learning rate: 0.000825, scenario: 0, slope: -6.052615733303921e-05, fluctuations: 0.0\n",
      "step: 73900 loss: 0.563381 time elapsed: 98.2378 learning rate: 0.000825, scenario: 0, slope: -5.869741404174578e-05, fluctuations: 0.0\n",
      "step: 73910 loss: 0.562881 time elapsed: 98.2510 learning rate: 0.000842, scenario: 1, slope: -5.6666693678367097e-05, fluctuations: 0.0\n",
      "step: 73920 loss: 0.562364 time elapsed: 98.2631 learning rate: 0.000930, scenario: 1, slope: -5.5057394949391135e-05, fluctuations: 0.0\n",
      "step: 73930 loss: 0.561806 time elapsed: 98.2752 learning rate: 0.001027, scenario: 1, slope: -5.3888511616624374e-05, fluctuations: 0.0\n",
      "step: 73940 loss: 0.561204 time elapsed: 98.2875 learning rate: 0.001135, scenario: 1, slope: -5.336111250703605e-05, fluctuations: 0.0\n",
      "step: 73950 loss: 0.560555 time elapsed: 98.2998 learning rate: 0.001254, scenario: 1, slope: -5.362592407068289e-05, fluctuations: 0.0\n",
      "step: 73960 loss: 0.559856 time elapsed: 98.3123 learning rate: 0.001385, scenario: 1, slope: -5.478175979250599e-05, fluctuations: 0.0\n",
      "step: 73970 loss: 0.559104 time elapsed: 98.3259 learning rate: 0.001499, scenario: 0, slope: -5.687410945462725e-05, fluctuations: 0.0\n",
      "step: 73980 loss: 0.558339 time elapsed: 98.3401 learning rate: 0.001499, scenario: 0, slope: -5.979662227970957e-05, fluctuations: 0.0\n",
      "step: 73990 loss: 0.557593 time elapsed: 98.3538 learning rate: 0.001499, scenario: 0, slope: -6.312717329784719e-05, fluctuations: 0.0\n",
      "step: 74000 loss: 0.556864 time elapsed: 98.3673 learning rate: 0.001499, scenario: 0, slope: -6.609718004640026e-05, fluctuations: 0.0\n",
      "step: 74010 loss: 0.556151 time elapsed: 98.3820 learning rate: 0.001499, scenario: 0, slope: -6.921679669718348e-05, fluctuations: 0.0\n",
      "step: 74020 loss: 0.555451 time elapsed: 98.3956 learning rate: 0.001499, scenario: 0, slope: -7.120341218611315e-05, fluctuations: 0.0\n",
      "step: 74030 loss: 0.554764 time elapsed: 98.4097 learning rate: 0.001499, scenario: 0, slope: -7.22836347490634e-05, fluctuations: 0.0\n",
      "step: 74040 loss: 0.554088 time elapsed: 98.4240 learning rate: 0.001499, scenario: 0, slope: -7.249263632701038e-05, fluctuations: 0.0\n",
      "step: 74050 loss: 0.553421 time elapsed: 98.4378 learning rate: 0.001499, scenario: 0, slope: -7.195513833545938e-05, fluctuations: 0.0\n",
      "step: 74060 loss: 0.552764 time elapsed: 98.4504 learning rate: 0.001499, scenario: 0, slope: -7.088733053013775e-05, fluctuations: 0.0\n",
      "step: 74070 loss: 0.552116 time elapsed: 98.4627 learning rate: 0.001499, scenario: 0, slope: -6.959974158009367e-05, fluctuations: 0.0\n",
      "step: 74080 loss: 0.551475 time elapsed: 98.4755 learning rate: 0.001499, scenario: 0, slope: -6.839260242024431e-05, fluctuations: 0.0\n",
      "step: 74090 loss: 0.550841 time elapsed: 98.4881 learning rate: 0.001499, scenario: 0, slope: -6.730289221618566e-05, fluctuations: 0.0\n",
      "step: 74100 loss: 0.550214 time elapsed: 98.5016 learning rate: 0.001499, scenario: 0, slope: -6.64098945474445e-05, fluctuations: 0.0\n",
      "step: 74110 loss: 0.549592 time elapsed: 98.5168 learning rate: 0.001499, scenario: 0, slope: -6.541651600519974e-05, fluctuations: 0.0\n",
      "step: 74120 loss: 0.548977 time elapsed: 98.5325 learning rate: 0.001499, scenario: 0, slope: -6.45950047197399e-05, fluctuations: 0.0\n",
      "step: 74130 loss: 0.548366 time elapsed: 98.5502 learning rate: 0.001499, scenario: 0, slope: -6.384117416864275e-05, fluctuations: 0.0\n",
      "step: 74140 loss: 0.547761 time elapsed: 98.5654 learning rate: 0.001499, scenario: 0, slope: -6.314691233722001e-05, fluctuations: 0.0\n",
      "step: 74150 loss: 0.547160 time elapsed: 98.5794 learning rate: 0.001499, scenario: 0, slope: -6.250536523169864e-05, fluctuations: 0.0\n",
      "step: 74160 loss: 0.546564 time elapsed: 98.5940 learning rate: 0.001499, scenario: 0, slope: -6.191071587305252e-05, fluctuations: 0.0\n",
      "step: 74170 loss: 0.545971 time elapsed: 98.6081 learning rate: 0.001499, scenario: 0, slope: -6.135800234022185e-05, fluctuations: 0.0\n",
      "step: 74180 loss: 0.545382 time elapsed: 98.6225 learning rate: 0.001499, scenario: 0, slope: -6.084296970749898e-05, fluctuations: 0.0\n",
      "step: 74190 loss: 0.544797 time elapsed: 98.6371 learning rate: 0.001499, scenario: 0, slope: -6.0361949889739484e-05, fluctuations: 0.0\n",
      "step: 74200 loss: 0.544216 time elapsed: 98.6498 learning rate: 0.001499, scenario: 0, slope: -5.9955476903822224e-05, fluctuations: 0.0\n",
      "step: 74210 loss: 0.543637 time elapsed: 98.6627 learning rate: 0.001499, scenario: 0, slope: -5.948964298932587e-05, fluctuations: 0.0\n",
      "step: 74220 loss: 0.543062 time elapsed: 98.6756 learning rate: 0.001499, scenario: 0, slope: -5.909316236111225e-05, fluctuations: 0.0\n",
      "step: 74230 loss: 0.542489 time elapsed: 98.6883 learning rate: 0.001499, scenario: 0, slope: -5.872018916482338e-05, fluctuations: 0.0\n",
      "step: 74240 loss: 0.541919 time elapsed: 98.7006 learning rate: 0.001499, scenario: 0, slope: -5.836883795065435e-05, fluctuations: 0.0\n",
      "step: 74250 loss: 0.541352 time elapsed: 98.7135 learning rate: 0.001499, scenario: 0, slope: -5.803743451402103e-05, fluctuations: 0.0\n",
      "step: 74260 loss: 0.540787 time elapsed: 98.7259 learning rate: 0.001499, scenario: 0, slope: -5.772448578194587e-05, fluctuations: 0.0\n",
      "step: 74270 loss: 0.540224 time elapsed: 98.7394 learning rate: 0.001499, scenario: 0, slope: -5.74286547214617e-05, fluctuations: 0.0\n",
      "step: 74280 loss: 0.539664 time elapsed: 98.7534 learning rate: 0.001499, scenario: 0, slope: -5.7148739346968816e-05, fluctuations: 0.0\n",
      "step: 74290 loss: 0.539105 time elapsed: 98.7673 learning rate: 0.001499, scenario: 0, slope: -5.688365508576471e-05, fluctuations: 0.0\n",
      "step: 74300 loss: 0.538549 time elapsed: 98.7809 learning rate: 0.001499, scenario: 0, slope: -5.6656946290044886e-05, fluctuations: 0.0\n",
      "step: 74310 loss: 0.537994 time elapsed: 98.7951 learning rate: 0.001499, scenario: 0, slope: -5.6394141694524646e-05, fluctuations: 0.0\n",
      "step: 74320 loss: 0.537442 time elapsed: 98.8089 learning rate: 0.001499, scenario: 0, slope: -5.616800757919661e-05, fluctuations: 0.0\n",
      "step: 74330 loss: 0.536891 time elapsed: 98.8226 learning rate: 0.001499, scenario: 0, slope: -5.595327472913856e-05, fluctuations: 0.0\n",
      "step: 74340 loss: 0.536341 time elapsed: 98.8364 learning rate: 0.001499, scenario: 0, slope: -5.5749262519575005e-05, fluctuations: 0.0\n",
      "step: 74350 loss: 0.535794 time elapsed: 98.8505 learning rate: 0.001499, scenario: 0, slope: -5.5555345758890146e-05, fluctuations: 0.0\n",
      "step: 74360 loss: 0.535247 time elapsed: 98.8633 learning rate: 0.001499, scenario: 0, slope: -5.537094883297739e-05, fluctuations: 0.0\n",
      "step: 74370 loss: 0.534702 time elapsed: 98.8766 learning rate: 0.001499, scenario: 0, slope: -5.5195540618273106e-05, fluctuations: 0.0\n",
      "step: 74380 loss: 0.534159 time elapsed: 98.8889 learning rate: 0.001499, scenario: 0, slope: -5.5028630044706364e-05, fluctuations: 0.0\n",
      "step: 74390 loss: 0.533616 time elapsed: 98.9012 learning rate: 0.001499, scenario: 0, slope: -5.486976221019289e-05, fluctuations: 0.0\n",
      "step: 74400 loss: 0.533075 time elapsed: 98.9134 learning rate: 0.001499, scenario: 0, slope: -5.4733308166972917e-05, fluctuations: 0.0\n",
      "step: 74410 loss: 0.532535 time elapsed: 98.9262 learning rate: 0.001499, scenario: 0, slope: -5.457449589692769e-05, fluctuations: 0.0\n",
      "step: 74420 loss: 0.531996 time elapsed: 98.9381 learning rate: 0.001499, scenario: 0, slope: -5.4437339664475834e-05, fluctuations: 0.0\n",
      "step: 74430 loss: 0.531458 time elapsed: 98.9524 learning rate: 0.001499, scenario: 0, slope: -5.4306705622972174e-05, fluctuations: 0.0\n",
      "step: 74440 loss: 0.530922 time elapsed: 98.9665 learning rate: 0.001499, scenario: 0, slope: -5.418227571116367e-05, fluctuations: 0.0\n",
      "step: 74450 loss: 0.530386 time elapsed: 98.9803 learning rate: 0.001499, scenario: 0, slope: -5.406375256008465e-05, fluctuations: 0.0\n",
      "step: 74460 loss: 0.529851 time elapsed: 98.9942 learning rate: 0.001499, scenario: 0, slope: -5.395085779665823e-05, fluctuations: 0.0\n",
      "step: 74470 loss: 0.529317 time elapsed: 99.0083 learning rate: 0.001499, scenario: 0, slope: -5.384333051761184e-05, fluctuations: 0.0\n",
      "step: 74480 loss: 0.528783 time elapsed: 99.0222 learning rate: 0.001499, scenario: 0, slope: -5.3740925913084435e-05, fluctuations: 0.0\n",
      "step: 74490 loss: 0.528251 time elapsed: 99.0360 learning rate: 0.001499, scenario: 0, slope: -5.364341397998285e-05, fluctuations: 0.0\n",
      "step: 74500 loss: 0.527719 time elapsed: 99.0499 learning rate: 0.001499, scenario: 0, slope: -5.355963656280398e-05, fluctuations: 0.0\n",
      "step: 74510 loss: 0.527255 time elapsed: 99.0632 learning rate: 0.001499, scenario: 0, slope: -5.338869988971932e-05, fluctuations: 0.0\n",
      "step: 74520 loss: 1.553661 time elapsed: 99.0757 learning rate: 0.001552, scenario: -1, slope: 0.0008103993544205595, fluctuations: 0.0\n",
      "step: 74530 loss: 94.188665 time elapsed: 99.0885 learning rate: 0.001404, scenario: -1, slope: 0.29466233598300906, fluctuations: 0.01\n",
      "step: 74540 loss: 1.227281 time elapsed: 99.1013 learning rate: 0.001270, scenario: -1, slope: 0.2689069462286303, fluctuations: 0.05\n",
      "step: 74550 loss: 2.876647 time elapsed: 99.1139 learning rate: 0.001148, scenario: -1, slope: 0.24017709530016232, fluctuations: 0.09\n",
      "step: 74560 loss: 1.846383 time elapsed: 99.1265 learning rate: 0.001038, scenario: -1, slope: 0.1942745046943488, fluctuations: 0.14\n",
      "step: 74570 loss: 1.169234 time elapsed: 99.1391 learning rate: 0.000939, scenario: -1, slope: 0.11886051056737744, fluctuations: 0.18\n",
      "step: 74580 loss: 0.604422 time elapsed: 99.1516 learning rate: 0.000849, scenario: -1, slope: 0.04243616810572223, fluctuations: 0.23\n",
      "step: 74590 loss: 0.584340 time elapsed: 99.1654 learning rate: 0.000816, scenario: 0, slope: -0.043475996671082255, fluctuations: 0.28\n",
      "step: 74600 loss: 0.551748 time elapsed: 99.1795 learning rate: 0.000816, scenario: 0, slope: -0.13041814559212228, fluctuations: 0.32\n",
      "step: 74610 loss: 0.539687 time elapsed: 99.1938 learning rate: 0.000816, scenario: 0, slope: -0.2526014691427877, fluctuations: 0.37\n",
      "step: 74620 loss: 0.538407 time elapsed: 99.2074 learning rate: 0.000816, scenario: 0, slope: -0.3956429001295287, fluctuations: 0.41\n",
      "step: 74630 loss: 0.535074 time elapsed: 99.2211 learning rate: 0.000816, scenario: 0, slope: -0.09909945959781677, fluctuations: 0.44\n",
      "step: 74640 loss: 0.534936 time elapsed: 99.2346 learning rate: 0.000816, scenario: 0, slope: -0.044876298975700994, fluctuations: 0.44\n",
      "step: 74650 loss: 0.534069 time elapsed: 99.2483 learning rate: 0.000816, scenario: 0, slope: -0.011329323337283733, fluctuations: 0.45\n",
      "step: 74660 loss: 0.533638 time elapsed: 99.2622 learning rate: 0.000816, scenario: 0, slope: -0.002678084688516024, fluctuations: 0.45\n",
      "step: 74670 loss: 0.533206 time elapsed: 99.2754 learning rate: 0.000816, scenario: 0, slope: -0.0008487230860535102, fluctuations: 0.4\n",
      "step: 74680 loss: 0.532787 time elapsed: 99.2881 learning rate: 0.000816, scenario: 0, slope: -0.0003525719182616108, fluctuations: 0.35\n",
      "step: 74690 loss: 0.532396 time elapsed: 99.3005 learning rate: 0.000816, scenario: 0, slope: -0.00011428435450962556, fluctuations: 0.31\n",
      "step: 74700 loss: 0.532013 time elapsed: 99.3126 learning rate: 0.000816, scenario: 0, slope: -7.92647493212246e-05, fluctuations: 0.27\n",
      "step: 74710 loss: 0.531640 time elapsed: 99.3254 learning rate: 0.000808, scenario: -1, slope: -5.3741710604885386e-05, fluctuations: 0.22\n",
      "step: 74720 loss: 0.531290 time elapsed: 99.3377 learning rate: 0.000731, scenario: -1, slope: -4.5454746804257813e-05, fluctuations: 0.17\n",
      "step: 74730 loss: 0.530979 time elapsed: 99.3499 learning rate: 0.000661, scenario: -1, slope: -4.168197676013107e-05, fluctuations: 0.12\n",
      "step: 74740 loss: 0.530701 time elapsed: 99.3620 learning rate: 0.000597, scenario: -1, slope: -3.8304140784683266e-05, fluctuations: 0.08\n",
      "step: 74750 loss: 0.530452 time elapsed: 99.3754 learning rate: 0.000540, scenario: -1, slope: -3.645247364176707e-05, fluctuations: 0.03\n",
      "step: 74760 loss: 0.530227 time elapsed: 99.3897 learning rate: 0.000537, scenario: 1, slope: -3.440681627823038e-05, fluctuations: 0.0\n",
      "step: 74770 loss: 0.529991 time elapsed: 99.4037 learning rate: 0.000594, scenario: 1, slope: -3.209926699492496e-05, fluctuations: 0.0\n",
      "step: 74780 loss: 0.529732 time elapsed: 99.4177 learning rate: 0.000656, scenario: 1, slope: -2.9963752456822827e-05, fluctuations: 0.0\n",
      "step: 74790 loss: 0.529447 time elapsed: 99.4313 learning rate: 0.000724, scenario: 1, slope: -2.820004241729833e-05, fluctuations: 0.0\n",
      "step: 74800 loss: 0.529136 time elapsed: 99.4455 learning rate: 0.000792, scenario: 1, slope: -2.7107006206782528e-05, fluctuations: 0.0\n",
      "step: 74810 loss: 0.528797 time elapsed: 99.4606 learning rate: 0.000875, scenario: 1, slope: -2.661117189338502e-05, fluctuations: 0.0\n",
      "step: 74820 loss: 0.528427 time elapsed: 99.4756 learning rate: 0.000967, scenario: 1, slope: -2.709895610558941e-05, fluctuations: 0.0\n",
      "step: 74830 loss: 0.528022 time elapsed: 99.4896 learning rate: 0.001068, scenario: 1, slope: -2.844592263063923e-05, fluctuations: 0.0\n",
      "step: 74840 loss: 0.527578 time elapsed: 99.5031 learning rate: 0.001180, scenario: 1, slope: -3.0541157103784644e-05, fluctuations: 0.0\n",
      "step: 74850 loss: 0.527092 time elapsed: 99.5164 learning rate: 0.001303, scenario: 1, slope: -3.3235799243167637e-05, fluctuations: 0.0\n",
      "step: 74860 loss: 0.526561 time elapsed: 99.5293 learning rate: 0.001439, scenario: 1, slope: -3.6343108223255716e-05, fluctuations: 0.0\n",
      "step: 74870 loss: 0.525981 time elapsed: 99.5423 learning rate: 0.001590, scenario: 1, slope: -3.974568117788083e-05, fluctuations: 0.0\n",
      "step: 74880 loss: 0.525346 time elapsed: 99.5549 learning rate: 0.001756, scenario: 1, slope: -4.346234095266282e-05, fluctuations: 0.0\n",
      "step: 74890 loss: 0.524653 time elapsed: 99.5676 learning rate: 0.001940, scenario: 1, slope: -4.752536768252066e-05, fluctuations: 0.0\n",
      "step: 74900 loss: 1.812414 time elapsed: 99.5804 learning rate: 0.002090, scenario: -1, slope: 6.847060773981809e-05, fluctuations: 0.0\n",
      "step: 74910 loss: 186.137299 time elapsed: 99.5949 learning rate: 0.001890, scenario: -1, slope: 1.594071940296025, fluctuations: 0.03\n",
      "step: 74920 loss: 236.194085 time elapsed: 99.6089 learning rate: 0.001710, scenario: -1, slope: 1.9360266724066284, fluctuations: 0.07\n",
      "step: 74930 loss: 48.513963 time elapsed: 99.6230 learning rate: 0.001546, scenario: -1, slope: 1.7557197772920368, fluctuations: 0.12\n",
      "step: 74940 loss: 29.732693 time elapsed: 99.6370 learning rate: 0.001398, scenario: -1, slope: 1.312337299068634, fluctuations: 0.15\n",
      "step: 74950 loss: 3.368407 time elapsed: 99.6512 learning rate: 0.001265, scenario: -1, slope: 0.822090399485204, fluctuations: 0.19\n",
      "step: 74960 loss: 3.537832 time elapsed: 99.6653 learning rate: 0.001144, scenario: -1, slope: 0.2501419989711005, fluctuations: 0.22\n",
      "step: 74970 loss: 1.593198 time elapsed: 99.6796 learning rate: 0.001099, scenario: 0, slope: -0.3016457975510532, fluctuations: 0.26\n",
      "step: 74980 loss: 1.012524 time elapsed: 99.6938 learning rate: 0.001099, scenario: 0, slope: -0.9331978838264878, fluctuations: 0.29\n",
      "step: 74990 loss: 0.923888 time elapsed: 99.7077 learning rate: 0.001099, scenario: 0, slope: -1.6821943912272266, fluctuations: 0.32\n",
      "step: 75000 loss: 0.874952 time elapsed: 99.7213 learning rate: 0.001099, scenario: 0, slope: -2.6274871758235814, fluctuations: 0.35\n",
      "step: 75010 loss: 0.824303 time elapsed: 99.7350 learning rate: 0.001099, scenario: 0, slope: -1.3428138561015952, fluctuations: 0.35\n",
      "step: 75020 loss: 0.794822 time elapsed: 99.7475 learning rate: 0.001099, scenario: 0, slope: -0.34081418174038447, fluctuations: 0.34\n",
      "step: 75030 loss: 0.777141 time elapsed: 99.7606 learning rate: 0.001099, scenario: 0, slope: -0.11554891986771215, fluctuations: 0.31\n",
      "step: 75040 loss: 0.761719 time elapsed: 99.7737 learning rate: 0.001099, scenario: 0, slope: -0.042048283617352876, fluctuations: 0.27\n",
      "step: 75050 loss: 0.747797 time elapsed: 99.7882 learning rate: 0.001099, scenario: 0, slope: -0.0205896179401245, fluctuations: 0.23\n",
      "step: 75060 loss: 0.735696 time elapsed: 99.8033 learning rate: 0.001099, scenario: 0, slope: -0.006877517613976309, fluctuations: 0.2\n",
      "step: 75070 loss: 0.724939 time elapsed: 99.8204 learning rate: 0.001099, scenario: 0, slope: -0.0037070538677072278, fluctuations: 0.17\n",
      "step: 75080 loss: 0.715226 time elapsed: 99.8361 learning rate: 0.001099, scenario: 0, slope: -0.0023243572778458305, fluctuations: 0.14\n",
      "step: 75090 loss: 0.706443 time elapsed: 99.8514 learning rate: 0.001099, scenario: 0, slope: -0.001743849476359033, fluctuations: 0.1\n",
      "step: 75100 loss: 0.698470 time elapsed: 99.8660 learning rate: 0.001099, scenario: 0, slope: -0.001440656006415998, fluctuations: 0.07\n",
      "step: 75110 loss: 0.691206 time elapsed: 99.8815 learning rate: 0.001099, scenario: 0, slope: -0.0011908903992243246, fluctuations: 0.04\n",
      "step: 75120 loss: 0.684574 time elapsed: 99.8957 learning rate: 0.001099, scenario: 0, slope: -0.0010710136468009299, fluctuations: 0.0\n",
      "step: 75130 loss: 0.678505 time elapsed: 99.9091 learning rate: 0.001099, scenario: 0, slope: -0.0009553376579464495, fluctuations: 0.0\n",
      "step: 75140 loss: 0.672939 time elapsed: 99.9228 learning rate: 0.001099, scenario: 0, slope: -0.0008603610837131723, fluctuations: 0.0\n",
      "step: 75150 loss: 0.667825 time elapsed: 99.9370 learning rate: 0.001099, scenario: 0, slope: -0.0007796464995390947, fluctuations: 0.0\n",
      "step: 75160 loss: 0.663117 time elapsed: 99.9514 learning rate: 0.001099, scenario: 0, slope: -0.0007093669706505594, fluctuations: 0.0\n",
      "step: 75170 loss: 0.658772 time elapsed: 99.9642 learning rate: 0.001099, scenario: 0, slope: -0.0006474020886746279, fluctuations: 0.0\n",
      "step: 75180 loss: 0.654754 time elapsed: 99.9776 learning rate: 0.001099, scenario: 0, slope: -0.0005923894963284944, fluctuations: 0.0\n",
      "step: 75190 loss: 0.651029 time elapsed: 99.9916 learning rate: 0.001099, scenario: 0, slope: -0.0005433160621975606, fluctuations: 0.0\n",
      "step: 75200 loss: 0.647568 time elapsed: 100.0065 learning rate: 0.001099, scenario: 0, slope: -0.0005035877393839214, fluctuations: 0.0\n",
      "step: 75210 loss: 0.644343 time elapsed: 100.0219 learning rate: 0.001099, scenario: 0, slope: -0.00046006294769208915, fluctuations: 0.0\n",
      "step: 75220 loss: 0.641332 time elapsed: 100.0366 learning rate: 0.001099, scenario: 0, slope: -0.00042476014205956517, fluctuations: 0.0\n",
      "step: 75230 loss: 0.638512 time elapsed: 100.0511 learning rate: 0.001099, scenario: 0, slope: -0.0003930647024865418, fluctuations: 0.0\n",
      "step: 75240 loss: 0.635866 time elapsed: 100.0655 learning rate: 0.001099, scenario: 0, slope: -0.0003645921495860114, fluctuations: 0.0\n",
      "step: 75250 loss: 0.633376 time elapsed: 100.0793 learning rate: 0.001099, scenario: 0, slope: -0.00033899978530009, fluctuations: 0.0\n",
      "step: 75260 loss: 0.631028 time elapsed: 100.0935 learning rate: 0.001099, scenario: 0, slope: -0.00031597997753499255, fluctuations: 0.0\n",
      "step: 75270 loss: 0.628808 time elapsed: 100.1118 learning rate: 0.001099, scenario: 0, slope: -0.000295255999546488, fluctuations: 0.0\n",
      "step: 75280 loss: 0.626704 time elapsed: 100.1259 learning rate: 0.001099, scenario: 0, slope: -0.0002765792587816323, fluctuations: 0.0\n",
      "step: 75290 loss: 0.624706 time elapsed: 100.1393 learning rate: 0.001099, scenario: 0, slope: -0.00025972710540582205, fluctuations: 0.0\n",
      "step: 75300 loss: 0.622805 time elapsed: 100.1517 learning rate: 0.001099, scenario: 0, slope: -0.0002459554575378175, fluctuations: 0.0\n",
      "step: 75310 loss: 0.620991 time elapsed: 100.1650 learning rate: 0.001099, scenario: 0, slope: -0.00023072338493305698, fluctuations: 0.0\n",
      "step: 75320 loss: 0.619257 time elapsed: 100.1772 learning rate: 0.001099, scenario: 0, slope: -0.00021823778231009567, fluctuations: 0.0\n",
      "step: 75330 loss: 0.617597 time elapsed: 100.1894 learning rate: 0.001099, scenario: 0, slope: -0.00020690461448792595, fluctuations: 0.0\n",
      "step: 75340 loss: 0.616004 time elapsed: 100.2030 learning rate: 0.001099, scenario: 0, slope: -0.00019660036213503562, fluctuations: 0.0\n",
      "step: 75350 loss: 0.614473 time elapsed: 100.2167 learning rate: 0.001099, scenario: 0, slope: -0.00018721556843993406, fluctuations: 0.0\n",
      "step: 75360 loss: 0.612999 time elapsed: 100.2313 learning rate: 0.001099, scenario: 0, slope: -0.00017865322290445786, fluctuations: 0.0\n",
      "step: 75370 loss: 0.611578 time elapsed: 100.2451 learning rate: 0.001099, scenario: 0, slope: -0.00017082731358742097, fluctuations: 0.0\n",
      "step: 75380 loss: 0.610205 time elapsed: 100.2587 learning rate: 0.001099, scenario: 0, slope: -0.0001636615412887973, fluctuations: 0.0\n",
      "step: 75390 loss: 0.608878 time elapsed: 100.2722 learning rate: 0.001099, scenario: 0, slope: -0.00015708818315378548, fluctuations: 0.0\n",
      "step: 75400 loss: 0.607591 time elapsed: 100.2857 learning rate: 0.001099, scenario: 0, slope: -0.00015162882038741556, fluctuations: 0.0\n",
      "step: 75410 loss: 0.606344 time elapsed: 100.3012 learning rate: 0.001099, scenario: 0, slope: -0.00014548480836737697, fluctuations: 0.0\n",
      "step: 75420 loss: 0.605132 time elapsed: 100.3143 learning rate: 0.001099, scenario: 0, slope: -0.00014035379512104056, fluctuations: 0.0\n",
      "step: 75430 loss: 0.603955 time elapsed: 100.3269 learning rate: 0.001099, scenario: 0, slope: -0.0001356117419551432, fluctuations: 0.0\n",
      "step: 75440 loss: 0.602808 time elapsed: 100.3399 learning rate: 0.001099, scenario: 0, slope: -0.00013122096837404337, fluctuations: 0.0\n",
      "step: 75450 loss: 0.601691 time elapsed: 100.3529 learning rate: 0.001099, scenario: 0, slope: -0.00012714789125923656, fluctuations: 0.0\n",
      "step: 75460 loss: 0.600602 time elapsed: 100.3658 learning rate: 0.001099, scenario: 0, slope: -0.0001233625562824213, fluctuations: 0.0\n",
      "step: 75470 loss: 0.599538 time elapsed: 100.3786 learning rate: 0.001099, scenario: 0, slope: -0.00011983822440035576, fluctuations: 0.0\n",
      "step: 75480 loss: 0.598499 time elapsed: 100.3911 learning rate: 0.001099, scenario: 0, slope: -0.00011655100676080558, fluctuations: 0.0\n",
      "step: 75490 loss: 0.597482 time elapsed: 100.4034 learning rate: 0.001099, scenario: 0, slope: -0.00011347954218667647, fluctuations: 0.0\n",
      "step: 75500 loss: 0.596487 time elapsed: 100.4173 learning rate: 0.001099, scenario: 0, slope: -0.00011088384927639051, fluctuations: 0.0\n",
      "step: 75510 loss: 0.595513 time elapsed: 100.4322 learning rate: 0.001099, scenario: 0, slope: -0.00010790938864786983, fluctuations: 0.0\n",
      "step: 75520 loss: 0.594557 time elapsed: 100.4463 learning rate: 0.001099, scenario: 0, slope: -0.00010537821142042879, fluctuations: 0.0\n",
      "step: 75530 loss: 0.593620 time elapsed: 100.4604 learning rate: 0.001099, scenario: 0, slope: -0.00010299739048897833, fluctuations: 0.0\n",
      "step: 75540 loss: 0.592701 time elapsed: 100.4745 learning rate: 0.001099, scenario: 0, slope: -0.00010075453159559403, fluctuations: 0.0\n",
      "step: 75550 loss: 0.591798 time elapsed: 100.4881 learning rate: 0.001099, scenario: 0, slope: -9.863848154904063e-05, fluctuations: 0.0\n",
      "step: 75560 loss: 0.590910 time elapsed: 100.5020 learning rate: 0.001099, scenario: 0, slope: -9.663919127434529e-05, fluctuations: 0.0\n",
      "step: 75570 loss: 0.590037 time elapsed: 100.5164 learning rate: 0.001099, scenario: 0, slope: -9.474759450659682e-05, fluctuations: 0.0\n",
      "step: 75580 loss: 0.589179 time elapsed: 100.5292 learning rate: 0.001099, scenario: 0, slope: -9.295550033266974e-05, fluctuations: 0.0\n",
      "step: 75590 loss: 0.588334 time elapsed: 100.5417 learning rate: 0.001099, scenario: 0, slope: -9.125549799280137e-05, fluctuations: 0.0\n",
      "step: 75600 loss: 0.587502 time elapsed: 100.5541 learning rate: 0.001099, scenario: 0, slope: -8.97986713565547e-05, fluctuations: 0.0\n",
      "step: 75610 loss: 0.586682 time elapsed: 100.5676 learning rate: 0.001099, scenario: 0, slope: -8.810553013636091e-05, fluctuations: 0.0\n",
      "step: 75620 loss: 0.585874 time elapsed: 100.5801 learning rate: 0.001099, scenario: 0, slope: -8.664393183255567e-05, fluctuations: 0.0\n",
      "step: 75630 loss: 0.585078 time elapsed: 100.5926 learning rate: 0.001099, scenario: 0, slope: -8.525103495156192e-05, fluctuations: 0.0\n",
      "step: 75640 loss: 0.584292 time elapsed: 100.6060 learning rate: 0.001099, scenario: 0, slope: -8.392224111856674e-05, fluctuations: 0.0\n",
      "step: 75650 loss: 0.583517 time elapsed: 100.6192 learning rate: 0.001099, scenario: 0, slope: -8.265335022656294e-05, fluctuations: 0.0\n",
      "step: 75660 loss: 0.582751 time elapsed: 100.6336 learning rate: 0.001099, scenario: 0, slope: -8.14405196370837e-05, fluctuations: 0.0\n",
      "step: 75670 loss: 0.581995 time elapsed: 100.6477 learning rate: 0.001099, scenario: 0, slope: -8.028022801153315e-05, fluctuations: 0.0\n",
      "step: 75680 loss: 0.581249 time elapsed: 100.6617 learning rate: 0.001099, scenario: 0, slope: -7.916924323632655e-05, fluctuations: 0.0\n",
      "step: 75690 loss: 0.580511 time elapsed: 100.6754 learning rate: 0.001099, scenario: 0, slope: -7.810459396105206e-05, fluctuations: 0.0\n",
      "step: 75700 loss: 0.579782 time elapsed: 100.6889 learning rate: 0.001099, scenario: 0, slope: -7.718376078161178e-05, fluctuations: 0.0\n",
      "step: 75710 loss: 0.579061 time elapsed: 100.7031 learning rate: 0.001099, scenario: 0, slope: -7.610357150350221e-05, fluctuations: 0.0\n",
      "step: 75720 loss: 0.578347 time elapsed: 100.7171 learning rate: 0.001099, scenario: 0, slope: -7.51623457139801e-05, fluctuations: 0.0\n",
      "step: 75730 loss: 0.577642 time elapsed: 100.7314 learning rate: 0.001099, scenario: 0, slope: -7.425771242936849e-05, fluctuations: 0.0\n",
      "step: 75740 loss: 0.576944 time elapsed: 100.7441 learning rate: 0.001099, scenario: 0, slope: -7.338767649576839e-05, fluctuations: 0.0\n",
      "step: 75750 loss: 0.576252 time elapsed: 100.7568 learning rate: 0.001099, scenario: 0, slope: -7.255038794946972e-05, fluctuations: 0.0\n",
      "step: 75760 loss: 0.575568 time elapsed: 100.7698 learning rate: 0.001099, scenario: 0, slope: -7.17441293219899e-05, fluctuations: 0.0\n",
      "step: 75770 loss: 0.574890 time elapsed: 100.7823 learning rate: 0.001099, scenario: 0, slope: -7.096730425700876e-05, fluctuations: 0.0\n",
      "step: 75780 loss: 0.574219 time elapsed: 100.7950 learning rate: 0.001099, scenario: 0, slope: -7.021842728311363e-05, fluctuations: 0.0\n",
      "step: 75790 loss: 0.573554 time elapsed: 100.8077 learning rate: 0.001099, scenario: 0, slope: -6.949611460469451e-05, fluctuations: 0.0\n",
      "step: 75800 loss: 0.572894 time elapsed: 100.8203 learning rate: 0.001099, scenario: 0, slope: -6.886767732065071e-05, fluctuations: 0.0\n",
      "step: 75810 loss: 0.572241 time elapsed: 100.8362 learning rate: 0.001099, scenario: 0, slope: -6.812610626631681e-05, fluctuations: 0.0\n",
      "step: 75820 loss: 0.571593 time elapsed: 100.8506 learning rate: 0.001099, scenario: 0, slope: -6.747608048787525e-05, fluctuations: 0.0\n",
      "step: 75830 loss: 0.570950 time elapsed: 100.8645 learning rate: 0.001099, scenario: 0, slope: -6.68479457676111e-05, fluctuations: 0.0\n",
      "step: 75840 loss: 0.570313 time elapsed: 100.8785 learning rate: 0.001099, scenario: 0, slope: -6.624071663359707e-05, fluctuations: 0.0\n",
      "step: 75850 loss: 0.569681 time elapsed: 100.8922 learning rate: 0.001099, scenario: 0, slope: -6.56534696916987e-05, fluctuations: 0.0\n",
      "step: 75860 loss: 0.569054 time elapsed: 100.9061 learning rate: 0.001099, scenario: 0, slope: -6.508533892677326e-05, fluctuations: 0.0\n",
      "step: 75870 loss: 0.568431 time elapsed: 100.9200 learning rate: 0.001099, scenario: 0, slope: -6.453551140029344e-05, fluctuations: 0.0\n",
      "step: 75880 loss: 0.567813 time elapsed: 100.9342 learning rate: 0.001099, scenario: 0, slope: -6.400322330263425e-05, fluctuations: 0.0\n",
      "step: 75890 loss: 0.567200 time elapsed: 100.9466 learning rate: 0.001099, scenario: 0, slope: -6.348775632595217e-05, fluctuations: 0.0\n",
      "step: 75900 loss: 0.566591 time elapsed: 100.9604 learning rate: 0.001099, scenario: 0, slope: -6.303765845150693e-05, fluctuations: 0.0\n",
      "step: 75910 loss: 0.565986 time elapsed: 100.9743 learning rate: 0.001099, scenario: 0, slope: -6.250462024926742e-05, fluctuations: 0.0\n",
      "step: 75920 loss: 0.565385 time elapsed: 100.9887 learning rate: 0.001099, scenario: 0, slope: -6.203571329317641e-05, fluctuations: 0.0\n",
      "step: 75930 loss: 0.564788 time elapsed: 101.0026 learning rate: 0.001099, scenario: 0, slope: -6.158114629213733e-05, fluctuations: 0.0\n",
      "step: 75940 loss: 0.564195 time elapsed: 101.0169 learning rate: 0.001099, scenario: 0, slope: -6.11403832971618e-05, fluctuations: 0.0\n",
      "step: 75950 loss: 0.563605 time elapsed: 101.0306 learning rate: 0.001099, scenario: 0, slope: -6.071291734162742e-05, fluctuations: 0.0\n",
      "step: 75960 loss: 0.563020 time elapsed: 101.0441 learning rate: 0.001099, scenario: 0, slope: -6.029826837464369e-05, fluctuations: 0.0\n",
      "step: 75970 loss: 0.562438 time elapsed: 101.0584 learning rate: 0.001099, scenario: 0, slope: -5.989598135050579e-05, fluctuations: 0.0\n",
      "step: 75980 loss: 0.561859 time elapsed: 101.0750 learning rate: 0.001099, scenario: 0, slope: -5.950562446168543e-05, fluctuations: 0.0\n",
      "step: 75990 loss: 0.561283 time elapsed: 101.0912 learning rate: 0.001099, scenario: 0, slope: -5.9126787505148056e-05, fluctuations: 0.0\n",
      "step: 76000 loss: 0.560711 time elapsed: 101.1070 learning rate: 0.001099, scenario: 0, slope: -5.879536096918807e-05, fluctuations: 0.0\n",
      "step: 76010 loss: 0.560142 time elapsed: 101.1232 learning rate: 0.001099, scenario: 0, slope: -5.84021316539432e-05, fluctuations: 0.0\n",
      "step: 76020 loss: 0.559576 time elapsed: 101.1384 learning rate: 0.001099, scenario: 0, slope: -5.8055587352095473e-05, fluctuations: 0.0\n",
      "step: 76030 loss: 0.559013 time elapsed: 101.1540 learning rate: 0.001099, scenario: 0, slope: -5.771910969120709e-05, fluctuations: 0.0\n",
      "step: 76040 loss: 0.558452 time elapsed: 101.1727 learning rate: 0.001099, scenario: 0, slope: -5.739237602230726e-05, fluctuations: 0.0\n",
      "step: 76050 loss: 0.557895 time elapsed: 101.1868 learning rate: 0.001099, scenario: 0, slope: -5.7075077813254116e-05, fluctuations: 0.0\n",
      "step: 76060 loss: 0.557340 time elapsed: 101.2000 learning rate: 0.001099, scenario: 0, slope: -5.67669197197219e-05, fluctuations: 0.0\n",
      "step: 76070 loss: 0.556788 time elapsed: 101.2128 learning rate: 0.001099, scenario: 0, slope: -5.64676187307515e-05, fluctuations: 0.0\n",
      "step: 76080 loss: 0.556238 time elapsed: 101.2259 learning rate: 0.001110, scenario: 1, slope: -5.617690338391973e-05, fluctuations: 0.0\n",
      "step: 76090 loss: 0.555666 time elapsed: 101.2396 learning rate: 0.001226, scenario: 1, slope: -5.594694361917835e-05, fluctuations: 0.0\n",
      "step: 76100 loss: 0.555037 time elapsed: 101.2531 learning rate: 0.001341, scenario: 1, slope: -5.599923778079201e-05, fluctuations: 0.0\n",
      "step: 76110 loss: 0.554376 time elapsed: 101.2679 learning rate: 0.001341, scenario: 0, slope: -5.662652271968993e-05, fluctuations: 0.0\n",
      "step: 76120 loss: 0.553720 time elapsed: 101.2823 learning rate: 0.001341, scenario: 0, slope: -5.765373592990795e-05, fluctuations: 0.0\n",
      "step: 76130 loss: 0.553067 time elapsed: 101.2962 learning rate: 0.001341, scenario: 0, slope: -5.896121904258064e-05, fluctuations: 0.0\n",
      "step: 76140 loss: 0.552419 time elapsed: 101.3100 learning rate: 0.001341, scenario: 0, slope: -6.040123768898705e-05, fluctuations: 0.0\n",
      "step: 76150 loss: 0.551774 time elapsed: 101.3235 learning rate: 0.001341, scenario: 0, slope: -6.18283323240407e-05, fluctuations: 0.0\n",
      "step: 76160 loss: 0.551132 time elapsed: 101.3371 learning rate: 0.001341, scenario: 0, slope: -6.309930470411207e-05, fluctuations: 0.0\n",
      "step: 76170 loss: 0.550494 time elapsed: 101.3514 learning rate: 0.001341, scenario: 0, slope: -6.407315768476844e-05, fluctuations: 0.0\n",
      "step: 76180 loss: 0.549860 time elapsed: 101.3646 learning rate: 0.001341, scenario: 0, slope: -6.461102122258861e-05, fluctuations: 0.0\n",
      "step: 76190 loss: 0.549228 time elapsed: 101.3782 learning rate: 0.001341, scenario: 0, slope: -6.463400190075743e-05, fluctuations: 0.0\n",
      "step: 76200 loss: 0.548600 time elapsed: 101.3906 learning rate: 0.001341, scenario: 0, slope: -6.436859796186902e-05, fluctuations: 0.0\n",
      "step: 76210 loss: 0.547975 time elapsed: 101.4040 learning rate: 0.001341, scenario: 0, slope: -6.398107601987087e-05, fluctuations: 0.0\n",
      "step: 76220 loss: 0.547352 time elapsed: 101.4168 learning rate: 0.001341, scenario: 0, slope: -6.363997536558437e-05, fluctuations: 0.0\n",
      "step: 76230 loss: 0.546733 time elapsed: 101.4292 learning rate: 0.001341, scenario: 0, slope: -6.33094978614341e-05, fluctuations: 0.0\n",
      "step: 76240 loss: 0.546116 time elapsed: 101.4419 learning rate: 0.001341, scenario: 0, slope: -6.298937946905076e-05, fluctuations: 0.0\n",
      "step: 76250 loss: 0.545503 time elapsed: 101.4546 learning rate: 0.001341, scenario: 0, slope: -6.26792907175978e-05, fluctuations: 0.0\n",
      "step: 76260 loss: 0.544891 time elapsed: 101.4697 learning rate: 0.001341, scenario: 0, slope: -6.237888922227828e-05, fluctuations: 0.0\n",
      "step: 76270 loss: 0.544282 time elapsed: 101.4845 learning rate: 0.001341, scenario: 0, slope: -6.208783690803451e-05, fluctuations: 0.0\n",
      "step: 76280 loss: 0.543677 time elapsed: 101.4994 learning rate: 0.001341, scenario: 0, slope: -6.180503721326907e-05, fluctuations: 0.0\n",
      "step: 76290 loss: 0.544330 time elapsed: 101.5135 learning rate: 0.001341, scenario: 0, slope: -6.016460167910327e-05, fluctuations: 0.0\n",
      "step: 76300 loss: 5.553691 time elapsed: 101.5276 learning rate: 0.001307, scenario: -1, slope: 0.00233626253895471, fluctuations: 0.0\n",
      "step: 76310 loss: 7.036744 time elapsed: 101.5426 learning rate: 0.001182, scenario: -1, slope: 0.05358690559473609, fluctuations: 0.02\n",
      "step: 76320 loss: 3.036533 time elapsed: 101.5569 learning rate: 0.001069, scenario: -1, slope: 0.05175362074771758, fluctuations: 0.04\n",
      "step: 76330 loss: 0.553328 time elapsed: 101.5713 learning rate: 0.000967, scenario: -1, slope: 0.043049850359614276, fluctuations: 0.08\n",
      "step: 76340 loss: 0.542803 time elapsed: 101.5847 learning rate: 0.000874, scenario: -1, slope: 0.031591394829817286, fluctuations: 0.12\n",
      "step: 76350 loss: 0.596074 time elapsed: 101.5974 learning rate: 0.000791, scenario: -1, slope: 0.017325996477841242, fluctuations: 0.16\n",
      "step: 76360 loss: 0.542022 time elapsed: 101.6108 learning rate: 0.000715, scenario: -1, slope: 0.0024918453701722474, fluctuations: 0.21\n",
      "step: 76370 loss: 0.541093 time elapsed: 101.6234 learning rate: 0.000708, scenario: 0, slope: -0.014665572973404067, fluctuations: 0.26\n",
      "step: 76380 loss: 0.540910 time elapsed: 101.6359 learning rate: 0.000708, scenario: 0, slope: -0.03408667498548735, fluctuations: 0.31\n",
      "step: 76390 loss: 0.540636 time elapsed: 101.6482 learning rate: 0.000708, scenario: 0, slope: -0.056843671691797854, fluctuations: 0.36\n",
      "step: 76400 loss: 0.540248 time elapsed: 101.6603 learning rate: 0.000708, scenario: 0, slope: -0.07619296894551396, fluctuations: 0.39\n",
      "step: 76410 loss: 0.539836 time elapsed: 101.6739 learning rate: 0.000708, scenario: 0, slope: -0.010678031187165975, fluctuations: 0.41\n",
      "step: 76420 loss: 0.539451 time elapsed: 101.6881 learning rate: 0.000708, scenario: 0, slope: -0.0028220301717086145, fluctuations: 0.43\n",
      "step: 76430 loss: 0.539101 time elapsed: 101.7022 learning rate: 0.000708, scenario: 0, slope: -0.0012428764690778065, fluctuations: 0.4\n",
      "step: 76440 loss: 0.538774 time elapsed: 101.7161 learning rate: 0.000708, scenario: 0, slope: -0.0003892225212705722, fluctuations: 0.36\n",
      "step: 76450 loss: 0.538456 time elapsed: 101.7303 learning rate: 0.000708, scenario: 0, slope: -0.00010039197336892517, fluctuations: 0.32\n",
      "step: 76460 loss: 0.538144 time elapsed: 101.7448 learning rate: 0.000673, scenario: -1, slope: -5.239250306843342e-05, fluctuations: 0.27\n",
      "step: 76470 loss: 0.537854 time elapsed: 101.7595 learning rate: 0.000609, scenario: -1, slope: -4.0310311412852094e-05, fluctuations: 0.22\n",
      "step: 76480 loss: 0.537591 time elapsed: 101.7739 learning rate: 0.000551, scenario: -1, slope: -3.5438422270213115e-05, fluctuations: 0.17\n",
      "step: 76490 loss: 0.537355 time elapsed: 101.7882 learning rate: 0.000498, scenario: -1, slope: -3.251532034469682e-05, fluctuations: 0.12\n",
      "step: 76500 loss: 0.537140 time elapsed: 101.8012 learning rate: 0.000455, scenario: -1, slope: -3.0161407411922576e-05, fluctuations: 0.1\n",
      "step: 76510 loss: 0.536944 time elapsed: 101.8148 learning rate: 0.000411, scenario: -1, slope: -2.8331269387047226e-05, fluctuations: 0.05\n",
      "step: 76520 loss: 0.536767 time elapsed: 101.8280 learning rate: 0.000385, scenario: 1, slope: -2.6917518424627663e-05, fluctuations: 0.0\n",
      "step: 76530 loss: 0.536588 time elapsed: 101.8409 learning rate: 0.000426, scenario: 1, slope: -2.5029669679529797e-05, fluctuations: 0.0\n",
      "step: 76540 loss: 0.536390 time elapsed: 101.8540 learning rate: 0.000470, scenario: 1, slope: -2.3224814519397347e-05, fluctuations: 0.0\n",
      "step: 76550 loss: 0.536171 time elapsed: 101.8679 learning rate: 0.000520, scenario: 1, slope: -2.1737774664728576e-05, fluctuations: 0.0\n",
      "step: 76560 loss: 0.535929 time elapsed: 101.8818 learning rate: 0.000574, scenario: 1, slope: -2.0798906687328234e-05, fluctuations: 0.0\n",
      "step: 76570 loss: 0.535662 time elapsed: 101.8960 learning rate: 0.000634, scenario: 1, slope: -2.0547940460469407e-05, fluctuations: 0.0\n",
      "step: 76580 loss: 0.535366 time elapsed: 101.9106 learning rate: 0.000700, scenario: 1, slope: -2.100543396698537e-05, fluctuations: 0.0\n",
      "step: 76590 loss: 0.535039 time elapsed: 101.9250 learning rate: 0.000773, scenario: 1, slope: -2.214740547272103e-05, fluctuations: 0.0\n",
      "step: 76600 loss: 0.534678 time elapsed: 101.9387 learning rate: 0.000846, scenario: 1, slope: -2.3713977894927142e-05, fluctuations: 0.0\n",
      "step: 76610 loss: 0.534282 time elapsed: 101.9532 learning rate: 0.000934, scenario: 1, slope: -2.6218320955083703e-05, fluctuations: 0.0\n",
      "step: 76620 loss: 0.533847 time elapsed: 101.9675 learning rate: 0.001032, scenario: 1, slope: -2.892244252415878e-05, fluctuations: 0.0\n",
      "step: 76630 loss: 0.533366 time elapsed: 101.9813 learning rate: 0.001140, scenario: 1, slope: -3.191365730228152e-05, fluctuations: 0.0\n",
      "step: 76640 loss: 0.532836 time elapsed: 101.9952 learning rate: 0.001259, scenario: 1, slope: -3.5195444820592914e-05, fluctuations: 0.0\n",
      "step: 76650 loss: 0.532252 time elapsed: 102.0088 learning rate: 0.001391, scenario: 1, slope: -3.879905546982549e-05, fluctuations: 0.0\n",
      "step: 76660 loss: 0.531626 time elapsed: 102.0222 learning rate: 0.001537, scenario: 1, slope: -4.274582306578111e-05, fluctuations: 0.0\n",
      "step: 76670 loss: 1196.162982 time elapsed: 102.0354 learning rate: 0.001513, scenario: -1, slope: 1.2723290317057607, fluctuations: 0.0\n",
      "step: 76680 loss: 315.367801 time elapsed: 102.0488 learning rate: 0.001369, scenario: -1, slope: 1.0804169840868634, fluctuations: 0.05\n",
      "step: 76690 loss: 36.417704 time elapsed: 102.0622 learning rate: 0.001238, scenario: -1, slope: 0.9435102194184625, fluctuations: 0.1\n",
      "step: 76700 loss: 19.298162 time elapsed: 102.0753 learning rate: 0.001131, scenario: -1, slope: 0.8112315045862486, fluctuations: 0.13\n",
      "step: 76710 loss: 11.108346 time elapsed: 102.0906 learning rate: 0.001023, scenario: -1, slope: 0.5481468377688152, fluctuations: 0.17\n",
      "step: 76720 loss: 1.019029 time elapsed: 102.1056 learning rate: 0.000925, scenario: -1, slope: 0.28484268485742775, fluctuations: 0.21\n",
      "step: 76730 loss: 1.480893 time elapsed: 102.1203 learning rate: 0.000845, scenario: 0, slope: -0.01282674783935813, fluctuations: 0.24\n",
      "step: 76740 loss: 1.138006 time elapsed: 102.1346 learning rate: 0.000845, scenario: 0, slope: -0.3176130709283459, fluctuations: 0.27\n",
      "step: 76750 loss: 0.775157 time elapsed: 102.1489 learning rate: 0.000845, scenario: 0, slope: -0.6622959039461596, fluctuations: 0.31\n",
      "step: 76760 loss: 0.693570 time elapsed: 102.1631 learning rate: 0.000845, scenario: 0, slope: -1.1111399711287964, fluctuations: 0.34\n",
      "step: 76770 loss: 0.686682 time elapsed: 102.1774 learning rate: 0.000845, scenario: 0, slope: -0.6950295591240744, fluctuations: 0.36\n",
      "step: 76780 loss: 0.668024 time elapsed: 102.1923 learning rate: 0.000845, scenario: 0, slope: -0.2783992072364865, fluctuations: 0.34\n",
      "step: 76790 loss: 0.652902 time elapsed: 102.2073 learning rate: 0.000845, scenario: 0, slope: -0.10788255461226745, fluctuations: 0.34\n",
      "step: 76800 loss: 0.644705 time elapsed: 102.2212 learning rate: 0.000845, scenario: 0, slope: -0.049768372060030125, fluctuations: 0.31\n",
      "step: 76810 loss: 0.638029 time elapsed: 102.2356 learning rate: 0.000845, scenario: 0, slope: -0.015308591344294808, fluctuations: 0.27\n",
      "step: 76820 loss: 0.632115 time elapsed: 102.2489 learning rate: 0.000845, scenario: 0, slope: -0.006728765274562955, fluctuations: 0.24\n",
      "step: 76830 loss: 0.627177 time elapsed: 102.2620 learning rate: 0.000845, scenario: 0, slope: -0.0031992121453163266, fluctuations: 0.2\n",
      "step: 76840 loss: 0.622872 time elapsed: 102.2752 learning rate: 0.000845, scenario: 0, slope: -0.0015177151482317066, fluctuations: 0.17\n",
      "step: 76850 loss: 0.619038 time elapsed: 102.2888 learning rate: 0.000845, scenario: 0, slope: -0.0009989243066991958, fluctuations: 0.14\n",
      "step: 76860 loss: 0.615609 time elapsed: 102.3033 learning rate: 0.000845, scenario: 0, slope: -0.0007763689076899894, fluctuations: 0.1\n",
      "step: 76870 loss: 0.612509 time elapsed: 102.3183 learning rate: 0.000845, scenario: 0, slope: -0.0005943849609420909, fluctuations: 0.07\n",
      "step: 76880 loss: 0.609685 time elapsed: 102.3328 learning rate: 0.000845, scenario: 0, slope: -0.0004926675346926323, fluctuations: 0.04\n",
      "step: 76890 loss: 0.607096 time elapsed: 102.3472 learning rate: 0.000845, scenario: 0, slope: -0.00043140837415462806, fluctuations: 0.01\n",
      "step: 76900 loss: 0.604709 time elapsed: 102.3615 learning rate: 0.000845, scenario: 0, slope: -0.00038643254869404005, fluctuations: 0.0\n",
      "step: 76910 loss: 0.602498 time elapsed: 102.3764 learning rate: 0.000845, scenario: 0, slope: -0.00033999534058241183, fluctuations: 0.0\n",
      "step: 76920 loss: 0.600441 time elapsed: 102.3907 learning rate: 0.000845, scenario: 0, slope: -0.0003063623797149795, fluctuations: 0.0\n",
      "step: 76930 loss: 0.598518 time elapsed: 102.4053 learning rate: 0.000845, scenario: 0, slope: -0.00027848057253452733, fluctuations: 0.0\n",
      "step: 76940 loss: 0.596715 time elapsed: 102.4192 learning rate: 0.000845, scenario: 0, slope: -0.00025495658200453997, fluctuations: 0.0\n",
      "step: 76950 loss: 0.595018 time elapsed: 102.4326 learning rate: 0.000845, scenario: 0, slope: -0.00023485874073912196, fluctuations: 0.0\n",
      "step: 76960 loss: 0.593417 time elapsed: 102.4455 learning rate: 0.000845, scenario: 0, slope: -0.00021749667126708035, fluctuations: 0.0\n",
      "step: 76970 loss: 0.591901 time elapsed: 102.4583 learning rate: 0.000845, scenario: 0, slope: -0.00020236418869774774, fluctuations: 0.0\n",
      "step: 76980 loss: 0.590463 time elapsed: 102.4722 learning rate: 0.000845, scenario: 0, slope: -0.0001890746787574197, fluctuations: 0.0\n",
      "step: 76990 loss: 0.589095 time elapsed: 102.4856 learning rate: 0.000845, scenario: 0, slope: -0.0001773252744344472, fluctuations: 0.0\n",
      "step: 77000 loss: 0.587791 time elapsed: 102.4983 learning rate: 0.000845, scenario: 0, slope: -0.00016786818626045075, fluctuations: 0.0\n",
      "step: 77010 loss: 0.586545 time elapsed: 102.5126 learning rate: 0.000845, scenario: 0, slope: -0.0001575332346701143, fluctuations: 0.0\n",
      "step: 77020 loss: 0.585352 time elapsed: 102.5273 learning rate: 0.000845, scenario: 0, slope: -0.00014913958809460953, fluctuations: 0.0\n",
      "step: 77030 loss: 0.584208 time elapsed: 102.5418 learning rate: 0.000845, scenario: 0, slope: -0.00014156529627012794, fluctuations: 0.0\n",
      "step: 77040 loss: 0.583109 time elapsed: 102.5561 learning rate: 0.000845, scenario: 0, slope: -0.00013470268070140984, fluctuations: 0.0\n",
      "step: 77050 loss: 0.582052 time elapsed: 102.5706 learning rate: 0.000845, scenario: 0, slope: -0.00012846166728786915, fluctuations: 0.0\n",
      "step: 77060 loss: 0.581034 time elapsed: 102.5849 learning rate: 0.000845, scenario: 0, slope: -0.0001227663908899639, fluctuations: 0.0\n",
      "step: 77070 loss: 0.580051 time elapsed: 102.5991 learning rate: 0.000845, scenario: 0, slope: -0.00011755255974224815, fluctuations: 0.0\n",
      "step: 77080 loss: 0.579101 time elapsed: 102.6137 learning rate: 0.000845, scenario: 0, slope: -0.00011276537993053481, fluctuations: 0.0\n",
      "step: 77090 loss: 0.578182 time elapsed: 102.6270 learning rate: 0.000845, scenario: 0, slope: -0.00010835790493212028, fluctuations: 0.0\n",
      "step: 77100 loss: 0.577292 time elapsed: 102.6400 learning rate: 0.000845, scenario: 0, slope: -0.00010468229812031108, fluctuations: 0.0\n",
      "step: 77110 loss: 0.576429 time elapsed: 102.6536 learning rate: 0.000845, scenario: 0, slope: -0.00010052582736680062, fluctuations: 0.0\n",
      "step: 77120 loss: 0.575590 time elapsed: 102.6664 learning rate: 0.000845, scenario: 0, slope: -9.703586209024497e-05, fluctuations: 0.0\n",
      "step: 77130 loss: 0.574776 time elapsed: 102.6788 learning rate: 0.000845, scenario: 0, slope: -9.37932903781981e-05, fluctuations: 0.0\n",
      "step: 77140 loss: 0.573983 time elapsed: 102.6911 learning rate: 0.000845, scenario: 0, slope: -9.077486560520966e-05, fluctuations: 0.0\n",
      "step: 77150 loss: 0.573212 time elapsed: 102.7036 learning rate: 0.000845, scenario: 0, slope: -8.796013322619409e-05, fluctuations: 0.0\n",
      "step: 77160 loss: 0.572460 time elapsed: 102.7165 learning rate: 0.000845, scenario: 0, slope: -8.533102662864518e-05, fluctuations: 0.0\n",
      "step: 77170 loss: 0.571726 time elapsed: 102.7304 learning rate: 0.000845, scenario: 0, slope: -8.287152985202086e-05, fluctuations: 0.0\n",
      "step: 77180 loss: 0.571009 time elapsed: 102.7441 learning rate: 0.000845, scenario: 0, slope: -8.056739498029377e-05, fluctuations: 0.0\n",
      "step: 77190 loss: 0.570309 time elapsed: 102.7579 learning rate: 0.000845, scenario: 0, slope: -7.840590443130636e-05, fluctuations: 0.0\n",
      "step: 77200 loss: 0.569624 time elapsed: 102.7715 learning rate: 0.000845, scenario: 0, slope: -7.657308704474762e-05, fluctuations: 0.0\n",
      "step: 77210 loss: 0.568954 time elapsed: 102.7857 learning rate: 0.000845, scenario: 0, slope: -7.446646410617012e-05, fluctuations: 0.0\n",
      "step: 77220 loss: 0.568298 time elapsed: 102.7993 learning rate: 0.000845, scenario: 0, slope: -7.266907254846172e-05, fluctuations: 0.0\n",
      "step: 77230 loss: 0.567654 time elapsed: 102.8128 learning rate: 0.000845, scenario: 0, slope: -7.097517369549384e-05, fluctuations: 0.0\n",
      "step: 77240 loss: 0.567023 time elapsed: 102.8267 learning rate: 0.000845, scenario: 0, slope: -6.937723146019354e-05, fluctuations: 0.0\n",
      "step: 77250 loss: 0.566404 time elapsed: 102.8397 learning rate: 0.000845, scenario: 0, slope: -6.786840469908347e-05, fluctuations: 0.0\n",
      "step: 77260 loss: 0.565796 time elapsed: 102.8522 learning rate: 0.000845, scenario: 0, slope: -6.644246884028925e-05, fluctuations: 0.0\n",
      "step: 77270 loss: 0.565198 time elapsed: 102.8644 learning rate: 0.000845, scenario: 0, slope: -6.509374804482573e-05, fluctuations: 0.0\n",
      "step: 77280 loss: 0.564610 time elapsed: 102.8767 learning rate: 0.000845, scenario: 0, slope: -6.381705628001265e-05, fluctuations: 0.0\n",
      "step: 77290 loss: 0.564032 time elapsed: 102.8888 learning rate: 0.000845, scenario: 0, slope: -6.260764595345517e-05, fluctuations: 0.0\n",
      "step: 77300 loss: 0.563463 time elapsed: 102.9012 learning rate: 0.000845, scenario: 0, slope: -6.157309615910637e-05, fluctuations: 0.0\n",
      "step: 77310 loss: 0.562902 time elapsed: 102.9136 learning rate: 0.000845, scenario: 0, slope: -6.037360735652632e-05, fluctuations: 0.0\n",
      "step: 77320 loss: 0.562350 time elapsed: 102.9270 learning rate: 0.000845, scenario: 0, slope: -5.934129840720775e-05, fluctuations: 0.0\n",
      "step: 77330 loss: 0.561805 time elapsed: 102.9410 learning rate: 0.000845, scenario: 0, slope: -5.8360844120825874e-05, fluctuations: 0.0\n",
      "step: 77340 loss: 0.561268 time elapsed: 102.9546 learning rate: 0.000845, scenario: 0, slope: -5.742911394000234e-05, fluctuations: 0.0\n",
      "step: 77350 loss: 0.560738 time elapsed: 102.9682 learning rate: 0.000862, scenario: 1, slope: -5.654321458322433e-05, fluctuations: 0.0\n",
      "step: 77360 loss: 0.560185 time elapsed: 102.9818 learning rate: 0.000952, scenario: 1, slope: -5.576705804231043e-05, fluctuations: 0.0\n",
      "step: 77370 loss: 0.559583 time elapsed: 102.9957 learning rate: 0.001052, scenario: 1, slope: -5.534701288568423e-05, fluctuations: 0.0\n",
      "step: 77380 loss: 0.558927 time elapsed: 103.0094 learning rate: 0.001162, scenario: 1, slope: -5.552122327106281e-05, fluctuations: 0.0\n",
      "step: 77390 loss: 0.558215 time elapsed: 103.0233 learning rate: 0.001270, scenario: 0, slope: -5.6476982104124085e-05, fluctuations: 0.0\n",
      "step: 77400 loss: 0.557475 time elapsed: 103.0365 learning rate: 0.001270, scenario: 0, slope: -5.80648609759595e-05, fluctuations: 0.0\n",
      "step: 77410 loss: 0.556749 time elapsed: 103.0496 learning rate: 0.001270, scenario: 0, slope: -6.0661012215122245e-05, fluctuations: 0.0\n",
      "step: 77420 loss: 0.556035 time elapsed: 103.0619 learning rate: 0.001270, scenario: 0, slope: -6.330589814036456e-05, fluctuations: 0.0\n",
      "step: 77430 loss: 0.555332 time elapsed: 103.0743 learning rate: 0.001270, scenario: 0, slope: -6.589834344134607e-05, fluctuations: 0.0\n",
      "step: 77440 loss: 0.554641 time elapsed: 103.0863 learning rate: 0.001270, scenario: 0, slope: -6.813652763238987e-05, fluctuations: 0.0\n",
      "step: 77450 loss: 0.553959 time elapsed: 103.0988 learning rate: 0.001270, scenario: 0, slope: -6.972901656833515e-05, fluctuations: 0.0\n",
      "step: 77460 loss: 0.553287 time elapsed: 103.1118 learning rate: 0.001270, scenario: 0, slope: -7.046831637253347e-05, fluctuations: 0.0\n",
      "step: 77470 loss: 0.552624 time elapsed: 103.1245 learning rate: 0.001270, scenario: 0, slope: -7.041169936832744e-05, fluctuations: 0.0\n",
      "step: 77480 loss: 0.551969 time elapsed: 103.1382 learning rate: 0.001270, scenario: 0, slope: -6.974758352351877e-05, fluctuations: 0.0\n",
      "step: 77490 loss: 0.551323 time elapsed: 103.1520 learning rate: 0.001270, scenario: 0, slope: -6.876371872858817e-05, fluctuations: 0.0\n",
      "step: 77500 loss: 0.550683 time elapsed: 103.1657 learning rate: 0.001270, scenario: 0, slope: -6.786960876908299e-05, fluctuations: 0.0\n",
      "step: 77510 loss: 0.550051 time elapsed: 103.1799 learning rate: 0.001270, scenario: 0, slope: -6.684321147828712e-05, fluctuations: 0.0\n",
      "step: 77520 loss: 0.549425 time elapsed: 103.1934 learning rate: 0.001270, scenario: 0, slope: -6.596986688149293e-05, fluctuations: 0.0\n",
      "step: 77530 loss: 0.548805 time elapsed: 103.2072 learning rate: 0.001270, scenario: 0, slope: -6.514953458672283e-05, fluctuations: 0.0\n",
      "step: 77540 loss: 0.548192 time elapsed: 103.2209 learning rate: 0.001270, scenario: 0, slope: -6.437857125255385e-05, fluctuations: 0.0\n",
      "step: 77550 loss: 0.547583 time elapsed: 103.2341 learning rate: 0.001270, scenario: 0, slope: -6.36535716742613e-05, fluctuations: 0.0\n",
      "step: 77560 loss: 0.546980 time elapsed: 103.2471 learning rate: 0.001270, scenario: 0, slope: -6.297138966325798e-05, fluctuations: 0.0\n",
      "step: 77570 loss: 0.546382 time elapsed: 103.2597 learning rate: 0.001270, scenario: 0, slope: -6.232912777439938e-05, fluctuations: 0.0\n",
      "step: 77580 loss: 0.545788 time elapsed: 103.2719 learning rate: 0.001270, scenario: 0, slope: -6.17241182458172e-05, fluctuations: 0.0\n",
      "step: 77590 loss: 0.545199 time elapsed: 103.2841 learning rate: 0.001270, scenario: 0, slope: -6.11539026933413e-05, fluctuations: 0.0\n",
      "step: 77600 loss: 0.544614 time elapsed: 103.2961 learning rate: 0.001270, scenario: 0, slope: -6.066857926098877e-05, fluctuations: 0.0\n",
      "step: 77610 loss: 0.544033 time elapsed: 103.3085 learning rate: 0.001270, scenario: 0, slope: -6.010895367344914e-05, fluctuations: 0.0\n",
      "step: 77620 loss: 0.543455 time elapsed: 103.3205 learning rate: 0.001270, scenario: 0, slope: -5.96301867597746e-05, fluctuations: 0.0\n",
      "step: 77630 loss: 0.542881 time elapsed: 103.3328 learning rate: 0.001270, scenario: 0, slope: -5.917811741769464e-05, fluctuations: 0.0\n",
      "step: 77640 loss: 0.542310 time elapsed: 103.3461 learning rate: 0.001270, scenario: 0, slope: -5.8751081723583034e-05, fluctuations: 0.0\n",
      "step: 77650 loss: 0.541743 time elapsed: 103.3598 learning rate: 0.001270, scenario: 0, slope: -5.834753547834006e-05, fluctuations: 0.0\n",
      "step: 77660 loss: 0.541178 time elapsed: 103.3732 learning rate: 0.001270, scenario: 0, slope: -5.79660442071005e-05, fluctuations: 0.0\n",
      "step: 77670 loss: 0.540616 time elapsed: 103.3867 learning rate: 0.001270, scenario: 0, slope: -5.760527416269502e-05, fluctuations: 0.0\n",
      "step: 77680 loss: 0.540057 time elapsed: 103.4003 learning rate: 0.001270, scenario: 0, slope: -5.726398421163276e-05, fluctuations: 0.0\n",
      "step: 77690 loss: 0.539500 time elapsed: 103.4135 learning rate: 0.001270, scenario: 0, slope: -5.694101849924501e-05, fluctuations: 0.0\n",
      "step: 77700 loss: 0.538946 time elapsed: 103.4267 learning rate: 0.001270, scenario: 0, slope: -5.666512473170517e-05, fluctuations: 0.0\n",
      "step: 77710 loss: 0.538394 time elapsed: 103.4411 learning rate: 0.001270, scenario: 0, slope: -5.634582349484923e-05, fluctuations: 0.0\n",
      "step: 77720 loss: 0.537844 time elapsed: 103.4553 learning rate: 0.001270, scenario: 0, slope: -5.607165205061486e-05, fluctuations: 0.0\n",
      "step: 77730 loss: 0.537296 time elapsed: 103.4682 learning rate: 0.001270, scenario: 0, slope: -5.58119100371596e-05, fluctuations: 0.0\n",
      "step: 77740 loss: 0.536750 time elapsed: 103.4816 learning rate: 0.001270, scenario: 0, slope: -5.556577953629857e-05, fluctuations: 0.0\n",
      "step: 77750 loss: 0.536206 time elapsed: 103.4942 learning rate: 0.001270, scenario: 0, slope: -5.533249595475704e-05, fluctuations: 0.0\n",
      "step: 77760 loss: 0.535664 time elapsed: 103.5066 learning rate: 0.001270, scenario: 0, slope: -5.511134418190328e-05, fluctuations: 0.0\n",
      "step: 77770 loss: 0.535123 time elapsed: 103.5192 learning rate: 0.001270, scenario: 0, slope: -5.4901655061791494e-05, fluctuations: 0.0\n",
      "step: 77780 loss: 0.534584 time elapsed: 103.5318 learning rate: 0.001270, scenario: 0, slope: -5.4702802147381605e-05, fluctuations: 0.0\n",
      "step: 77790 loss: 0.534047 time elapsed: 103.5442 learning rate: 0.001270, scenario: 0, slope: -5.451419871173483e-05, fluctuations: 0.0\n",
      "step: 77800 loss: 0.533510 time elapsed: 103.5575 learning rate: 0.001270, scenario: 0, slope: -5.4352763821817066e-05, fluctuations: 0.0\n",
      "step: 77810 loss: 0.532975 time elapsed: 103.5723 learning rate: 0.001270, scenario: 0, slope: -5.416557563722829e-05, fluctuations: 0.0\n",
      "step: 77820 loss: 0.532442 time elapsed: 103.5863 learning rate: 0.001270, scenario: 0, slope: -5.4004557367483405e-05, fluctuations: 0.0\n",
      "step: 77830 loss: 0.531909 time elapsed: 103.5998 learning rate: 0.001270, scenario: 0, slope: -5.385178678291978e-05, fluctuations: 0.0\n",
      "step: 77840 loss: 0.531378 time elapsed: 103.6134 learning rate: 0.001270, scenario: 0, slope: -5.370683835208214e-05, fluctuations: 0.0\n",
      "step: 77850 loss: 0.530842 time elapsed: 103.6268 learning rate: 0.001349, scenario: 1, slope: -5.3575557285866875e-05, fluctuations: 0.0\n",
      "step: 77860 loss: 0.530261 time elapsed: 103.6409 learning rate: 0.001446, scenario: 0, slope: -5.3608309500023563e-05, fluctuations: 0.0\n",
      "step: 77870 loss: 0.529660 time elapsed: 103.6551 learning rate: 0.001446, scenario: 0, slope: -5.397992141113613e-05, fluctuations: 0.0\n",
      "step: 77880 loss: 0.529061 time elapsed: 103.6689 learning rate: 0.001446, scenario: 0, slope: -5.4630309646422446e-05, fluctuations: 0.0\n",
      "step: 77890 loss: 0.585786 time elapsed: 103.6818 learning rate: 0.001490, scenario: 1, slope: -9.33147905830735e-06, fluctuations: 0.0\n",
      "step: 77900 loss: 61.325893 time elapsed: 103.6951 learning rate: 0.001368, scenario: -1, slope: 0.17779973582810693, fluctuations: 0.01\n",
      "step: 77910 loss: 20.643131 time elapsed: 103.7091 learning rate: 0.001237, scenario: -1, slope: 0.24875526798222244, fluctuations: 0.05\n",
      "step: 77920 loss: 5.318777 time elapsed: 103.7232 learning rate: 0.001119, scenario: -1, slope: 0.2257896378790366, fluctuations: 0.09\n",
      "step: 77930 loss: 2.381447 time elapsed: 103.7370 learning rate: 0.001012, scenario: -1, slope: 0.18800627453023802, fluctuations: 0.14\n",
      "step: 77940 loss: 0.920142 time elapsed: 103.7513 learning rate: 0.000915, scenario: -1, slope: 0.1299960066292026, fluctuations: 0.19\n",
      "step: 77950 loss: 0.665147 time elapsed: 103.7656 learning rate: 0.000828, scenario: -1, slope: 0.039901151736764943, fluctuations: 0.21\n",
      "step: 77960 loss: 0.582332 time elapsed: 103.7802 learning rate: 0.000787, scenario: 0, slope: -0.03522852226712579, fluctuations: 0.24\n",
      "step: 77970 loss: 0.558154 time elapsed: 103.7952 learning rate: 0.000787, scenario: 0, slope: -0.11335934302343958, fluctuations: 0.28\n",
      "step: 77980 loss: 0.539538 time elapsed: 103.8097 learning rate: 0.000787, scenario: 0, slope: -0.20992519319782654, fluctuations: 0.33\n",
      "step: 77990 loss: 0.534543 time elapsed: 103.8243 learning rate: 0.000787, scenario: 0, slope: -0.3407325338745893, fluctuations: 0.37\n",
      "step: 78000 loss: 0.533042 time elapsed: 103.8387 learning rate: 0.000787, scenario: 0, slope: -0.20477972217800472, fluctuations: 0.39\n",
      "step: 78010 loss: 0.531968 time elapsed: 103.8537 learning rate: 0.000787, scenario: 0, slope: -0.04389686126497797, fluctuations: 0.4\n",
      "step: 78020 loss: 0.531524 time elapsed: 103.8695 learning rate: 0.000787, scenario: 0, slope: -0.01215459963094826, fluctuations: 0.4\n",
      "step: 78030 loss: 0.531047 time elapsed: 103.8832 learning rate: 0.000787, scenario: 0, slope: -0.0035765706142406595, fluctuations: 0.36\n",
      "step: 78040 loss: 0.530630 time elapsed: 103.8961 learning rate: 0.000787, scenario: 0, slope: -0.0017328033511540594, fluctuations: 0.31\n",
      "step: 78050 loss: 0.530247 time elapsed: 103.9103 learning rate: 0.000787, scenario: 0, slope: -0.0004755152309429783, fluctuations: 0.3\n",
      "step: 78060 loss: 0.529873 time elapsed: 103.9243 learning rate: 0.000787, scenario: 0, slope: -0.00016951837846087702, fluctuations: 0.26\n",
      "step: 78070 loss: 0.529515 time elapsed: 103.9385 learning rate: 0.000787, scenario: 0, slope: -8.592830695717992e-05, fluctuations: 0.22\n",
      "step: 78080 loss: 0.529165 time elapsed: 103.9527 learning rate: 0.000787, scenario: 0, slope: -5.44377248243972e-05, fluctuations: 0.18\n",
      "step: 78090 loss: 0.528831 time elapsed: 103.9668 learning rate: 0.000726, scenario: -1, slope: -4.487023921445836e-05, fluctuations: 0.13\n",
      "step: 78100 loss: 0.528532 time elapsed: 103.9818 learning rate: 0.000663, scenario: -1, slope: -3.982343326288126e-05, fluctuations: 0.09\n",
      "step: 78110 loss: 0.528262 time elapsed: 103.9978 learning rate: 0.000600, scenario: -1, slope: -3.677794769367806e-05, fluctuations: 0.04\n",
      "step: 78120 loss: 0.528021 time elapsed: 104.0121 learning rate: 0.000543, scenario: -1, slope: -3.471625901673047e-05, fluctuations: 0.01\n",
      "step: 78130 loss: 0.527793 time elapsed: 104.0267 learning rate: 0.000585, scenario: 1, slope: -3.2698426050810325e-05, fluctuations: 0.0\n",
      "step: 78140 loss: 0.527545 time elapsed: 104.0412 learning rate: 0.000646, scenario: 1, slope: -3.072204251188151e-05, fluctuations: 0.0\n",
      "step: 78150 loss: 0.527273 time elapsed: 104.0558 learning rate: 0.000713, scenario: 1, slope: -2.900192889973466e-05, fluctuations: 0.0\n",
      "step: 78160 loss: 0.526974 time elapsed: 104.0701 learning rate: 0.000788, scenario: 1, slope: -2.7706855596857007e-05, fluctuations: 0.0\n",
      "step: 78170 loss: 0.526647 time elapsed: 104.0890 learning rate: 0.000870, scenario: 1, slope: -2.7000256041582148e-05, fluctuations: 0.0\n",
      "step: 78180 loss: 0.526288 time elapsed: 104.1052 learning rate: 0.000961, scenario: 1, slope: -2.703771708871944e-05, fluctuations: 0.0\n",
      "step: 78190 loss: 0.525894 time elapsed: 104.1197 learning rate: 0.001062, scenario: 1, slope: -2.7940668361485755e-05, fluctuations: 0.0\n",
      "step: 78200 loss: 0.525462 time elapsed: 104.1334 learning rate: 0.001161, scenario: 1, slope: -2.9475912456153496e-05, fluctuations: 0.0\n",
      "step: 78210 loss: 0.524993 time elapsed: 104.1485 learning rate: 0.001283, scenario: 1, slope: -3.213609967464502e-05, fluctuations: 0.0\n",
      "step: 78220 loss: 0.524479 time elapsed: 104.1619 learning rate: 0.001417, scenario: 1, slope: -3.5124130418861004e-05, fluctuations: 0.0\n",
      "step: 78230 loss: 0.523915 time elapsed: 104.1759 learning rate: 0.001566, scenario: 1, slope: -3.84663347871843e-05, fluctuations: 0.0\n",
      "step: 78240 loss: 0.523297 time elapsed: 104.1912 learning rate: 0.001729, scenario: 1, slope: -4.2120483863287074e-05, fluctuations: 0.0\n",
      "step: 78250 loss: 0.522774 time elapsed: 104.2062 learning rate: 0.001910, scenario: 1, slope: -4.600998436761498e-05, fluctuations: 0.0\n",
      "step: 78260 loss: 1070.434428 time elapsed: 104.2211 learning rate: 0.001844, scenario: -1, slope: 0.9840202492996583, fluctuations: 0.01\n",
      "step: 78270 loss: 397.550426 time elapsed: 104.2359 learning rate: 0.001667, scenario: -1, slope: 1.4241342951784186, fluctuations: 0.05\n",
      "step: 78280 loss: 30.617176 time elapsed: 104.2505 learning rate: 0.001508, scenario: -1, slope: 1.2380919448452397, fluctuations: 0.1\n",
      "step: 78290 loss: 8.694982 time elapsed: 104.2652 learning rate: 0.001364, scenario: -1, slope: 1.0749257126377718, fluctuations: 0.14\n",
      "step: 78300 loss: 9.751604 time elapsed: 104.2793 learning rate: 0.001246, scenario: -1, slope: 0.7761489250648221, fluctuations: 0.17\n",
      "step: 78310 loss: 3.435345 time elapsed: 104.2951 learning rate: 0.001127, scenario: -1, slope: 0.3699395183247723, fluctuations: 0.21\n",
      "step: 78320 loss: 1.686917 time elapsed: 104.3106 learning rate: 0.001029, scenario: 0, slope: -0.008545912156384691, fluctuations: 0.25\n",
      "step: 78330 loss: 0.876127 time elapsed: 104.3247 learning rate: 0.001029, scenario: 0, slope: -0.4348752367648467, fluctuations: 0.28\n",
      "step: 78340 loss: 0.934646 time elapsed: 104.3389 learning rate: 0.001029, scenario: 0, slope: -0.9047426585108691, fluctuations: 0.31\n",
      "step: 78350 loss: 0.832545 time elapsed: 104.3531 learning rate: 0.001029, scenario: 0, slope: -1.5380506480537655, fluctuations: 0.35\n",
      "step: 78360 loss: 0.767867 time elapsed: 104.3673 learning rate: 0.001029, scenario: 0, slope: -0.6720512145907831, fluctuations: 0.37\n",
      "step: 78370 loss: 0.754529 time elapsed: 104.3803 learning rate: 0.001029, scenario: 0, slope: -0.3839572567295417, fluctuations: 0.35\n",
      "step: 78380 loss: 0.736421 time elapsed: 104.3946 learning rate: 0.001029, scenario: 0, slope: -0.2263632877489511, fluctuations: 0.34\n",
      "step: 78390 loss: 0.722266 time elapsed: 104.4099 learning rate: 0.001029, scenario: 0, slope: -0.07412922862134765, fluctuations: 0.3\n",
      "step: 78400 loss: 0.711191 time elapsed: 104.4247 learning rate: 0.001029, scenario: 0, slope: -0.025732084723327754, fluctuations: 0.27\n",
      "step: 78410 loss: 0.701107 time elapsed: 104.4401 learning rate: 0.001029, scenario: 0, slope: -0.009537388548478342, fluctuations: 0.23\n",
      "step: 78420 loss: 0.692344 time elapsed: 104.4548 learning rate: 0.001029, scenario: 0, slope: -0.0037272923962770065, fluctuations: 0.2\n",
      "step: 78430 loss: 0.684486 time elapsed: 104.4695 learning rate: 0.001029, scenario: 0, slope: -0.002492968374999136, fluctuations: 0.16\n",
      "step: 78440 loss: 0.677425 time elapsed: 104.4843 learning rate: 0.001029, scenario: 0, slope: -0.0015658593438080506, fluctuations: 0.13\n",
      "step: 78450 loss: 0.671053 time elapsed: 104.4993 learning rate: 0.001029, scenario: 0, slope: -0.0011655690685047895, fluctuations: 0.1\n",
      "step: 78460 loss: 0.665272 time elapsed: 104.5141 learning rate: 0.001029, scenario: 0, slope: -0.0009953816606873434, fluctuations: 0.06\n",
      "step: 78470 loss: 0.660010 time elapsed: 104.5277 learning rate: 0.001029, scenario: 0, slope: -0.0008629015239762112, fluctuations: 0.03\n",
      "step: 78480 loss: 0.655200 time elapsed: 104.5415 learning rate: 0.001029, scenario: 0, slope: -0.0007747409248179732, fluctuations: 0.0\n",
      "step: 78490 loss: 0.650790 time elapsed: 104.5554 learning rate: 0.001029, scenario: 0, slope: -0.0006933718050022645, fluctuations: 0.0\n",
      "step: 78500 loss: 0.646732 time elapsed: 104.5692 learning rate: 0.001029, scenario: 0, slope: -0.0006312659070297686, fluctuations: 0.0\n",
      "step: 78510 loss: 0.642987 time elapsed: 104.5833 learning rate: 0.001029, scenario: 0, slope: -0.0005662476692813241, fluctuations: 0.0\n",
      "step: 78520 loss: 0.639518 time elapsed: 104.5965 learning rate: 0.001029, scenario: 0, slope: -0.0005153894906331983, fluctuations: 0.0\n",
      "step: 78530 loss: 0.636296 time elapsed: 104.6118 learning rate: 0.001029, scenario: 0, slope: -0.00047094439244208723, fluctuations: 0.0\n",
      "step: 78540 loss: 0.633295 time elapsed: 104.6267 learning rate: 0.001029, scenario: 0, slope: -0.00043190899882700645, fluctuations: 0.0\n",
      "step: 78550 loss: 0.630490 time elapsed: 104.6411 learning rate: 0.001029, scenario: 0, slope: -0.00039748058704524816, fluctuations: 0.0\n",
      "step: 78560 loss: 0.627862 time elapsed: 104.6558 learning rate: 0.001029, scenario: 0, slope: -0.0003670162172197977, fluctuations: 0.0\n",
      "step: 78570 loss: 0.625393 time elapsed: 104.6707 learning rate: 0.001029, scenario: 0, slope: -0.0003399778354577999, fluctuations: 0.0\n",
      "step: 78580 loss: 0.623066 time elapsed: 104.6853 learning rate: 0.001029, scenario: 0, slope: -0.00031591533870565033, fluctuations: 0.0\n",
      "step: 78590 loss: 0.620869 time elapsed: 104.6997 learning rate: 0.001029, scenario: 0, slope: -0.0002944466846698752, fluctuations: 0.0\n",
      "step: 78600 loss: 0.618788 time elapsed: 104.7137 learning rate: 0.001029, scenario: 0, slope: -0.0002770721127828806, fluctuations: 0.0\n",
      "step: 78610 loss: 0.616813 time elapsed: 104.7275 learning rate: 0.001029, scenario: 0, slope: -0.000258032582332881, fluctuations: 0.0\n",
      "step: 78620 loss: 0.614934 time elapsed: 104.7415 learning rate: 0.001029, scenario: 0, slope: -0.00024256575977410586, fluctuations: 0.0\n",
      "step: 78630 loss: 0.613142 time elapsed: 104.7557 learning rate: 0.001029, scenario: 0, slope: -0.0002286365221960899, fluctuations: 0.0\n",
      "step: 78640 loss: 0.611430 time elapsed: 104.7693 learning rate: 0.001029, scenario: 0, slope: -0.0002160637971580045, fluctuations: 0.0\n",
      "step: 78650 loss: 0.609791 time elapsed: 104.7830 learning rate: 0.001029, scenario: 0, slope: -0.00020469016274291617, fluctuations: 0.0\n",
      "step: 78660 loss: 0.608219 time elapsed: 104.7957 learning rate: 0.001029, scenario: 0, slope: -0.0001943784645784185, fluctuations: 0.0\n",
      "step: 78670 loss: 0.606709 time elapsed: 104.8103 learning rate: 0.001029, scenario: 0, slope: -0.00018500896977145917, fluctuations: 0.0\n",
      "step: 78680 loss: 0.605254 time elapsed: 104.8253 learning rate: 0.001029, scenario: 0, slope: -0.00017647696014739614, fluctuations: 0.0\n",
      "step: 78690 loss: 0.603852 time elapsed: 104.8406 learning rate: 0.001029, scenario: 0, slope: -0.00016869068799332985, fluctuations: 0.0\n",
      "step: 78700 loss: 0.602498 time elapsed: 104.8568 learning rate: 0.001029, scenario: 0, slope: -0.00016225389194855186, fluctuations: 0.0\n",
      "step: 78710 loss: 0.601189 time elapsed: 104.8727 learning rate: 0.001029, scenario: 0, slope: -0.00015504301752753392, fluctuations: 0.0\n",
      "step: 78720 loss: 0.599921 time elapsed: 104.8869 learning rate: 0.001029, scenario: 0, slope: -0.00014904852152461352, fluctuations: 0.0\n",
      "step: 78730 loss: 0.598691 time elapsed: 104.9018 learning rate: 0.001029, scenario: 0, slope: -0.00014353119525512806, fluctuations: 0.0\n",
      "step: 78740 loss: 0.597497 time elapsed: 104.9162 learning rate: 0.001029, scenario: 0, slope: -0.0001384425112950102, fluctuations: 0.0\n",
      "step: 78750 loss: 0.596337 time elapsed: 104.9290 learning rate: 0.001029, scenario: 0, slope: -0.0001337395516463323, fluctuations: 0.0\n",
      "step: 78760 loss: 0.595208 time elapsed: 104.9430 learning rate: 0.001029, scenario: 0, slope: -0.00012938430459416694, fluctuations: 0.0\n",
      "step: 78770 loss: 0.594108 time elapsed: 104.9572 learning rate: 0.001029, scenario: 0, slope: -0.0001253430560873747, fluctuations: 0.0\n",
      "step: 78780 loss: 0.593035 time elapsed: 104.9708 learning rate: 0.001029, scenario: 0, slope: -0.00012158586205087595, fluctuations: 0.0\n",
      "step: 78790 loss: 0.591988 time elapsed: 104.9848 learning rate: 0.001029, scenario: 0, slope: -0.00011808609012630668, fluctuations: 0.0\n",
      "step: 78800 loss: 0.590965 time elapsed: 104.9984 learning rate: 0.001029, scenario: 0, slope: -0.00011513673289535599, fluctuations: 0.0\n",
      "step: 78810 loss: 0.589965 time elapsed: 105.0123 learning rate: 0.001029, scenario: 0, slope: -0.00011176650159832973, fluctuations: 0.0\n",
      "step: 78820 loss: 0.588987 time elapsed: 105.0273 learning rate: 0.001029, scenario: 0, slope: -0.00010890664135212717, fluctuations: 0.0\n",
      "step: 78830 loss: 0.588029 time elapsed: 105.0412 learning rate: 0.001029, scenario: 0, slope: -0.00010622354838063772, fluctuations: 0.0\n",
      "step: 78840 loss: 0.587090 time elapsed: 105.0551 learning rate: 0.001029, scenario: 0, slope: -0.00010370209755281792, fluctuations: 0.0\n",
      "step: 78850 loss: 0.586170 time elapsed: 105.0688 learning rate: 0.001029, scenario: 0, slope: -0.00010132872773658264, fluctuations: 0.0\n",
      "step: 78860 loss: 0.585267 time elapsed: 105.0825 learning rate: 0.001029, scenario: 0, slope: -9.909126386840562e-05, fluctuations: 0.0\n",
      "step: 78870 loss: 0.584380 time elapsed: 105.0964 learning rate: 0.001029, scenario: 0, slope: -9.697876066620238e-05, fluctuations: 0.0\n",
      "step: 78880 loss: 0.583509 time elapsed: 105.1102 learning rate: 0.001029, scenario: 0, slope: -9.498136518112624e-05, fluctuations: 0.0\n",
      "step: 78890 loss: 0.582653 time elapsed: 105.1245 learning rate: 0.001029, scenario: 0, slope: -9.309019576896283e-05, fluctuations: 0.0\n",
      "step: 78900 loss: 0.581812 time elapsed: 105.1376 learning rate: 0.001029, scenario: 0, slope: -9.147232419616546e-05, fluctuations: 0.0\n",
      "step: 78910 loss: 0.580983 time elapsed: 105.1518 learning rate: 0.001029, scenario: 0, slope: -8.959523745385528e-05, fluctuations: 0.0\n",
      "step: 78920 loss: 0.580168 time elapsed: 105.1644 learning rate: 0.001029, scenario: 0, slope: -8.797764257689175e-05, fluctuations: 0.0\n",
      "step: 78930 loss: 0.579366 time elapsed: 105.1781 learning rate: 0.001029, scenario: 0, slope: -8.64385050092088e-05, fluctuations: 0.0\n",
      "step: 78940 loss: 0.578575 time elapsed: 105.1916 learning rate: 0.001029, scenario: 0, slope: -8.497242744007047e-05, fluctuations: 0.0\n",
      "step: 78950 loss: 0.577795 time elapsed: 105.2055 learning rate: 0.001029, scenario: 0, slope: -8.357450322433558e-05, fluctuations: 0.0\n",
      "step: 78960 loss: 0.577026 time elapsed: 105.2204 learning rate: 0.001029, scenario: 0, slope: -8.224026511638068e-05, fluctuations: 0.0\n",
      "step: 78970 loss: 0.576268 time elapsed: 105.2355 learning rate: 0.001029, scenario: 0, slope: -8.096563973575242e-05, fluctuations: 0.0\n",
      "step: 78980 loss: 0.575520 time elapsed: 105.2498 learning rate: 0.001029, scenario: 0, slope: -7.974690708657727e-05, fluctuations: 0.0\n",
      "step: 78990 loss: 0.574781 time elapsed: 105.2639 learning rate: 0.001029, scenario: 0, slope: -7.858066453681427e-05, fluctuations: 0.0\n",
      "step: 79000 loss: 0.574052 time elapsed: 105.2782 learning rate: 0.001029, scenario: 0, slope: -7.757334336445851e-05, fluctuations: 0.0\n",
      "step: 79010 loss: 0.573332 time elapsed: 105.2933 learning rate: 0.001029, scenario: 0, slope: -7.639343703283857e-05, fluctuations: 0.0\n",
      "step: 79020 loss: 0.572620 time elapsed: 105.3076 learning rate: 0.001029, scenario: 0, slope: -7.536696194428858e-05, fluctuations: 0.0\n",
      "step: 79030 loss: 0.571916 time elapsed: 105.3219 learning rate: 0.001029, scenario: 0, slope: -7.438194840831169e-05, fluctuations: 0.0\n",
      "step: 79040 loss: 0.571220 time elapsed: 105.3362 learning rate: 0.001029, scenario: 0, slope: -7.343616342993868e-05, fluctuations: 0.0\n",
      "step: 79050 loss: 0.570532 time elapsed: 105.3498 learning rate: 0.001029, scenario: 0, slope: -7.252754389768612e-05, fluctuations: 0.0\n",
      "step: 79060 loss: 0.569851 time elapsed: 105.3629 learning rate: 0.001029, scenario: 0, slope: -7.165418031674221e-05, fluctuations: 0.0\n",
      "step: 79070 loss: 0.569177 time elapsed: 105.3762 learning rate: 0.001029, scenario: 0, slope: -7.081430224857962e-05, fluctuations: 0.0\n",
      "step: 79080 loss: 0.568510 time elapsed: 105.3888 learning rate: 0.001029, scenario: 0, slope: -7.000626527123032e-05, fluctuations: 0.0\n",
      "step: 79090 loss: 0.567850 time elapsed: 105.4026 learning rate: 0.001029, scenario: 0, slope: -6.922853929556908e-05, fluctuations: 0.0\n",
      "step: 79100 loss: 0.567196 time elapsed: 105.4156 learning rate: 0.001029, scenario: 0, slope: -6.855332105286282e-05, fluctuations: 0.0\n",
      "step: 79110 loss: 0.566548 time elapsed: 105.4296 learning rate: 0.001029, scenario: 0, slope: -6.775840989173848e-05, fluctuations: 0.0\n",
      "step: 79120 loss: 0.565906 time elapsed: 105.4441 learning rate: 0.001029, scenario: 0, slope: -6.706342896555151e-05, fluctuations: 0.0\n",
      "step: 79130 loss: 0.565269 time elapsed: 105.4585 learning rate: 0.001029, scenario: 0, slope: -6.639358804873155e-05, fluctuations: 0.0\n",
      "step: 79140 loss: 0.564638 time elapsed: 105.4730 learning rate: 0.001029, scenario: 0, slope: -6.574779154936184e-05, fluctuations: 0.0\n",
      "step: 79150 loss: 0.564013 time elapsed: 105.4875 learning rate: 0.001029, scenario: 0, slope: -6.512500944167808e-05, fluctuations: 0.0\n",
      "step: 79160 loss: 0.563392 time elapsed: 105.5024 learning rate: 0.001029, scenario: 0, slope: -6.452427177806537e-05, fluctuations: 0.0\n",
      "step: 79170 loss: 0.562777 time elapsed: 105.5169 learning rate: 0.001029, scenario: 0, slope: -6.394466375376945e-05, fluctuations: 0.0\n",
      "step: 79180 loss: 0.562166 time elapsed: 105.5312 learning rate: 0.001029, scenario: 0, slope: -6.338532126619967e-05, fluctuations: 0.0\n",
      "step: 79190 loss: 0.561560 time elapsed: 105.5455 learning rate: 0.001029, scenario: 0, slope: -6.284542691735212e-05, fluctuations: 0.0\n",
      "step: 79200 loss: 0.560958 time elapsed: 105.5597 learning rate: 0.001029, scenario: 0, slope: -6.23755093707376e-05, fluctuations: 0.0\n",
      "step: 79210 loss: 0.560360 time elapsed: 105.5732 learning rate: 0.001029, scenario: 0, slope: -6.182092531812795e-05, fluctuations: 0.0\n",
      "step: 79220 loss: 0.559767 time elapsed: 105.5859 learning rate: 0.001029, scenario: 0, slope: -6.133488612574619e-05, fluctuations: 0.0\n",
      "step: 79230 loss: 0.559178 time elapsed: 105.5985 learning rate: 0.001029, scenario: 0, slope: -6.086542561653092e-05, fluctuations: 0.0\n",
      "step: 79240 loss: 0.558592 time elapsed: 105.6107 learning rate: 0.001029, scenario: 0, slope: -6.041191246764566e-05, fluctuations: 0.0\n",
      "step: 79250 loss: 0.558011 time elapsed: 105.6232 learning rate: 0.001029, scenario: 0, slope: -5.9973745091282736e-05, fluctuations: 0.0\n",
      "step: 79260 loss: 0.557433 time elapsed: 105.6364 learning rate: 0.001029, scenario: 0, slope: -5.955034967614018e-05, fluctuations: 0.0\n",
      "step: 79270 loss: 0.556858 time elapsed: 105.6509 learning rate: 0.001029, scenario: 0, slope: -5.914117841083001e-05, fluctuations: 0.0\n",
      "step: 79280 loss: 0.556287 time elapsed: 105.6648 learning rate: 0.001029, scenario: 0, slope: -5.8745707869429964e-05, fluctuations: 0.0\n",
      "step: 79290 loss: 0.555719 time elapsed: 105.6786 learning rate: 0.001029, scenario: 0, slope: -5.8363437542101695e-05, fluctuations: 0.0\n",
      "step: 79300 loss: 0.555155 time elapsed: 105.6920 learning rate: 0.001029, scenario: 0, slope: -5.8030284166560215e-05, fluctuations: 0.0\n",
      "step: 79310 loss: 0.554593 time elapsed: 105.7062 learning rate: 0.001029, scenario: 0, slope: -5.763660214969172e-05, fluctuations: 0.0\n",
      "step: 79320 loss: 0.554034 time elapsed: 105.7199 learning rate: 0.001029, scenario: 0, slope: -5.7291139155135295e-05, fluctuations: 0.0\n",
      "step: 79330 loss: 0.553479 time elapsed: 105.7330 learning rate: 0.001029, scenario: 0, slope: -5.695707836658516e-05, fluctuations: 0.0\n",
      "step: 79340 loss: 0.552926 time elapsed: 105.7465 learning rate: 0.001029, scenario: 0, slope: -5.66340158958025e-05, fluctuations: 0.0\n",
      "step: 79350 loss: 0.552375 time elapsed: 105.7594 learning rate: 0.001029, scenario: 0, slope: -5.6321564239226974e-05, fluctuations: 0.0\n",
      "step: 79360 loss: 0.551828 time elapsed: 105.7721 learning rate: 0.001029, scenario: 0, slope: -5.601935147122815e-05, fluctuations: 0.0\n",
      "step: 79370 loss: 0.551282 time elapsed: 105.7848 learning rate: 0.001029, scenario: 0, slope: -5.572702049652445e-05, fluctuations: 0.0\n",
      "step: 79380 loss: 0.550728 time elapsed: 105.7971 learning rate: 0.001115, scenario: 1, slope: -5.546201850030059e-05, fluctuations: 0.0\n",
      "step: 79390 loss: 0.550122 time elapsed: 105.8094 learning rate: 0.001231, scenario: 1, slope: -5.542187989141542e-05, fluctuations: 0.0\n",
      "step: 79400 loss: 0.549463 time elapsed: 105.8217 learning rate: 0.001281, scenario: 0, slope: -5.5801778569700725e-05, fluctuations: 0.0\n",
      "step: 79410 loss: 0.548799 time elapsed: 105.8349 learning rate: 0.001281, scenario: 0, slope: -5.685771503487865e-05, fluctuations: 0.0\n",
      "step: 79420 loss: 0.548138 time elapsed: 105.8493 learning rate: 0.001281, scenario: 0, slope: -5.821776841206443e-05, fluctuations: 0.0\n",
      "step: 79430 loss: 0.547481 time elapsed: 105.8630 learning rate: 0.001281, scenario: 0, slope: -5.978984203473174e-05, fluctuations: 0.0\n",
      "step: 79440 loss: 0.546828 time elapsed: 105.8773 learning rate: 0.001281, scenario: 0, slope: -6.141288246782143e-05, fluctuations: 0.0\n",
      "step: 79450 loss: 0.546179 time elapsed: 105.8916 learning rate: 0.001281, scenario: 0, slope: -6.292833399088378e-05, fluctuations: 0.0\n",
      "step: 79460 loss: 0.545533 time elapsed: 105.9056 learning rate: 0.001281, scenario: 0, slope: -6.418007860292363e-05, fluctuations: 0.0\n",
      "step: 79470 loss: 0.544891 time elapsed: 105.9191 learning rate: 0.001281, scenario: 0, slope: -6.501435031852285e-05, fluctuations: 0.0\n",
      "step: 79480 loss: 0.544251 time elapsed: 105.9327 learning rate: 0.001281, scenario: 0, slope: -6.529872084604511e-05, fluctuations: 0.0\n",
      "step: 79490 loss: 0.543615 time elapsed: 105.9465 learning rate: 0.001281, scenario: 0, slope: -6.51241744781534e-05, fluctuations: 0.0\n",
      "step: 79500 loss: 0.542982 time elapsed: 105.9595 learning rate: 0.001281, scenario: 0, slope: -6.481042508083564e-05, fluctuations: 0.0\n",
      "step: 79510 loss: 0.542352 time elapsed: 105.9725 learning rate: 0.001281, scenario: 0, slope: -6.443043313434217e-05, fluctuations: 0.0\n",
      "step: 79520 loss: 0.541725 time elapsed: 105.9848 learning rate: 0.001281, scenario: 0, slope: -6.409741315144287e-05, fluctuations: 0.0\n",
      "step: 79530 loss: 0.541100 time elapsed: 105.9970 learning rate: 0.001281, scenario: 0, slope: -6.377593668254626e-05, fluctuations: 0.0\n",
      "step: 79540 loss: 0.540478 time elapsed: 106.0094 learning rate: 0.001281, scenario: 0, slope: -6.34655948624634e-05, fluctuations: 0.0\n",
      "step: 79550 loss: 0.539859 time elapsed: 106.0216 learning rate: 0.001281, scenario: 0, slope: -6.316595255913024e-05, fluctuations: 0.0\n",
      "step: 79560 loss: 0.539242 time elapsed: 106.0337 learning rate: 0.001281, scenario: 0, slope: -6.287658001052718e-05, fluctuations: 0.0\n",
      "step: 79570 loss: 0.538628 time elapsed: 106.0459 learning rate: 0.001281, scenario: 0, slope: -6.259706311646552e-05, fluctuations: 0.0\n",
      "step: 79580 loss: 0.538016 time elapsed: 106.0593 learning rate: 0.001281, scenario: 0, slope: -6.23270062948337e-05, fluctuations: 0.0\n",
      "step: 79590 loss: 0.537406 time elapsed: 106.0730 learning rate: 0.001281, scenario: 0, slope: -6.206603277331154e-05, fluctuations: 0.0\n",
      "step: 79600 loss: 0.536798 time elapsed: 106.0868 learning rate: 0.001281, scenario: 0, slope: -6.183862613648198e-05, fluctuations: 0.0\n",
      "step: 79610 loss: 0.536193 time elapsed: 106.1009 learning rate: 0.001281, scenario: 0, slope: -6.156991892094604e-05, fluctuations: 0.0\n",
      "step: 79620 loss: 0.535589 time elapsed: 106.1148 learning rate: 0.001281, scenario: 0, slope: -6.133411287501652e-05, fluctuations: 0.0\n",
      "step: 79630 loss: 0.534987 time elapsed: 106.1282 learning rate: 0.001281, scenario: 0, slope: -6.110605693496497e-05, fluctuations: 0.0\n",
      "step: 79640 loss: 0.534388 time elapsed: 106.1419 learning rate: 0.001281, scenario: 0, slope: -6.0885456959848206e-05, fluctuations: 0.0\n",
      "step: 79650 loss: 0.533790 time elapsed: 106.1555 learning rate: 0.001281, scenario: 0, slope: -6.067203281959753e-05, fluctuations: 0.0\n",
      "step: 79660 loss: 0.533194 time elapsed: 106.1693 learning rate: 0.001281, scenario: 0, slope: -6.04655176403688e-05, fluctuations: 0.0\n",
      "step: 79670 loss: 0.532599 time elapsed: 106.1818 learning rate: 0.001281, scenario: 0, slope: -6.026565709153783e-05, fluctuations: 0.0\n",
      "step: 79680 loss: 0.532007 time elapsed: 106.1944 learning rate: 0.001281, scenario: 0, slope: -6.007220871218169e-05, fluctuations: 0.0\n",
      "step: 79690 loss: 0.531415 time elapsed: 106.2070 learning rate: 0.001281, scenario: 0, slope: -5.988494127493502e-05, fluctuations: 0.0\n",
      "step: 79700 loss: 0.530826 time elapsed: 106.2195 learning rate: 0.001281, scenario: 0, slope: -5.972150275508331e-05, fluctuations: 0.0\n",
      "step: 79710 loss: 0.530238 time elapsed: 106.2324 learning rate: 0.001281, scenario: 0, slope: -5.9528076914681204e-05, fluctuations: 0.0\n",
      "step: 79720 loss: 0.529651 time elapsed: 106.2447 learning rate: 0.001281, scenario: 0, slope: -5.935806845546053e-05, fluctuations: 0.0\n",
      "step: 79730 loss: 0.529066 time elapsed: 106.2571 learning rate: 0.001281, scenario: 0, slope: -5.919340998247254e-05, fluctuations: 0.0\n",
      "step: 79740 loss: 0.528490 time elapsed: 106.2711 learning rate: 0.001281, scenario: 0, slope: -5.9025608708527815e-05, fluctuations: 0.0\n",
      "step: 79750 loss: 0.543352 time elapsed: 106.2857 learning rate: 0.001307, scenario: 1, slope: -4.2243544113710104e-05, fluctuations: 0.0\n",
      "step: 79760 loss: 40.413254 time elapsed: 106.3000 learning rate: 0.001212, scenario: -1, slope: 0.06132731448293746, fluctuations: 0.0\n",
      "step: 79770 loss: 1.629751 time elapsed: 106.3139 learning rate: 0.001096, scenario: -1, slope: 0.07196861434897242, fluctuations: 0.03\n",
      "step: 79780 loss: 1.413935 time elapsed: 106.3280 learning rate: 0.000991, scenario: -1, slope: 0.0653556269781982, fluctuations: 0.06\n",
      "step: 79790 loss: 0.546622 time elapsed: 106.3422 learning rate: 0.000897, scenario: -1, slope: 0.05320093717211746, fluctuations: 0.1\n",
      "step: 79800 loss: 0.689277 time elapsed: 106.3560 learning rate: 0.000819, scenario: -1, slope: 0.039174463827055354, fluctuations: 0.14\n",
      "step: 79810 loss: 0.533428 time elapsed: 106.3700 learning rate: 0.000741, scenario: -1, slope: 0.018128930128368147, fluctuations: 0.19\n",
      "step: 79820 loss: 0.533097 time elapsed: 106.3875 learning rate: 0.000683, scenario: 0, slope: -0.0032965144121477564, fluctuations: 0.24\n",
      "step: 79830 loss: 0.528019 time elapsed: 106.4006 learning rate: 0.000683, scenario: 0, slope: -0.02755028494127274, fluctuations: 0.29\n",
      "step: 79840 loss: 0.526834 time elapsed: 106.4133 learning rate: 0.000683, scenario: 0, slope: -0.05503736158934716, fluctuations: 0.34\n",
      "step: 79850 loss: 0.526344 time elapsed: 106.4260 learning rate: 0.000683, scenario: 0, slope: -0.08851514817743314, fluctuations: 0.39\n",
      "step: 79860 loss: 0.525972 time elapsed: 106.4388 learning rate: 0.000683, scenario: 0, slope: -0.06490413400171609, fluctuations: 0.43\n",
      "step: 79870 loss: 0.525627 time elapsed: 106.4516 learning rate: 0.000683, scenario: 0, slope: -0.009935916952674373, fluctuations: 0.46\n",
      "step: 79880 loss: 0.525292 time elapsed: 106.4643 learning rate: 0.000683, scenario: 0, slope: -0.004501603252437461, fluctuations: 0.47\n",
      "step: 79890 loss: 0.524962 time elapsed: 106.4782 learning rate: 0.000683, scenario: 0, slope: -0.0012749133925492847, fluctuations: 0.43\n",
      "step: 79900 loss: 0.524636 time elapsed: 106.4924 learning rate: 0.000683, scenario: 0, slope: -0.0003949827669965987, fluctuations: 0.39\n",
      "step: 79910 loss: 0.524312 time elapsed: 106.5075 learning rate: 0.000683, scenario: 0, slope: -0.00013729527039492513, fluctuations: 0.34\n",
      "step: 79920 loss: 0.523991 time elapsed: 106.5217 learning rate: 0.000677, scenario: 0, slope: -6.147474457845347e-05, fluctuations: 0.29\n",
      "step: 79930 loss: 0.523684 time elapsed: 106.5358 learning rate: 0.000618, scenario: -1, slope: -4.166267499698092e-05, fluctuations: 0.24\n",
      "step: 79940 loss: 0.523406 time elapsed: 106.5499 learning rate: 0.000559, scenario: -1, slope: -3.534819995123839e-05, fluctuations: 0.19\n",
      "step: 79950 loss: 0.523155 time elapsed: 106.5640 learning rate: 0.000506, scenario: -1, slope: -3.271351379317359e-05, fluctuations: 0.14\n",
      "step: 79960 loss: 0.522929 time elapsed: 106.5779 learning rate: 0.000457, scenario: -1, slope: -3.105388601197772e-05, fluctuations: 0.09\n",
      "step: 79970 loss: 0.522725 time elapsed: 106.5921 learning rate: 0.000413, scenario: -1, slope: -2.9590298117885012e-05, fluctuations: 0.04\n",
      "step: 79980 loss: 0.522539 time elapsed: 106.6057 learning rate: 0.000395, scenario: 1, slope: -2.8069242867945886e-05, fluctuations: 0.0\n",
      "step: 79990 loss: 0.522350 time elapsed: 106.6186 learning rate: 0.000436, scenario: 1, slope: -2.6193060286195196e-05, fluctuations: 0.0\n",
      "step: 80000 loss: 0.522140 time elapsed: 106.6310 learning rate: 0.000477, scenario: 1, slope: -2.4568888692663576e-05, fluctuations: 0.0\n",
      "step: 80010 loss: 0.521910 time elapsed: 106.6441 learning rate: 0.000527, scenario: 1, slope: -2.2902301847955504e-05, fluctuations: 0.0\n",
      "step: 80020 loss: 0.521656 time elapsed: 106.6567 learning rate: 0.000582, scenario: 1, slope: -2.191705382252552e-05, fluctuations: 0.0\n",
      "step: 80030 loss: 0.521375 time elapsed: 106.6691 learning rate: 0.000643, scenario: 1, slope: -2.1631621709256454e-05, fluctuations: 0.0\n",
      "step: 80040 loss: 0.521066 time elapsed: 106.6819 learning rate: 0.000711, scenario: 1, slope: -2.20949188186219e-05, fluctuations: 0.0\n",
      "step: 80050 loss: 0.520724 time elapsed: 106.6960 learning rate: 0.000785, scenario: 1, slope: -2.3278690366883702e-05, fluctuations: 0.0\n",
      "step: 80060 loss: 0.520346 time elapsed: 106.7102 learning rate: 0.000867, scenario: 1, slope: -2.5120434431179015e-05, fluctuations: 0.0\n",
      "step: 80070 loss: 0.519929 time elapsed: 106.7240 learning rate: 0.000958, scenario: 1, slope: -2.752245130784733e-05, fluctuations: 0.0\n",
      "step: 80080 loss: 0.519470 time elapsed: 106.7379 learning rate: 0.001058, scenario: 1, slope: -3.0350632408256316e-05, fluctuations: 0.0\n",
      "step: 80090 loss: 0.518963 time elapsed: 106.7517 learning rate: 0.001169, scenario: 1, slope: -3.3493049321296795e-05, fluctuations: 0.0\n",
      "step: 80100 loss: 0.518404 time elapsed: 106.7651 learning rate: 0.001278, scenario: 1, slope: -3.659878273287999e-05, fluctuations: 0.0\n",
      "step: 80110 loss: 0.517795 time elapsed: 106.7793 learning rate: 0.001412, scenario: 1, slope: -4.0766419090586144e-05, fluctuations: 0.0\n",
      "step: 80120 loss: 0.626130 time elapsed: 106.7929 learning rate: 0.001537, scenario: -1, slope: 3.104530138310088e-05, fluctuations: 0.0\n",
      "step: 80130 loss: 390.078246 time elapsed: 106.8060 learning rate: 0.001390, scenario: -1, slope: 0.9352086971499678, fluctuations: 0.02\n",
      "step: 80140 loss: 144.692589 time elapsed: 106.8188 learning rate: 0.001257, scenario: -1, slope: 0.9750512423765311, fluctuations: 0.07\n",
      "step: 80150 loss: 29.730500 time elapsed: 106.8316 learning rate: 0.001137, scenario: -1, slope: 0.8630746407863106, fluctuations: 0.12\n",
      "step: 80160 loss: 11.156727 time elapsed: 106.8438 learning rate: 0.001028, scenario: -1, slope: 0.6736715154015597, fluctuations: 0.16\n",
      "step: 80170 loss: 5.354986 time elapsed: 106.8568 learning rate: 0.000930, scenario: -1, slope: 0.39512513500236535, fluctuations: 0.19\n",
      "step: 80180 loss: 0.832314 time elapsed: 106.8695 learning rate: 0.000841, scenario: -1, slope: 0.13626636043144352, fluctuations: 0.23\n",
      "step: 80190 loss: 1.133740 time elapsed: 106.8820 learning rate: 0.000800, scenario: 0, slope: -0.15166603637431295, fluctuations: 0.26\n",
      "step: 80200 loss: 0.754765 time elapsed: 106.8961 learning rate: 0.000800, scenario: 0, slope: -0.41866100692315644, fluctuations: 0.29\n",
      "step: 80210 loss: 0.646601 time elapsed: 106.9109 learning rate: 0.000800, scenario: 0, slope: -0.8110600864119755, fluctuations: 0.33\n",
      "step: 80220 loss: 0.648990 time elapsed: 106.9249 learning rate: 0.000800, scenario: 0, slope: -1.3168045905937327, fluctuations: 0.36\n",
      "step: 80230 loss: 0.618804 time elapsed: 106.9388 learning rate: 0.000800, scenario: 0, slope: -0.4738024867005587, fluctuations: 0.37\n",
      "step: 80240 loss: 0.610576 time elapsed: 106.9524 learning rate: 0.000800, scenario: 0, slope: -0.1611693897171968, fluctuations: 0.35\n",
      "step: 80250 loss: 0.604041 time elapsed: 106.9658 learning rate: 0.000800, scenario: 0, slope: -0.04989837606924866, fluctuations: 0.35\n",
      "step: 80260 loss: 0.598344 time elapsed: 106.9794 learning rate: 0.000800, scenario: 0, slope: -0.01744950305062471, fluctuations: 0.31\n",
      "step: 80270 loss: 0.594224 time elapsed: 106.9936 learning rate: 0.000800, scenario: 0, slope: -0.007974625293655035, fluctuations: 0.27\n",
      "step: 80280 loss: 0.590482 time elapsed: 107.0071 learning rate: 0.000800, scenario: 0, slope: -0.004007888710713228, fluctuations: 0.23\n",
      "step: 80290 loss: 0.587312 time elapsed: 107.0199 learning rate: 0.000800, scenario: 0, slope: -0.001560854923618108, fluctuations: 0.2\n",
      "step: 80300 loss: 0.584485 time elapsed: 107.0322 learning rate: 0.000800, scenario: 0, slope: -0.0009139981934384542, fluctuations: 0.17\n",
      "step: 80310 loss: 0.581957 time elapsed: 107.0448 learning rate: 0.000800, scenario: 0, slope: -0.0006198898705839422, fluctuations: 0.13\n",
      "step: 80320 loss: 0.579669 time elapsed: 107.0572 learning rate: 0.000800, scenario: 0, slope: -0.00044636113995681897, fluctuations: 0.1\n",
      "step: 80330 loss: 0.577582 time elapsed: 107.0694 learning rate: 0.000800, scenario: 0, slope: -0.0003659192050168104, fluctuations: 0.07\n",
      "step: 80340 loss: 0.575665 time elapsed: 107.0815 learning rate: 0.000800, scenario: 0, slope: -0.00031883412358739676, fluctuations: 0.03\n",
      "step: 80350 loss: 0.573892 time elapsed: 107.0938 learning rate: 0.000800, scenario: 0, slope: -0.0002826108384762305, fluctuations: 0.0\n",
      "step: 80360 loss: 0.572244 time elapsed: 107.1078 learning rate: 0.000800, scenario: 0, slope: -0.000251455338304523, fluctuations: 0.0\n",
      "step: 80370 loss: 0.570705 time elapsed: 107.1215 learning rate: 0.000800, scenario: 0, slope: -0.0002265224462988072, fluctuations: 0.0\n",
      "step: 80380 loss: 0.569261 time elapsed: 107.1356 learning rate: 0.000800, scenario: 0, slope: -0.0002061829807954537, fluctuations: 0.0\n",
      "step: 80390 loss: 0.567902 time elapsed: 107.1496 learning rate: 0.000800, scenario: 0, slope: -0.00018916589233200794, fluctuations: 0.0\n",
      "step: 80400 loss: 0.566617 time elapsed: 107.1632 learning rate: 0.000800, scenario: 0, slope: -0.0001760744531749142, fluctuations: 0.0\n",
      "step: 80410 loss: 0.565399 time elapsed: 107.1775 learning rate: 0.000800, scenario: 0, slope: -0.00016233706563320086, fluctuations: 0.0\n",
      "step: 80420 loss: 0.564241 time elapsed: 107.1914 learning rate: 0.000800, scenario: 0, slope: -0.000151589407759016, fluctuations: 0.0\n",
      "step: 80430 loss: 0.563137 time elapsed: 107.2051 learning rate: 0.000800, scenario: 0, slope: -0.0001421895105402261, fluctuations: 0.0\n",
      "step: 80440 loss: 0.562082 time elapsed: 107.2199 learning rate: 0.000800, scenario: 0, slope: -0.00013390580160569607, fluctuations: 0.0\n",
      "step: 80450 loss: 0.561072 time elapsed: 107.2342 learning rate: 0.000800, scenario: 0, slope: -0.00012655646779919857, fluctuations: 0.0\n",
      "step: 80460 loss: 0.560103 time elapsed: 107.2470 learning rate: 0.000800, scenario: 0, slope: -0.00011999639245200578, fluctuations: 0.0\n",
      "step: 80470 loss: 0.559172 time elapsed: 107.2594 learning rate: 0.000800, scenario: 0, slope: -0.00011410860699654073, fluctuations: 0.0\n",
      "step: 80480 loss: 0.558274 time elapsed: 107.2731 learning rate: 0.000800, scenario: 0, slope: -0.00010879781865975203, fluctuations: 0.0\n",
      "step: 80490 loss: 0.557409 time elapsed: 107.2856 learning rate: 0.000800, scenario: 0, slope: -0.00010398572241539373, fluctuations: 0.0\n",
      "step: 80500 loss: 0.556573 time elapsed: 107.2978 learning rate: 0.000800, scenario: 0, slope: -0.00010002736385262308, fluctuations: 0.0\n",
      "step: 80510 loss: 0.555764 time elapsed: 107.3110 learning rate: 0.000800, scenario: 0, slope: -9.560881386414475e-05, fluctuations: 0.0\n",
      "step: 80520 loss: 0.554980 time elapsed: 107.3253 learning rate: 0.000800, scenario: 0, slope: -9.194431626926028e-05, fluctuations: 0.0\n",
      "step: 80530 loss: 0.554220 time elapsed: 107.3389 learning rate: 0.000800, scenario: 0, slope: -8.857542108913365e-05, fluctuations: 0.0\n",
      "step: 80540 loss: 0.553482 time elapsed: 107.3525 learning rate: 0.000800, scenario: 0, slope: -8.546932281980693e-05, fluctuations: 0.0\n",
      "step: 80550 loss: 0.552764 time elapsed: 107.3661 learning rate: 0.000800, scenario: 0, slope: -8.259792165372993e-05, fluctuations: 0.0\n",
      "step: 80560 loss: 0.552065 time elapsed: 107.3798 learning rate: 0.000800, scenario: 0, slope: -7.99370133883506e-05, fluctuations: 0.0\n",
      "step: 80570 loss: 0.551385 time elapsed: 107.3933 learning rate: 0.000800, scenario: 0, slope: -7.746563846935341e-05, fluctuations: 0.0\n",
      "step: 80580 loss: 0.550721 time elapsed: 107.4071 learning rate: 0.000800, scenario: 0, slope: -7.516555565930091e-05, fluctuations: 0.0\n",
      "step: 80590 loss: 0.550073 time elapsed: 107.4211 learning rate: 0.000800, scenario: 0, slope: -7.302081390393233e-05, fluctuations: 0.0\n",
      "step: 80600 loss: 0.549440 time elapsed: 107.4339 learning rate: 0.000800, scenario: 0, slope: -7.121174743752033e-05, fluctuations: 0.0\n",
      "step: 80610 loss: 0.548821 time elapsed: 107.4465 learning rate: 0.000800, scenario: 0, slope: -6.914296075524471e-05, fluctuations: 0.0\n",
      "step: 80620 loss: 0.548215 time elapsed: 107.4590 learning rate: 0.000800, scenario: 0, slope: -6.738654396752082e-05, fluctuations: 0.0\n",
      "step: 80630 loss: 0.547621 time elapsed: 107.4714 learning rate: 0.000800, scenario: 0, slope: -6.573842085312505e-05, fluctuations: 0.0\n",
      "step: 80640 loss: 0.547040 time elapsed: 107.4837 learning rate: 0.000800, scenario: 0, slope: -6.418991003034513e-05, fluctuations: 0.0\n",
      "step: 80650 loss: 0.546469 time elapsed: 107.4968 learning rate: 0.000800, scenario: 0, slope: -6.273324039383782e-05, fluctuations: 0.0\n",
      "step: 80660 loss: 0.545909 time elapsed: 107.5097 learning rate: 0.000800, scenario: 0, slope: -6.136143361728535e-05, fluctuations: 0.0\n",
      "step: 80670 loss: 0.545359 time elapsed: 107.5233 learning rate: 0.000800, scenario: 0, slope: -6.0068204503336935e-05, fluctuations: 0.0\n",
      "step: 80680 loss: 0.544819 time elapsed: 107.5373 learning rate: 0.000800, scenario: 0, slope: -5.884787608941587e-05, fluctuations: 0.0\n",
      "step: 80690 loss: 0.544287 time elapsed: 107.5511 learning rate: 0.000800, scenario: 0, slope: -5.769530701038977e-05, fluctuations: 0.0\n",
      "step: 80700 loss: 0.543764 time elapsed: 107.5645 learning rate: 0.000800, scenario: 0, slope: -5.6712061936753146e-05, fluctuations: 0.0\n",
      "step: 80710 loss: 0.543249 time elapsed: 107.5787 learning rate: 0.000800, scenario: 0, slope: -5.557519351662352e-05, fluctuations: 0.0\n",
      "step: 80720 loss: 0.542741 time elapsed: 107.5922 learning rate: 0.000824, scenario: 1, slope: -5.459982354385748e-05, fluctuations: 0.0\n",
      "step: 80730 loss: 0.542207 time elapsed: 107.6061 learning rate: 0.000910, scenario: 1, slope: -5.3757787117032394e-05, fluctuations: 0.0\n",
      "step: 80740 loss: 0.541627 time elapsed: 107.6200 learning rate: 0.001005, scenario: 1, slope: -5.329022053336819e-05, fluctuations: 0.0\n",
      "step: 80750 loss: 0.540997 time elapsed: 107.6343 learning rate: 0.001110, scenario: 1, slope: -5.341767085162561e-05, fluctuations: 0.0\n",
      "step: 80760 loss: 0.540313 time elapsed: 107.6472 learning rate: 0.001227, scenario: 1, slope: -5.431068879913434e-05, fluctuations: 0.0\n",
      "step: 80770 loss: 0.539593 time elapsed: 107.6599 learning rate: 0.001239, scenario: 0, slope: -5.605143196854062e-05, fluctuations: 0.0\n",
      "step: 80780 loss: 0.538885 time elapsed: 107.6725 learning rate: 0.001239, scenario: 0, slope: -5.8417157178483784e-05, fluctuations: 0.0\n",
      "step: 80790 loss: 0.538190 time elapsed: 107.6848 learning rate: 0.001239, scenario: 0, slope: -6.10696110409725e-05, fluctuations: 0.0\n",
      "step: 80800 loss: 0.537508 time elapsed: 107.6967 learning rate: 0.001239, scenario: 0, slope: -6.34327181279074e-05, fluctuations: 0.0\n",
      "step: 80810 loss: 0.536838 time elapsed: 107.7097 learning rate: 0.001239, scenario: 0, slope: -6.59456996381393e-05, fluctuations: 0.0\n",
      "step: 80820 loss: 0.536179 time elapsed: 107.7223 learning rate: 0.001239, scenario: 0, slope: -6.755627016147297e-05, fluctuations: 0.0\n",
      "step: 80830 loss: 0.535530 time elapsed: 107.7357 learning rate: 0.001239, scenario: 0, slope: -6.831739145465576e-05, fluctuations: 0.0\n",
      "step: 80840 loss: 0.534890 time elapsed: 107.7495 learning rate: 0.001239, scenario: 0, slope: -6.828280341236316e-05, fluctuations: 0.0\n",
      "step: 80850 loss: 0.534260 time elapsed: 107.7630 learning rate: 0.001239, scenario: 0, slope: -6.762168582332857e-05, fluctuations: 0.0\n",
      "step: 80860 loss: 0.533637 time elapsed: 107.7766 learning rate: 0.001239, scenario: 0, slope: -6.659759636697306e-05, fluctuations: 0.0\n",
      "step: 80870 loss: 0.533022 time elapsed: 107.7902 learning rate: 0.001239, scenario: 0, slope: -6.553369459115658e-05, fluctuations: 0.0\n",
      "step: 80880 loss: 0.532415 time elapsed: 107.8040 learning rate: 0.001239, scenario: 0, slope: -6.454097542628028e-05, fluctuations: 0.0\n",
      "step: 80890 loss: 0.531814 time elapsed: 107.8175 learning rate: 0.001239, scenario: 0, slope: -6.361540367598693e-05, fluctuations: 0.0\n",
      "step: 80900 loss: 0.531219 time elapsed: 107.8309 learning rate: 0.001239, scenario: 0, slope: -6.283581170169765e-05, fluctuations: 0.0\n",
      "step: 80910 loss: 0.530631 time elapsed: 107.8448 learning rate: 0.001239, scenario: 0, slope: -6.194658712769524e-05, fluctuations: 0.0\n",
      "step: 80920 loss: 0.530048 time elapsed: 107.8576 learning rate: 0.001239, scenario: 0, slope: -6.11943749424861e-05, fluctuations: 0.0\n",
      "step: 80930 loss: 0.529470 time elapsed: 107.8698 learning rate: 0.001239, scenario: 0, slope: -6.0491552880957786e-05, fluctuations: 0.0\n",
      "step: 80940 loss: 0.528897 time elapsed: 107.8822 learning rate: 0.001239, scenario: 0, slope: -5.983448900374597e-05, fluctuations: 0.0\n",
      "step: 80950 loss: 0.528328 time elapsed: 107.8943 learning rate: 0.001239, scenario: 0, slope: -5.9219858810471174e-05, fluctuations: 0.0\n",
      "step: 80960 loss: 0.527764 time elapsed: 107.9066 learning rate: 0.001239, scenario: 0, slope: -5.8644616177091504e-05, fluctuations: 0.0\n",
      "step: 80970 loss: 0.527204 time elapsed: 107.9188 learning rate: 0.001239, scenario: 0, slope: -5.8105966312045775e-05, fluctuations: 0.0\n",
      "step: 80980 loss: 0.526647 time elapsed: 107.9311 learning rate: 0.001239, scenario: 0, slope: -5.7601341459149604e-05, fluctuations: 0.0\n",
      "step: 80990 loss: 0.526094 time elapsed: 107.9446 learning rate: 0.001239, scenario: 0, slope: -5.712837930631468e-05, fluctuations: 0.0\n",
      "step: 81000 loss: 0.525544 time elapsed: 107.9588 learning rate: 0.001239, scenario: 0, slope: -5.672798293164345e-05, fluctuations: 0.0\n",
      "step: 81010 loss: 0.524998 time elapsed: 107.9738 learning rate: 0.001239, scenario: 0, slope: -5.626890833942749e-05, fluctuations: 0.0\n",
      "step: 81020 loss: 0.524454 time elapsed: 107.9884 learning rate: 0.001239, scenario: 0, slope: -5.587854025317629e-05, fluctuations: 0.0\n",
      "step: 81030 loss: 0.523913 time elapsed: 108.0031 learning rate: 0.001239, scenario: 0, slope: -5.5512087678251086e-05, fluctuations: 0.0\n",
      "step: 81040 loss: 0.523375 time elapsed: 108.0180 learning rate: 0.001239, scenario: 0, slope: -5.516796726656335e-05, fluctuations: 0.0\n",
      "step: 81050 loss: 0.522839 time elapsed: 108.0324 learning rate: 0.001239, scenario: 0, slope: -5.484471335875912e-05, fluctuations: 0.0\n",
      "step: 81060 loss: 0.522305 time elapsed: 108.0465 learning rate: 0.001239, scenario: 0, slope: -5.454096819966082e-05, fluctuations: 0.0\n",
      "step: 81070 loss: 0.521774 time elapsed: 108.0609 learning rate: 0.001239, scenario: 0, slope: -5.4255473103227684e-05, fluctuations: 0.0\n",
      "step: 81080 loss: 0.521244 time elapsed: 108.0742 learning rate: 0.001239, scenario: 0, slope: -5.398706045571365e-05, fluctuations: 0.0\n",
      "step: 81090 loss: 0.520717 time elapsed: 108.0875 learning rate: 0.001239, scenario: 0, slope: -5.3734646460769286e-05, fluctuations: 0.0\n",
      "step: 81100 loss: 0.520191 time elapsed: 108.1004 learning rate: 0.001239, scenario: 0, slope: -5.352031928184706e-05, fluctuations: 0.0\n",
      "step: 81110 loss: 0.519667 time elapsed: 108.1135 learning rate: 0.001239, scenario: 0, slope: -5.327385934090511e-05, fluctuations: 0.0\n",
      "step: 81120 loss: 0.519145 time elapsed: 108.1269 learning rate: 0.001239, scenario: 0, slope: -5.306368121929713e-05, fluctuations: 0.0\n",
      "step: 81130 loss: 0.518624 time elapsed: 108.1401 learning rate: 0.001239, scenario: 0, slope: -5.286588125928294e-05, fluctuations: 0.0\n",
      "step: 81140 loss: 0.518104 time elapsed: 108.1545 learning rate: 0.001239, scenario: 0, slope: -5.267970666575369e-05, fluctuations: 0.0\n",
      "step: 81150 loss: 0.517586 time elapsed: 108.1700 learning rate: 0.001239, scenario: 0, slope: -5.250445655829442e-05, fluctuations: 0.0\n",
      "step: 81160 loss: 0.517069 time elapsed: 108.1850 learning rate: 0.001239, scenario: 0, slope: -5.2339478106326056e-05, fluctuations: 0.0\n",
      "step: 81170 loss: 0.516553 time elapsed: 108.1995 learning rate: 0.001239, scenario: 0, slope: -5.218416297572376e-05, fluctuations: 0.0\n",
      "step: 81180 loss: 0.516024 time elapsed: 108.2136 learning rate: 0.001355, scenario: 1, slope: -5.2063200669266346e-05, fluctuations: 0.0\n",
      "step: 81190 loss: 0.515446 time elapsed: 108.2280 learning rate: 0.001424, scenario: 0, slope: -5.217958516989728e-05, fluctuations: 0.0\n",
      "step: 81200 loss: 0.514860 time elapsed: 108.2424 learning rate: 0.001424, scenario: 0, slope: -5.258815611998981e-05, fluctuations: 0.0\n",
      "step: 81210 loss: 21.251843 time elapsed: 108.2574 learning rate: 0.001417, scenario: -1, slope: 0.015310498092037774, fluctuations: 0.0\n",
      "step: 81220 loss: 42.274457 time elapsed: 108.2719 learning rate: 0.001281, scenario: -1, slope: 0.22063949684360237, fluctuations: 0.03\n",
      "step: 81230 loss: 10.806380 time elapsed: 108.2854 learning rate: 0.001159, scenario: -1, slope: 0.22512648895988227, fluctuations: 0.08\n",
      "step: 81240 loss: 4.272375 time elapsed: 108.2986 learning rate: 0.001048, scenario: -1, slope: 0.19698233993493308, fluctuations: 0.12\n",
      "step: 81250 loss: 1.110135 time elapsed: 108.3114 learning rate: 0.000948, scenario: -1, slope: 0.1521172008847749, fluctuations: 0.17\n",
      "step: 81260 loss: 0.927225 time elapsed: 108.3250 learning rate: 0.000857, scenario: -1, slope: 0.08674031044041426, fluctuations: 0.21\n",
      "step: 81270 loss: 0.676435 time elapsed: 108.3385 learning rate: 0.000775, scenario: -1, slope: 0.018127600802082384, fluctuations: 0.25\n",
      "step: 81280 loss: 0.567793 time elapsed: 108.3518 learning rate: 0.000752, scenario: 0, slope: -0.053006563735459936, fluctuations: 0.29\n",
      "step: 81290 loss: 0.534186 time elapsed: 108.3663 learning rate: 0.000752, scenario: 0, slope: -0.1313320325461154, fluctuations: 0.33\n",
      "step: 81300 loss: 0.524397 time elapsed: 108.3807 learning rate: 0.000752, scenario: 0, slope: -0.2200230497205971, fluctuations: 0.37\n",
      "step: 81310 loss: 0.520019 time elapsed: 108.3958 learning rate: 0.000752, scenario: 0, slope: -0.3544905889470757, fluctuations: 0.41\n",
      "step: 81320 loss: 0.518574 time elapsed: 108.4104 learning rate: 0.000752, scenario: 0, slope: -0.0822719229147742, fluctuations: 0.41\n",
      "step: 81330 loss: 0.518093 time elapsed: 108.4248 learning rate: 0.000752, scenario: 0, slope: -0.030858098433521233, fluctuations: 0.4\n",
      "step: 81340 loss: 0.517650 time elapsed: 108.4398 learning rate: 0.000752, scenario: 0, slope: -0.006095919908438389, fluctuations: 0.38\n",
      "step: 81350 loss: 0.517226 time elapsed: 108.4545 learning rate: 0.000752, scenario: 0, slope: -0.0026252839704283787, fluctuations: 0.34\n",
      "step: 81360 loss: 0.516854 time elapsed: 108.4692 learning rate: 0.000752, scenario: 0, slope: -0.0008029550204067786, fluctuations: 0.3\n",
      "step: 81370 loss: 0.516493 time elapsed: 108.4856 learning rate: 0.000752, scenario: 0, slope: -0.000256222797818678, fluctuations: 0.26\n",
      "step: 81380 loss: 0.516145 time elapsed: 108.4993 learning rate: 0.000752, scenario: 0, slope: -0.0001363698097021537, fluctuations: 0.22\n",
      "step: 81390 loss: 0.515806 time elapsed: 108.5122 learning rate: 0.000752, scenario: 0, slope: -7.323064831683551e-05, fluctuations: 0.18\n",
      "step: 81400 loss: 0.515476 time elapsed: 108.5252 learning rate: 0.000745, scenario: 0, slope: -5.40161782230375e-05, fluctuations: 0.14\n",
      "step: 81410 loss: 0.515167 time elapsed: 108.5387 learning rate: 0.000673, scenario: -1, slope: -4.125903192004921e-05, fluctuations: 0.1\n",
      "step: 81420 loss: 0.514892 time elapsed: 108.5517 learning rate: 0.000609, scenario: -1, slope: -3.696139358274084e-05, fluctuations: 0.06\n",
      "step: 81430 loss: 0.514646 time elapsed: 108.5646 learning rate: 0.000551, scenario: -1, slope: -3.457313269869533e-05, fluctuations: 0.02\n",
      "step: 81440 loss: 0.514427 time elapsed: 108.5782 learning rate: 0.000498, scenario: -1, slope: -3.241009264606117e-05, fluctuations: 0.01\n",
      "step: 81450 loss: 0.514222 time elapsed: 108.5927 learning rate: 0.000526, scenario: 1, slope: -3.045255381931648e-05, fluctuations: 0.0\n",
      "step: 81460 loss: 0.514000 time elapsed: 108.6067 learning rate: 0.000581, scenario: 1, slope: -2.847006402928938e-05, fluctuations: 0.0\n",
      "step: 81470 loss: 0.513756 time elapsed: 108.6203 learning rate: 0.000642, scenario: 1, slope: -2.6698194575611578e-05, fluctuations: 0.0\n",
      "step: 81480 loss: 0.513490 time elapsed: 108.6338 learning rate: 0.000709, scenario: 1, slope: -2.531128506190619e-05, fluctuations: 0.0\n",
      "step: 81490 loss: 0.513197 time elapsed: 108.6475 learning rate: 0.000783, scenario: 1, slope: -2.4482387286528118e-05, fluctuations: 0.0\n",
      "step: 81500 loss: 0.512877 time elapsed: 108.6609 learning rate: 0.000857, scenario: 1, slope: -2.434633194280475e-05, fluctuations: 0.0\n",
      "step: 81510 loss: 0.512528 time elapsed: 108.6751 learning rate: 0.000946, scenario: 1, slope: -2.5074468595994273e-05, fluctuations: 0.0\n",
      "step: 81520 loss: 0.512147 time elapsed: 108.6892 learning rate: 0.001045, scenario: 1, slope: -2.653566208793616e-05, fluctuations: 0.0\n",
      "step: 81530 loss: 0.511730 time elapsed: 108.7026 learning rate: 0.001155, scenario: 1, slope: -2.864361233063384e-05, fluctuations: 0.0\n",
      "step: 81540 loss: 0.511273 time elapsed: 108.7156 learning rate: 0.001275, scenario: 1, slope: -3.1252432762095174e-05, fluctuations: 0.0\n",
      "step: 81550 loss: 0.510773 time elapsed: 108.7279 learning rate: 0.001409, scenario: 1, slope: -3.419239603395588e-05, fluctuations: 0.0\n",
      "step: 81560 loss: 0.510225 time elapsed: 108.7405 learning rate: 0.001556, scenario: 1, slope: -3.7406889647083006e-05, fluctuations: 0.0\n",
      "step: 81570 loss: 0.509626 time elapsed: 108.7530 learning rate: 0.001719, scenario: 1, slope: -4.0924725945674704e-05, fluctuations: 0.0\n",
      "step: 81580 loss: 0.766258 time elapsed: 108.7655 learning rate: 0.001871, scenario: -1, slope: 0.00013208602582851948, fluctuations: 0.0\n",
      "step: 81590 loss: 654.522154 time elapsed: 108.7777 learning rate: 0.001692, scenario: -1, slope: 1.387586105534958, fluctuations: 0.02\n",
      "step: 81600 loss: 50.635968 time elapsed: 108.7910 learning rate: 0.001546, scenario: -1, slope: 1.2380665512063025, fluctuations: 0.06\n",
      "step: 81610 loss: 57.385967 time elapsed: 108.8055 learning rate: 0.001398, scenario: -1, slope: 1.1224307227306, fluctuations: 0.11\n",
      "step: 81620 loss: 17.081335 time elapsed: 108.8191 learning rate: 0.001264, scenario: -1, slope: 0.863918966752091, fluctuations: 0.15\n",
      "step: 81630 loss: 2.441344 time elapsed: 108.8332 learning rate: 0.001143, scenario: -1, slope: 0.5505765604128144, fluctuations: 0.19\n",
      "step: 81640 loss: 2.401657 time elapsed: 108.8469 learning rate: 0.001034, scenario: -1, slope: 0.20992376959799097, fluctuations: 0.23\n",
      "step: 81650 loss: 0.853786 time elapsed: 108.8607 learning rate: 0.000983, scenario: 0, slope: -0.17733458205892355, fluctuations: 0.26\n",
      "step: 81660 loss: 0.943982 time elapsed: 108.8745 learning rate: 0.000983, scenario: 0, slope: -0.5767660537447863, fluctuations: 0.29\n",
      "step: 81670 loss: 0.809553 time elapsed: 108.8891 learning rate: 0.000983, scenario: 0, slope: -1.0555129421573468, fluctuations: 0.33\n",
      "step: 81680 loss: 0.727468 time elapsed: 108.9032 learning rate: 0.000983, scenario: 0, slope: -1.7197246900831877, fluctuations: 0.36\n",
      "step: 81690 loss: 0.718300 time elapsed: 108.9161 learning rate: 0.000983, scenario: 0, slope: -0.4872960185503163, fluctuations: 0.36\n",
      "step: 81700 loss: 0.698857 time elapsed: 108.9288 learning rate: 0.000983, scenario: 0, slope: -0.22380278947036425, fluctuations: 0.35\n",
      "step: 81710 loss: 0.685248 time elapsed: 108.9419 learning rate: 0.000983, scenario: 0, slope: -0.08722697928959142, fluctuations: 0.33\n",
      "step: 81720 loss: 0.675012 time elapsed: 108.9542 learning rate: 0.000983, scenario: 0, slope: -0.033707272653841276, fluctuations: 0.29\n",
      "step: 81730 loss: 0.665534 time elapsed: 108.9666 learning rate: 0.000983, scenario: 0, slope: -0.014952617570929513, fluctuations: 0.25\n",
      "step: 81740 loss: 0.657493 time elapsed: 108.9789 learning rate: 0.000983, scenario: 0, slope: -0.004908743318275555, fluctuations: 0.22\n",
      "step: 81750 loss: 0.650280 time elapsed: 108.9915 learning rate: 0.000983, scenario: 0, slope: -0.002969283771730576, fluctuations: 0.18\n",
      "step: 81760 loss: 0.643839 time elapsed: 109.0061 learning rate: 0.000983, scenario: 0, slope: -0.0016444239446943738, fluctuations: 0.15\n",
      "step: 81770 loss: 0.638053 time elapsed: 109.0206 learning rate: 0.000983, scenario: 0, slope: -0.0011303722881089771, fluctuations: 0.12\n",
      "step: 81780 loss: 0.632825 time elapsed: 109.0349 learning rate: 0.000983, scenario: 0, slope: -0.0009455560816484212, fluctuations: 0.08\n",
      "step: 81790 loss: 0.628087 time elapsed: 109.0486 learning rate: 0.000983, scenario: 0, slope: -0.0007969770444496664, fluctuations: 0.05\n",
      "step: 81800 loss: 0.623776 time elapsed: 109.0625 learning rate: 0.000983, scenario: 0, slope: -0.0007133556203377764, fluctuations: 0.02\n",
      "step: 81810 loss: 0.619838 time elapsed: 109.0766 learning rate: 0.000983, scenario: 0, slope: -0.0006335065312748788, fluctuations: 0.0\n",
      "step: 81820 loss: 0.616231 time elapsed: 109.0907 learning rate: 0.000983, scenario: 0, slope: -0.0005676139353950296, fluctuations: 0.0\n",
      "step: 81830 loss: 0.612914 time elapsed: 109.1047 learning rate: 0.000983, scenario: 0, slope: -0.0005119569890145758, fluctuations: 0.0\n",
      "step: 81840 loss: 0.609855 time elapsed: 109.1182 learning rate: 0.000983, scenario: 0, slope: -0.00046389259280045053, fluctuations: 0.0\n",
      "step: 81850 loss: 0.607024 time elapsed: 109.1311 learning rate: 0.000983, scenario: 0, slope: -0.0004220672233233146, fluctuations: 0.0\n",
      "step: 81860 loss: 0.604396 time elapsed: 109.1433 learning rate: 0.000983, scenario: 0, slope: -0.0003854546400144686, fluctuations: 0.0\n",
      "step: 81870 loss: 0.601950 time elapsed: 109.1559 learning rate: 0.000983, scenario: 0, slope: -0.000353264646013937, fluctuations: 0.0\n",
      "step: 81880 loss: 0.599664 time elapsed: 109.1687 learning rate: 0.000983, scenario: 0, slope: -0.00032487352965882243, fluctuations: 0.0\n",
      "step: 81890 loss: 0.597524 time elapsed: 109.1810 learning rate: 0.000983, scenario: 0, slope: -0.0002997597847801159, fluctuations: 0.0\n",
      "step: 81900 loss: 0.595513 time elapsed: 109.1931 learning rate: 0.000983, scenario: 0, slope: -0.00027959853974045734, fluctuations: 0.0\n",
      "step: 81910 loss: 0.593620 time elapsed: 109.2058 learning rate: 0.000983, scenario: 0, slope: -0.0002576894520332752, fluctuations: 0.0\n",
      "step: 81920 loss: 0.591831 time elapsed: 109.2197 learning rate: 0.000983, scenario: 0, slope: -0.0002400478938047334, fluctuations: 0.0\n",
      "step: 81930 loss: 0.590137 time elapsed: 109.2339 learning rate: 0.000983, scenario: 0, slope: -0.0002242930906996463, fluctuations: 0.0\n",
      "step: 81940 loss: 0.588528 time elapsed: 109.2474 learning rate: 0.000983, scenario: 0, slope: -0.00021019210390291586, fluctuations: 0.0\n",
      "step: 81950 loss: 0.586998 time elapsed: 109.2611 learning rate: 0.000983, scenario: 0, slope: -0.00019754371695125328, fluctuations: 0.0\n",
      "step: 81960 loss: 0.585538 time elapsed: 109.2746 learning rate: 0.000983, scenario: 0, slope: -0.0001861736696791764, fluctuations: 0.0\n",
      "step: 81970 loss: 0.584143 time elapsed: 109.2881 learning rate: 0.000983, scenario: 0, slope: -0.00017593069122385745, fluctuations: 0.0\n",
      "step: 81980 loss: 0.582806 time elapsed: 109.3013 learning rate: 0.000983, scenario: 0, slope: -0.0001666831930048717, fluctuations: 0.0\n",
      "step: 81990 loss: 0.581523 time elapsed: 109.3151 learning rate: 0.000983, scenario: 0, slope: -0.00015831648616962827, fluctuations: 0.0\n",
      "step: 82000 loss: 0.580289 time elapsed: 109.3286 learning rate: 0.000983, scenario: 0, slope: -0.00015145649404912703, fluctuations: 0.0\n",
      "step: 82010 loss: 0.579101 time elapsed: 109.3423 learning rate: 0.000983, scenario: 0, slope: -0.00014383744530513306, fluctuations: 0.0\n",
      "step: 82020 loss: 0.577954 time elapsed: 109.3548 learning rate: 0.000983, scenario: 0, slope: -0.00013756080123777668, fluctuations: 0.0\n",
      "step: 82030 loss: 0.576845 time elapsed: 109.3672 learning rate: 0.000983, scenario: 0, slope: -0.00013183318076190852, fluctuations: 0.0\n",
      "step: 82040 loss: 0.575771 time elapsed: 109.3795 learning rate: 0.000983, scenario: 0, slope: -0.00012659543149984382, fluctuations: 0.0\n",
      "step: 82050 loss: 0.574730 time elapsed: 109.3915 learning rate: 0.000983, scenario: 0, slope: -0.00012179550170264629, fluctuations: 0.0\n",
      "step: 82060 loss: 0.573720 time elapsed: 109.4035 learning rate: 0.000983, scenario: 0, slope: -0.0001173875236516231, fluctuations: 0.0\n",
      "step: 82070 loss: 0.572738 time elapsed: 109.4155 learning rate: 0.000983, scenario: 0, slope: -0.00011333102360167758, fluctuations: 0.0\n",
      "step: 82080 loss: 0.571782 time elapsed: 109.4288 learning rate: 0.000983, scenario: 0, slope: -0.00010959023951684155, fluctuations: 0.0\n",
      "step: 82090 loss: 0.570851 time elapsed: 109.4430 learning rate: 0.000983, scenario: 0, slope: -0.00010613353085029845, fluctuations: 0.0\n",
      "step: 82100 loss: 0.569942 time elapsed: 109.4564 learning rate: 0.000983, scenario: 0, slope: -0.0001032421430646955, fluctuations: 0.0\n",
      "step: 82110 loss: 0.569055 time elapsed: 109.4701 learning rate: 0.000983, scenario: 0, slope: -9.996338393426037e-05, fluctuations: 0.0\n",
      "step: 82120 loss: 0.568188 time elapsed: 109.4834 learning rate: 0.000983, scenario: 0, slope: -9.720299735506308e-05, fluctuations: 0.0\n",
      "step: 82130 loss: 0.567340 time elapsed: 109.4969 learning rate: 0.000983, scenario: 0, slope: -9.463206786923395e-05, fluctuations: 0.0\n",
      "step: 82140 loss: 0.566510 time elapsed: 109.5106 learning rate: 0.000983, scenario: 0, slope: -9.223310774200161e-05, fluctuations: 0.0\n",
      "step: 82150 loss: 0.565697 time elapsed: 109.5245 learning rate: 0.000983, scenario: 0, slope: -8.999052548971549e-05, fluctuations: 0.0\n",
      "step: 82160 loss: 0.564899 time elapsed: 109.5390 learning rate: 0.000983, scenario: 0, slope: -8.78904025823661e-05, fluctuations: 0.0\n",
      "step: 82170 loss: 0.564117 time elapsed: 109.5518 learning rate: 0.000983, scenario: 0, slope: -8.592029805414487e-05, fluctuations: 0.0\n",
      "step: 82180 loss: 0.563348 time elapsed: 109.5646 learning rate: 0.000983, scenario: 0, slope: -8.406907733023703e-05, fluctuations: 0.0\n",
      "step: 82190 loss: 0.562593 time elapsed: 109.5770 learning rate: 0.000983, scenario: 0, slope: -8.232676209401464e-05, fluctuations: 0.0\n",
      "step: 82200 loss: 0.561851 time elapsed: 109.5894 learning rate: 0.000983, scenario: 0, slope: -8.084437293048737e-05, fluctuations: 0.0\n",
      "step: 82210 loss: 0.561120 time elapsed: 109.6027 learning rate: 0.000983, scenario: 0, slope: -7.913394109799978e-05, fluctuations: 0.0\n",
      "step: 82220 loss: 0.560401 time elapsed: 109.6151 learning rate: 0.000983, scenario: 0, slope: -7.7668151259705e-05, fluctuations: 0.0\n",
      "step: 82230 loss: 0.559694 time elapsed: 109.6275 learning rate: 0.000983, scenario: 0, slope: -7.628050696406066e-05, fluctuations: 0.0\n",
      "step: 82240 loss: 0.558996 time elapsed: 109.6417 learning rate: 0.000983, scenario: 0, slope: -7.496512379673119e-05, fluctuations: 0.0\n",
      "step: 82250 loss: 0.558309 time elapsed: 109.6552 learning rate: 0.000983, scenario: 0, slope: -7.371668500968354e-05, fluctuations: 0.0\n",
      "step: 82260 loss: 0.557631 time elapsed: 109.6690 learning rate: 0.000983, scenario: 0, slope: -7.253037976813618e-05, fluctuations: 0.0\n",
      "step: 82270 loss: 0.556962 time elapsed: 109.6825 learning rate: 0.000983, scenario: 0, slope: -7.140184854510966e-05, fluctuations: 0.0\n",
      "step: 82280 loss: 0.556302 time elapsed: 109.6963 learning rate: 0.000983, scenario: 0, slope: -7.032713479263054e-05, fluctuations: 0.0\n",
      "step: 82290 loss: 0.555650 time elapsed: 109.7096 learning rate: 0.000983, scenario: 0, slope: -6.930264213177178e-05, fluctuations: 0.0\n",
      "step: 82300 loss: 0.555006 time elapsed: 109.7231 learning rate: 0.000983, scenario: 0, slope: -6.842082557005448e-05, fluctuations: 0.0\n",
      "step: 82310 loss: 0.554370 time elapsed: 109.7373 learning rate: 0.000983, scenario: 0, slope: -6.739151196557863e-05, fluctuations: 0.0\n",
      "step: 82320 loss: 0.553741 time elapsed: 109.7516 learning rate: 0.000983, scenario: 0, slope: -6.649916184098755e-05, fluctuations: 0.0\n",
      "step: 82330 loss: 0.553119 time elapsed: 109.7645 learning rate: 0.000983, scenario: 0, slope: -6.564555108873325e-05, fluctuations: 0.0\n",
      "step: 82340 loss: 0.552504 time elapsed: 109.7770 learning rate: 0.000983, scenario: 0, slope: -6.482839319545635e-05, fluctuations: 0.0\n",
      "step: 82350 loss: 0.551895 time elapsed: 109.7895 learning rate: 0.000983, scenario: 0, slope: -6.40455890353132e-05, fluctuations: 0.0\n",
      "step: 82360 loss: 0.551292 time elapsed: 109.8025 learning rate: 0.000983, scenario: 0, slope: -6.32952081359324e-05, fluctuations: 0.0\n",
      "step: 82370 loss: 0.550696 time elapsed: 109.8153 learning rate: 0.000983, scenario: 0, slope: -6.25754719830382e-05, fluctuations: 0.0\n",
      "step: 82380 loss: 0.550105 time elapsed: 109.8279 learning rate: 0.000983, scenario: 0, slope: -6.188473913134396e-05, fluctuations: 0.0\n",
      "step: 82390 loss: 0.549520 time elapsed: 109.8405 learning rate: 0.000983, scenario: 0, slope: -6.122149191853545e-05, fluctuations: 0.0\n",
      "step: 82400 loss: 0.548940 time elapsed: 109.8544 learning rate: 0.000983, scenario: 0, slope: -6.0646905663718935e-05, fluctuations: 0.0\n",
      "step: 82410 loss: 0.548366 time elapsed: 109.8695 learning rate: 0.000983, scenario: 0, slope: -5.997193275983912e-05, fluctuations: 0.0\n",
      "step: 82420 loss: 0.547796 time elapsed: 109.8839 learning rate: 0.000983, scenario: 0, slope: -5.9383103815427735e-05, fluctuations: 0.0\n",
      "step: 82430 loss: 0.547231 time elapsed: 109.8980 learning rate: 0.000983, scenario: 0, slope: -5.88167085619142e-05, fluctuations: 0.0\n",
      "step: 82440 loss: 0.546670 time elapsed: 109.9117 learning rate: 0.000983, scenario: 0, slope: -5.82716935764014e-05, fluctuations: 0.0\n",
      "step: 82450 loss: 0.546114 time elapsed: 109.9252 learning rate: 0.000983, scenario: 0, slope: -5.774707442643431e-05, fluctuations: 0.0\n",
      "step: 82460 loss: 0.545563 time elapsed: 109.9390 learning rate: 0.000983, scenario: 0, slope: -5.7241929582088554e-05, fluctuations: 0.0\n",
      "step: 82470 loss: 0.545015 time elapsed: 109.9525 learning rate: 0.000983, scenario: 0, slope: -5.675539495739784e-05, fluctuations: 0.0\n",
      "step: 82480 loss: 0.544471 time elapsed: 109.9662 learning rate: 0.000983, scenario: 0, slope: -5.628665901256689e-05, fluctuations: 0.0\n",
      "step: 82490 loss: 0.543932 time elapsed: 109.9787 learning rate: 0.000983, scenario: 0, slope: -5.5834958356354216e-05, fluctuations: 0.0\n",
      "step: 82500 loss: 0.543396 time elapsed: 109.9912 learning rate: 0.000983, scenario: 0, slope: -5.544239766874009e-05, fluctuations: 0.0\n",
      "step: 82510 loss: 0.542863 time elapsed: 110.0044 learning rate: 0.000983, scenario: 0, slope: -5.497982677720766e-05, fluctuations: 0.0\n",
      "step: 82520 loss: 0.542329 time elapsed: 110.0192 learning rate: 0.001044, scenario: 1, slope: -5.458129745837505e-05, fluctuations: 0.0\n",
      "step: 82530 loss: 0.541751 time elapsed: 110.0334 learning rate: 0.001153, scenario: 1, slope: -5.435339413387413e-05, fluctuations: 0.0\n",
      "step: 82540 loss: 0.541117 time elapsed: 110.0461 learning rate: 0.001274, scenario: 1, slope: -5.456459057531211e-05, fluctuations: 0.0\n",
      "step: 82550 loss: 0.540442 time elapsed: 110.0591 learning rate: 0.001286, scenario: 0, slope: -5.541260634953023e-05, fluctuations: 0.0\n",
      "step: 82560 loss: 0.539771 time elapsed: 110.0732 learning rate: 0.001286, scenario: 0, slope: -5.6810230127005234e-05, fluctuations: 0.0\n",
      "step: 82570 loss: 0.539106 time elapsed: 110.0875 learning rate: 0.001286, scenario: 0, slope: -5.855485009884757e-05, fluctuations: 0.0\n",
      "step: 82580 loss: 0.538446 time elapsed: 110.1016 learning rate: 0.001286, scenario: 0, slope: -6.044728621580565e-05, fluctuations: 0.0\n",
      "step: 82590 loss: 0.537792 time elapsed: 110.1153 learning rate: 0.001286, scenario: 0, slope: -6.229235424026225e-05, fluctuations: 0.0\n",
      "step: 82600 loss: 0.537142 time elapsed: 110.1288 learning rate: 0.001286, scenario: 0, slope: -6.375425334424353e-05, fluctuations: 0.0\n",
      "step: 82610 loss: 0.536498 time elapsed: 110.1433 learning rate: 0.001286, scenario: 0, slope: -6.507906918140077e-05, fluctuations: 0.0\n",
      "step: 82620 loss: 0.535858 time elapsed: 110.1572 learning rate: 0.001286, scenario: 0, slope: -6.5655822729988e-05, fluctuations: 0.0\n",
      "step: 82630 loss: 0.535223 time elapsed: 110.1714 learning rate: 0.001286, scenario: 0, slope: -6.562477422665563e-05, fluctuations: 0.0\n",
      "step: 82640 loss: 0.534592 time elapsed: 110.1858 learning rate: 0.001286, scenario: 0, slope: -6.52108234520674e-05, fluctuations: 0.0\n",
      "step: 82650 loss: 0.533965 time elapsed: 110.1985 learning rate: 0.001286, scenario: 0, slope: -6.471021231042541e-05, fluctuations: 0.0\n",
      "step: 82660 loss: 0.533343 time elapsed: 110.2107 learning rate: 0.001286, scenario: 0, slope: -6.422881445618645e-05, fluctuations: 0.0\n",
      "step: 82670 loss: 0.532724 time elapsed: 110.2233 learning rate: 0.001286, scenario: 0, slope: -6.376664322692806e-05, fluctuations: 0.0\n",
      "step: 82680 loss: 0.532109 time elapsed: 110.2357 learning rate: 0.001286, scenario: 0, slope: -6.332296648809461e-05, fluctuations: 0.0\n",
      "step: 82690 loss: 0.531497 time elapsed: 110.2483 learning rate: 0.001286, scenario: 0, slope: -6.289696526845404e-05, fluctuations: 0.0\n",
      "step: 82700 loss: 0.530889 time elapsed: 110.2609 learning rate: 0.001286, scenario: 0, slope: -6.252800066644278e-05, fluctuations: 0.0\n",
      "step: 82710 loss: 0.530284 time elapsed: 110.2760 learning rate: 0.001286, scenario: 0, slope: -6.209475077195308e-05, fluctuations: 0.0\n",
      "step: 82720 loss: 0.529682 time elapsed: 110.2898 learning rate: 0.001286, scenario: 0, slope: -6.171700278542897e-05, fluctuations: 0.0\n",
      "step: 82730 loss: 0.529083 time elapsed: 110.3040 learning rate: 0.001286, scenario: 0, slope: -6.135387013941875e-05, fluctuations: 0.0\n",
      "step: 82740 loss: 0.528488 time elapsed: 110.3179 learning rate: 0.001286, scenario: 0, slope: -6.100468318595361e-05, fluctuations: 0.0\n",
      "step: 82750 loss: 0.527895 time elapsed: 110.3314 learning rate: 0.001286, scenario: 0, slope: -6.066880881701165e-05, fluctuations: 0.0\n",
      "step: 82760 loss: 0.527304 time elapsed: 110.3456 learning rate: 0.001286, scenario: 0, slope: -6.0345648346087415e-05, fluctuations: 0.0\n",
      "step: 82770 loss: 0.526717 time elapsed: 110.3596 learning rate: 0.001286, scenario: 0, slope: -6.003463544418797e-05, fluctuations: 0.0\n",
      "step: 82780 loss: 0.526132 time elapsed: 110.3734 learning rate: 0.001286, scenario: 0, slope: -5.973523418720247e-05, fluctuations: 0.0\n",
      "step: 82790 loss: 0.525549 time elapsed: 110.3873 learning rate: 0.001286, scenario: 0, slope: -5.944693722523989e-05, fluctuations: 0.0\n",
      "step: 82800 loss: 0.524969 time elapsed: 110.3997 learning rate: 0.001286, scenario: 0, slope: -5.9196566480793745e-05, fluctuations: 0.0\n",
      "step: 82810 loss: 0.524391 time elapsed: 110.4128 learning rate: 0.001286, scenario: 0, slope: -5.8901759505782984e-05, fluctuations: 0.0\n",
      "step: 82820 loss: 0.523815 time elapsed: 110.4253 learning rate: 0.001286, scenario: 0, slope: -5.8643992074970596e-05, fluctuations: 0.0\n",
      "step: 82830 loss: 0.523241 time elapsed: 110.4380 learning rate: 0.001286, scenario: 0, slope: -5.839555269927261e-05, fluctuations: 0.0\n",
      "step: 82840 loss: 0.522669 time elapsed: 110.4505 learning rate: 0.001286, scenario: 0, slope: -5.81560533601255e-05, fluctuations: 0.0\n",
      "step: 82850 loss: 0.522100 time elapsed: 110.4632 learning rate: 0.001286, scenario: 0, slope: -5.792512587379161e-05, fluctuations: 0.0\n",
      "step: 82860 loss: 0.521532 time elapsed: 110.4769 learning rate: 0.001286, scenario: 0, slope: -5.7702420740799864e-05, fluctuations: 0.0\n",
      "step: 82870 loss: 0.520966 time elapsed: 110.4908 learning rate: 0.001286, scenario: 0, slope: -5.748760606727608e-05, fluctuations: 0.0\n",
      "step: 82880 loss: 0.520401 time elapsed: 110.5048 learning rate: 0.001286, scenario: 0, slope: -5.72803665522608e-05, fluctuations: 0.0\n",
      "step: 82890 loss: 0.519839 time elapsed: 110.5187 learning rate: 0.001286, scenario: 0, slope: -5.7080402537685885e-05, fluctuations: 0.0\n",
      "step: 82900 loss: 0.519278 time elapsed: 110.5329 learning rate: 0.001286, scenario: 0, slope: -5.6906419710238746e-05, fluctuations: 0.0\n",
      "step: 82910 loss: 0.518719 time elapsed: 110.5474 learning rate: 0.001286, scenario: 0, slope: -5.670117529226372e-05, fluctuations: 0.0\n",
      "step: 82920 loss: 0.518161 time elapsed: 110.5609 learning rate: 0.001286, scenario: 0, slope: -5.652138319527832e-05, fluctuations: 0.0\n",
      "step: 82930 loss: 0.517604 time elapsed: 110.5743 learning rate: 0.001286, scenario: 0, slope: -5.6347805950671884e-05, fluctuations: 0.0\n",
      "step: 82940 loss: 0.517051 time elapsed: 110.5899 learning rate: 0.001286, scenario: 0, slope: -5.617857349895587e-05, fluctuations: 0.0\n",
      "step: 82950 loss: 0.519444 time elapsed: 110.6028 learning rate: 0.001286, scenario: 0, slope: -5.283677006017417e-05, fluctuations: 0.0\n",
      "step: 82960 loss: 18.733220 time elapsed: 110.6156 learning rate: 0.001242, scenario: -1, slope: 0.019572716175105795, fluctuations: 0.0\n",
      "step: 82970 loss: 8.120762 time elapsed: 110.6282 learning rate: 0.001123, scenario: -1, slope: 0.07115131122969606, fluctuations: 0.02\n",
      "step: 82980 loss: 3.731508 time elapsed: 110.6408 learning rate: 0.001015, scenario: -1, slope: 0.06743229898646948, fluctuations: 0.05\n",
      "step: 82990 loss: 0.928883 time elapsed: 110.6532 learning rate: 0.000918, scenario: -1, slope: 0.05514482409457485, fluctuations: 0.09\n",
      "step: 83000 loss: 0.788831 time elapsed: 110.6655 learning rate: 0.000839, scenario: -1, slope: 0.042081204890183944, fluctuations: 0.13\n",
      "step: 83010 loss: 0.519767 time elapsed: 110.6808 learning rate: 0.000759, scenario: -1, slope: 0.02200241509082529, fluctuations: 0.18\n",
      "step: 83020 loss: 0.526166 time elapsed: 110.6948 learning rate: 0.000686, scenario: -1, slope: 0.0013799144978279743, fluctuations: 0.23\n",
      "step: 83030 loss: 0.517695 time elapsed: 110.7089 learning rate: 0.000686, scenario: 0, slope: -0.022110240691208195, fluctuations: 0.28\n",
      "step: 83040 loss: 0.515690 time elapsed: 110.7229 learning rate: 0.000686, scenario: 0, slope: -0.048557383396647384, fluctuations: 0.33\n",
      "step: 83050 loss: 0.515026 time elapsed: 110.7370 learning rate: 0.000686, scenario: 0, slope: -0.0800723319882019, fluctuations: 0.38\n",
      "step: 83060 loss: 0.514628 time elapsed: 110.7510 learning rate: 0.000686, scenario: 0, slope: -0.09137303461611629, fluctuations: 0.43\n",
      "step: 83070 loss: 0.514288 time elapsed: 110.7649 learning rate: 0.000686, scenario: 0, slope: -0.01903609921516793, fluctuations: 0.45\n",
      "step: 83080 loss: 0.513966 time elapsed: 110.7788 learning rate: 0.000686, scenario: 0, slope: -0.003829188270413142, fluctuations: 0.47\n",
      "step: 83090 loss: 0.513651 time elapsed: 110.7936 learning rate: 0.000686, scenario: 0, slope: -0.0013599436213537432, fluctuations: 0.44\n",
      "step: 83100 loss: 0.513341 time elapsed: 110.8067 learning rate: 0.000686, scenario: 0, slope: -0.0005508300325133497, fluctuations: 0.4\n",
      "step: 83110 loss: 0.513033 time elapsed: 110.8198 learning rate: 0.000686, scenario: 0, slope: -0.000175927789076277, fluctuations: 0.35\n",
      "step: 83120 loss: 0.512728 time elapsed: 110.8324 learning rate: 0.000686, scenario: 0, slope: -7.244611009261045e-05, fluctuations: 0.3\n",
      "step: 83130 loss: 0.512430 time elapsed: 110.8450 learning rate: 0.000640, scenario: -1, slope: -4.3553512019081594e-05, fluctuations: 0.25\n",
      "step: 83140 loss: 0.512159 time elapsed: 110.8574 learning rate: 0.000578, scenario: -1, slope: -3.482730772572944e-05, fluctuations: 0.2\n",
      "step: 83150 loss: 0.511914 time elapsed: 110.8706 learning rate: 0.000523, scenario: -1, slope: -3.159612868017381e-05, fluctuations: 0.15\n",
      "step: 83160 loss: 0.511693 time elapsed: 110.8847 learning rate: 0.000473, scenario: -1, slope: -2.985449530710478e-05, fluctuations: 0.1\n",
      "step: 83170 loss: 0.511494 time elapsed: 110.8988 learning rate: 0.000428, scenario: -1, slope: -2.8450372606011855e-05, fluctuations: 0.05\n",
      "step: 83180 loss: 0.511313 time elapsed: 110.9130 learning rate: 0.000393, scenario: 1, slope: -2.709588993889214e-05, fluctuations: 0.0\n",
      "step: 83190 loss: 0.511135 time elapsed: 110.9267 learning rate: 0.000434, scenario: 1, slope: -2.5350328377053396e-05, fluctuations: 0.0\n",
      "step: 83200 loss: 0.510938 time elapsed: 110.9406 learning rate: 0.000475, scenario: 1, slope: -2.3802699737296406e-05, fluctuations: 0.0\n",
      "step: 83210 loss: 0.510722 time elapsed: 110.9548 learning rate: 0.000524, scenario: 1, slope: -2.2156742766720928e-05, fluctuations: 0.0\n",
      "step: 83220 loss: 0.510484 time elapsed: 110.9686 learning rate: 0.000579, scenario: 1, slope: -2.1107974840201688e-05, fluctuations: 0.0\n",
      "step: 83230 loss: 0.510221 time elapsed: 110.9830 learning rate: 0.000640, scenario: 1, slope: -2.0691052788142603e-05, fluctuations: 0.0\n",
      "step: 83240 loss: 0.509931 time elapsed: 110.9962 learning rate: 0.000707, scenario: 1, slope: -2.0990714183079792e-05, fluctuations: 0.0\n",
      "step: 83250 loss: 0.509610 time elapsed: 111.0090 learning rate: 0.000781, scenario: 1, slope: -2.198819358075686e-05, fluctuations: 0.0\n",
      "step: 83260 loss: 0.509256 time elapsed: 111.0215 learning rate: 0.000862, scenario: 1, slope: -2.363019688111753e-05, fluctuations: 0.0\n",
      "step: 83270 loss: 0.508865 time elapsed: 111.0345 learning rate: 0.000952, scenario: 1, slope: -2.5830014008875594e-05, fluctuations: 0.0\n",
      "step: 83280 loss: 0.508434 time elapsed: 111.0470 learning rate: 0.001052, scenario: 1, slope: -2.846622151393985e-05, fluctuations: 0.0\n",
      "step: 83290 loss: 0.507958 time elapsed: 111.0595 learning rate: 0.001162, scenario: 1, slope: -3.141416305741602e-05, fluctuations: 0.0\n",
      "step: 83300 loss: 0.507434 time elapsed: 111.0717 learning rate: 0.001271, scenario: 1, slope: -3.432781027900468e-05, fluctuations: 0.0\n",
      "step: 83310 loss: 0.506862 time elapsed: 111.0866 learning rate: 0.001404, scenario: 1, slope: -3.8237888077168844e-05, fluctuations: 0.0\n",
      "step: 83320 loss: 0.546929 time elapsed: 111.1009 learning rate: 0.001551, scenario: 1, slope: -1.3533889040479356e-05, fluctuations: 0.0\n",
      "step: 83330 loss: 47.873466 time elapsed: 111.1149 learning rate: 0.001410, scenario: -1, slope: 0.7046029432369515, fluctuations: 0.02\n",
      "step: 83340 loss: 52.735530 time elapsed: 111.1288 learning rate: 0.001275, scenario: -1, slope: 0.7639985696838434, fluctuations: 0.07\n",
      "step: 83350 loss: 9.666316 time elapsed: 111.1430 learning rate: 0.001153, scenario: -1, slope: 0.7473828628505506, fluctuations: 0.11\n",
      "step: 83360 loss: 5.394643 time elapsed: 111.1567 learning rate: 0.001043, scenario: -1, slope: 0.590181163874208, fluctuations: 0.15\n",
      "step: 83370 loss: 1.176773 time elapsed: 111.1703 learning rate: 0.000943, scenario: -1, slope: 0.3827148761845948, fluctuations: 0.19\n",
      "step: 83380 loss: 2.278139 time elapsed: 111.1840 learning rate: 0.000853, scenario: -1, slope: 0.13052949813404913, fluctuations: 0.22\n",
      "step: 83390 loss: 0.678605 time elapsed: 111.1979 learning rate: 0.000811, scenario: 0, slope: -0.10777703491930758, fluctuations: 0.26\n",
      "step: 83400 loss: 0.791111 time elapsed: 111.2103 learning rate: 0.000811, scenario: 0, slope: -0.34622647612825613, fluctuations: 0.29\n",
      "step: 83410 loss: 0.651505 time elapsed: 111.2231 learning rate: 0.000811, scenario: 0, slope: -0.6900425768346926, fluctuations: 0.33\n",
      "step: 83420 loss: 0.615380 time elapsed: 111.2356 learning rate: 0.000811, scenario: 0, slope: -1.1211690344282645, fluctuations: 0.36\n",
      "step: 83430 loss: 0.606263 time elapsed: 111.2482 learning rate: 0.000811, scenario: 0, slope: -0.6681697676302155, fluctuations: 0.36\n",
      "step: 83440 loss: 0.592197 time elapsed: 111.2609 learning rate: 0.000811, scenario: 0, slope: -0.1821582649877884, fluctuations: 0.36\n",
      "step: 83450 loss: 0.587666 time elapsed: 111.2736 learning rate: 0.000811, scenario: 0, slope: -0.08427425076683194, fluctuations: 0.34\n",
      "step: 83460 loss: 0.582229 time elapsed: 111.2862 learning rate: 0.000811, scenario: 0, slope: -0.028710110911779826, fluctuations: 0.32\n",
      "step: 83470 loss: 0.578449 time elapsed: 111.2997 learning rate: 0.000811, scenario: 0, slope: -0.010094629637144215, fluctuations: 0.28\n",
      "step: 83480 loss: 0.575028 time elapsed: 111.3137 learning rate: 0.000811, scenario: 0, slope: -0.00312825499694207, fluctuations: 0.25\n",
      "step: 83490 loss: 0.572111 time elapsed: 111.3280 learning rate: 0.000811, scenario: 0, slope: -0.0015863493290934171, fluctuations: 0.22\n",
      "step: 83500 loss: 0.569504 time elapsed: 111.3417 learning rate: 0.000811, scenario: 0, slope: -0.0009913270597269918, fluctuations: 0.18\n",
      "step: 83510 loss: 0.567167 time elapsed: 111.3560 learning rate: 0.000811, scenario: 0, slope: -0.0005374547694030501, fluctuations: 0.15\n",
      "step: 83520 loss: 0.565044 time elapsed: 111.3699 learning rate: 0.000811, scenario: 0, slope: -0.0004271455940502824, fluctuations: 0.11\n",
      "step: 83530 loss: 0.563103 time elapsed: 111.3841 learning rate: 0.000811, scenario: 0, slope: -0.0003375339273465008, fluctuations: 0.08\n",
      "step: 83540 loss: 0.561314 time elapsed: 111.3979 learning rate: 0.000811, scenario: 0, slope: -0.0002929727072194281, fluctuations: 0.04\n",
      "step: 83550 loss: 0.559657 time elapsed: 111.4115 learning rate: 0.000811, scenario: 0, slope: -0.00025895398179912504, fluctuations: 0.01\n",
      "step: 83560 loss: 0.558114 time elapsed: 111.4242 learning rate: 0.000811, scenario: 0, slope: -0.00023255483446453946, fluctuations: 0.0\n",
      "step: 83570 loss: 0.556669 time elapsed: 111.4366 learning rate: 0.000811, scenario: 0, slope: -0.0002101127373220335, fluctuations: 0.0\n",
      "step: 83580 loss: 0.555313 time elapsed: 111.4486 learning rate: 0.000811, scenario: 0, slope: -0.00019176505653315193, fluctuations: 0.0\n",
      "step: 83590 loss: 0.554033 time elapsed: 111.4608 learning rate: 0.000811, scenario: 0, slope: -0.0001763890586643884, fluctuations: 0.0\n",
      "step: 83600 loss: 0.552823 time elapsed: 111.4729 learning rate: 0.000811, scenario: 0, slope: -0.00016452260292033143, fluctuations: 0.0\n",
      "step: 83610 loss: 0.551675 time elapsed: 111.4860 learning rate: 0.000811, scenario: 0, slope: -0.00015202631530113227, fluctuations: 0.0\n",
      "step: 83620 loss: 0.550582 time elapsed: 111.4982 learning rate: 0.000811, scenario: 0, slope: -0.0001422111349056982, fluctuations: 0.0\n",
      "step: 83630 loss: 0.549540 time elapsed: 111.5115 learning rate: 0.000811, scenario: 0, slope: -0.00013359460340658493, fluctuations: 0.0\n",
      "step: 83640 loss: 0.548544 time elapsed: 111.5253 learning rate: 0.000811, scenario: 0, slope: -0.00012597422613004684, fluctuations: 0.0\n",
      "step: 83650 loss: 0.547589 time elapsed: 111.5396 learning rate: 0.000811, scenario: 0, slope: -0.00011919071833271554, fluctuations: 0.0\n",
      "step: 83660 loss: 0.546673 time elapsed: 111.5531 learning rate: 0.000811, scenario: 0, slope: -0.00011311704099300057, fluctuations: 0.0\n",
      "step: 83670 loss: 0.545792 time elapsed: 111.5666 learning rate: 0.000811, scenario: 0, slope: -0.00010765050293558336, fluctuations: 0.0\n",
      "step: 83680 loss: 0.544944 time elapsed: 111.5803 learning rate: 0.000811, scenario: 0, slope: -0.00010270724115844829, fluctuations: 0.0\n",
      "step: 83690 loss: 0.544125 time elapsed: 111.5943 learning rate: 0.000811, scenario: 0, slope: -9.821808578244625e-05, fluctuations: 0.0\n",
      "step: 83700 loss: 0.543333 time elapsed: 111.6081 learning rate: 0.000811, scenario: 0, slope: -9.451835790469432e-05, fluctuations: 0.0\n",
      "step: 83710 loss: 0.542568 time elapsed: 111.6232 learning rate: 0.000811, scenario: 0, slope: -9.038128069805009e-05, fluctuations: 0.0\n",
      "step: 83720 loss: 0.541826 time elapsed: 111.6365 learning rate: 0.000811, scenario: 0, slope: -8.69446904453171e-05, fluctuations: 0.0\n",
      "step: 83730 loss: 0.541107 time elapsed: 111.6495 learning rate: 0.000811, scenario: 0, slope: -8.378113507959447e-05, fluctuations: 0.0\n",
      "step: 83740 loss: 0.540408 time elapsed: 111.6623 learning rate: 0.000811, scenario: 0, slope: -8.086102687432931e-05, fluctuations: 0.0\n",
      "step: 83750 loss: 0.539728 time elapsed: 111.6758 learning rate: 0.000811, scenario: 0, slope: -7.815891981698041e-05, fluctuations: 0.0\n",
      "step: 83760 loss: 0.539067 time elapsed: 111.6892 learning rate: 0.000811, scenario: 0, slope: -7.565281166215655e-05, fluctuations: 0.0\n",
      "step: 83770 loss: 0.538422 time elapsed: 111.7024 learning rate: 0.000811, scenario: 0, slope: -7.332358137124497e-05, fluctuations: 0.0\n",
      "step: 83780 loss: 0.537793 time elapsed: 111.7159 learning rate: 0.000811, scenario: 0, slope: -7.115453252090964e-05, fluctuations: 0.0\n",
      "step: 83790 loss: 0.537180 time elapsed: 111.7302 learning rate: 0.000811, scenario: 0, slope: -6.913102030474352e-05, fluctuations: 0.0\n",
      "step: 83800 loss: 0.536580 time elapsed: 111.7445 learning rate: 0.000811, scenario: 0, slope: -6.742359964258275e-05, fluctuations: 0.0\n",
      "step: 83810 loss: 0.535994 time elapsed: 111.7597 learning rate: 0.000811, scenario: 0, slope: -6.547049834253451e-05, fluctuations: 0.0\n",
      "step: 83820 loss: 0.535420 time elapsed: 111.7745 learning rate: 0.000811, scenario: 0, slope: -6.381195322461005e-05, fluctuations: 0.0\n",
      "step: 83830 loss: 0.534858 time elapsed: 111.7889 learning rate: 0.000811, scenario: 0, slope: -6.225548727558961e-05, fluctuations: 0.0\n",
      "step: 83840 loss: 0.534307 time elapsed: 111.8034 learning rate: 0.000811, scenario: 0, slope: -6.079303520174897e-05, fluctuations: 0.0\n",
      "step: 83850 loss: 0.533766 time elapsed: 111.8178 learning rate: 0.000811, scenario: 0, slope: -5.941736399440199e-05, fluctuations: 0.0\n",
      "step: 83860 loss: 0.533236 time elapsed: 111.8328 learning rate: 0.000811, scenario: 0, slope: -5.8121967182897154e-05, fluctuations: 0.0\n",
      "step: 83870 loss: 0.532715 time elapsed: 111.8465 learning rate: 0.000811, scenario: 0, slope: -5.690097481400606e-05, fluctuations: 0.0\n",
      "step: 83880 loss: 0.532202 time elapsed: 111.8602 learning rate: 0.000811, scenario: 0, slope: -5.5749076504776755e-05, fluctuations: 0.0\n",
      "step: 83890 loss: 0.531698 time elapsed: 111.8737 learning rate: 0.000811, scenario: 0, slope: -5.4661455412927236e-05, fluctuations: 0.0\n",
      "step: 83900 loss: 0.531203 time elapsed: 111.8864 learning rate: 0.000811, scenario: 0, slope: -5.3733925027511765e-05, fluctuations: 0.0\n",
      "step: 83910 loss: 0.530696 time elapsed: 111.9001 learning rate: 0.000896, scenario: 1, slope: -5.269594344805208e-05, fluctuations: 0.0\n",
      "step: 83920 loss: 0.530147 time elapsed: 111.9125 learning rate: 0.000990, scenario: 1, slope: -5.205314831521185e-05, fluctuations: 0.0\n",
      "step: 83930 loss: 0.529550 time elapsed: 111.9265 learning rate: 0.001093, scenario: 1, slope: -5.193257909148645e-05, fluctuations: 0.0\n",
      "step: 83940 loss: 0.528903 time elapsed: 111.9410 learning rate: 0.001208, scenario: 1, slope: -5.2513317894099584e-05, fluctuations: 0.0\n",
      "step: 83950 loss: 0.528204 time elapsed: 111.9558 learning rate: 0.001282, scenario: 0, slope: -5.3921616862146034e-05, fluctuations: 0.0\n",
      "step: 83960 loss: 0.527501 time elapsed: 111.9700 learning rate: 0.001282, scenario: 0, slope: -5.608936323601506e-05, fluctuations: 0.0\n",
      "step: 83970 loss: 0.526813 time elapsed: 111.9844 learning rate: 0.001282, scenario: 0, slope: -5.868813829629943e-05, fluctuations: 0.0\n",
      "step: 83980 loss: 0.526138 time elapsed: 111.9987 learning rate: 0.001282, scenario: 0, slope: -6.137905608185677e-05, fluctuations: 0.0\n",
      "step: 83990 loss: 0.525475 time elapsed: 112.0127 learning rate: 0.001282, scenario: 0, slope: -6.383728692612185e-05, fluctuations: 0.0\n",
      "step: 84000 loss: 0.524825 time elapsed: 112.0271 learning rate: 0.001282, scenario: 0, slope: -6.559293622132108e-05, fluctuations: 0.0\n",
      "step: 84010 loss: 0.524185 time elapsed: 112.0412 learning rate: 0.001282, scenario: 0, slope: -6.685858309427613e-05, fluctuations: 0.0\n",
      "step: 84020 loss: 0.523555 time elapsed: 112.0544 learning rate: 0.001282, scenario: 0, slope: -6.713501927015722e-05, fluctuations: 0.0\n",
      "step: 84030 loss: 0.522934 time elapsed: 112.0670 learning rate: 0.001282, scenario: 0, slope: -6.670434517377458e-05, fluctuations: 0.0\n",
      "step: 84040 loss: 0.522322 time elapsed: 112.0794 learning rate: 0.001282, scenario: 0, slope: -6.578099785063263e-05, fluctuations: 0.0\n",
      "step: 84050 loss: 0.521718 time elapsed: 112.0922 learning rate: 0.001282, scenario: 0, slope: -6.467116397720769e-05, fluctuations: 0.0\n",
      "step: 84060 loss: 0.521122 time elapsed: 112.1047 learning rate: 0.001282, scenario: 0, slope: -6.361723924145756e-05, fluctuations: 0.0\n",
      "step: 84070 loss: 0.520533 time elapsed: 112.1172 learning rate: 0.001282, scenario: 0, slope: -6.26376429953737e-05, fluctuations: 0.0\n",
      "step: 84080 loss: 0.519950 time elapsed: 112.1303 learning rate: 0.001282, scenario: 0, slope: -6.172699991922102e-05, fluctuations: 0.0\n",
      "step: 84090 loss: 0.519373 time elapsed: 112.1444 learning rate: 0.001282, scenario: 0, slope: -6.0879978260041845e-05, fluctuations: 0.0\n",
      "step: 84100 loss: 0.518803 time elapsed: 112.1580 learning rate: 0.001282, scenario: 0, slope: -6.016792255279757e-05, fluctuations: 0.0\n",
      "step: 84110 loss: 0.518237 time elapsed: 112.1726 learning rate: 0.001282, scenario: 0, slope: -5.9357248987412374e-05, fluctuations: 0.0\n",
      "step: 84120 loss: 0.517676 time elapsed: 112.1865 learning rate: 0.001282, scenario: 0, slope: -5.867278030031865e-05, fluctuations: 0.0\n",
      "step: 84130 loss: 0.517120 time elapsed: 112.2005 learning rate: 0.001282, scenario: 0, slope: -5.8034371289959146e-05, fluctuations: 0.0\n",
      "step: 84140 loss: 0.516569 time elapsed: 112.2142 learning rate: 0.001282, scenario: 0, slope: -5.743854558514704e-05, fluctuations: 0.0\n",
      "step: 84150 loss: 0.516021 time elapsed: 112.2281 learning rate: 0.001282, scenario: 0, slope: -5.6882127921563243e-05, fluctuations: 0.0\n",
      "step: 84160 loss: 0.515478 time elapsed: 112.2418 learning rate: 0.001282, scenario: 0, slope: -5.6362213983246995e-05, fluctuations: 0.0\n",
      "step: 84170 loss: 0.514938 time elapsed: 112.2562 learning rate: 0.001282, scenario: 0, slope: -5.5876143603655034e-05, fluctuations: 0.0\n",
      "step: 84180 loss: 0.514401 time elapsed: 112.2688 learning rate: 0.001282, scenario: 0, slope: -5.542147705559345e-05, fluctuations: 0.0\n",
      "step: 84190 loss: 0.513867 time elapsed: 112.2816 learning rate: 0.001282, scenario: 0, slope: -5.499597406436446e-05, fluctuations: 0.0\n",
      "step: 84200 loss: 0.513337 time elapsed: 112.2943 learning rate: 0.001282, scenario: 0, slope: -5.4636250622456216e-05, fluctuations: 0.0\n",
      "step: 84210 loss: 0.512809 time elapsed: 112.3075 learning rate: 0.001282, scenario: 0, slope: -5.4224385265960994e-05, fluctuations: 0.0\n",
      "step: 84220 loss: 0.512284 time elapsed: 112.3201 learning rate: 0.001282, scenario: 0, slope: -5.387465861713045e-05, fluctuations: 0.0\n",
      "step: 84230 loss: 0.511761 time elapsed: 112.3328 learning rate: 0.001282, scenario: 0, slope: -5.3546785838774775e-05, fluctuations: 0.0\n",
      "step: 84240 loss: 0.511240 time elapsed: 112.3466 learning rate: 0.001282, scenario: 0, slope: -5.3239281927827387e-05, fluctuations: 0.0\n",
      "step: 84250 loss: 0.510722 time elapsed: 112.3610 learning rate: 0.001282, scenario: 0, slope: -5.295077560698603e-05, fluctuations: 0.0\n",
      "step: 84260 loss: 0.510206 time elapsed: 112.3748 learning rate: 0.001282, scenario: 0, slope: -5.26799996963398e-05, fluctuations: 0.0\n",
      "step: 84270 loss: 0.509691 time elapsed: 112.3886 learning rate: 0.001282, scenario: 0, slope: -5.242578241084382e-05, fluctuations: 0.0\n",
      "step: 84280 loss: 0.509179 time elapsed: 112.4021 learning rate: 0.001282, scenario: 0, slope: -5.218703947744603e-05, fluctuations: 0.0\n",
      "step: 84290 loss: 0.508668 time elapsed: 112.4159 learning rate: 0.001282, scenario: 0, slope: -5.196276698191268e-05, fluctuations: 0.0\n",
      "step: 84300 loss: 0.508158 time elapsed: 112.4297 learning rate: 0.001282, scenario: 0, slope: -5.17725238374695e-05, fluctuations: 0.0\n",
      "step: 84310 loss: 0.507650 time elapsed: 112.4438 learning rate: 0.001282, scenario: 0, slope: -5.155398100168241e-05, fluctuations: 0.0\n",
      "step: 84320 loss: 0.507144 time elapsed: 112.4573 learning rate: 0.001282, scenario: 0, slope: -5.136780579880685e-05, fluctuations: 0.0\n",
      "step: 84330 loss: 0.506639 time elapsed: 112.4701 learning rate: 0.001282, scenario: 0, slope: -5.119276726506853e-05, fluctuations: 0.0\n",
      "step: 84340 loss: 0.506121 time elapsed: 112.4829 learning rate: 0.001402, scenario: 1, slope: -5.1052901139818946e-05, fluctuations: 0.0\n",
      "step: 84350 loss: 0.505552 time elapsed: 112.4954 learning rate: 0.001503, scenario: 0, slope: -5.114980591310731e-05, fluctuations: 0.0\n",
      "step: 84360 loss: 0.504966 time elapsed: 112.5079 learning rate: 0.001503, scenario: 0, slope: -5.164024551852376e-05, fluctuations: 0.0\n",
      "step: 84370 loss: 11.445643 time elapsed: 112.5201 learning rate: 0.001480, scenario: -1, slope: 0.008001624213829337, fluctuations: 0.0\n",
      "step: 84380 loss: 42.048027 time elapsed: 112.5322 learning rate: 0.001339, scenario: -1, slope: 0.25343089274287345, fluctuations: 0.03\n",
      "step: 84390 loss: 3.078069 time elapsed: 112.5460 learning rate: 0.001211, scenario: -1, slope: 0.2785127905305379, fluctuations: 0.07\n",
      "step: 84400 loss: 3.138914 time elapsed: 112.5600 learning rate: 0.001106, scenario: -1, slope: 0.25166897680933636, fluctuations: 0.11\n",
      "step: 84410 loss: 2.301867 time elapsed: 112.5745 learning rate: 0.001000, scenario: -1, slope: 0.18809970250162497, fluctuations: 0.16\n",
      "step: 84420 loss: 0.810892 time elapsed: 112.5883 learning rate: 0.000905, scenario: -1, slope: 0.11854298337385591, fluctuations: 0.21\n",
      "step: 84430 loss: 0.537699 time elapsed: 112.6021 learning rate: 0.000818, scenario: -1, slope: 0.030208426166604345, fluctuations: 0.25\n",
      "step: 84440 loss: 0.552374 time elapsed: 112.6156 learning rate: 0.000794, scenario: 0, slope: -0.060905726055289544, fluctuations: 0.29\n",
      "step: 84450 loss: 0.527414 time elapsed: 112.6295 learning rate: 0.000794, scenario: 0, slope: -0.15972592479852332, fluctuations: 0.33\n",
      "step: 84460 loss: 0.516797 time elapsed: 112.6435 learning rate: 0.000794, scenario: 0, slope: -0.2801928344419868, fluctuations: 0.37\n",
      "step: 84470 loss: 0.513807 time elapsed: 112.6596 learning rate: 0.000794, scenario: 0, slope: -0.43379958090882403, fluctuations: 0.4\n",
      "step: 84480 loss: 0.511097 time elapsed: 112.6734 learning rate: 0.000794, scenario: 0, slope: -0.12748808793579813, fluctuations: 0.4\n",
      "step: 84490 loss: 0.510032 time elapsed: 112.6870 learning rate: 0.000794, scenario: 0, slope: -0.03595833815073427, fluctuations: 0.4\n",
      "step: 84500 loss: 0.509382 time elapsed: 112.6999 learning rate: 0.000794, scenario: 0, slope: -0.011169579508820525, fluctuations: 0.4\n",
      "step: 84510 loss: 0.508894 time elapsed: 112.7130 learning rate: 0.000794, scenario: 0, slope: -0.0028844092939471952, fluctuations: 0.36\n",
      "step: 84520 loss: 0.508472 time elapsed: 112.7261 learning rate: 0.000794, scenario: 0, slope: -0.0009130940865606616, fluctuations: 0.32\n",
      "step: 84530 loss: 0.508077 time elapsed: 112.7389 learning rate: 0.000794, scenario: 0, slope: -0.0004822971419211094, fluctuations: 0.27\n",
      "step: 84540 loss: 0.507705 time elapsed: 112.7525 learning rate: 0.000794, scenario: 0, slope: -0.00013941468239117707, fluctuations: 0.24\n",
      "step: 84550 loss: 0.507346 time elapsed: 112.7669 learning rate: 0.000794, scenario: 0, slope: -7.985284950311341e-05, fluctuations: 0.2\n",
      "step: 84560 loss: 0.506999 time elapsed: 112.7812 learning rate: 0.000794, scenario: 0, slope: -5.449059220371588e-05, fluctuations: 0.16\n",
      "step: 84570 loss: 0.506666 time elapsed: 112.7956 learning rate: 0.000733, scenario: -1, slope: -4.363893332708471e-05, fluctuations: 0.12\n",
      "step: 84580 loss: 0.506370 time elapsed: 112.8094 learning rate: 0.000662, scenario: -1, slope: -3.9348722776253565e-05, fluctuations: 0.08\n",
      "step: 84590 loss: 0.506107 time elapsed: 112.8231 learning rate: 0.000599, scenario: -1, slope: -3.6630728570173145e-05, fluctuations: 0.04\n",
      "step: 84600 loss: 0.505873 time elapsed: 112.8367 learning rate: 0.000556, scenario: 1, slope: -3.4953953446120526e-05, fluctuations: 0.0\n",
      "step: 84610 loss: 0.505642 time elapsed: 112.8508 learning rate: 0.000614, scenario: 1, slope: -3.2515205459371545e-05, fluctuations: 0.0\n",
      "step: 84620 loss: 0.505389 time elapsed: 112.8643 learning rate: 0.000678, scenario: 1, slope: -3.050927851583166e-05, fluctuations: 0.0\n",
      "step: 84630 loss: 0.505113 time elapsed: 112.8779 learning rate: 0.000749, scenario: 1, slope: -2.881108188193401e-05, fluctuations: 0.0\n",
      "step: 84640 loss: 0.504811 time elapsed: 112.8914 learning rate: 0.000827, scenario: 1, slope: -2.75725791840214e-05, fluctuations: 0.0\n",
      "step: 84650 loss: 0.504481 time elapsed: 112.9053 learning rate: 0.000914, scenario: 1, slope: -2.6948612430191972e-05, fluctuations: 0.0\n",
      "step: 84660 loss: 0.504121 time elapsed: 112.9191 learning rate: 0.001009, scenario: 1, slope: -2.707682629360341e-05, fluctuations: 0.0\n",
      "step: 84670 loss: 0.503727 time elapsed: 112.9319 learning rate: 0.001115, scenario: 1, slope: -2.8062477886630363e-05, fluctuations: 0.0\n",
      "step: 84680 loss: 0.503298 time elapsed: 112.9443 learning rate: 0.001232, scenario: 1, slope: -2.986252855557508e-05, fluctuations: 0.0\n",
      "step: 84690 loss: 0.502828 time elapsed: 112.9572 learning rate: 0.001361, scenario: 1, slope: -3.232951308133868e-05, fluctuations: 0.0\n",
      "step: 84700 loss: 0.502316 time elapsed: 112.9712 learning rate: 0.001488, scenario: 1, slope: -3.496831424269362e-05, fluctuations: 0.0\n",
      "step: 84710 loss: 0.501761 time elapsed: 112.9856 learning rate: 0.001644, scenario: 1, slope: -3.8524725291612914e-05, fluctuations: 0.0\n",
      "step: 84720 loss: 0.501156 time elapsed: 112.9995 learning rate: 0.001816, scenario: 1, slope: -4.203768482529614e-05, fluctuations: 0.0\n",
      "step: 84730 loss: 0.695454 time elapsed: 113.0132 learning rate: 0.001976, scenario: -1, slope: 8.981976128958365e-05, fluctuations: 0.0\n",
      "step: 84740 loss: 637.768082 time elapsed: 113.0268 learning rate: 0.001787, scenario: -1, slope: 1.4067758267149333, fluctuations: 0.02\n",
      "step: 84750 loss: 198.240987 time elapsed: 113.0405 learning rate: 0.001616, scenario: -1, slope: 1.3322626036933227, fluctuations: 0.07\n",
      "step: 84760 loss: 55.962075 time elapsed: 113.0542 learning rate: 0.001462, scenario: -1, slope: 1.1997451291965788, fluctuations: 0.11\n",
      "step: 84770 loss: 20.131733 time elapsed: 113.0676 learning rate: 0.001322, scenario: -1, slope: 0.9229155995859027, fluctuations: 0.15\n",
      "step: 84780 loss: 6.150080 time elapsed: 113.0820 learning rate: 0.001195, scenario: -1, slope: 0.582009881965803, fluctuations: 0.19\n",
      "step: 84790 loss: 1.515761 time elapsed: 113.0947 learning rate: 0.001081, scenario: -1, slope: 0.21656180539434103, fluctuations: 0.23\n",
      "step: 84800 loss: 1.286003 time elapsed: 113.1074 learning rate: 0.001039, scenario: 0, slope: -0.14552498298744995, fluctuations: 0.26\n",
      "step: 84810 loss: 1.028443 time elapsed: 113.1205 learning rate: 0.001039, scenario: 0, slope: -0.6179180464948916, fluctuations: 0.29\n",
      "step: 84820 loss: 0.761474 time elapsed: 113.1329 learning rate: 0.001039, scenario: 0, slope: -1.1313859488882148, fluctuations: 0.33\n",
      "step: 84830 loss: 0.758267 time elapsed: 113.1451 learning rate: 0.001039, scenario: 0, slope: -1.8491740239993606, fluctuations: 0.36\n",
      "step: 84840 loss: 0.725181 time elapsed: 113.1576 learning rate: 0.001039, scenario: 0, slope: -0.6186860181844678, fluctuations: 0.37\n",
      "step: 84850 loss: 0.703933 time elapsed: 113.1712 learning rate: 0.001039, scenario: 0, slope: -0.291911952509018, fluctuations: 0.35\n",
      "step: 84860 loss: 0.692164 time elapsed: 113.1854 learning rate: 0.001039, scenario: 0, slope: -0.08126386557088937, fluctuations: 0.34\n",
      "step: 84870 loss: 0.680129 time elapsed: 113.1992 learning rate: 0.001039, scenario: 0, slope: -0.028684645743356964, fluctuations: 0.3\n",
      "step: 84880 loss: 0.670668 time elapsed: 113.2128 learning rate: 0.001039, scenario: 0, slope: -0.01331939816436232, fluctuations: 0.26\n",
      "step: 84890 loss: 0.662135 time elapsed: 113.2264 learning rate: 0.001039, scenario: 0, slope: -0.005209992634176193, fluctuations: 0.23\n",
      "step: 84900 loss: 0.654628 time elapsed: 113.2402 learning rate: 0.001039, scenario: 0, slope: -0.0028539862975748456, fluctuations: 0.2\n",
      "step: 84910 loss: 0.647925 time elapsed: 113.2554 learning rate: 0.001039, scenario: 0, slope: -0.001612709743926527, fluctuations: 0.16\n",
      "step: 84920 loss: 0.641896 time elapsed: 113.2703 learning rate: 0.001039, scenario: 0, slope: -0.001206854285325454, fluctuations: 0.13\n",
      "step: 84930 loss: 0.636457 time elapsed: 113.2849 learning rate: 0.001039, scenario: 0, slope: -0.000978729755239543, fluctuations: 0.09\n",
      "step: 84940 loss: 0.631524 time elapsed: 113.2977 learning rate: 0.001039, scenario: 0, slope: -0.0008254927131782572, fluctuations: 0.06\n",
      "step: 84950 loss: 0.627034 time elapsed: 113.3122 learning rate: 0.001039, scenario: 0, slope: -0.0007375035521273935, fluctuations: 0.02\n",
      "step: 84960 loss: 0.622930 time elapsed: 113.3264 learning rate: 0.001039, scenario: 0, slope: -0.0006604676345162129, fluctuations: 0.0\n",
      "step: 84970 loss: 0.619166 time elapsed: 113.3407 learning rate: 0.001039, scenario: 0, slope: -0.0005916477247378197, fluctuations: 0.0\n",
      "step: 84980 loss: 0.615700 time elapsed: 113.3546 learning rate: 0.001039, scenario: 0, slope: -0.0005333548835610797, fluctuations: 0.0\n",
      "step: 84990 loss: 0.612499 time elapsed: 113.3681 learning rate: 0.001039, scenario: 0, slope: -0.00048333069044453674, fluctuations: 0.0\n",
      "step: 85000 loss: 0.609531 time elapsed: 113.3826 learning rate: 0.001039, scenario: 0, slope: -0.00044402242363825406, fluctuations: 0.0\n",
      "step: 85010 loss: 0.606771 time elapsed: 113.3973 learning rate: 0.001039, scenario: 0, slope: -0.00040208469155954084, fluctuations: 0.0\n",
      "step: 85020 loss: 0.604196 time elapsed: 113.4119 learning rate: 0.001039, scenario: 0, slope: -0.0003688956607187766, fluctuations: 0.0\n",
      "step: 85030 loss: 0.601786 time elapsed: 113.4266 learning rate: 0.001039, scenario: 0, slope: -0.0003396937648319901, fluctuations: 0.0\n",
      "step: 85040 loss: 0.599523 time elapsed: 113.4415 learning rate: 0.001039, scenario: 0, slope: -0.00031391926622669977, fluctuations: 0.0\n",
      "step: 85050 loss: 0.597393 time elapsed: 113.4561 learning rate: 0.001039, scenario: 0, slope: -0.0002911015604746605, fluctuations: 0.0\n",
      "step: 85060 loss: 0.595383 time elapsed: 113.4710 learning rate: 0.001039, scenario: 0, slope: -0.0002708446000866164, fluctuations: 0.0\n",
      "step: 85070 loss: 0.593479 time elapsed: 113.4855 learning rate: 0.001039, scenario: 0, slope: -0.0002528117978122895, fluctuations: 0.0\n",
      "step: 85080 loss: 0.591673 time elapsed: 113.5001 learning rate: 0.001039, scenario: 0, slope: -0.0002367162073985952, fluctuations: 0.0\n",
      "step: 85090 loss: 0.589955 time elapsed: 113.5130 learning rate: 0.001039, scenario: 0, slope: -0.00022231214087283936, fluctuations: 0.0\n",
      "step: 85100 loss: 0.588316 time elapsed: 113.5270 learning rate: 0.001039, scenario: 0, slope: -0.00021061974697086022, fluctuations: 0.0\n",
      "step: 85110 loss: 0.586751 time elapsed: 113.5415 learning rate: 0.001039, scenario: 0, slope: -0.0001977640279035617, fluctuations: 0.0\n",
      "step: 85120 loss: 0.585252 time elapsed: 113.5553 learning rate: 0.001039, scenario: 0, slope: -0.00018728148949133887, fluctuations: 0.0\n",
      "step: 85130 loss: 0.583813 time elapsed: 113.5689 learning rate: 0.001039, scenario: 0, slope: -0.00017780531893401717, fluctuations: 0.0\n",
      "step: 85140 loss: 0.582430 time elapsed: 113.5830 learning rate: 0.001039, scenario: 0, slope: -0.00016921783397940918, fluctuations: 0.0\n",
      "step: 85150 loss: 0.581099 time elapsed: 113.5978 learning rate: 0.001039, scenario: 0, slope: -0.00016141676499234066, fluctuations: 0.0\n",
      "step: 85160 loss: 0.579815 time elapsed: 113.6128 learning rate: 0.001039, scenario: 0, slope: -0.0001543130572882349, fluctuations: 0.0\n",
      "step: 85170 loss: 0.578574 time elapsed: 113.6278 learning rate: 0.001039, scenario: 0, slope: -0.0001478290128817195, fluctuations: 0.0\n",
      "step: 85180 loss: 0.577373 time elapsed: 113.6423 learning rate: 0.001039, scenario: 0, slope: -0.00014189671460911288, fluctuations: 0.0\n",
      "step: 85190 loss: 0.576210 time elapsed: 113.6571 learning rate: 0.001039, scenario: 0, slope: -0.00013645668620001627, fluctuations: 0.0\n",
      "step: 85200 loss: 0.575081 time elapsed: 113.6716 learning rate: 0.001039, scenario: 0, slope: -0.00013193829201062907, fluctuations: 0.0\n",
      "step: 85210 loss: 0.573985 time elapsed: 113.6867 learning rate: 0.001039, scenario: 0, slope: -0.00012685105197977802, fluctuations: 0.0\n",
      "step: 85220 loss: 0.572918 time elapsed: 113.7013 learning rate: 0.001039, scenario: 0, slope: -0.00012259922476778247, fluctuations: 0.0\n",
      "step: 85230 loss: 0.571880 time elapsed: 113.7149 learning rate: 0.001039, scenario: 0, slope: -0.0001186656724136156, fluctuations: 0.0\n",
      "step: 85240 loss: 0.570868 time elapsed: 113.7281 learning rate: 0.001039, scenario: 0, slope: -0.00011501895345779847, fluctuations: 0.0\n",
      "step: 85250 loss: 0.569881 time elapsed: 113.7422 learning rate: 0.001039, scenario: 0, slope: -0.00011163125040468869, fluctuations: 0.0\n",
      "step: 85260 loss: 0.568916 time elapsed: 113.7555 learning rate: 0.001039, scenario: 0, slope: -0.00010847791184221088, fluctuations: 0.0\n",
      "step: 85270 loss: 0.567974 time elapsed: 113.7687 learning rate: 0.001039, scenario: 0, slope: -0.00010553705668683835, fluctuations: 0.0\n",
      "step: 85280 loss: 0.567052 time elapsed: 113.7811 learning rate: 0.001039, scenario: 0, slope: -0.00010278923155168558, fluctuations: 0.0\n",
      "step: 85290 loss: 0.566149 time elapsed: 113.7948 learning rate: 0.001039, scenario: 0, slope: -0.00010021711362644756, fluctuations: 0.0\n",
      "step: 85300 loss: 0.565265 time elapsed: 113.8102 learning rate: 0.001039, scenario: 0, slope: -9.803963250919753e-05, fluctuations: 0.0\n",
      "step: 85310 loss: 0.564397 time elapsed: 113.8257 learning rate: 0.001039, scenario: 0, slope: -9.553984629515622e-05, fluctuations: 0.0\n",
      "step: 85320 loss: 0.563547 time elapsed: 113.8405 learning rate: 0.001039, scenario: 0, slope: -9.340854492080939e-05, fluctuations: 0.0\n",
      "step: 85330 loss: 0.562711 time elapsed: 113.8552 learning rate: 0.001039, scenario: 0, slope: -9.140028069551701e-05, fluctuations: 0.0\n",
      "step: 85340 loss: 0.561891 time elapsed: 113.8701 learning rate: 0.001039, scenario: 0, slope: -8.950511872418047e-05, fluctuations: 0.0\n",
      "step: 85350 loss: 0.561084 time elapsed: 113.8846 learning rate: 0.001039, scenario: 0, slope: -8.771412665977296e-05, fluctuations: 0.0\n",
      "step: 85360 loss: 0.560291 time elapsed: 113.8997 learning rate: 0.001039, scenario: 0, slope: -8.601926051741701e-05, fluctuations: 0.0\n",
      "step: 85370 loss: 0.559511 time elapsed: 113.9144 learning rate: 0.001039, scenario: 0, slope: -8.441326452403085e-05, fluctuations: 0.0\n",
      "step: 85380 loss: 0.558743 time elapsed: 113.9285 learning rate: 0.001039, scenario: 0, slope: -8.288958316668779e-05, fluctuations: 0.0\n",
      "step: 85390 loss: 0.557986 time elapsed: 113.9428 learning rate: 0.001039, scenario: 0, slope: -8.144228385675188e-05, fluctuations: 0.0\n",
      "step: 85400 loss: 0.557240 time elapsed: 113.9561 learning rate: 0.001039, scenario: 0, slope: -8.020056594356847e-05, fluctuations: 0.0\n",
      "step: 85410 loss: 0.556505 time elapsed: 113.9697 learning rate: 0.001039, scenario: 0, slope: -7.875581521106142e-05, fluctuations: 0.0\n",
      "step: 85420 loss: 0.555780 time elapsed: 113.9831 learning rate: 0.001039, scenario: 0, slope: -7.750732189728933e-05, fluctuations: 0.0\n",
      "step: 85430 loss: 0.555065 time elapsed: 113.9962 learning rate: 0.001039, scenario: 0, slope: -7.631646292189935e-05, fluctuations: 0.0\n",
      "step: 85440 loss: 0.554359 time elapsed: 114.0102 learning rate: 0.001039, scenario: 0, slope: -7.517954598692843e-05, fluctuations: 0.0\n",
      "step: 85450 loss: 0.553662 time elapsed: 114.0253 learning rate: 0.001039, scenario: 0, slope: -7.409319582639987e-05, fluctuations: 0.0\n",
      "step: 85460 loss: 0.552973 time elapsed: 114.0407 learning rate: 0.001039, scenario: 0, slope: -7.305432171384431e-05, fluctuations: 0.0\n",
      "step: 85470 loss: 0.552293 time elapsed: 114.0554 learning rate: 0.001039, scenario: 0, slope: -7.2060088625584e-05, fluctuations: 0.0\n",
      "step: 85480 loss: 0.551621 time elapsed: 114.0697 learning rate: 0.001039, scenario: 0, slope: -7.110789162120571e-05, fluctuations: 0.0\n",
      "step: 85490 loss: 0.550956 time elapsed: 114.0842 learning rate: 0.001039, scenario: 0, slope: -7.019533305867572e-05, fluctuations: 0.0\n",
      "step: 85500 loss: 0.550298 time elapsed: 114.0979 learning rate: 0.001039, scenario: 0, slope: -6.940609056931253e-05, fluctuations: 0.0\n",
      "step: 85510 loss: 0.549647 time elapsed: 114.1131 learning rate: 0.001039, scenario: 0, slope: -6.848045768434189e-05, fluctuations: 0.0\n",
      "step: 85520 loss: 0.549004 time elapsed: 114.1277 learning rate: 0.001039, scenario: 0, slope: -6.767421030268556e-05, fluctuations: 0.0\n",
      "step: 85530 loss: 0.548366 time elapsed: 114.1455 learning rate: 0.001039, scenario: 0, slope: -6.689970970051918e-05, fluctuations: 0.0\n",
      "step: 85540 loss: 0.547735 time elapsed: 114.1594 learning rate: 0.001039, scenario: 0, slope: -6.615533095985068e-05, fluctuations: 0.0\n",
      "step: 85550 loss: 0.547110 time elapsed: 114.1736 learning rate: 0.001039, scenario: 0, slope: -6.54395631984694e-05, fluctuations: 0.0\n",
      "step: 85560 loss: 0.546491 time elapsed: 114.1872 learning rate: 0.001039, scenario: 0, slope: -6.47509992635117e-05, fluctuations: 0.0\n",
      "step: 85570 loss: 0.545877 time elapsed: 114.2008 learning rate: 0.001039, scenario: 0, slope: -6.408832649514677e-05, fluctuations: 0.0\n",
      "step: 85580 loss: 0.545268 time elapsed: 114.2138 learning rate: 0.001039, scenario: 0, slope: -6.345031844212437e-05, fluctuations: 0.0\n",
      "step: 85590 loss: 0.544665 time elapsed: 114.2273 learning rate: 0.001039, scenario: 0, slope: -6.283582742380244e-05, fluctuations: 0.0\n",
      "step: 85600 loss: 0.544067 time elapsed: 114.2416 learning rate: 0.001039, scenario: 0, slope: -6.230200220342607e-05, fluctuations: 0.0\n",
      "step: 85610 loss: 0.543474 time elapsed: 114.2564 learning rate: 0.001039, scenario: 0, slope: -6.167316019524339e-05, fluctuations: 0.0\n",
      "step: 85620 loss: 0.542885 time elapsed: 114.2709 learning rate: 0.001039, scenario: 0, slope: -6.112302561735016e-05, fluctuations: 0.0\n",
      "step: 85630 loss: 0.542301 time elapsed: 114.2852 learning rate: 0.001039, scenario: 0, slope: -6.059248105224189e-05, fluctuations: 0.0\n",
      "step: 85640 loss: 0.541721 time elapsed: 114.2995 learning rate: 0.001039, scenario: 0, slope: -6.0080684830645636e-05, fluctuations: 0.0\n",
      "step: 85650 loss: 0.541146 time elapsed: 114.3130 learning rate: 0.001039, scenario: 0, slope: -5.95868427055218e-05, fluctuations: 0.0\n",
      "step: 85660 loss: 0.540574 time elapsed: 114.3270 learning rate: 0.001039, scenario: 0, slope: -5.911020426299439e-05, fluctuations: 0.0\n",
      "step: 85670 loss: 0.540007 time elapsed: 114.3414 learning rate: 0.001039, scenario: 0, slope: -5.865005967574936e-05, fluctuations: 0.0\n",
      "step: 85680 loss: 0.539443 time elapsed: 114.3558 learning rate: 0.001039, scenario: 0, slope: -5.820573676174261e-05, fluctuations: 0.0\n",
      "step: 85690 loss: 0.538883 time elapsed: 114.3705 learning rate: 0.001039, scenario: 0, slope: -5.7776598316262525e-05, fluctuations: 0.0\n",
      "step: 85700 loss: 0.538327 time elapsed: 114.3837 learning rate: 0.001039, scenario: 0, slope: -5.740285605613588e-05, fluctuations: 0.0\n",
      "step: 85710 loss: 0.537774 time elapsed: 114.3970 learning rate: 0.001039, scenario: 0, slope: -5.696148658201556e-05, fluctuations: 0.0\n",
      "step: 85720 loss: 0.537224 time elapsed: 114.4097 learning rate: 0.001039, scenario: 0, slope: -5.657439304018163e-05, fluctuations: 0.0\n",
      "step: 85730 loss: 0.536677 time elapsed: 114.4224 learning rate: 0.001039, scenario: 0, slope: -5.6200239623445005e-05, fluctuations: 0.0\n",
      "step: 85740 loss: 0.536134 time elapsed: 114.4360 learning rate: 0.001039, scenario: 0, slope: -5.583853173239297e-05, fluctuations: 0.0\n",
      "step: 85750 loss: 0.535593 time elapsed: 114.4499 learning rate: 0.001039, scenario: 0, slope: -5.548879807865438e-05, fluctuations: 0.0\n",
      "step: 85760 loss: 0.535056 time elapsed: 114.4634 learning rate: 0.001039, scenario: 0, slope: -5.5150589280523675e-05, fluctuations: 0.0\n",
      "step: 85770 loss: 0.534521 time elapsed: 114.4774 learning rate: 0.001039, scenario: 0, slope: -5.482347657451789e-05, fluctuations: 0.0\n",
      "step: 85780 loss: 0.533989 time elapsed: 114.4912 learning rate: 0.001039, scenario: 0, slope: -5.45070506285686e-05, fluctuations: 0.0\n",
      "step: 85790 loss: 0.533460 time elapsed: 114.5048 learning rate: 0.001039, scenario: 0, slope: -5.4200920448152196e-05, fluctuations: 0.0\n",
      "step: 85800 loss: 0.532933 time elapsed: 114.5184 learning rate: 0.001039, scenario: 0, slope: -5.393389698617868e-05, fluctuations: 0.0\n",
      "step: 85810 loss: 0.532401 time elapsed: 114.5327 learning rate: 0.001113, scenario: 1, slope: -5.362883708205927e-05, fluctuations: 0.0\n",
      "step: 85820 loss: 0.531821 time elapsed: 114.5466 learning rate: 0.001230, scenario: 1, slope: -5.3543421697947624e-05, fluctuations: 0.0\n",
      "step: 85830 loss: 0.531189 time elapsed: 114.5601 learning rate: 0.001293, scenario: 0, slope: -5.3911631937489356e-05, fluctuations: 0.0\n",
      "step: 85840 loss: 0.530547 time elapsed: 114.5726 learning rate: 0.001293, scenario: 0, slope: -5.480462850499826e-05, fluctuations: 0.0\n",
      "step: 85850 loss: 0.529909 time elapsed: 114.5852 learning rate: 0.001293, scenario: 0, slope: -5.6075741786063564e-05, fluctuations: 0.0\n",
      "step: 85860 loss: 0.529276 time elapsed: 114.5974 learning rate: 0.001293, scenario: 0, slope: -5.756672289555718e-05, fluctuations: 0.0\n",
      "step: 85870 loss: 0.528646 time elapsed: 114.6099 learning rate: 0.001293, scenario: 0, slope: -5.91217740366743e-05, fluctuations: 0.0\n",
      "step: 85880 loss: 0.528019 time elapsed: 114.6221 learning rate: 0.001293, scenario: 0, slope: -6.058758573029624e-05, fluctuations: 0.0\n",
      "step: 85890 loss: 0.527397 time elapsed: 114.6345 learning rate: 0.001293, scenario: 0, slope: -6.18132816201478e-05, fluctuations: 0.0\n",
      "step: 85900 loss: 0.526777 time elapsed: 114.6479 learning rate: 0.001293, scenario: 0, slope: -6.258830413663201e-05, fluctuations: 0.0\n",
      "step: 85910 loss: 0.526161 time elapsed: 114.6627 learning rate: 0.001293, scenario: 0, slope: -6.296390853390939e-05, fluctuations: 0.0\n",
      "step: 85920 loss: 0.525548 time elapsed: 114.6765 learning rate: 0.001293, scenario: 0, slope: -6.281339033479591e-05, fluctuations: 0.0\n",
      "step: 85930 loss: 0.524939 time elapsed: 114.6902 learning rate: 0.001293, scenario: 0, slope: -6.246595509827364e-05, fluctuations: 0.0\n",
      "step: 85940 loss: 0.524332 time elapsed: 114.7038 learning rate: 0.001293, scenario: 0, slope: -6.2117200685711e-05, fluctuations: 0.0\n",
      "step: 85950 loss: 0.523728 time elapsed: 114.7174 learning rate: 0.001293, scenario: 0, slope: -6.17802047647522e-05, fluctuations: 0.0\n",
      "step: 85960 loss: 0.523127 time elapsed: 114.7310 learning rate: 0.001293, scenario: 0, slope: -6.145470246157563e-05, fluctuations: 0.0\n",
      "step: 85970 loss: 0.522528 time elapsed: 114.7443 learning rate: 0.001293, scenario: 0, slope: -6.114029657865552e-05, fluctuations: 0.0\n",
      "step: 85980 loss: 0.521932 time elapsed: 114.7589 learning rate: 0.001293, scenario: 0, slope: -6.083655922174479e-05, fluctuations: 0.0\n",
      "step: 85990 loss: 0.521339 time elapsed: 114.7736 learning rate: 0.001293, scenario: 0, slope: -6.054306649040393e-05, fluctuations: 0.0\n",
      "step: 86000 loss: 0.520748 time elapsed: 114.7865 learning rate: 0.001293, scenario: 0, slope: -6.028734408738531e-05, fluctuations: 0.0\n",
      "step: 86010 loss: 0.520160 time elapsed: 114.7994 learning rate: 0.001293, scenario: 0, slope: -5.9985198857873144e-05, fluctuations: 0.0\n",
      "step: 86020 loss: 0.519574 time elapsed: 114.8115 learning rate: 0.001293, scenario: 0, slope: -5.9720062335359724e-05, fluctuations: 0.0\n",
      "step: 86030 loss: 0.518990 time elapsed: 114.8238 learning rate: 0.001293, scenario: 0, slope: -5.946364693982583e-05, fluctuations: 0.0\n",
      "step: 86040 loss: 0.518408 time elapsed: 114.8362 learning rate: 0.001293, scenario: 0, slope: -5.9215616702816934e-05, fluctuations: 0.0\n",
      "step: 86050 loss: 0.517828 time elapsed: 114.8482 learning rate: 0.001293, scenario: 0, slope: -5.897565201201394e-05, fluctuations: 0.0\n",
      "step: 86060 loss: 0.517250 time elapsed: 114.8620 learning rate: 0.001293, scenario: 0, slope: -5.874344869501514e-05, fluctuations: 0.0\n",
      "step: 86070 loss: 0.516674 time elapsed: 114.8760 learning rate: 0.001293, scenario: 0, slope: -5.851871714635605e-05, fluctuations: 0.0\n",
      "step: 86080 loss: 0.516100 time elapsed: 114.8902 learning rate: 0.001293, scenario: 0, slope: -5.830118150382531e-05, fluctuations: 0.0\n",
      "step: 86090 loss: 0.515528 time elapsed: 114.9041 learning rate: 0.001293, scenario: 0, slope: -5.8090578873850494e-05, fluctuations: 0.0\n",
      "step: 86100 loss: 0.514958 time elapsed: 114.9180 learning rate: 0.001293, scenario: 0, slope: -5.790675682670036e-05, fluctuations: 0.0\n",
      "step: 86110 loss: 0.514389 time elapsed: 114.9322 learning rate: 0.001293, scenario: 0, slope: -5.76891816007136e-05, fluctuations: 0.0\n",
      "step: 86120 loss: 0.513822 time elapsed: 114.9459 learning rate: 0.001293, scenario: 0, slope: -5.749791967606171e-05, fluctuations: 0.0\n",
      "step: 86130 loss: 0.513256 time elapsed: 114.9598 learning rate: 0.001293, scenario: 0, slope: -5.731264591338956e-05, fluctuations: 0.0\n",
      "step: 86140 loss: 0.512704 time elapsed: 114.9739 learning rate: 0.001293, scenario: 0, slope: -5.712011236107816e-05, fluctuations: 0.0\n",
      "step: 86150 loss: 0.541940 time elapsed: 114.9876 learning rate: 0.001332, scenario: 1, slope: -2.571970825353947e-05, fluctuations: 0.0\n",
      "step: 86160 loss: 33.247512 time elapsed: 115.0005 learning rate: 0.001211, scenario: -1, slope: 0.055648311840206695, fluctuations: 0.01\n",
      "step: 86170 loss: 2.181642 time elapsed: 115.0132 learning rate: 0.001095, scenario: -1, slope: 0.07021148524546719, fluctuations: 0.03\n",
      "step: 86180 loss: 2.745008 time elapsed: 115.0260 learning rate: 0.000990, scenario: -1, slope: 0.06366126416072333, fluctuations: 0.06\n",
      "step: 86190 loss: 1.131108 time elapsed: 115.0384 learning rate: 0.000896, scenario: -1, slope: 0.05026910071021919, fluctuations: 0.1\n",
      "step: 86200 loss: 0.647137 time elapsed: 115.0507 learning rate: 0.000818, scenario: -1, slope: 0.03594619495173091, fluctuations: 0.14\n",
      "step: 86210 loss: 0.561095 time elapsed: 115.0636 learning rate: 0.000740, scenario: -1, slope: 0.015158663010062096, fluctuations: 0.19\n",
      "step: 86220 loss: 0.523945 time elapsed: 115.0777 learning rate: 0.000690, scenario: 0, slope: -0.005844845899927606, fluctuations: 0.24\n",
      "step: 86230 loss: 0.516537 time elapsed: 115.0925 learning rate: 0.000690, scenario: 0, slope: -0.029456062505602235, fluctuations: 0.29\n",
      "step: 86240 loss: 0.512909 time elapsed: 115.1062 learning rate: 0.000690, scenario: 0, slope: -0.056256533418282594, fluctuations: 0.34\n",
      "step: 86250 loss: 0.511245 time elapsed: 115.1200 learning rate: 0.000690, scenario: 0, slope: -0.0891923034244495, fluctuations: 0.39\n",
      "step: 86260 loss: 0.510417 time elapsed: 115.1339 learning rate: 0.000690, scenario: 0, slope: -0.03897506599706601, fluctuations: 0.43\n",
      "step: 86270 loss: 0.509917 time elapsed: 115.1479 learning rate: 0.000690, scenario: 0, slope: -0.014190159444811301, fluctuations: 0.45\n",
      "step: 86280 loss: 0.509539 time elapsed: 115.1617 learning rate: 0.000690, scenario: 0, slope: -0.0023170767433795046, fluctuations: 0.45\n",
      "step: 86290 loss: 0.509205 time elapsed: 115.1758 learning rate: 0.000690, scenario: 0, slope: -0.0007310019371341539, fluctuations: 0.41\n",
      "step: 86300 loss: 0.508889 time elapsed: 115.1903 learning rate: 0.000690, scenario: 0, slope: -0.0002988117458590525, fluctuations: 0.37\n",
      "step: 86310 loss: 0.508579 time elapsed: 115.2040 learning rate: 0.000690, scenario: 0, slope: -0.00013046239180989708, fluctuations: 0.32\n",
      "step: 86320 loss: 0.508272 time elapsed: 115.2170 learning rate: 0.000690, scenario: 0, slope: -6.076650989126891e-05, fluctuations: 0.27\n",
      "step: 86330 loss: 0.507971 time elapsed: 115.2294 learning rate: 0.000643, scenario: -1, slope: -3.879825159591351e-05, fluctuations: 0.22\n",
      "step: 86340 loss: 0.507698 time elapsed: 115.2417 learning rate: 0.000581, scenario: -1, slope: -3.316903962507658e-05, fluctuations: 0.17\n",
      "step: 86350 loss: 0.507451 time elapsed: 115.2541 learning rate: 0.000526, scenario: -1, slope: -3.120239942173575e-05, fluctuations: 0.12\n",
      "step: 86360 loss: 0.507228 time elapsed: 115.2665 learning rate: 0.000475, scenario: -1, slope: -2.994744276970524e-05, fluctuations: 0.07\n",
      "step: 86370 loss: 0.507026 time elapsed: 115.2789 learning rate: 0.000430, scenario: -1, slope: -2.8755641836527996e-05, fluctuations: 0.02\n",
      "step: 86380 loss: 0.506836 time elapsed: 115.2929 learning rate: 0.000454, scenario: 1, slope: -2.7310663705741538e-05, fluctuations: 0.0\n",
      "step: 86390 loss: 0.506629 time elapsed: 115.3067 learning rate: 0.000502, scenario: 1, slope: -2.5703821533406737e-05, fluctuations: 0.0\n",
      "step: 86400 loss: 0.506401 time elapsed: 115.3201 learning rate: 0.000549, scenario: 1, slope: -2.4371899622580946e-05, fluctuations: 0.0\n",
      "step: 86410 loss: 0.506151 time elapsed: 115.3342 learning rate: 0.000606, scenario: 1, slope: -2.31024405775533e-05, fluctuations: 0.0\n",
      "step: 86420 loss: 0.505874 time elapsed: 115.3480 learning rate: 0.000669, scenario: 1, slope: -2.2487927450408193e-05, fluctuations: 0.0\n",
      "step: 86430 loss: 0.505569 time elapsed: 115.3618 learning rate: 0.000739, scenario: 1, slope: -2.257509380591405e-05, fluctuations: 0.0\n",
      "step: 86440 loss: 0.505233 time elapsed: 115.3756 learning rate: 0.000817, scenario: 1, slope: -2.3425486017270127e-05, fluctuations: 0.0\n",
      "step: 86450 loss: 0.504861 time elapsed: 115.3892 learning rate: 0.000902, scenario: 1, slope: -2.499253218060733e-05, fluctuations: 0.0\n",
      "step: 86460 loss: 0.504451 time elapsed: 115.4034 learning rate: 0.000997, scenario: 1, slope: -2.719233901314334e-05, fluctuations: 0.0\n",
      "step: 86470 loss: 0.503999 time elapsed: 115.4162 learning rate: 0.001101, scenario: 1, slope: -2.9904008905703534e-05, fluctuations: 0.0\n",
      "step: 86480 loss: 0.503501 time elapsed: 115.4284 learning rate: 0.001216, scenario: 1, slope: -3.297966093908282e-05, fluctuations: 0.0\n",
      "step: 86490 loss: 0.502952 time elapsed: 115.4408 learning rate: 0.001343, scenario: 1, slope: -3.637235854359751e-05, fluctuations: 0.0\n",
      "step: 86500 loss: 0.502350 time elapsed: 115.4530 learning rate: 0.001469, scenario: 1, slope: -3.972215177810757e-05, fluctuations: 0.0\n",
      "step: 86510 loss: 108.440301 time elapsed: 115.4658 learning rate: 0.001506, scenario: -1, slope: 0.07546769413267607, fluctuations: 0.0\n",
      "step: 86520 loss: 41.282808 time elapsed: 115.4781 learning rate: 0.001362, scenario: -1, slope: 0.8436393389976674, fluctuations: 0.04\n",
      "step: 86530 loss: 96.917973 time elapsed: 115.4906 learning rate: 0.001232, scenario: -1, slope: 0.9806831410007827, fluctuations: 0.08\n",
      "step: 86540 loss: 22.622907 time elapsed: 115.5045 learning rate: 0.001114, scenario: -1, slope: 0.8047578960857722, fluctuations: 0.12\n",
      "step: 86550 loss: 9.035695 time elapsed: 115.5188 learning rate: 0.001007, scenario: -1, slope: 0.584496607461176, fluctuations: 0.16\n",
      "step: 86560 loss: 3.101821 time elapsed: 115.5329 learning rate: 0.000911, scenario: -1, slope: 0.3278818632185354, fluctuations: 0.2\n",
      "step: 86570 loss: 0.915865 time elapsed: 115.5468 learning rate: 0.000824, scenario: -1, slope: 0.054810934309853704, fluctuations: 0.24\n",
      "step: 86580 loss: 0.810261 time elapsed: 115.5605 learning rate: 0.000816, scenario: 0, slope: -0.24769710430797343, fluctuations: 0.27\n",
      "step: 86590 loss: 0.733885 time elapsed: 115.5739 learning rate: 0.000816, scenario: 0, slope: -0.5699491620557625, fluctuations: 0.3\n",
      "step: 86600 loss: 0.613723 time elapsed: 115.5876 learning rate: 0.000816, scenario: 0, slope: -0.9383568686064044, fluctuations: 0.34\n",
      "step: 86610 loss: 0.613714 time elapsed: 115.6019 learning rate: 0.000816, scenario: 0, slope: -1.4515735051283087, fluctuations: 0.37\n",
      "step: 86620 loss: 0.592853 time elapsed: 115.6158 learning rate: 0.000816, scenario: 0, slope: -0.5271182959677418, fluctuations: 0.36\n",
      "step: 86630 loss: 0.586908 time elapsed: 115.6305 learning rate: 0.000816, scenario: 0, slope: -0.10018103014658816, fluctuations: 0.35\n",
      "step: 86640 loss: 0.581115 time elapsed: 115.6429 learning rate: 0.000816, scenario: 0, slope: -0.0356294020960905, fluctuations: 0.35\n",
      "step: 86650 loss: 0.576611 time elapsed: 115.6556 learning rate: 0.000816, scenario: 0, slope: -0.012445092803964898, fluctuations: 0.31\n",
      "step: 86660 loss: 0.572999 time elapsed: 115.6684 learning rate: 0.000816, scenario: 0, slope: -0.005414707716553731, fluctuations: 0.27\n",
      "step: 86670 loss: 0.569803 time elapsed: 115.6823 learning rate: 0.000816, scenario: 0, slope: -0.0020776557298729744, fluctuations: 0.24\n",
      "step: 86680 loss: 0.567026 time elapsed: 115.6946 learning rate: 0.000816, scenario: 0, slope: -0.0012964295712127238, fluctuations: 0.2\n",
      "step: 86690 loss: 0.564526 time elapsed: 115.7070 learning rate: 0.000816, scenario: 0, slope: -0.0006860460594085445, fluctuations: 0.17\n",
      "step: 86700 loss: 0.562274 time elapsed: 115.7209 learning rate: 0.000816, scenario: 0, slope: -0.0004889199913462559, fluctuations: 0.14\n",
      "step: 86710 loss: 0.560218 time elapsed: 115.7351 learning rate: 0.000816, scenario: 0, slope: -0.0003770890858914618, fluctuations: 0.1\n",
      "step: 86720 loss: 0.558329 time elapsed: 115.7488 learning rate: 0.000816, scenario: 0, slope: -0.00031455357580085233, fluctuations: 0.07\n",
      "step: 86730 loss: 0.556584 time elapsed: 115.7622 learning rate: 0.000816, scenario: 0, slope: -0.00027704820417426474, fluctuations: 0.03\n",
      "step: 86740 loss: 0.554962 time elapsed: 115.7760 learning rate: 0.000816, scenario: 0, slope: -0.00024882306711840584, fluctuations: 0.0\n",
      "step: 86750 loss: 0.553447 time elapsed: 115.7894 learning rate: 0.000816, scenario: 0, slope: -0.00022362717684092459, fluctuations: 0.0\n",
      "step: 86760 loss: 0.552026 time elapsed: 115.8030 learning rate: 0.000816, scenario: 0, slope: -0.00020321759307548865, fluctuations: 0.0\n",
      "step: 86770 loss: 0.550689 time elapsed: 115.8164 learning rate: 0.000816, scenario: 0, slope: -0.00018633507698880285, fluctuations: 0.0\n",
      "step: 86780 loss: 0.549426 time elapsed: 115.8313 learning rate: 0.000816, scenario: 0, slope: -0.0001720416343720531, fluctuations: 0.0\n",
      "step: 86790 loss: 0.548230 time elapsed: 115.8441 learning rate: 0.000816, scenario: 0, slope: -0.0001597884942986974, fluctuations: 0.0\n",
      "step: 86800 loss: 0.547093 time elapsed: 115.8564 learning rate: 0.000816, scenario: 0, slope: -0.00015015724440809637, fluctuations: 0.0\n",
      "step: 86810 loss: 0.546010 time elapsed: 115.8694 learning rate: 0.000816, scenario: 0, slope: -0.0001398526519436163, fluctuations: 0.0\n",
      "step: 86820 loss: 0.544977 time elapsed: 115.8818 learning rate: 0.000816, scenario: 0, slope: -0.00013164237891898507, fluctuations: 0.0\n",
      "step: 86830 loss: 0.543987 time elapsed: 115.8941 learning rate: 0.000816, scenario: 0, slope: -0.00012434899868700908, fluctuations: 0.0\n",
      "step: 86840 loss: 0.543039 time elapsed: 115.9066 learning rate: 0.000816, scenario: 0, slope: -0.00011783086897114557, fluctuations: 0.0\n",
      "step: 86850 loss: 0.542128 time elapsed: 115.9205 learning rate: 0.000816, scenario: 0, slope: -0.00011197424670562794, fluctuations: 0.0\n",
      "step: 86860 loss: 0.541251 time elapsed: 115.9347 learning rate: 0.000816, scenario: 0, slope: -0.00010668651607513619, fluctuations: 0.0\n",
      "step: 86870 loss: 0.540406 time elapsed: 115.9484 learning rate: 0.000816, scenario: 0, slope: -0.00010189153494328535, fluctuations: 0.0\n",
      "step: 86880 loss: 0.539591 time elapsed: 115.9620 learning rate: 0.000816, scenario: 0, slope: -9.752612047897988e-05, fluctuations: 0.0\n",
      "step: 86890 loss: 0.538802 time elapsed: 115.9755 learning rate: 0.000816, scenario: 0, slope: -9.353741622166305e-05, fluctuations: 0.0\n",
      "step: 86900 loss: 0.538039 time elapsed: 115.9889 learning rate: 0.000816, scenario: 0, slope: -9.02327083395647e-05, fluctuations: 0.0\n",
      "step: 86910 loss: 0.537299 time elapsed: 116.0029 learning rate: 0.000816, scenario: 0, slope: -8.651871103791548e-05, fluctuations: 0.0\n",
      "step: 86920 loss: 0.536581 time elapsed: 116.0165 learning rate: 0.000816, scenario: 0, slope: -8.34186470216847e-05, fluctuations: 0.0\n",
      "step: 86930 loss: 0.535883 time elapsed: 116.0307 learning rate: 0.000816, scenario: 0, slope: -8.055297496066262e-05, fluctuations: 0.0\n",
      "step: 86940 loss: 0.535205 time elapsed: 116.0439 learning rate: 0.000816, scenario: 0, slope: -7.789776818707641e-05, fluctuations: 0.0\n",
      "step: 86950 loss: 0.534544 time elapsed: 116.0569 learning rate: 0.000816, scenario: 0, slope: -7.543226204347715e-05, fluctuations: 0.0\n",
      "step: 86960 loss: 0.533901 time elapsed: 116.0691 learning rate: 0.000816, scenario: 0, slope: -7.313834947681342e-05, fluctuations: 0.0\n",
      "step: 86970 loss: 0.533272 time elapsed: 116.0815 learning rate: 0.000816, scenario: 0, slope: -7.100016941859e-05, fluctuations: 0.0\n",
      "step: 86980 loss: 0.532659 time elapsed: 116.0938 learning rate: 0.000816, scenario: 0, slope: -6.900376876835684e-05, fluctuations: 0.0\n",
      "step: 86990 loss: 0.532060 time elapsed: 116.1059 learning rate: 0.000816, scenario: 0, slope: -6.713682319212688e-05, fluctuations: 0.0\n",
      "step: 87000 loss: 0.531473 time elapsed: 116.1178 learning rate: 0.000816, scenario: 0, slope: -6.555819906403672e-05, fluctuations: 0.0\n",
      "step: 87010 loss: 0.530899 time elapsed: 116.1316 learning rate: 0.000816, scenario: 0, slope: -6.374879063243433e-05, fluctuations: 0.0\n",
      "step: 87020 loss: 0.530337 time elapsed: 116.1450 learning rate: 0.000816, scenario: 0, slope: -6.22092959977868e-05, fluctuations: 0.0\n",
      "step: 87030 loss: 0.529786 time elapsed: 116.1591 learning rate: 0.000816, scenario: 0, slope: -6.076214174294317e-05, fluctuations: 0.0\n",
      "step: 87040 loss: 0.529245 time elapsed: 116.1726 learning rate: 0.000816, scenario: 0, slope: -5.9400336205711316e-05, fluctuations: 0.0\n",
      "step: 87050 loss: 0.528714 time elapsed: 116.1865 learning rate: 0.000816, scenario: 0, slope: -5.811757705877334e-05, fluctuations: 0.0\n",
      "step: 87060 loss: 0.528192 time elapsed: 116.2000 learning rate: 0.000816, scenario: 0, slope: -5.690816716296035e-05, fluctuations: 0.0\n",
      "step: 87070 loss: 0.527680 time elapsed: 116.2140 learning rate: 0.000816, scenario: 0, slope: -5.576694246581247e-05, fluctuations: 0.0\n",
      "step: 87080 loss: 0.527175 time elapsed: 116.2278 learning rate: 0.000816, scenario: 0, slope: -5.468920999489389e-05, fluctuations: 0.0\n",
      "step: 87090 loss: 0.526678 time elapsed: 116.2423 learning rate: 0.000816, scenario: 0, slope: -5.367069434777367e-05, fluctuations: 0.0\n",
      "step: 87100 loss: 0.526186 time elapsed: 116.2553 learning rate: 0.000849, scenario: 1, slope: -5.2802580003115496e-05, fluctuations: 0.0\n",
      "step: 87110 loss: 0.525667 time elapsed: 116.2683 learning rate: 0.000938, scenario: 1, slope: -5.190890325863118e-05, fluctuations: 0.0\n",
      "step: 87120 loss: 0.525102 time elapsed: 116.2809 learning rate: 0.001036, scenario: 1, slope: -5.1491766119220464e-05, fluctuations: 0.0\n",
      "step: 87130 loss: 0.524489 time elapsed: 116.2931 learning rate: 0.001144, scenario: 1, slope: -5.166661644664479e-05, fluctuations: 0.0\n",
      "step: 87140 loss: 0.523825 time elapsed: 116.3055 learning rate: 0.001264, scenario: 1, slope: -5.2592442861519314e-05, fluctuations: 0.0\n",
      "step: 87150 loss: 0.523121 time elapsed: 116.3177 learning rate: 0.001289, scenario: 0, slope: -5.435348633842845e-05, fluctuations: 0.0\n",
      "step: 87160 loss: 0.522426 time elapsed: 116.3300 learning rate: 0.001289, scenario: 0, slope: -5.674927963552903e-05, fluctuations: 0.0\n",
      "step: 87170 loss: 0.521746 time elapsed: 116.3433 learning rate: 0.001289, scenario: 0, slope: -5.9436802793137635e-05, fluctuations: 0.0\n",
      "step: 87180 loss: 0.521078 time elapsed: 116.3573 learning rate: 0.001289, scenario: 0, slope: -6.208505814614915e-05, fluctuations: 0.0\n",
      "step: 87190 loss: 0.520422 time elapsed: 116.3707 learning rate: 0.001289, scenario: 0, slope: -6.437634870405067e-05, fluctuations: 0.0\n",
      "step: 87200 loss: 0.519776 time elapsed: 116.3839 learning rate: 0.001289, scenario: 0, slope: -6.588189928996076e-05, fluctuations: 0.0\n",
      "step: 87210 loss: 0.519141 time elapsed: 116.3980 learning rate: 0.001289, scenario: 0, slope: -6.680805103326384e-05, fluctuations: 0.0\n",
      "step: 87220 loss: 0.518516 time elapsed: 116.4117 learning rate: 0.001289, scenario: 0, slope: -6.681953517014099e-05, fluctuations: 0.0\n",
      "step: 87230 loss: 0.517899 time elapsed: 116.4252 learning rate: 0.001289, scenario: 0, slope: -6.62004674176923e-05, fluctuations: 0.0\n",
      "step: 87240 loss: 0.517290 time elapsed: 116.4387 learning rate: 0.001289, scenario: 0, slope: -6.520012554325214e-05, fluctuations: 0.0\n",
      "step: 87250 loss: 0.516689 time elapsed: 116.4529 learning rate: 0.001289, scenario: 0, slope: -6.413781966453809e-05, fluctuations: 0.0\n",
      "step: 87260 loss: 0.516095 time elapsed: 116.4674 learning rate: 0.001289, scenario: 0, slope: -6.31480768979153e-05, fluctuations: 0.0\n",
      "step: 87270 loss: 0.515508 time elapsed: 116.4802 learning rate: 0.001289, scenario: 0, slope: -6.222774625420956e-05, fluctuations: 0.0\n",
      "step: 87280 loss: 0.514927 time elapsed: 116.4926 learning rate: 0.001289, scenario: 0, slope: -6.137160633391623e-05, fluctuations: 0.0\n",
      "step: 87290 loss: 0.514351 time elapsed: 116.5048 learning rate: 0.001289, scenario: 0, slope: -6.0574631894631604e-05, fluctuations: 0.0\n",
      "step: 87300 loss: 0.513781 time elapsed: 116.5167 learning rate: 0.001289, scenario: 0, slope: -5.990408398069426e-05, fluctuations: 0.0\n",
      "step: 87310 loss: 0.513216 time elapsed: 116.5294 learning rate: 0.001289, scenario: 0, slope: -5.9139945607195686e-05, fluctuations: 0.0\n",
      "step: 87320 loss: 0.512656 time elapsed: 116.5416 learning rate: 0.001289, scenario: 0, slope: -5.8494084977638776e-05, fluctuations: 0.0\n",
      "step: 87330 loss: 0.512100 time elapsed: 116.5554 learning rate: 0.001289, scenario: 0, slope: -5.789104103901687e-05, fluctuations: 0.0\n",
      "step: 87340 loss: 0.511549 time elapsed: 116.5694 learning rate: 0.001289, scenario: 0, slope: -5.732758170836954e-05, fluctuations: 0.0\n",
      "step: 87350 loss: 0.511001 time elapsed: 116.5833 learning rate: 0.001289, scenario: 0, slope: -5.680075482256055e-05, fluctuations: 0.0\n",
      "step: 87360 loss: 0.510457 time elapsed: 116.5967 learning rate: 0.001289, scenario: 0, slope: -5.630786044487771e-05, fluctuations: 0.0\n",
      "step: 87370 loss: 0.509916 time elapsed: 116.6101 learning rate: 0.001289, scenario: 0, slope: -5.5846426229882417e-05, fluctuations: 0.0\n",
      "step: 87380 loss: 0.509379 time elapsed: 116.6234 learning rate: 0.001289, scenario: 0, slope: -5.541418555774372e-05, fluctuations: 0.0\n",
      "step: 87390 loss: 0.508844 time elapsed: 116.6369 learning rate: 0.001289, scenario: 0, slope: -5.500905810754259e-05, fluctuations: 0.0\n",
      "step: 87400 loss: 0.508313 time elapsed: 116.6501 learning rate: 0.001289, scenario: 0, slope: -5.4666042262674434e-05, fluctuations: 0.0\n",
      "step: 87410 loss: 0.507784 time elapsed: 116.6647 learning rate: 0.001289, scenario: 0, slope: -5.42726511491886e-05, fluctuations: 0.0\n",
      "step: 87420 loss: 0.507257 time elapsed: 116.6779 learning rate: 0.001289, scenario: 0, slope: -5.393799585362599e-05, fluctuations: 0.0\n",
      "step: 87430 loss: 0.506733 time elapsed: 116.6906 learning rate: 0.001289, scenario: 0, slope: -5.362367598506152e-05, fluctuations: 0.0\n",
      "step: 87440 loss: 0.506211 time elapsed: 116.7029 learning rate: 0.001289, scenario: 0, slope: -5.332831704365712e-05, fluctuations: 0.0\n",
      "step: 87450 loss: 0.505692 time elapsed: 116.7153 learning rate: 0.001289, scenario: 0, slope: -5.305065066784043e-05, fluctuations: 0.0\n",
      "step: 87460 loss: 0.505174 time elapsed: 116.7275 learning rate: 0.001289, scenario: 0, slope: -5.2789505560839536e-05, fluctuations: 0.0\n",
      "step: 87470 loss: 0.504658 time elapsed: 116.7395 learning rate: 0.001289, scenario: 0, slope: -5.254379928182303e-05, fluctuations: 0.0\n",
      "step: 87480 loss: 0.504144 time elapsed: 116.7516 learning rate: 0.001289, scenario: 0, slope: -5.231253080787813e-05, fluctuations: 0.0\n",
      "step: 87490 loss: 0.503632 time elapsed: 116.7650 learning rate: 0.001289, scenario: 0, slope: -5.2094773783201175e-05, fluctuations: 0.0\n",
      "step: 87500 loss: 0.503121 time elapsed: 116.7785 learning rate: 0.001289, scenario: 0, slope: -5.19096344338287e-05, fluctuations: 0.0\n",
      "step: 87510 loss: 0.502611 time elapsed: 116.7929 learning rate: 0.001289, scenario: 0, slope: -5.1696425732708155e-05, fluctuations: 0.0\n",
      "step: 87520 loss: 0.502103 time elapsed: 116.8066 learning rate: 0.001289, scenario: 0, slope: -5.1514302814770236e-05, fluctuations: 0.0\n",
      "step: 87530 loss: 0.501596 time elapsed: 116.8203 learning rate: 0.001289, scenario: 0, slope: -5.134261783334185e-05, fluctuations: 0.0\n",
      "step: 87540 loss: 0.501091 time elapsed: 116.8337 learning rate: 0.001289, scenario: 0, slope: -5.1180735974082094e-05, fluctuations: 0.0\n",
      "step: 87550 loss: 0.500587 time elapsed: 116.8471 learning rate: 0.001289, scenario: 0, slope: -5.102806753148072e-05, fluctuations: 0.0\n",
      "step: 87560 loss: 0.500083 time elapsed: 116.8607 learning rate: 0.001289, scenario: 0, slope: -5.088406436606946e-05, fluctuations: 0.0\n",
      "step: 87570 loss: 0.499581 time elapsed: 116.8740 learning rate: 0.001289, scenario: 0, slope: -5.074821666075616e-05, fluctuations: 0.0\n",
      "step: 87580 loss: 0.499080 time elapsed: 116.8889 learning rate: 0.001289, scenario: 0, slope: -5.0620049924717226e-05, fluctuations: 0.0\n",
      "step: 87590 loss: 0.498580 time elapsed: 116.9016 learning rate: 0.001289, scenario: 0, slope: -5.049910017647593e-05, fluctuations: 0.0\n",
      "step: 87600 loss: 0.498111 time elapsed: 116.9140 learning rate: 0.001289, scenario: 0, slope: -5.038038099177399e-05, fluctuations: 0.0\n",
      "step: 87610 loss: 0.930627 time elapsed: 116.9270 learning rate: 0.001362, scenario: -1, slope: 0.0003108103444453036, fluctuations: 0.0\n",
      "step: 87620 loss: 9.415717 time elapsed: 116.9391 learning rate: 0.001231, scenario: -1, slope: 0.06842739184683493, fluctuations: 0.02\n",
      "step: 87630 loss: 5.105427 time elapsed: 116.9516 learning rate: 0.001114, scenario: -1, slope: 0.08037030451091255, fluctuations: 0.06\n",
      "step: 87640 loss: 1.015560 time elapsed: 116.9642 learning rate: 0.001007, scenario: -1, slope: 0.07153426484913518, fluctuations: 0.1\n",
      "step: 87650 loss: 1.083876 time elapsed: 116.9779 learning rate: 0.000911, scenario: -1, slope: 0.054676598675028276, fluctuations: 0.14\n",
      "step: 87660 loss: 0.632960 time elapsed: 116.9924 learning rate: 0.000824, scenario: -1, slope: 0.035514632231657274, fluctuations: 0.19\n",
      "step: 87670 loss: 0.516500 time elapsed: 117.0064 learning rate: 0.000745, scenario: -1, slope: 0.013020700484714692, fluctuations: 0.24\n",
      "step: 87680 loss: 0.507853 time elapsed: 117.0200 learning rate: 0.000716, scenario: 0, slope: -0.012137834214792772, fluctuations: 0.29\n",
      "step: 87690 loss: 0.498359 time elapsed: 117.0336 learning rate: 0.000716, scenario: 0, slope: -0.040659928906860154, fluctuations: 0.33\n",
      "step: 87700 loss: 0.498508 time elapsed: 117.0470 learning rate: 0.000716, scenario: 0, slope: -0.0695937879209228, fluctuations: 0.37\n",
      "step: 87710 loss: 0.496701 time elapsed: 117.0612 learning rate: 0.000716, scenario: 0, slope: -0.1176895643862005, fluctuations: 0.42\n",
      "step: 87720 loss: 0.496360 time elapsed: 117.0748 learning rate: 0.000716, scenario: 0, slope: -0.04092210151553387, fluctuations: 0.43\n",
      "step: 87730 loss: 0.495862 time elapsed: 117.0889 learning rate: 0.000716, scenario: 0, slope: -0.00737154264062309, fluctuations: 0.45\n",
      "step: 87740 loss: 0.495566 time elapsed: 117.1024 learning rate: 0.000716, scenario: 0, slope: -0.0031828347715755524, fluctuations: 0.41\n",
      "step: 87750 loss: 0.495277 time elapsed: 117.1152 learning rate: 0.000716, scenario: 0, slope: -0.0005023874271526815, fluctuations: 0.37\n",
      "step: 87760 loss: 0.495003 time elapsed: 117.1277 learning rate: 0.000716, scenario: 0, slope: -0.00027261301013166574, fluctuations: 0.32\n",
      "step: 87770 loss: 0.494737 time elapsed: 117.1401 learning rate: 0.000716, scenario: 0, slope: -0.00011304219151980131, fluctuations: 0.27\n",
      "step: 87780 loss: 0.494471 time elapsed: 117.1524 learning rate: 0.000716, scenario: 0, slope: -5.4905244912196816e-05, fluctuations: 0.23\n",
      "step: 87790 loss: 0.494213 time elapsed: 117.1647 learning rate: 0.000660, scenario: -1, slope: -3.813907734814751e-05, fluctuations: 0.18\n",
      "step: 87800 loss: 0.493979 time elapsed: 117.1770 learning rate: 0.000603, scenario: -1, slope: -3.0920656334233966e-05, fluctuations: 0.14\n",
      "step: 87810 loss: 0.493765 time elapsed: 117.1912 learning rate: 0.000546, scenario: -1, slope: -2.7602125094888296e-05, fluctuations: 0.09\n",
      "step: 87820 loss: 0.493572 time elapsed: 117.2054 learning rate: 0.000493, scenario: -1, slope: -2.5849147534548698e-05, fluctuations: 0.05\n",
      "step: 87830 loss: 0.493397 time elapsed: 117.2193 learning rate: 0.000446, scenario: -1, slope: -2.4860124658437414e-05, fluctuations: 0.01\n",
      "step: 87840 loss: 0.493230 time elapsed: 117.2328 learning rate: 0.000481, scenario: 1, slope: -2.3544020168232797e-05, fluctuations: 0.0\n",
      "step: 87850 loss: 0.493047 time elapsed: 117.2466 learning rate: 0.000531, scenario: 1, slope: -2.219296495548452e-05, fluctuations: 0.0\n",
      "step: 87860 loss: 0.492845 time elapsed: 117.2600 learning rate: 0.000587, scenario: 1, slope: -2.0986054497406844e-05, fluctuations: 0.0\n",
      "step: 87870 loss: 0.492621 time elapsed: 117.2735 learning rate: 0.000648, scenario: 1, slope: -2.008947930389521e-05, fluctuations: 0.0\n",
      "step: 87880 loss: 0.492373 time elapsed: 117.2871 learning rate: 0.000716, scenario: 1, slope: -1.96710747525887e-05, fluctuations: 0.0\n",
      "step: 87890 loss: 0.492100 time elapsed: 117.3012 learning rate: 0.000791, scenario: 1, slope: -1.9878268507391433e-05, fluctuations: 0.0\n",
      "step: 87900 loss: 0.491797 time elapsed: 117.3137 learning rate: 0.000865, scenario: 1, slope: -2.0633155633558616e-05, fluctuations: 0.0\n",
      "step: 87910 loss: 0.491467 time elapsed: 117.3268 learning rate: 0.000955, scenario: 1, slope: -2.2235274021302858e-05, fluctuations: 0.0\n",
      "step: 87920 loss: 0.491101 time elapsed: 117.3390 learning rate: 0.001055, scenario: 1, slope: -2.4258211840263424e-05, fluctuations: 0.0\n",
      "step: 87930 loss: 0.490698 time elapsed: 117.3512 learning rate: 0.001166, scenario: 1, slope: -2.6710319362147655e-05, fluctuations: 0.0\n",
      "step: 87940 loss: 0.490254 time elapsed: 117.3632 learning rate: 0.001287, scenario: 1, slope: -2.946554100026196e-05, fluctuations: 0.0\n",
      "step: 87950 loss: 0.489763 time elapsed: 117.3753 learning rate: 0.001422, scenario: 1, slope: -3.2496931680192084e-05, fluctuations: 0.0\n",
      "step: 87960 loss: 0.489222 time elapsed: 117.3874 learning rate: 0.001571, scenario: 1, slope: -3.5834347630357615e-05, fluctuations: 0.0\n",
      "step: 87970 loss: 5.094956 time elapsed: 117.4007 learning rate: 0.001643, scenario: -1, slope: 0.0031373377228885945, fluctuations: 0.0\n",
      "step: 87980 loss: 142.101212 time elapsed: 117.4145 learning rate: 0.001486, scenario: -1, slope: 0.981771653765256, fluctuations: 0.03\n",
      "step: 87990 loss: 111.507336 time elapsed: 117.4283 learning rate: 0.001343, scenario: -1, slope: 1.1938257804387895, fluctuations: 0.07\n",
      "step: 88000 loss: 12.895229 time elapsed: 117.4419 learning rate: 0.001227, scenario: -1, slope: 1.04171773313766, fluctuations: 0.11\n",
      "step: 88010 loss: 2.019098 time elapsed: 117.4561 learning rate: 0.001110, scenario: -1, slope: 0.7940486314968644, fluctuations: 0.16\n",
      "step: 88020 loss: 3.359451 time elapsed: 117.4695 learning rate: 0.001004, scenario: -1, slope: 0.4878212365495687, fluctuations: 0.2\n",
      "step: 88030 loss: 1.496187 time elapsed: 117.4831 learning rate: 0.000908, scenario: -1, slope: 0.12426779006235275, fluctuations: 0.23\n",
      "step: 88040 loss: 0.785644 time elapsed: 117.4966 learning rate: 0.000881, scenario: 0, slope: -0.21822267309359156, fluctuations: 0.27\n",
      "step: 88050 loss: 0.664350 time elapsed: 117.5101 learning rate: 0.000881, scenario: 0, slope: -0.6034605151280341, fluctuations: 0.3\n",
      "step: 88060 loss: 0.635409 time elapsed: 117.5235 learning rate: 0.000881, scenario: 0, slope: -1.074137727370689, fluctuations: 0.34\n",
      "step: 88070 loss: 0.599845 time elapsed: 117.5360 learning rate: 0.000881, scenario: 0, slope: -1.7315777596189061, fluctuations: 0.37\n",
      "step: 88080 loss: 0.594426 time elapsed: 117.5485 learning rate: 0.000881, scenario: 0, slope: -0.6368681192024483, fluctuations: 0.36\n",
      "step: 88090 loss: 0.583835 time elapsed: 117.5608 learning rate: 0.000881, scenario: 0, slope: -0.12495935477193346, fluctuations: 0.36\n",
      "step: 88100 loss: 0.578985 time elapsed: 117.5731 learning rate: 0.000881, scenario: 0, slope: -0.058277562309356344, fluctuations: 0.34\n",
      "step: 88110 loss: 0.574003 time elapsed: 117.5859 learning rate: 0.000881, scenario: 0, slope: -0.020831502903208732, fluctuations: 0.29\n",
      "step: 88120 loss: 0.570192 time elapsed: 117.5985 learning rate: 0.000881, scenario: 0, slope: -0.0053512230722776504, fluctuations: 0.26\n",
      "step: 88130 loss: 0.566713 time elapsed: 117.6119 learning rate: 0.000881, scenario: 0, slope: -0.0024596993874704213, fluctuations: 0.22\n",
      "step: 88140 loss: 0.563654 time elapsed: 117.6256 learning rate: 0.000881, scenario: 0, slope: -0.001047281026200203, fluctuations: 0.19\n",
      "step: 88150 loss: 0.560897 time elapsed: 117.6395 learning rate: 0.000881, scenario: 0, slope: -0.0007665888699240417, fluctuations: 0.15\n",
      "step: 88160 loss: 0.558409 time elapsed: 117.6531 learning rate: 0.000881, scenario: 0, slope: -0.00048273716328248964, fluctuations: 0.12\n",
      "step: 88170 loss: 0.556151 time elapsed: 117.6671 learning rate: 0.000881, scenario: 0, slope: -0.0004040138205337951, fluctuations: 0.08\n",
      "step: 88180 loss: 0.554092 time elapsed: 117.6811 learning rate: 0.000881, scenario: 0, slope: -0.00033885470262759915, fluctuations: 0.05\n",
      "step: 88190 loss: 0.552207 time elapsed: 117.6949 learning rate: 0.000881, scenario: 0, slope: -0.0003034961804182974, fluctuations: 0.01\n",
      "step: 88200 loss: 0.550473 time elapsed: 117.7083 learning rate: 0.000881, scenario: 0, slope: -0.0002747073953655361, fluctuations: 0.0\n",
      "step: 88210 loss: 0.548873 time elapsed: 117.7230 learning rate: 0.000881, scenario: 0, slope: -0.00024450097019773615, fluctuations: 0.0\n",
      "step: 88220 loss: 0.547389 time elapsed: 117.7361 learning rate: 0.000881, scenario: 0, slope: -0.00022159657415363738, fluctuations: 0.0\n",
      "step: 88230 loss: 0.546009 time elapsed: 117.7487 learning rate: 0.000881, scenario: 0, slope: -0.0002019930407180902, fluctuations: 0.0\n",
      "step: 88240 loss: 0.544719 time elapsed: 117.7611 learning rate: 0.000881, scenario: 0, slope: -0.00018497937226155846, fluctuations: 0.0\n",
      "step: 88250 loss: 0.543510 time elapsed: 117.7732 learning rate: 0.000881, scenario: 0, slope: -0.00017014771763331285, fluctuations: 0.0\n",
      "step: 88260 loss: 0.542373 time elapsed: 117.7853 learning rate: 0.000881, scenario: 0, slope: -0.0001571586592889781, fluctuations: 0.0\n",
      "step: 88270 loss: 0.541299 time elapsed: 117.7974 learning rate: 0.000881, scenario: 0, slope: -0.00014574838216127446, fluctuations: 0.0\n",
      "step: 88280 loss: 0.540282 time elapsed: 117.8096 learning rate: 0.000881, scenario: 0, slope: -0.0001356965238770723, fluctuations: 0.0\n",
      "step: 88290 loss: 0.539316 time elapsed: 117.8231 learning rate: 0.000881, scenario: 0, slope: -0.00012681811579675855, fluctuations: 0.0\n",
      "step: 88300 loss: 0.538395 time elapsed: 117.8366 learning rate: 0.000881, scenario: 0, slope: -0.00011970080128092301, fluctuations: 0.0\n",
      "step: 88310 loss: 0.537515 time elapsed: 117.8507 learning rate: 0.000881, scenario: 0, slope: -0.00011197766538613259, fluctuations: 0.0\n",
      "step: 88320 loss: 0.536672 time elapsed: 117.8642 learning rate: 0.000881, scenario: 0, slope: -0.00010576747371737111, fluctuations: 0.0\n",
      "step: 88330 loss: 0.535863 time elapsed: 117.8779 learning rate: 0.000881, scenario: 0, slope: -0.00010022761518079715, fluctuations: 0.0\n",
      "step: 88340 loss: 0.535083 time elapsed: 117.8916 learning rate: 0.000881, scenario: 0, slope: -9.527357082874505e-05, fluctuations: 0.0\n",
      "step: 88350 loss: 0.534331 time elapsed: 117.9054 learning rate: 0.000881, scenario: 0, slope: -9.083245379482805e-05, fluctuations: 0.0\n",
      "step: 88360 loss: 0.533604 time elapsed: 117.9194 learning rate: 0.000881, scenario: 0, slope: -8.684128513307258e-05, fluctuations: 0.0\n",
      "step: 88370 loss: 0.532899 time elapsed: 117.9334 learning rate: 0.000881, scenario: 0, slope: -8.324555024483463e-05, fluctuations: 0.0\n",
      "step: 88380 loss: 0.532216 time elapsed: 117.9464 learning rate: 0.000881, scenario: 0, slope: -7.999798207946925e-05, fluctuations: 0.0\n",
      "step: 88390 loss: 0.531551 time elapsed: 117.9587 learning rate: 0.000881, scenario: 0, slope: -7.705753259675514e-05, fluctuations: 0.0\n",
      "step: 88400 loss: 0.530904 time elapsed: 117.9707 learning rate: 0.000881, scenario: 0, slope: -7.464410909899858e-05, fluctuations: 0.0\n",
      "step: 88410 loss: 0.530272 time elapsed: 117.9840 learning rate: 0.000881, scenario: 0, slope: -7.195978822319199e-05, fluctuations: 0.0\n",
      "step: 88420 loss: 0.529656 time elapsed: 117.9964 learning rate: 0.000881, scenario: 0, slope: -6.974427082301233e-05, fluctuations: 0.0\n",
      "step: 88430 loss: 0.529053 time elapsed: 118.0090 learning rate: 0.000881, scenario: 0, slope: -6.771825346465254e-05, fluctuations: 0.0\n",
      "step: 88440 loss: 0.528463 time elapsed: 118.0215 learning rate: 0.000881, scenario: 0, slope: -6.586100923122325e-05, fluctuations: 0.0\n",
      "step: 88450 loss: 0.527885 time elapsed: 118.0348 learning rate: 0.000881, scenario: 0, slope: -6.41543817693068e-05, fluctuations: 0.0\n",
      "step: 88460 loss: 0.527318 time elapsed: 118.0487 learning rate: 0.000881, scenario: 0, slope: -6.258244405868891e-05, fluctuations: 0.0\n",
      "step: 88470 loss: 0.526761 time elapsed: 118.0626 learning rate: 0.000881, scenario: 0, slope: -6.113120482228874e-05, fluctuations: 0.0\n",
      "step: 88480 loss: 0.526214 time elapsed: 118.0759 learning rate: 0.000881, scenario: 0, slope: -5.9788355582373405e-05, fluctuations: 0.0\n",
      "step: 88490 loss: 0.525676 time elapsed: 118.0897 learning rate: 0.000881, scenario: 0, slope: -5.8543052462143006e-05, fluctuations: 0.0\n",
      "step: 88500 loss: 0.525146 time elapsed: 118.1034 learning rate: 0.000881, scenario: 0, slope: -5.7497750186341184e-05, fluctuations: 0.0\n",
      "step: 88510 loss: 0.524624 time elapsed: 118.1178 learning rate: 0.000881, scenario: 0, slope: -5.630792689179392e-05, fluctuations: 0.0\n",
      "step: 88520 loss: 0.524110 time elapsed: 118.1316 learning rate: 0.000881, scenario: 0, slope: -5.530216759749532e-05, fluctuations: 0.0\n",
      "step: 88530 loss: 0.523603 time elapsed: 118.1453 learning rate: 0.000881, scenario: 0, slope: -5.436181750529052e-05, fluctuations: 0.0\n",
      "step: 88540 loss: 0.523102 time elapsed: 118.1602 learning rate: 0.000881, scenario: 0, slope: -5.348098826662306e-05, fluctuations: 0.0\n",
      "step: 88550 loss: 0.522608 time elapsed: 118.1742 learning rate: 0.000899, scenario: 1, slope: -5.265444351945025e-05, fluctuations: 0.0\n",
      "step: 88560 loss: 0.522092 time elapsed: 118.1867 learning rate: 0.000993, scenario: 1, slope: -5.193967703075358e-05, fluctuations: 0.0\n",
      "step: 88570 loss: 0.521529 time elapsed: 118.1990 learning rate: 0.001096, scenario: 1, slope: -5.156505343172077e-05, fluctuations: 0.0\n",
      "step: 88580 loss: 0.520916 time elapsed: 118.2119 learning rate: 0.001211, scenario: 1, slope: -5.175216082179087e-05, fluctuations: 0.0\n",
      "step: 88590 loss: 0.520249 time elapsed: 118.2243 learning rate: 0.001325, scenario: 0, slope: -5.267607721973677e-05, fluctuations: 0.0\n",
      "step: 88600 loss: 0.519556 time elapsed: 118.2365 learning rate: 0.001325, scenario: 0, slope: -5.419384959384272e-05, fluctuations: 0.0\n",
      "step: 88610 loss: 0.518875 time elapsed: 118.2512 learning rate: 0.001325, scenario: 0, slope: -5.666998245961008e-05, fluctuations: 0.0\n",
      "step: 88620 loss: 0.518204 time elapsed: 118.2652 learning rate: 0.001325, scenario: 0, slope: -5.9195311497743396e-05, fluctuations: 0.0\n",
      "step: 88630 loss: 0.517543 time elapsed: 118.2789 learning rate: 0.001325, scenario: 0, slope: -6.167887696727521e-05, fluctuations: 0.0\n",
      "step: 88640 loss: 0.516892 time elapsed: 118.2925 learning rate: 0.001325, scenario: 0, slope: -6.383748381731849e-05, fluctuations: 0.0\n",
      "step: 88650 loss: 0.516249 time elapsed: 118.3061 learning rate: 0.001325, scenario: 0, slope: -6.539707260688461e-05, fluctuations: 0.0\n",
      "step: 88660 loss: 0.515615 time elapsed: 118.3196 learning rate: 0.001325, scenario: 0, slope: -6.61614079664568e-05, fluctuations: 0.0\n",
      "step: 88670 loss: 0.514989 time elapsed: 118.3331 learning rate: 0.001325, scenario: 0, slope: -6.618118598242967e-05, fluctuations: 0.0\n",
      "step: 88680 loss: 0.514370 time elapsed: 118.3467 learning rate: 0.001325, scenario: 0, slope: -6.562995100139951e-05, fluctuations: 0.0\n",
      "step: 88690 loss: 0.513758 time elapsed: 118.3603 learning rate: 0.001325, scenario: 0, slope: -6.477490577859045e-05, fluctuations: 0.0\n",
      "step: 88700 loss: 0.513153 time elapsed: 118.3741 learning rate: 0.001325, scenario: 0, slope: -6.399253738241865e-05, fluctuations: 0.0\n",
      "step: 88710 loss: 0.512554 time elapsed: 118.3873 learning rate: 0.001325, scenario: 0, slope: -6.309198880041665e-05, fluctuations: 0.0\n",
      "step: 88720 loss: 0.511961 time elapsed: 118.3998 learning rate: 0.001325, scenario: 0, slope: -6.232295581361007e-05, fluctuations: 0.0\n",
      "step: 88730 loss: 0.511373 time elapsed: 118.4122 learning rate: 0.001325, scenario: 0, slope: -6.159762824084521e-05, fluctuations: 0.0\n",
      "step: 88740 loss: 0.510791 time elapsed: 118.4242 learning rate: 0.001325, scenario: 0, slope: -6.091275225991332e-05, fluctuations: 0.0\n",
      "step: 88750 loss: 0.510214 time elapsed: 118.4362 learning rate: 0.001325, scenario: 0, slope: -6.02653607626157e-05, fluctuations: 0.0\n",
      "step: 88760 loss: 0.509642 time elapsed: 118.4499 learning rate: 0.001325, scenario: 0, slope: -5.965277128953958e-05, fluctuations: 0.0\n",
      "step: 88770 loss: 0.509074 time elapsed: 118.4636 learning rate: 0.001325, scenario: 0, slope: -5.90725607072175e-05, fluctuations: 0.0\n",
      "step: 88780 loss: 0.508510 time elapsed: 118.4772 learning rate: 0.001325, scenario: 0, slope: -5.852253569241608e-05, fluctuations: 0.0\n",
      "step: 88790 loss: 0.507951 time elapsed: 118.4910 learning rate: 0.001325, scenario: 0, slope: -5.8000704996409504e-05, fluctuations: 0.0\n",
      "step: 88800 loss: 0.507396 time elapsed: 118.5042 learning rate: 0.001325, scenario: 0, slope: -5.75536612420146e-05, fluctuations: 0.0\n",
      "step: 88810 loss: 0.506844 time elapsed: 118.5183 learning rate: 0.001325, scenario: 0, slope: -5.7034528689281295e-05, fluctuations: 0.0\n",
      "step: 88820 loss: 0.506296 time elapsed: 118.5316 learning rate: 0.001325, scenario: 0, slope: -5.658700751171527e-05, fluctuations: 0.0\n",
      "step: 88830 loss: 0.505752 time elapsed: 118.5453 learning rate: 0.001325, scenario: 0, slope: -5.6161295991086025e-05, fluctuations: 0.0\n",
      "step: 88840 loss: 0.505210 time elapsed: 118.5587 learning rate: 0.001325, scenario: 0, slope: -5.57561083883811e-05, fluctuations: 0.0\n",
      "step: 88850 loss: 0.504672 time elapsed: 118.5718 learning rate: 0.001325, scenario: 0, slope: -5.537025724613024e-05, fluctuations: 0.0\n",
      "step: 88860 loss: 0.504137 time elapsed: 118.5843 learning rate: 0.001325, scenario: 0, slope: -5.5002643462694816e-05, fluctuations: 0.0\n",
      "step: 88870 loss: 0.503604 time elapsed: 118.5967 learning rate: 0.001325, scenario: 0, slope: -5.465224766276056e-05, fluctuations: 0.0\n",
      "step: 88880 loss: 0.503074 time elapsed: 118.6090 learning rate: 0.001325, scenario: 0, slope: -5.431812266238434e-05, fluctuations: 0.0\n",
      "step: 88890 loss: 0.502547 time elapsed: 118.6215 learning rate: 0.001325, scenario: 0, slope: -5.399938686212013e-05, fluctuations: 0.0\n",
      "step: 88900 loss: 0.502023 time elapsed: 118.6336 learning rate: 0.001325, scenario: 0, slope: -5.3725002012697366e-05, fluctuations: 0.0\n",
      "step: 88910 loss: 0.501500 time elapsed: 118.6462 learning rate: 0.001325, scenario: 0, slope: -5.340485015142303e-05, fluctuations: 0.0\n",
      "step: 88920 loss: 0.500980 time elapsed: 118.6598 learning rate: 0.001325, scenario: 0, slope: -5.312756487196181e-05, fluctuations: 0.0\n",
      "step: 88930 loss: 0.500462 time elapsed: 118.6734 learning rate: 0.001325, scenario: 0, slope: -5.2862691413806684e-05, fluctuations: 0.0\n",
      "step: 88940 loss: 0.499946 time elapsed: 118.6869 learning rate: 0.001325, scenario: 0, slope: -5.2609600936718875e-05, fluctuations: 0.0\n",
      "step: 88950 loss: 0.499432 time elapsed: 118.7005 learning rate: 0.001325, scenario: 0, slope: -5.2367703662831963e-05, fluctuations: 0.0\n",
      "step: 88960 loss: 0.498920 time elapsed: 118.7142 learning rate: 0.001325, scenario: 0, slope: -5.213644592494627e-05, fluctuations: 0.0\n",
      "step: 88970 loss: 0.498410 time elapsed: 118.7278 learning rate: 0.001325, scenario: 0, slope: -5.191530749606745e-05, fluctuations: 0.0\n",
      "step: 88980 loss: 0.497901 time elapsed: 118.7413 learning rate: 0.001325, scenario: 0, slope: -5.170379916597476e-05, fluctuations: 0.0\n",
      "step: 88990 loss: 0.497395 time elapsed: 118.7550 learning rate: 0.001325, scenario: 0, slope: -5.150146053532018e-05, fluctuations: 0.0\n",
      "step: 89000 loss: 0.496889 time elapsed: 118.7683 learning rate: 0.001325, scenario: 0, slope: -5.132683696459825e-05, fluctuations: 0.0\n",
      "step: 89010 loss: 0.496386 time elapsed: 118.7823 learning rate: 0.001325, scenario: 0, slope: -5.112258291536553e-05, fluctuations: 0.0\n",
      "step: 89020 loss: 0.495883 time elapsed: 118.7948 learning rate: 0.001325, scenario: 0, slope: -5.0945249890210745e-05, fluctuations: 0.0\n",
      "step: 89030 loss: 0.495382 time elapsed: 118.8071 learning rate: 0.001325, scenario: 0, slope: -5.077549495236375e-05, fluctuations: 0.0\n",
      "step: 89040 loss: 0.494883 time elapsed: 118.8193 learning rate: 0.001325, scenario: 0, slope: -5.061255807623728e-05, fluctuations: 0.0\n",
      "step: 89050 loss: 0.495290 time elapsed: 118.8317 learning rate: 0.001338, scenario: 1, slope: -4.9493964456886054e-05, fluctuations: 0.0\n",
      "step: 89060 loss: 22.599619 time elapsed: 118.8441 learning rate: 0.001317, scenario: -1, slope: 0.020285503411661326, fluctuations: 0.0\n",
      "step: 89070 loss: 32.375737 time elapsed: 118.8564 learning rate: 0.001191, scenario: -1, slope: 0.1598916149140278, fluctuations: 0.02\n",
      "step: 89080 loss: 1.677008 time elapsed: 118.8705 learning rate: 0.001078, scenario: -1, slope: 0.14309686574326366, fluctuations: 0.06\n",
      "step: 89090 loss: 1.989200 time elapsed: 118.8842 learning rate: 0.000974, scenario: -1, slope: 0.12114322675892662, fluctuations: 0.1\n",
      "step: 89100 loss: 0.787484 time elapsed: 118.8978 learning rate: 0.000890, scenario: -1, slope: 0.09283245113940758, fluctuations: 0.14\n",
      "step: 89110 loss: 0.713215 time elapsed: 118.9122 learning rate: 0.000805, scenario: -1, slope: 0.04980870107026261, fluctuations: 0.19\n",
      "step: 89120 loss: 0.525310 time elapsed: 118.9265 learning rate: 0.000728, scenario: -1, slope: 0.006651885205070533, fluctuations: 0.24\n",
      "step: 89130 loss: 0.505034 time elapsed: 118.9405 learning rate: 0.000721, scenario: 0, slope: -0.041854626583747674, fluctuations: 0.29\n",
      "step: 89140 loss: 0.507220 time elapsed: 118.9542 learning rate: 0.000721, scenario: 0, slope: -0.09644080730568344, fluctuations: 0.33\n",
      "step: 89150 loss: 0.497061 time elapsed: 118.9676 learning rate: 0.000721, scenario: 0, slope: -0.1618850884989442, fluctuations: 0.38\n",
      "step: 89160 loss: 0.496193 time elapsed: 118.9811 learning rate: 0.000721, scenario: 0, slope: -0.2234438657784933, fluctuations: 0.43\n",
      "step: 89170 loss: 0.495927 time elapsed: 118.9962 learning rate: 0.000721, scenario: 0, slope: -0.03432742873526064, fluctuations: 0.44\n",
      "step: 89180 loss: 0.495195 time elapsed: 119.0095 learning rate: 0.000721, scenario: 0, slope: -0.016217270588192292, fluctuations: 0.45\n",
      "step: 89190 loss: 0.494781 time elapsed: 119.0223 learning rate: 0.000721, scenario: 0, slope: -0.003569900259528704, fluctuations: 0.45\n",
      "step: 89200 loss: 0.494464 time elapsed: 119.0344 learning rate: 0.000721, scenario: 0, slope: -0.0014352401469130724, fluctuations: 0.42\n",
      "step: 89210 loss: 0.494139 time elapsed: 119.0471 learning rate: 0.000721, scenario: 0, slope: -0.0003421519429914374, fluctuations: 0.37\n",
      "step: 89220 loss: 0.493821 time elapsed: 119.0594 learning rate: 0.000721, scenario: 0, slope: -0.00013859207558113677, fluctuations: 0.32\n",
      "step: 89230 loss: 0.493514 time elapsed: 119.0719 learning rate: 0.000721, scenario: 0, slope: -8.104311882661942e-05, fluctuations: 0.27\n",
      "step: 89240 loss: 0.493216 time elapsed: 119.0860 learning rate: 0.000692, scenario: -1, slope: -4.260546080929598e-05, fluctuations: 0.23\n",
      "step: 89250 loss: 0.492940 time elapsed: 119.0999 learning rate: 0.000626, scenario: -1, slope: -3.698601565254147e-05, fluctuations: 0.18\n",
      "step: 89260 loss: 0.492694 time elapsed: 119.1140 learning rate: 0.000566, scenario: -1, slope: -3.333163600369606e-05, fluctuations: 0.13\n",
      "step: 89270 loss: 0.492473 time elapsed: 119.1277 learning rate: 0.000512, scenario: -1, slope: -3.0441745845470314e-05, fluctuations: 0.09\n",
      "step: 89280 loss: 0.492274 time elapsed: 119.1412 learning rate: 0.000463, scenario: -1, slope: -2.8882704233826767e-05, fluctuations: 0.05\n",
      "step: 89290 loss: 0.492095 time elapsed: 119.1548 learning rate: 0.000425, scenario: 1, slope: -2.7395875913237654e-05, fluctuations: 0.0\n",
      "step: 89300 loss: 0.491920 time elapsed: 119.1681 learning rate: 0.000465, scenario: 1, slope: -2.5696253538300353e-05, fluctuations: 0.0\n",
      "step: 89310 loss: 0.491728 time elapsed: 119.1822 learning rate: 0.000514, scenario: 1, slope: -2.369242825283167e-05, fluctuations: 0.0\n",
      "step: 89320 loss: 0.491517 time elapsed: 119.1965 learning rate: 0.000568, scenario: 1, slope: -2.2119256700109197e-05, fluctuations: 0.0\n",
      "step: 89330 loss: 0.491285 time elapsed: 119.2111 learning rate: 0.000627, scenario: 1, slope: -2.097663535359761e-05, fluctuations: 0.0\n",
      "step: 89340 loss: 0.491030 time elapsed: 119.2238 learning rate: 0.000693, scenario: 1, slope: -2.0445713878284247e-05, fluctuations: 0.0\n",
      "step: 89350 loss: 0.490749 time elapsed: 119.2363 learning rate: 0.000765, scenario: 1, slope: -2.0627367682008345e-05, fluctuations: 0.0\n",
      "step: 89360 loss: 0.490440 time elapsed: 119.2490 learning rate: 0.000845, scenario: 1, slope: -2.150381987564145e-05, fluctuations: 0.0\n",
      "step: 89370 loss: 0.490100 time elapsed: 119.2613 learning rate: 0.000933, scenario: 1, slope: -2.3011297839401305e-05, fluctuations: 0.0\n",
      "step: 89380 loss: 0.489726 time elapsed: 119.2736 learning rate: 0.001031, scenario: 1, slope: -2.5055437075379802e-05, fluctuations: 0.0\n",
      "step: 89390 loss: 0.489316 time elapsed: 119.2857 learning rate: 0.001139, scenario: 1, slope: -2.750969787944993e-05, fluctuations: 0.0\n",
      "step: 89400 loss: 0.488866 time elapsed: 119.2993 learning rate: 0.001246, scenario: 1, slope: -2.9960841323476507e-05, fluctuations: 0.0\n",
      "step: 89410 loss: 0.488376 time elapsed: 119.3140 learning rate: 0.001376, scenario: 1, slope: -3.323835791198995e-05, fluctuations: 0.0\n",
      "step: 89420 loss: 0.487838 time elapsed: 119.3276 learning rate: 0.001520, scenario: 1, slope: -3.649170842658974e-05, fluctuations: 0.0\n",
      "step: 89430 loss: 0.487248 time elapsed: 119.3411 learning rate: 0.001679, scenario: 1, slope: -4.003674004254864e-05, fluctuations: 0.0\n",
      "step: 89440 loss: 6.928163 time elapsed: 119.3546 learning rate: 0.001755, scenario: -1, slope: 0.0043891548352158844, fluctuations: 0.0\n",
      "step: 89450 loss: 309.996976 time elapsed: 119.3681 learning rate: 0.001588, scenario: -1, slope: 1.3500622085447724, fluctuations: 0.03\n",
      "step: 89460 loss: 134.466720 time elapsed: 119.3817 learning rate: 0.001436, scenario: -1, slope: 1.4017361022852723, fluctuations: 0.08\n",
      "step: 89470 loss: 43.039205 time elapsed: 119.3950 learning rate: 0.001299, scenario: -1, slope: 1.2028063697309164, fluctuations: 0.12\n",
      "step: 89480 loss: 15.789406 time elapsed: 119.4087 learning rate: 0.001174, scenario: -1, slope: 0.880354099833072, fluctuations: 0.16\n",
      "step: 89490 loss: 4.011731 time elapsed: 119.4223 learning rate: 0.001062, scenario: -1, slope: 0.5101145389572511, fluctuations: 0.2\n",
      "step: 89500 loss: 1.315118 time elapsed: 119.4346 learning rate: 0.000970, scenario: -1, slope: 0.14242544755198944, fluctuations: 0.23\n",
      "step: 89510 loss: 0.958071 time elapsed: 119.4473 learning rate: 0.000951, scenario: 0, slope: -0.3091470702205699, fluctuations: 0.27\n",
      "step: 89520 loss: 0.846624 time elapsed: 119.4594 learning rate: 0.000951, scenario: 0, slope: -0.7640118462146323, fluctuations: 0.3\n",
      "step: 89530 loss: 0.661770 time elapsed: 119.4715 learning rate: 0.000951, scenario: 0, slope: -1.332256948977153, fluctuations: 0.34\n",
      "step: 89540 loss: 0.666027 time elapsed: 119.4836 learning rate: 0.000951, scenario: 0, slope: -2.1503946845561757, fluctuations: 0.37\n",
      "step: 89550 loss: 0.630820 time elapsed: 119.4959 learning rate: 0.000951, scenario: 0, slope: -0.5184209149150717, fluctuations: 0.37\n",
      "step: 89560 loss: 0.622402 time elapsed: 119.5092 learning rate: 0.000951, scenario: 0, slope: -0.19162464285600883, fluctuations: 0.35\n",
      "step: 89570 loss: 0.612415 time elapsed: 119.5228 learning rate: 0.000951, scenario: 0, slope: -0.0529418581330375, fluctuations: 0.35\n",
      "step: 89580 loss: 0.605689 time elapsed: 119.5366 learning rate: 0.000951, scenario: 0, slope: -0.01958016926549779, fluctuations: 0.31\n",
      "step: 89590 loss: 0.599848 time elapsed: 119.5502 learning rate: 0.000951, scenario: 0, slope: -0.009397406002030397, fluctuations: 0.27\n",
      "step: 89600 loss: 0.594845 time elapsed: 119.5634 learning rate: 0.000951, scenario: 0, slope: -0.0037138933443473274, fluctuations: 0.24\n",
      "step: 89610 loss: 0.590435 time elapsed: 119.5776 learning rate: 0.000951, scenario: 0, slope: -0.0020848527855991564, fluctuations: 0.2\n",
      "step: 89620 loss: 0.586493 time elapsed: 119.5911 learning rate: 0.000951, scenario: 0, slope: -0.001076414635574357, fluctuations: 0.17\n",
      "step: 89630 loss: 0.582932 time elapsed: 119.6047 learning rate: 0.000951, scenario: 0, slope: -0.0007974022468982297, fluctuations: 0.13\n",
      "step: 89640 loss: 0.579690 time elapsed: 119.6183 learning rate: 0.000951, scenario: 0, slope: -0.0005948853727608413, fluctuations: 0.1\n",
      "step: 89650 loss: 0.576716 time elapsed: 119.6321 learning rate: 0.000951, scenario: 0, slope: -0.0004970259634218961, fluctuations: 0.07\n",
      "step: 89660 loss: 0.573976 time elapsed: 119.6448 learning rate: 0.000951, scenario: 0, slope: -0.0004367324304876075, fluctuations: 0.03\n",
      "step: 89670 loss: 0.571436 time elapsed: 119.6572 learning rate: 0.000951, scenario: 0, slope: -0.00039277425308595473, fluctuations: 0.0\n",
      "step: 89680 loss: 0.569074 time elapsed: 119.6694 learning rate: 0.000951, scenario: 0, slope: -0.0003526090497701671, fluctuations: 0.0\n",
      "step: 89690 loss: 0.566868 time elapsed: 119.6817 learning rate: 0.000951, scenario: 0, slope: -0.000320004946657456, fluctuations: 0.0\n",
      "step: 89700 loss: 0.564800 time elapsed: 119.6937 learning rate: 0.000951, scenario: 0, slope: -0.00029533695563222677, fluctuations: 0.0\n",
      "step: 89710 loss: 0.562857 time elapsed: 119.7067 learning rate: 0.000951, scenario: 0, slope: -0.00026964854893058425, fluctuations: 0.0\n",
      "step: 89720 loss: 0.561025 time elapsed: 119.7204 learning rate: 0.000951, scenario: 0, slope: -0.00024964562266170025, fluctuations: 0.0\n",
      "step: 89730 loss: 0.559293 time elapsed: 119.7341 learning rate: 0.000951, scenario: 0, slope: -0.00023218291350768247, fluctuations: 0.0\n",
      "step: 89740 loss: 0.557652 time elapsed: 119.7475 learning rate: 0.000951, scenario: 0, slope: -0.00021680891034384997, fluctuations: 0.0\n",
      "step: 89750 loss: 0.556094 time elapsed: 119.7608 learning rate: 0.000951, scenario: 0, slope: -0.00020317665359443032, fluctuations: 0.0\n",
      "step: 89760 loss: 0.554610 time elapsed: 119.7743 learning rate: 0.000951, scenario: 0, slope: -0.0001910153154041896, fluctuations: 0.0\n",
      "step: 89770 loss: 0.553196 time elapsed: 119.7878 learning rate: 0.000951, scenario: 0, slope: -0.00018010855247320735, fluctuations: 0.0\n",
      "step: 89780 loss: 0.551845 time elapsed: 119.8013 learning rate: 0.000951, scenario: 0, slope: -0.00017028075886940043, fluctuations: 0.0\n",
      "step: 89790 loss: 0.550552 time elapsed: 119.8150 learning rate: 0.000951, scenario: 0, slope: -0.0001613876198810167, fluctuations: 0.0\n",
      "step: 89800 loss: 0.549313 time elapsed: 119.8282 learning rate: 0.000951, scenario: 0, slope: -0.00015408336752716014, fluctuations: 0.0\n",
      "step: 89810 loss: 0.548123 time elapsed: 119.8423 learning rate: 0.000951, scenario: 0, slope: -0.00014594488885397034, fluctuations: 0.0\n",
      "step: 89820 loss: 0.546979 time elapsed: 119.8558 learning rate: 0.000951, scenario: 0, slope: -0.00013920973633437474, fluctuations: 0.0\n",
      "step: 89830 loss: 0.545878 time elapsed: 119.8685 learning rate: 0.000951, scenario: 0, slope: -0.0001330314267319229, fluctuations: 0.0\n",
      "step: 89840 loss: 0.544817 time elapsed: 119.8811 learning rate: 0.000951, scenario: 0, slope: -0.00012734808542947833, fluctuations: 0.0\n",
      "step: 89850 loss: 0.543792 time elapsed: 119.8933 learning rate: 0.000951, scenario: 0, slope: -0.00012210648031141962, fluctuations: 0.0\n",
      "step: 89860 loss: 0.542802 time elapsed: 119.9055 learning rate: 0.000951, scenario: 0, slope: -0.00011726059374453074, fluctuations: 0.0\n",
      "step: 89870 loss: 0.541844 time elapsed: 119.9177 learning rate: 0.000951, scenario: 0, slope: -0.00011277046789178229, fluctuations: 0.0\n",
      "step: 89880 loss: 0.540917 time elapsed: 119.9310 learning rate: 0.000951, scenario: 0, slope: -0.00010860126285711529, fluctuations: 0.0\n",
      "step: 89890 loss: 0.540018 time elapsed: 119.9447 learning rate: 0.000951, scenario: 0, slope: -0.00010472248233611935, fluctuations: 0.0\n",
      "step: 89900 loss: 0.539145 time elapsed: 119.9583 learning rate: 0.000951, scenario: 0, slope: -0.00010145768017608128, fluctuations: 0.0\n",
      "step: 89910 loss: 0.538297 time elapsed: 119.9726 learning rate: 0.000951, scenario: 0, slope: -9.773218861858302e-05, fluctuations: 0.0\n",
      "step: 89920 loss: 0.537473 time elapsed: 119.9861 learning rate: 0.000951, scenario: 0, slope: -9.457614719355536e-05, fluctuations: 0.0\n",
      "step: 89930 loss: 0.536671 time elapsed: 119.9996 learning rate: 0.000951, scenario: 0, slope: -9.162065098239285e-05, fluctuations: 0.0\n",
      "step: 89940 loss: 0.535890 time elapsed: 120.0133 learning rate: 0.000951, scenario: 0, slope: -8.884917105596861e-05, fluctuations: 0.0\n",
      "step: 89950 loss: 0.535129 time elapsed: 120.0268 learning rate: 0.000951, scenario: 0, slope: -8.624693781556299e-05, fluctuations: 0.0\n",
      "step: 89960 loss: 0.534386 time elapsed: 120.0401 learning rate: 0.000951, scenario: 0, slope: -8.38007122257996e-05, fluctuations: 0.0\n",
      "step: 89970 loss: 0.533661 time elapsed: 120.0538 learning rate: 0.000951, scenario: 0, slope: -8.149859079206128e-05, fluctuations: 0.0\n",
      "step: 89980 loss: 0.532953 time elapsed: 120.0663 learning rate: 0.000951, scenario: 0, slope: -7.932983891207234e-05, fluctuations: 0.0\n",
      "step: 89990 loss: 0.532260 time elapsed: 120.0792 learning rate: 0.000951, scenario: 0, slope: -7.728474816682378e-05, fluctuations: 0.0\n",
      "step: 90000 loss: 0.531582 time elapsed: 120.0914 learning rate: 0.000951, scenario: 0, slope: -7.55426027562433e-05, fluctuations: 0.0\n",
      "step: 90010 loss: 0.530919 time elapsed: 120.1043 learning rate: 0.000951, scenario: 0, slope: -7.353112973301281e-05, fluctuations: 0.0\n",
      "step: 90020 loss: 0.530269 time elapsed: 120.1166 learning rate: 0.000951, scenario: 0, slope: -7.180729718435311e-05, fluctuations: 0.0\n",
      "step: 90030 loss: 0.529631 time elapsed: 120.1289 learning rate: 0.000951, scenario: 0, slope: -7.017634689224423e-05, fluctuations: 0.0\n",
      "step: 90040 loss: 0.529006 time elapsed: 120.1425 learning rate: 0.000951, scenario: 0, slope: -6.863217086495518e-05, fluctuations: 0.0\n",
      "step: 90050 loss: 0.528392 time elapsed: 120.1566 learning rate: 0.000951, scenario: 0, slope: -6.716916340339568e-05, fluctuations: 0.0\n",
      "step: 90060 loss: 0.527789 time elapsed: 120.1707 learning rate: 0.000951, scenario: 0, slope: -6.578216962384583e-05, fluctuations: 0.0\n",
      "step: 90070 loss: 0.527197 time elapsed: 120.1842 learning rate: 0.000951, scenario: 0, slope: -6.44664404252715e-05, fluctuations: 0.0\n",
      "step: 90080 loss: 0.526614 time elapsed: 120.1977 learning rate: 0.000951, scenario: 0, slope: -6.321759296470943e-05, fluctuations: 0.0\n",
      "step: 90090 loss: 0.526041 time elapsed: 120.2114 learning rate: 0.000951, scenario: 0, slope: -6.203157585264902e-05, fluctuations: 0.0\n",
      "step: 90100 loss: 0.525477 time elapsed: 120.2250 learning rate: 0.000951, scenario: 0, slope: -6.101477469583987e-05, fluctuations: 0.0\n",
      "step: 90110 loss: 0.524921 time elapsed: 120.2390 learning rate: 0.000951, scenario: 0, slope: -5.983330340303179e-05, fluctuations: 0.0\n",
      "step: 90120 loss: 0.524374 time elapsed: 120.2525 learning rate: 0.000951, scenario: 0, slope: -5.881434285161894e-05, fluctuations: 0.0\n",
      "step: 90130 loss: 0.523834 time elapsed: 120.2684 learning rate: 0.000951, scenario: 0, slope: -5.7844756390846764e-05, fluctuations: 0.0\n",
      "step: 90140 loss: 0.523302 time elapsed: 120.2812 learning rate: 0.000951, scenario: 0, slope: -5.692175195896778e-05, fluctuations: 0.0\n",
      "step: 90150 loss: 0.522777 time elapsed: 120.2939 learning rate: 0.000951, scenario: 0, slope: -5.6042728449451214e-05, fluctuations: 0.0\n",
      "step: 90160 loss: 0.522258 time elapsed: 120.3064 learning rate: 0.000951, scenario: 0, slope: -5.520526010235904e-05, fluctuations: 0.0\n",
      "step: 90170 loss: 0.521746 time elapsed: 120.3183 learning rate: 0.000951, scenario: 0, slope: -5.440708241844259e-05, fluctuations: 0.0\n",
      "step: 90180 loss: 0.521240 time elapsed: 120.3308 learning rate: 0.000951, scenario: 0, slope: -5.3646079415787356e-05, fluctuations: 0.0\n",
      "step: 90190 loss: 0.520740 time elapsed: 120.3430 learning rate: 0.000951, scenario: 0, slope: -5.2920272073653024e-05, fluctuations: 0.0\n",
      "step: 90200 loss: 0.520241 time elapsed: 120.3565 learning rate: 0.000999, scenario: 1, slope: -5.229851588960175e-05, fluctuations: 0.0\n",
      "step: 90210 loss: 0.519708 time elapsed: 120.3710 learning rate: 0.001104, scenario: 1, slope: -5.171048544293649e-05, fluctuations: 0.0\n",
      "step: 90220 loss: 0.519127 time elapsed: 120.3852 learning rate: 0.001219, scenario: 1, slope: -5.158341908550193e-05, fluctuations: 0.0\n",
      "step: 90230 loss: 0.518494 time elapsed: 120.3987 learning rate: 0.001347, scenario: 1, slope: -5.2064634420228945e-05, fluctuations: 0.0\n",
      "step: 90240 loss: 0.517815 time elapsed: 120.4122 learning rate: 0.001388, scenario: 0, slope: -5.330483228015117e-05, fluctuations: 0.0\n",
      "step: 90250 loss: 0.517140 time elapsed: 120.4255 learning rate: 0.001388, scenario: 0, slope: -5.5197690111333077e-05, fluctuations: 0.0\n",
      "step: 90260 loss: 0.516475 time elapsed: 120.4389 learning rate: 0.001388, scenario: 0, slope: -5.746539328301333e-05, fluctuations: 0.0\n",
      "step: 90270 loss: 0.515819 time elapsed: 120.4526 learning rate: 0.001388, scenario: 0, slope: -5.983447796122611e-05, fluctuations: 0.0\n",
      "step: 90280 loss: 0.515172 time elapsed: 120.4663 learning rate: 0.001388, scenario: 0, slope: -6.204003626630294e-05, fluctuations: 0.0\n",
      "step: 90290 loss: 0.514534 time elapsed: 120.4795 learning rate: 0.001388, scenario: 0, slope: -6.38253187724731e-05, fluctuations: 0.0\n",
      "step: 90300 loss: 0.513903 time elapsed: 120.4921 learning rate: 0.001388, scenario: 0, slope: -6.486972862521979e-05, fluctuations: 0.0\n",
      "step: 90310 loss: 0.513280 time elapsed: 120.5048 learning rate: 0.001388, scenario: 0, slope: -6.531276795048483e-05, fluctuations: 0.0\n",
      "step: 90320 loss: 0.512664 time elapsed: 120.5171 learning rate: 0.001388, scenario: 0, slope: -6.503338704759001e-05, fluctuations: 0.0\n",
      "step: 90330 loss: 0.512055 time elapsed: 120.5294 learning rate: 0.001388, scenario: 0, slope: -6.433328949791048e-05, fluctuations: 0.0\n",
      "step: 90340 loss: 0.511452 time elapsed: 120.5418 learning rate: 0.001388, scenario: 0, slope: -6.351745945471604e-05, fluctuations: 0.0\n",
      "step: 90350 loss: 0.510855 time elapsed: 120.5539 learning rate: 0.001388, scenario: 0, slope: -6.274346681357333e-05, fluctuations: 0.0\n",
      "step: 90360 loss: 0.510263 time elapsed: 120.5671 learning rate: 0.001388, scenario: 0, slope: -6.201326425536503e-05, fluctuations: 0.0\n",
      "step: 90370 loss: 0.509677 time elapsed: 120.5811 learning rate: 0.001388, scenario: 0, slope: -6.132425907717192e-05, fluctuations: 0.0\n",
      "step: 90380 loss: 0.509096 time elapsed: 120.5947 learning rate: 0.001388, scenario: 0, slope: -6.06738147242223e-05, fluctuations: 0.0\n",
      "step: 90390 loss: 0.508519 time elapsed: 120.6086 learning rate: 0.001388, scenario: 0, slope: -6.005941594563668e-05, fluctuations: 0.0\n",
      "step: 90400 loss: 0.507947 time elapsed: 120.6217 learning rate: 0.001388, scenario: 0, slope: -5.953533137151021e-05, fluctuations: 0.0\n",
      "step: 90410 loss: 0.507380 time elapsed: 120.6360 learning rate: 0.001388, scenario: 0, slope: -5.892953241131857e-05, fluctuations: 0.0\n",
      "step: 90420 loss: 0.506816 time elapsed: 120.6496 learning rate: 0.001388, scenario: 0, slope: -5.8409856603253244e-05, fluctuations: 0.0\n",
      "step: 90430 loss: 0.506257 time elapsed: 120.6636 learning rate: 0.001388, scenario: 0, slope: -5.79178239980887e-05, fluctuations: 0.0\n",
      "step: 90440 loss: 0.505700 time elapsed: 120.6773 learning rate: 0.001388, scenario: 0, slope: -5.7451710735933014e-05, fluctuations: 0.0\n",
      "step: 90450 loss: 0.505148 time elapsed: 120.6911 learning rate: 0.001388, scenario: 0, slope: -5.7009920276000006e-05, fluctuations: 0.0\n",
      "step: 90460 loss: 0.504599 time elapsed: 120.7038 learning rate: 0.001388, scenario: 0, slope: -5.659097273598422e-05, fluctuations: 0.0\n",
      "step: 90470 loss: 0.504052 time elapsed: 120.7170 learning rate: 0.001388, scenario: 0, slope: -5.619349518749448e-05, fluctuations: 0.0\n",
      "step: 90480 loss: 0.503509 time elapsed: 120.7293 learning rate: 0.001388, scenario: 0, slope: -5.581621284453231e-05, fluctuations: 0.0\n",
      "step: 90490 loss: 0.502969 time elapsed: 120.7417 learning rate: 0.001388, scenario: 0, slope: -5.5457941063542415e-05, fluctuations: 0.0\n",
      "step: 90500 loss: 0.502431 time elapsed: 120.7539 learning rate: 0.001388, scenario: 0, slope: -5.515083827354076e-05, fluctuations: 0.0\n",
      "step: 90510 loss: 0.501896 time elapsed: 120.7668 learning rate: 0.001388, scenario: 0, slope: -5.479409836352486e-05, fluctuations: 0.0\n",
      "step: 90520 loss: 0.501364 time elapsed: 120.7812 learning rate: 0.001388, scenario: 0, slope: -5.448654665726748e-05, fluctuations: 0.0\n",
      "step: 90530 loss: 0.500833 time elapsed: 120.7951 learning rate: 0.001388, scenario: 0, slope: -5.4194032421539455e-05, fluctuations: 0.0\n",
      "step: 90540 loss: 0.500305 time elapsed: 120.8090 learning rate: 0.001388, scenario: 0, slope: -5.391572484971384e-05, fluctuations: 0.0\n",
      "step: 90550 loss: 0.499779 time elapsed: 120.8227 learning rate: 0.001388, scenario: 0, slope: -5.365084827848036e-05, fluctuations: 0.0\n",
      "step: 90560 loss: 0.499255 time elapsed: 120.8365 learning rate: 0.001388, scenario: 0, slope: -5.339867799411051e-05, fluctuations: 0.0\n",
      "step: 90570 loss: 0.498733 time elapsed: 120.8501 learning rate: 0.001388, scenario: 0, slope: -5.315853639187023e-05, fluctuations: 0.0\n",
      "step: 90580 loss: 0.498213 time elapsed: 120.8637 learning rate: 0.001388, scenario: 0, slope: -5.292978945629288e-05, fluctuations: 0.0\n",
      "step: 90590 loss: 0.497695 time elapsed: 120.8774 learning rate: 0.001388, scenario: 0, slope: -5.271184353193171e-05, fluctuations: 0.0\n",
      "step: 90600 loss: 0.497178 time elapsed: 120.8908 learning rate: 0.001388, scenario: 0, slope: -5.252446658747324e-05, fluctuations: 0.0\n",
      "step: 90610 loss: 0.496663 time elapsed: 120.9049 learning rate: 0.001388, scenario: 0, slope: -5.230616434804802e-05, fluctuations: 0.0\n",
      "step: 90620 loss: 0.496149 time elapsed: 120.9174 learning rate: 0.001388, scenario: 0, slope: -5.2117420078806846e-05, fluctuations: 0.0\n",
      "step: 90630 loss: 0.495637 time elapsed: 120.9301 learning rate: 0.001388, scenario: 0, slope: -5.193744999298135e-05, fluctuations: 0.0\n",
      "step: 90640 loss: 0.495127 time elapsed: 120.9427 learning rate: 0.001388, scenario: 0, slope: -5.1765822274249866e-05, fluctuations: 0.0\n",
      "step: 90650 loss: 0.494617 time elapsed: 120.9550 learning rate: 0.001388, scenario: 0, slope: -5.1602130893780466e-05, fluctuations: 0.0\n",
      "step: 90660 loss: 0.494109 time elapsed: 120.9678 learning rate: 0.001388, scenario: 0, slope: -5.1445993809227606e-05, fluctuations: 0.0\n",
      "step: 90670 loss: 0.493602 time elapsed: 120.9801 learning rate: 0.001388, scenario: 0, slope: -5.1297051302999406e-05, fluctuations: 0.0\n",
      "step: 90680 loss: 0.493096 time elapsed: 120.9939 learning rate: 0.001388, scenario: 0, slope: -5.11549644468988e-05, fluctuations: 0.0\n",
      "step: 90690 loss: 0.492591 time elapsed: 121.0082 learning rate: 0.001388, scenario: 0, slope: -5.1019411304373997e-05, fluctuations: 0.0\n",
      "step: 90700 loss: 0.492091 time elapsed: 121.0219 learning rate: 0.001388, scenario: 0, slope: -5.0901050118006566e-05, fluctuations: 0.0\n",
      "step: 90710 loss: 0.500324 time elapsed: 121.0363 learning rate: 0.001430, scenario: 1, slope: -4.164245950237836e-05, fluctuations: 0.0\n",
      "step: 90720 loss: 46.756727 time elapsed: 121.0496 learning rate: 0.001326, scenario: -1, slope: 0.07042666252947737, fluctuations: 0.0\n",
      "step: 90730 loss: 2.391966 time elapsed: 121.0628 learning rate: 0.001199, scenario: -1, slope: 0.08232101191534329, fluctuations: 0.03\n",
      "step: 90740 loss: 3.129658 time elapsed: 121.0764 learning rate: 0.001085, scenario: -1, slope: 0.07941088859734993, fluctuations: 0.06\n",
      "step: 90750 loss: 1.285867 time elapsed: 121.0905 learning rate: 0.000981, scenario: -1, slope: 0.06421076457208225, fluctuations: 0.1\n",
      "step: 90760 loss: 0.636626 time elapsed: 121.1046 learning rate: 0.000887, scenario: -1, slope: 0.04723096792116279, fluctuations: 0.15\n",
      "step: 90770 loss: 0.564408 time elapsed: 121.1185 learning rate: 0.000802, scenario: -1, slope: 0.022552875746710735, fluctuations: 0.19\n",
      "step: 90780 loss: 0.512922 time elapsed: 121.1342 learning rate: 0.000733, scenario: 0, slope: -0.002469994946524614, fluctuations: 0.24\n",
      "step: 90790 loss: 0.499939 time elapsed: 121.1472 learning rate: 0.000733, scenario: 0, slope: -0.030622360884978408, fluctuations: 0.29\n",
      "step: 90800 loss: 0.492899 time elapsed: 121.1598 learning rate: 0.000733, scenario: 0, slope: -0.059446041932311844, fluctuations: 0.34\n",
      "step: 90810 loss: 0.490758 time elapsed: 121.1728 learning rate: 0.000733, scenario: 0, slope: -0.10215997086159705, fluctuations: 0.39\n",
      "step: 90820 loss: 0.490423 time elapsed: 121.1849 learning rate: 0.000733, scenario: 0, slope: -0.07768908479430592, fluctuations: 0.43\n",
      "step: 90830 loss: 0.490154 time elapsed: 121.1976 learning rate: 0.000733, scenario: 0, slope: -0.021916593371230943, fluctuations: 0.45\n",
      "step: 90840 loss: 0.489847 time elapsed: 121.2129 learning rate: 0.000733, scenario: 0, slope: -0.003527808890651735, fluctuations: 0.42\n",
      "step: 90850 loss: 0.489560 time elapsed: 121.2272 learning rate: 0.000733, scenario: 0, slope: -0.0010404085451574182, fluctuations: 0.38\n",
      "step: 90860 loss: 0.489286 time elapsed: 121.2415 learning rate: 0.000733, scenario: 0, slope: -0.0003035691427814597, fluctuations: 0.34\n",
      "step: 90870 loss: 0.489016 time elapsed: 121.2552 learning rate: 0.000733, scenario: 0, slope: -0.00011470570850169922, fluctuations: 0.29\n",
      "step: 90880 loss: 0.488749 time elapsed: 121.2688 learning rate: 0.000733, scenario: 0, slope: -5.248563056528168e-05, fluctuations: 0.24\n",
      "step: 90890 loss: 0.488489 time elapsed: 121.2826 learning rate: 0.000683, scenario: -1, slope: -3.725401593295578e-05, fluctuations: 0.19\n",
      "step: 90900 loss: 0.488252 time elapsed: 121.2960 learning rate: 0.000624, scenario: -1, slope: -3.271387967811235e-05, fluctuations: 0.14\n",
      "step: 90910 loss: 0.488036 time elapsed: 121.3101 learning rate: 0.000564, scenario: -1, slope: -2.7822673422508687e-05, fluctuations: 0.09\n",
      "step: 90920 loss: 0.487840 time elapsed: 121.3241 learning rate: 0.000510, scenario: -1, slope: -2.6453739015248227e-05, fluctuations: 0.04\n",
      "step: 90930 loss: 0.487663 time elapsed: 121.3387 learning rate: 0.000488, scenario: 1, slope: -2.518525766894574e-05, fluctuations: 0.0\n",
      "step: 90940 loss: 0.487482 time elapsed: 121.3519 learning rate: 0.000539, scenario: 1, slope: -2.3799022938841832e-05, fluctuations: 0.0\n",
      "step: 90950 loss: 0.487282 time elapsed: 121.3651 learning rate: 0.000595, scenario: 1, slope: -2.2542447521232622e-05, fluctuations: 0.0\n",
      "step: 90960 loss: 0.487060 time elapsed: 121.3776 learning rate: 0.000657, scenario: 1, slope: -2.150796097503128e-05, fluctuations: 0.0\n",
      "step: 90970 loss: 0.486815 time elapsed: 121.3899 learning rate: 0.000726, scenario: 1, slope: -2.0838718680439663e-05, fluctuations: 0.0\n",
      "step: 90980 loss: 0.486545 time elapsed: 121.4026 learning rate: 0.000802, scenario: 1, slope: -2.0684715673611154e-05, fluctuations: 0.0\n",
      "step: 90990 loss: 0.486246 time elapsed: 121.4164 learning rate: 0.000886, scenario: 1, slope: -2.1179744108048733e-05, fluctuations: 0.0\n",
      "step: 91000 loss: 0.485916 time elapsed: 121.4299 learning rate: 0.000969, scenario: 1, slope: -2.221002830389175e-05, fluctuations: 0.0\n",
      "step: 91010 loss: 0.485555 time elapsed: 121.4441 learning rate: 0.001070, scenario: 1, slope: -2.41533912766784e-05, fluctuations: 0.0\n",
      "step: 91020 loss: 0.485156 time elapsed: 121.4577 learning rate: 0.001182, scenario: 1, slope: -2.647081500317068e-05, fluctuations: 0.0\n",
      "step: 91030 loss: 0.484717 time elapsed: 121.4711 learning rate: 0.001306, scenario: 1, slope: -2.9181673711053346e-05, fluctuations: 0.0\n",
      "step: 91040 loss: 0.484232 time elapsed: 121.4846 learning rate: 0.001443, scenario: 1, slope: -3.2179787970674486e-05, fluctuations: 0.0\n",
      "step: 91050 loss: 0.483697 time elapsed: 121.4980 learning rate: 0.001594, scenario: 1, slope: -3.547694160522796e-05, fluctuations: 0.0\n",
      "step: 91060 loss: 0.483884 time elapsed: 121.5116 learning rate: 0.001761, scenario: 1, slope: -3.856460613761482e-05, fluctuations: 0.0\n",
      "step: 91070 loss: 730.008608 time elapsed: 121.5253 learning rate: 0.001666, scenario: -1, slope: 1.0988566439261653, fluctuations: 0.01\n",
      "step: 91080 loss: 230.767621 time elapsed: 121.5380 learning rate: 0.001506, scenario: -1, slope: 1.6435142910365943, fluctuations: 0.05\n",
      "step: 91090 loss: 31.191997 time elapsed: 121.5505 learning rate: 0.001362, scenario: -1, slope: 1.4676012301494974, fluctuations: 0.1\n",
      "step: 91100 loss: 7.325929 time elapsed: 121.5628 learning rate: 0.001245, scenario: -1, slope: 1.2200332730883199, fluctuations: 0.13\n",
      "step: 91110 loss: 8.875341 time elapsed: 121.5756 learning rate: 0.001126, scenario: -1, slope: 0.7846236892862484, fluctuations: 0.17\n",
      "step: 91120 loss: 1.627438 time elapsed: 121.5879 learning rate: 0.001018, scenario: -1, slope: 0.3698229575751504, fluctuations: 0.21\n",
      "step: 91130 loss: 1.549848 time elapsed: 121.6002 learning rate: 0.000939, scenario: 0, slope: -0.09840509247911124, fluctuations: 0.24\n",
      "step: 91140 loss: 0.818286 time elapsed: 121.6137 learning rate: 0.000939, scenario: 0, slope: -0.568532214032741, fluctuations: 0.28\n",
      "step: 91150 loss: 0.689343 time elapsed: 121.6277 learning rate: 0.000939, scenario: 0, slope: -1.1206025540638826, fluctuations: 0.31\n",
      "step: 91160 loss: 0.685051 time elapsed: 121.6413 learning rate: 0.000939, scenario: 0, slope: -1.8350936212631839, fluctuations: 0.34\n",
      "step: 91170 loss: 0.649251 time elapsed: 121.6552 learning rate: 0.000939, scenario: 0, slope: -1.5030860830990154, fluctuations: 0.36\n",
      "step: 91180 loss: 0.633650 time elapsed: 121.6691 learning rate: 0.000939, scenario: 0, slope: -0.2471162183811068, fluctuations: 0.35\n",
      "step: 91190 loss: 0.626340 time elapsed: 121.6829 learning rate: 0.000939, scenario: 0, slope: -0.1157869403242039, fluctuations: 0.34\n",
      "step: 91200 loss: 0.618190 time elapsed: 121.6963 learning rate: 0.000939, scenario: 0, slope: -0.039626625059492854, fluctuations: 0.3\n",
      "step: 91210 loss: 0.611707 time elapsed: 121.7104 learning rate: 0.000939, scenario: 0, slope: -0.012845000529751809, fluctuations: 0.26\n",
      "step: 91220 loss: 0.606157 time elapsed: 121.7241 learning rate: 0.000939, scenario: 0, slope: -0.006096228745176883, fluctuations: 0.22\n",
      "step: 91230 loss: 0.601125 time elapsed: 121.7379 learning rate: 0.000939, scenario: 0, slope: -0.002211714815854032, fluctuations: 0.19\n",
      "step: 91240 loss: 0.596659 time elapsed: 121.7506 learning rate: 0.000939, scenario: 0, slope: -0.0012414131610969054, fluctuations: 0.16\n",
      "step: 91250 loss: 0.592628 time elapsed: 121.7631 learning rate: 0.000939, scenario: 0, slope: -0.000920358424338424, fluctuations: 0.12\n",
      "step: 91260 loss: 0.588977 time elapsed: 121.7755 learning rate: 0.000939, scenario: 0, slope: -0.0006716489462575888, fluctuations: 0.09\n",
      "step: 91270 loss: 0.585657 time elapsed: 121.7877 learning rate: 0.000939, scenario: 0, slope: -0.0005559425843103415, fluctuations: 0.06\n",
      "step: 91280 loss: 0.582623 time elapsed: 121.8001 learning rate: 0.000939, scenario: 0, slope: -0.0004953057364484895, fluctuations: 0.02\n",
      "step: 91290 loss: 0.579840 time elapsed: 121.8124 learning rate: 0.000939, scenario: 0, slope: -0.000442267988245695, fluctuations: 0.0\n",
      "step: 91300 loss: 0.577277 time elapsed: 121.8263 learning rate: 0.000939, scenario: 0, slope: -0.00040056956938254424, fluctuations: 0.0\n",
      "step: 91310 loss: 0.574908 time elapsed: 121.8408 learning rate: 0.000939, scenario: 0, slope: -0.0003583043461669459, fluctuations: 0.0\n",
      "step: 91320 loss: 0.572711 time elapsed: 121.8546 learning rate: 0.000939, scenario: 0, slope: -0.0003256821499927345, fluctuations: 0.0\n",
      "step: 91330 loss: 0.570664 time elapsed: 121.8687 learning rate: 0.000939, scenario: 0, slope: -0.0002975016954092196, fluctuations: 0.0\n",
      "step: 91340 loss: 0.568752 time elapsed: 121.8824 learning rate: 0.000939, scenario: 0, slope: -0.000272930808811242, fluctuations: 0.0\n",
      "step: 91350 loss: 0.566960 time elapsed: 121.8962 learning rate: 0.000939, scenario: 0, slope: -0.0002514041942615962, fluctuations: 0.0\n",
      "step: 91360 loss: 0.565275 time elapsed: 121.9098 learning rate: 0.000939, scenario: 0, slope: -0.00023247267679325575, fluctuations: 0.0\n",
      "step: 91370 loss: 0.563685 time elapsed: 121.9234 learning rate: 0.000939, scenario: 0, slope: -0.00021576639216910692, fluctuations: 0.0\n",
      "step: 91380 loss: 0.562181 time elapsed: 121.9371 learning rate: 0.000939, scenario: 0, slope: -0.00020097969183603844, fluctuations: 0.0\n",
      "step: 91390 loss: 0.560755 time elapsed: 121.9505 learning rate: 0.000939, scenario: 0, slope: -0.00018785406863906117, fluctuations: 0.0\n",
      "step: 91400 loss: 0.559398 time elapsed: 121.9629 learning rate: 0.000939, scenario: 0, slope: -0.0001772794907539444, fluctuations: 0.0\n",
      "step: 91410 loss: 0.558104 time elapsed: 121.9759 learning rate: 0.000939, scenario: 0, slope: -0.0001657420101516811, fluctuations: 0.0\n",
      "step: 91420 loss: 0.556867 time elapsed: 121.9881 learning rate: 0.000939, scenario: 0, slope: -0.000156408715027941, fluctuations: 0.0\n",
      "step: 91430 loss: 0.555682 time elapsed: 122.0005 learning rate: 0.000939, scenario: 0, slope: -0.00014803322930622705, fluctuations: 0.0\n",
      "step: 91440 loss: 0.554545 time elapsed: 122.0129 learning rate: 0.000939, scenario: 0, slope: -0.0001404972998205988, fluctuations: 0.0\n",
      "step: 91450 loss: 0.553450 time elapsed: 122.0257 learning rate: 0.000939, scenario: 0, slope: -0.00013369888240163063, fluctuations: 0.0\n",
      "step: 91460 loss: 0.552396 time elapsed: 122.0401 learning rate: 0.000939, scenario: 0, slope: -0.0001275497291290083, fluctuations: 0.0\n",
      "step: 91470 loss: 0.551377 time elapsed: 122.0543 learning rate: 0.000939, scenario: 0, slope: -0.00012197336936857547, fluctuations: 0.0\n",
      "step: 91480 loss: 0.550392 time elapsed: 122.0680 learning rate: 0.000939, scenario: 0, slope: -0.00011690341095461858, fluctuations: 0.0\n",
      "step: 91490 loss: 0.549438 time elapsed: 122.0816 learning rate: 0.000939, scenario: 0, slope: -0.00011228210600023776, fluctuations: 0.0\n",
      "step: 91500 loss: 0.548513 time elapsed: 122.0950 learning rate: 0.000939, scenario: 0, slope: -0.00010846480560810228, fluctuations: 0.0\n",
      "step: 91510 loss: 0.547614 time elapsed: 122.1090 learning rate: 0.000939, scenario: 0, slope: -0.00010419057988543239, fluctuations: 0.0\n",
      "step: 91520 loss: 0.546739 time elapsed: 122.1227 learning rate: 0.000939, scenario: 0, slope: -0.00010063803510549725, fluctuations: 0.0\n",
      "step: 91530 loss: 0.545888 time elapsed: 122.1361 learning rate: 0.000939, scenario: 0, slope: -9.736786755958769e-05, fluctuations: 0.0\n",
      "step: 91540 loss: 0.545057 time elapsed: 122.1495 learning rate: 0.000939, scenario: 0, slope: -9.435056996015853e-05, fluctuations: 0.0\n",
      "step: 91550 loss: 0.544247 time elapsed: 122.1630 learning rate: 0.000939, scenario: 0, slope: -9.156021200965179e-05, fluctuations: 0.0\n",
      "step: 91560 loss: 0.543455 time elapsed: 122.1755 learning rate: 0.000939, scenario: 0, slope: -8.897396838776825e-05, fluctuations: 0.0\n",
      "step: 91570 loss: 0.542681 time elapsed: 122.1879 learning rate: 0.000939, scenario: 0, slope: -8.657171287600612e-05, fluctuations: 0.0\n",
      "step: 91580 loss: 0.541923 time elapsed: 122.2000 learning rate: 0.000939, scenario: 0, slope: -8.433566881824556e-05, fluctuations: 0.0\n",
      "step: 91590 loss: 0.541181 time elapsed: 122.2122 learning rate: 0.000939, scenario: 0, slope: -8.22501076645672e-05, fluctuations: 0.0\n",
      "step: 91600 loss: 0.540453 time elapsed: 122.2243 learning rate: 0.000939, scenario: 0, slope: -8.049021023325891e-05, fluctuations: 0.0\n",
      "step: 91610 loss: 0.539739 time elapsed: 122.2378 learning rate: 0.000939, scenario: 0, slope: -7.847623357873982e-05, fluctuations: 0.0\n",
      "step: 91620 loss: 0.539037 time elapsed: 122.2515 learning rate: 0.000939, scenario: 0, slope: -7.676453210271427e-05, fluctuations: 0.0\n",
      "step: 91630 loss: 0.538349 time elapsed: 122.2651 learning rate: 0.000939, scenario: 0, slope: -7.515617258197224e-05, fluctuations: 0.0\n",
      "step: 91640 loss: 0.537671 time elapsed: 122.2783 learning rate: 0.000939, scenario: 0, slope: -7.36423955568601e-05, fluctuations: 0.0\n",
      "step: 91650 loss: 0.537005 time elapsed: 122.2922 learning rate: 0.000939, scenario: 0, slope: -7.221536634183611e-05, fluctuations: 0.0\n",
      "step: 91660 loss: 0.536350 time elapsed: 122.3057 learning rate: 0.000939, scenario: 0, slope: -7.086806422520895e-05, fluctuations: 0.0\n",
      "step: 91670 loss: 0.535704 time elapsed: 122.3192 learning rate: 0.000939, scenario: 0, slope: -6.959418598802122e-05, fluctuations: 0.0\n",
      "step: 91680 loss: 0.535068 time elapsed: 122.3329 learning rate: 0.000939, scenario: 0, slope: -6.83880617983755e-05, fluctuations: 0.0\n",
      "step: 91690 loss: 0.534441 time elapsed: 122.3466 learning rate: 0.000939, scenario: 0, slope: -6.724458181096587e-05, fluctuations: 0.0\n",
      "step: 91700 loss: 0.533823 time elapsed: 122.3599 learning rate: 0.000939, scenario: 0, slope: -6.626518783288983e-05, fluctuations: 0.0\n",
      "step: 91710 loss: 0.533213 time elapsed: 122.3746 learning rate: 0.000939, scenario: 0, slope: -6.512753824509929e-05, fluctuations: 0.0\n",
      "step: 91720 loss: 0.532610 time elapsed: 122.3878 learning rate: 0.000939, scenario: 0, slope: -6.414601684099037e-05, fluctuations: 0.0\n",
      "step: 91730 loss: 0.532016 time elapsed: 122.4002 learning rate: 0.000939, scenario: 0, slope: -6.321113178757497e-05, fluctuations: 0.0\n",
      "step: 91740 loss: 0.531428 time elapsed: 122.4124 learning rate: 0.000939, scenario: 0, slope: -6.231975679373664e-05, fluctuations: 0.0\n",
      "step: 91750 loss: 0.530848 time elapsed: 122.4251 learning rate: 0.000939, scenario: 0, slope: -6.146904207564744e-05, fluctuations: 0.0\n",
      "step: 91760 loss: 0.530274 time elapsed: 122.4377 learning rate: 0.000939, scenario: 0, slope: -6.0656385102630005e-05, fluctuations: 0.0\n",
      "step: 91770 loss: 0.529707 time elapsed: 122.4500 learning rate: 0.000939, scenario: 0, slope: -5.987940481572964e-05, fluctuations: 0.0\n",
      "step: 91780 loss: 0.529146 time elapsed: 122.4633 learning rate: 0.000939, scenario: 0, slope: -5.913591887528275e-05, fluctuations: 0.0\n",
      "step: 91790 loss: 0.528590 time elapsed: 122.4771 learning rate: 0.000939, scenario: 0, slope: -5.842392355420003e-05, fluctuations: 0.0\n",
      "step: 91800 loss: 0.528040 time elapsed: 122.4908 learning rate: 0.000939, scenario: 0, slope: -5.780852601731089e-05, fluctuations: 0.0\n",
      "step: 91810 loss: 0.527496 time elapsed: 122.5053 learning rate: 0.000939, scenario: 0, slope: -5.7087178182127795e-05, fluctuations: 0.0\n",
      "step: 91820 loss: 0.526957 time elapsed: 122.5198 learning rate: 0.000939, scenario: 0, slope: -5.645916345760788e-05, fluctuations: 0.0\n",
      "step: 91830 loss: 0.526423 time elapsed: 122.5340 learning rate: 0.000939, scenario: 0, slope: -5.585608356391992e-05, fluctuations: 0.0\n",
      "step: 91840 loss: 0.525894 time elapsed: 122.5484 learning rate: 0.000939, scenario: 0, slope: -5.5276597822240385e-05, fluctuations: 0.0\n",
      "step: 91850 loss: 0.525370 time elapsed: 122.5629 learning rate: 0.000939, scenario: 0, slope: -5.471946320263504e-05, fluctuations: 0.0\n",
      "step: 91860 loss: 0.524850 time elapsed: 122.5774 learning rate: 0.000939, scenario: 0, slope: -5.418352550124161e-05, fluctuations: 0.0\n",
      "step: 91870 loss: 0.524334 time elapsed: 122.5918 learning rate: 0.000939, scenario: 0, slope: -5.366771144850807e-05, fluctuations: 0.0\n",
      "step: 91880 loss: 0.523823 time elapsed: 122.6062 learning rate: 0.000939, scenario: 0, slope: -5.317102163803256e-05, fluctuations: 0.0\n",
      "step: 91890 loss: 0.523314 time elapsed: 122.6201 learning rate: 0.000977, scenario: 1, slope: -5.2693722148057585e-05, fluctuations: 0.0\n",
      "step: 91900 loss: 0.522772 time elapsed: 122.6335 learning rate: 0.001069, scenario: 1, slope: -5.235989352193189e-05, fluctuations: 0.0\n",
      "step: 91910 loss: 0.522183 time elapsed: 122.6466 learning rate: 0.001181, scenario: 1, slope: -5.2338315238826585e-05, fluctuations: 0.0\n",
      "step: 91920 loss: 0.521540 time elapsed: 122.6604 learning rate: 0.001266, scenario: 0, slope: -5.2909894515430556e-05, fluctuations: 0.0\n",
      "step: 91930 loss: 0.520882 time elapsed: 122.6752 learning rate: 0.001266, scenario: 0, slope: -5.412981375161939e-05, fluctuations: 0.0\n",
      "step: 91940 loss: 0.520230 time elapsed: 122.6904 learning rate: 0.001266, scenario: 0, slope: -5.581035182372829e-05, fluctuations: 0.0\n",
      "step: 91950 loss: 0.519585 time elapsed: 122.7055 learning rate: 0.001266, scenario: 0, slope: -5.773416261402351e-05, fluctuations: 0.0\n",
      "step: 91960 loss: 0.518946 time elapsed: 122.7202 learning rate: 0.001266, scenario: 0, slope: -5.968890559779782e-05, fluctuations: 0.0\n",
      "step: 91970 loss: 0.518314 time elapsed: 122.7350 learning rate: 0.001266, scenario: 0, slope: -6.146723975452671e-05, fluctuations: 0.0\n",
      "step: 91980 loss: 0.517688 time elapsed: 122.7493 learning rate: 0.001266, scenario: 0, slope: -6.286664978434917e-05, fluctuations: 0.0\n",
      "step: 91990 loss: 0.517067 time elapsed: 122.7633 learning rate: 0.001266, scenario: 0, slope: -6.369045960702221e-05, fluctuations: 0.0\n",
      "step: 92000 loss: 0.516453 time elapsed: 122.7776 learning rate: 0.001266, scenario: 0, slope: -6.387005985607354e-05, fluctuations: 0.0\n",
      "step: 92010 loss: 0.515843 time elapsed: 122.7927 learning rate: 0.001266, scenario: 0, slope: -6.352932178006557e-05, fluctuations: 0.0\n",
      "step: 92020 loss: 0.515239 time elapsed: 122.8071 learning rate: 0.001266, scenario: 0, slope: -6.294356938146553e-05, fluctuations: 0.0\n",
      "step: 92030 loss: 0.514640 time elapsed: 122.8217 learning rate: 0.001266, scenario: 0, slope: -6.234976059680651e-05, fluctuations: 0.0\n",
      "step: 92040 loss: 0.514045 time elapsed: 122.8361 learning rate: 0.001266, scenario: 0, slope: -6.177936109601643e-05, fluctuations: 0.0\n",
      "step: 92050 loss: 0.513455 time elapsed: 122.8498 learning rate: 0.001266, scenario: 0, slope: -6.123157754258397e-05, fluctuations: 0.0\n",
      "step: 92060 loss: 0.512870 time elapsed: 122.8629 learning rate: 0.001266, scenario: 0, slope: -6.0705401321850205e-05, fluctuations: 0.0\n",
      "step: 92070 loss: 0.512288 time elapsed: 122.8773 learning rate: 0.001266, scenario: 0, slope: -6.019979779043602e-05, fluctuations: 0.0\n",
      "step: 92080 loss: 0.511711 time elapsed: 122.8924 learning rate: 0.001266, scenario: 0, slope: -5.971376835605888e-05, fluctuations: 0.0\n",
      "step: 92090 loss: 0.511138 time elapsed: 122.9075 learning rate: 0.001266, scenario: 0, slope: -5.9246368421871266e-05, fluctuations: 0.0\n",
      "step: 92100 loss: 0.510568 time elapsed: 122.9220 learning rate: 0.001266, scenario: 0, slope: -5.884090184048369e-05, fluctuations: 0.0\n",
      "step: 92110 loss: 0.510002 time elapsed: 122.9373 learning rate: 0.001266, scenario: 0, slope: -5.836396087452952e-05, fluctuations: 0.0\n",
      "step: 92120 loss: 0.509440 time elapsed: 122.9521 learning rate: 0.001266, scenario: 0, slope: -5.794733860185968e-05, fluctuations: 0.0\n",
      "step: 92130 loss: 0.508881 time elapsed: 122.9669 learning rate: 0.001266, scenario: 0, slope: -5.754610925517975e-05, fluctuations: 0.0\n",
      "step: 92140 loss: 0.508325 time elapsed: 122.9811 learning rate: 0.001266, scenario: 0, slope: -5.7159582593120305e-05, fluctuations: 0.0\n",
      "step: 92150 loss: 0.507772 time elapsed: 122.9953 learning rate: 0.001266, scenario: 0, slope: -5.678710898847571e-05, fluctuations: 0.0\n",
      "step: 92160 loss: 0.507223 time elapsed: 123.0100 learning rate: 0.001266, scenario: 0, slope: -5.6428076391114465e-05, fluctuations: 0.0\n",
      "step: 92170 loss: 0.506676 time elapsed: 123.0237 learning rate: 0.001266, scenario: 0, slope: -5.608190755924709e-05, fluctuations: 0.0\n",
      "step: 92180 loss: 0.506132 time elapsed: 123.0377 learning rate: 0.001266, scenario: 0, slope: -5.5748057535788703e-05, fluctuations: 0.0\n",
      "step: 92190 loss: 0.505591 time elapsed: 123.0511 learning rate: 0.001266, scenario: 0, slope: -5.5426011346682634e-05, fluctuations: 0.0\n",
      "step: 92200 loss: 0.505053 time elapsed: 123.0635 learning rate: 0.001266, scenario: 0, slope: -5.5145858927793034e-05, fluctuations: 0.0\n",
      "step: 92210 loss: 0.504517 time elapsed: 123.0776 learning rate: 0.001266, scenario: 0, slope: -5.481540805779054e-05, fluctuations: 0.0\n",
      "step: 92220 loss: 0.503984 time elapsed: 123.0925 learning rate: 0.001266, scenario: 0, slope: -5.452595288720879e-05, fluctuations: 0.0\n",
      "step: 92230 loss: 0.503453 time elapsed: 123.1077 learning rate: 0.001266, scenario: 0, slope: -5.4246502034380676e-05, fluctuations: 0.0\n",
      "step: 92240 loss: 0.502924 time elapsed: 123.1222 learning rate: 0.001266, scenario: 0, slope: -5.397666224769635e-05, fluctuations: 0.0\n",
      "step: 92250 loss: 0.502398 time elapsed: 123.1369 learning rate: 0.001266, scenario: 0, slope: -5.371606001301444e-05, fluctuations: 0.0\n",
      "step: 92260 loss: 0.501874 time elapsed: 123.1520 learning rate: 0.001266, scenario: 0, slope: -5.346434029816818e-05, fluctuations: 0.0\n",
      "step: 92270 loss: 0.501352 time elapsed: 123.1662 learning rate: 0.001266, scenario: 0, slope: -5.3221165395618273e-05, fluctuations: 0.0\n",
      "step: 92280 loss: 0.500831 time elapsed: 123.1801 learning rate: 0.001266, scenario: 0, slope: -5.2986213853528247e-05, fluctuations: 0.0\n",
      "step: 92290 loss: 0.500313 time elapsed: 123.1943 learning rate: 0.001266, scenario: 0, slope: -5.275917948810203e-05, fluctuations: 0.0\n",
      "step: 92300 loss: 0.499797 time elapsed: 123.2088 learning rate: 0.001266, scenario: 0, slope: -5.256137626506943e-05, fluctuations: 0.0\n",
      "step: 92310 loss: 0.499282 time elapsed: 123.2248 learning rate: 0.001266, scenario: 0, slope: -5.232770847404778e-05, fluctuations: 0.0\n",
      "step: 92320 loss: 0.498770 time elapsed: 123.2384 learning rate: 0.001266, scenario: 0, slope: -5.212272789960933e-05, fluctuations: 0.0\n",
      "step: 92330 loss: 0.498258 time elapsed: 123.2509 learning rate: 0.001266, scenario: 0, slope: -5.1924575134689845e-05, fluctuations: 0.0\n",
      "step: 92340 loss: 0.497749 time elapsed: 123.2634 learning rate: 0.001266, scenario: 0, slope: -5.173300788032926e-05, fluctuations: 0.0\n",
      "step: 92350 loss: 0.497241 time elapsed: 123.2758 learning rate: 0.001266, scenario: 0, slope: -5.154779451852639e-05, fluctuations: 0.0\n",
      "step: 92360 loss: 0.496735 time elapsed: 123.2882 learning rate: 0.001266, scenario: 0, slope: -5.1368713523572834e-05, fluctuations: 0.0\n",
      "step: 92370 loss: 0.496230 time elapsed: 123.3017 learning rate: 0.001266, scenario: 0, slope: -5.119555291344474e-05, fluctuations: 0.0\n",
      "step: 92380 loss: 0.495727 time elapsed: 123.3160 learning rate: 0.001266, scenario: 0, slope: -5.102810973717714e-05, fluctuations: 0.0\n",
      "step: 92390 loss: 0.495225 time elapsed: 123.3298 learning rate: 0.001266, scenario: 0, slope: -5.086618959604325e-05, fluctuations: 0.0\n",
      "step: 92400 loss: 0.494724 time elapsed: 123.3436 learning rate: 0.001266, scenario: 0, slope: -5.072502953116052e-05, fluctuations: 0.0\n",
      "step: 92410 loss: 0.494225 time elapsed: 123.3576 learning rate: 0.001266, scenario: 0, slope: -5.055818076634858e-05, fluctuations: 0.0\n",
      "step: 92420 loss: 0.493727 time elapsed: 123.3711 learning rate: 0.001266, scenario: 0, slope: -5.041157250193664e-05, fluctuations: 0.0\n",
      "step: 92430 loss: 0.493503 time elapsed: 123.3847 learning rate: 0.001266, scenario: 0, slope: -4.997264268585992e-05, fluctuations: 0.0\n",
      "step: 92440 loss: 5.142568 time elapsed: 123.3983 learning rate: 0.001272, scenario: -1, slope: 0.004094394841960398, fluctuations: 0.0\n",
      "step: 92450 loss: 0.541513 time elapsed: 123.4119 learning rate: 0.001150, scenario: -1, slope: 0.10279149225909505, fluctuations: 0.02\n",
      "step: 92460 loss: 6.943938 time elapsed: 123.4258 learning rate: 0.001040, scenario: -1, slope: 0.11154629945535009, fluctuations: 0.05\n",
      "step: 92470 loss: 2.286847 time elapsed: 123.4399 learning rate: 0.000941, scenario: -1, slope: 0.09410562030965408, fluctuations: 0.09\n",
      "step: 92480 loss: 0.845422 time elapsed: 123.4527 learning rate: 0.000851, scenario: -1, slope: 0.0736443421142398, fluctuations: 0.14\n",
      "step: 92490 loss: 0.708650 time elapsed: 123.4652 learning rate: 0.000769, scenario: -1, slope: 0.04214641839970977, fluctuations: 0.18\n",
      "step: 92500 loss: 0.560815 time elapsed: 123.4776 learning rate: 0.000703, scenario: -1, slope: 0.01512977198374017, fluctuations: 0.23\n",
      "step: 92510 loss: 0.500376 time elapsed: 123.4904 learning rate: 0.000682, scenario: 0, slope: -0.025826441406126926, fluctuations: 0.28\n",
      "step: 92520 loss: 0.494463 time elapsed: 123.5025 learning rate: 0.000682, scenario: 0, slope: -0.06620755614863166, fluctuations: 0.33\n",
      "step: 92530 loss: 0.495631 time elapsed: 123.5166 learning rate: 0.000682, scenario: 0, slope: -0.11241043217733197, fluctuations: 0.37\n",
      "step: 92540 loss: 0.493623 time elapsed: 123.5307 learning rate: 0.000682, scenario: 0, slope: -0.17058837699784238, fluctuations: 0.42\n",
      "step: 92550 loss: 0.492537 time elapsed: 123.5448 learning rate: 0.000682, scenario: 0, slope: -0.03719113688734636, fluctuations: 0.45\n",
      "step: 92560 loss: 0.492207 time elapsed: 123.5589 learning rate: 0.000682, scenario: 0, slope: -0.008782306345763281, fluctuations: 0.46\n",
      "step: 92570 loss: 0.491931 time elapsed: 123.5725 learning rate: 0.000682, scenario: 0, slope: -0.00229205478478657, fluctuations: 0.44\n",
      "step: 92580 loss: 0.491628 time elapsed: 123.5859 learning rate: 0.000682, scenario: 0, slope: -0.0003978803677708219, fluctuations: 0.4\n",
      "step: 92590 loss: 0.491334 time elapsed: 123.5995 learning rate: 0.000682, scenario: 0, slope: -8.243541305176978e-05, fluctuations: 0.35\n",
      "step: 92600 loss: 0.491052 time elapsed: 123.6130 learning rate: 0.000682, scenario: 0, slope: -0.0001457277661953222, fluctuations: 0.3\n",
      "step: 92610 loss: 0.490776 time elapsed: 123.6270 learning rate: 0.000675, scenario: 0, slope: -5.6523066800978534e-05, fluctuations: 0.25\n",
      "step: 92620 loss: 0.490511 time elapsed: 123.6408 learning rate: 0.000623, scenario: -1, slope: -4.2767685794205176e-05, fluctuations: 0.2\n",
      "step: 92630 loss: 0.490271 time elapsed: 123.6553 learning rate: 0.000563, scenario: -1, slope: -2.9763059807733465e-05, fluctuations: 0.16\n",
      "step: 92640 loss: 0.490054 time elapsed: 123.6681 learning rate: 0.000510, scenario: -1, slope: -2.8676868362800426e-05, fluctuations: 0.11\n",
      "step: 92650 loss: 0.489859 time elapsed: 123.6804 learning rate: 0.000461, scenario: -1, slope: -2.7032115384163935e-05, fluctuations: 0.06\n",
      "step: 92660 loss: 0.489683 time elapsed: 123.6938 learning rate: 0.000417, scenario: -1, slope: -2.5792065960671583e-05, fluctuations: 0.01\n",
      "step: 92670 loss: 0.489516 time elapsed: 123.7062 learning rate: 0.000449, scenario: 1, slope: -2.426238335191452e-05, fluctuations: 0.0\n",
      "step: 92680 loss: 0.489332 time elapsed: 123.7186 learning rate: 0.000496, scenario: 1, slope: -2.27263379903995e-05, fluctuations: 0.0\n",
      "step: 92690 loss: 0.489129 time elapsed: 123.7325 learning rate: 0.000548, scenario: 1, slope: -2.1365667747651745e-05, fluctuations: 0.0\n",
      "step: 92700 loss: 0.488905 time elapsed: 123.7460 learning rate: 0.000599, scenario: 1, slope: -2.0425664449633655e-05, fluctuations: 0.0\n",
      "step: 92710 loss: 0.488660 time elapsed: 123.7604 learning rate: 0.000662, scenario: 1, slope: -1.9826888498665876e-05, fluctuations: 0.0\n",
      "step: 92720 loss: 0.488390 time elapsed: 123.7739 learning rate: 0.000731, scenario: 1, slope: -1.994732837725783e-05, fluctuations: 0.0\n",
      "step: 92730 loss: 0.488093 time elapsed: 123.7875 learning rate: 0.000808, scenario: 1, slope: -2.0735922461869366e-05, fluctuations: 0.0\n",
      "step: 92740 loss: 0.487765 time elapsed: 123.8013 learning rate: 0.000892, scenario: 1, slope: -2.2145508616505604e-05, fluctuations: 0.0\n",
      "step: 92750 loss: 0.487404 time elapsed: 123.8148 learning rate: 0.000986, scenario: 1, slope: -2.4096386591657708e-05, fluctuations: 0.0\n",
      "step: 92760 loss: 0.487006 time elapsed: 123.8285 learning rate: 0.001089, scenario: 1, slope: -2.647680327144604e-05, fluctuations: 0.0\n",
      "step: 92770 loss: 0.486568 time elapsed: 123.8420 learning rate: 0.001203, scenario: 1, slope: -2.915713943747236e-05, fluctuations: 0.0\n",
      "step: 92780 loss: 0.486086 time elapsed: 123.8560 learning rate: 0.001328, scenario: 1, slope: -3.2107980328064206e-05, fluctuations: 0.0\n",
      "step: 92790 loss: 0.485556 time elapsed: 123.8690 learning rate: 0.001467, scenario: 1, slope: -3.535713622753483e-05, fluctuations: 0.0\n",
      "step: 92800 loss: 0.485099 time elapsed: 123.8814 learning rate: 0.001605, scenario: 1, slope: -3.854484437792655e-05, fluctuations: 0.0\n",
      "step: 92810 loss: 842.293868 time elapsed: 123.8943 learning rate: 0.001549, scenario: -1, slope: 0.7154240315563075, fluctuations: 0.01\n",
      "step: 92820 loss: 222.785607 time elapsed: 123.9075 learning rate: 0.001401, scenario: -1, slope: 1.13121684436563, fluctuations: 0.05\n",
      "step: 92830 loss: 10.740888 time elapsed: 123.9210 learning rate: 0.001267, scenario: -1, slope: 0.9854239934755707, fluctuations: 0.1\n",
      "step: 92840 loss: 10.738013 time elapsed: 123.9372 learning rate: 0.001146, scenario: -1, slope: 0.8154935837179447, fluctuations: 0.14\n",
      "step: 92850 loss: 1.260209 time elapsed: 123.9511 learning rate: 0.001036, scenario: -1, slope: 0.5667445478079326, fluctuations: 0.18\n",
      "step: 92860 loss: 2.424349 time elapsed: 123.9657 learning rate: 0.000937, scenario: -1, slope: 0.28755462519194647, fluctuations: 0.22\n",
      "step: 92870 loss: 0.933691 time elapsed: 123.9794 learning rate: 0.000856, scenario: 0, slope: -0.032988618594601, fluctuations: 0.25\n",
      "step: 92880 loss: 0.932751 time elapsed: 123.9935 learning rate: 0.000856, scenario: 0, slope: -0.35368202073565164, fluctuations: 0.28\n",
      "step: 92890 loss: 0.616968 time elapsed: 124.0077 learning rate: 0.000856, scenario: 0, slope: -0.7159508335492749, fluctuations: 0.32\n",
      "step: 92900 loss: 0.620866 time elapsed: 124.0212 learning rate: 0.000856, scenario: 0, slope: -1.1422161169960667, fluctuations: 0.35\n",
      "step: 92910 loss: 0.594750 time elapsed: 124.0357 learning rate: 0.000856, scenario: 0, slope: -0.6148557689643737, fluctuations: 0.38\n",
      "step: 92920 loss: 0.579524 time elapsed: 124.0509 learning rate: 0.000856, scenario: 0, slope: -0.18548700484207029, fluctuations: 0.36\n",
      "step: 92930 loss: 0.574872 time elapsed: 124.0640 learning rate: 0.000856, scenario: 0, slope: -0.13609560859334258, fluctuations: 0.34\n",
      "step: 92940 loss: 0.568598 time elapsed: 124.0767 learning rate: 0.000856, scenario: 0, slope: -0.04234818638694864, fluctuations: 0.32\n",
      "step: 92950 loss: 0.564782 time elapsed: 124.0898 learning rate: 0.000856, scenario: 0, slope: -0.0160247646329795, fluctuations: 0.28\n",
      "step: 92960 loss: 0.561186 time elapsed: 124.1025 learning rate: 0.000856, scenario: 0, slope: -0.003796679375321725, fluctuations: 0.25\n",
      "step: 92970 loss: 0.558147 time elapsed: 124.1153 learning rate: 0.000856, scenario: 0, slope: -0.002323832885366597, fluctuations: 0.21\n",
      "step: 92980 loss: 0.555420 time elapsed: 124.1288 learning rate: 0.000856, scenario: 0, slope: -0.0009896332151157088, fluctuations: 0.18\n",
      "step: 92990 loss: 0.552961 time elapsed: 124.1430 learning rate: 0.000856, scenario: 0, slope: -0.0006363950769445072, fluctuations: 0.15\n",
      "step: 93000 loss: 0.550722 time elapsed: 124.1572 learning rate: 0.000856, scenario: 0, slope: -0.00046575299623263496, fluctuations: 0.12\n",
      "step: 93010 loss: 0.548665 time elapsed: 124.1714 learning rate: 0.000856, scenario: 0, slope: -0.00035419533874215647, fluctuations: 0.08\n",
      "step: 93020 loss: 0.546766 time elapsed: 124.1853 learning rate: 0.000856, scenario: 0, slope: -0.0003080656920188487, fluctuations: 0.04\n",
      "step: 93030 loss: 0.545003 time elapsed: 124.1989 learning rate: 0.000856, scenario: 0, slope: -0.00027180477837473294, fluctuations: 0.01\n",
      "step: 93040 loss: 0.543358 time elapsed: 124.2127 learning rate: 0.000856, scenario: 0, slope: -0.0002446386648613365, fluctuations: 0.0\n",
      "step: 93050 loss: 0.541818 time elapsed: 124.2261 learning rate: 0.000856, scenario: 0, slope: -0.00022160524657097286, fluctuations: 0.0\n",
      "step: 93060 loss: 0.540372 time elapsed: 124.2401 learning rate: 0.000856, scenario: 0, slope: -0.00020285737954642695, fluctuations: 0.0\n",
      "step: 93070 loss: 0.539008 time elapsed: 124.2550 learning rate: 0.000856, scenario: 0, slope: -0.0001870771125458809, fluctuations: 0.0\n",
      "step: 93080 loss: 0.537719 time elapsed: 124.2694 learning rate: 0.000856, scenario: 0, slope: -0.00017354028972013433, fluctuations: 0.0\n",
      "step: 93090 loss: 0.536497 time elapsed: 124.2818 learning rate: 0.000856, scenario: 0, slope: -0.0001617740296070948, fluctuations: 0.0\n",
      "step: 93100 loss: 0.535336 time elapsed: 124.2942 learning rate: 0.000856, scenario: 0, slope: -0.00015241636254139464, fluctuations: 0.0\n",
      "step: 93110 loss: 0.534230 time elapsed: 124.3072 learning rate: 0.000856, scenario: 0, slope: -0.00014229277019285326, fluctuations: 0.0\n",
      "step: 93120 loss: 0.533174 time elapsed: 124.3195 learning rate: 0.000856, scenario: 0, slope: -0.00013414311261941182, fluctuations: 0.0\n",
      "step: 93130 loss: 0.532164 time elapsed: 124.3317 learning rate: 0.000856, scenario: 0, slope: -0.000126842397995289, fluctuations: 0.0\n",
      "step: 93140 loss: 0.531196 time elapsed: 124.3453 learning rate: 0.000856, scenario: 0, slope: -0.00012027088757056994, fluctuations: 0.0\n",
      "step: 93150 loss: 0.530266 time elapsed: 124.3590 learning rate: 0.000856, scenario: 0, slope: -0.00011433073065643395, fluctuations: 0.0\n",
      "step: 93160 loss: 0.529372 time elapsed: 124.3731 learning rate: 0.000856, scenario: 0, slope: -0.00010894079003826766, fluctuations: 0.0\n",
      "step: 93170 loss: 0.528511 time elapsed: 124.3867 learning rate: 0.000856, scenario: 0, slope: -0.00010403309464658133, fluctuations: 0.0\n",
      "step: 93180 loss: 0.527680 time elapsed: 124.4003 learning rate: 0.000856, scenario: 0, slope: -9.955019920295911e-05, fluctuations: 0.0\n",
      "step: 93190 loss: 0.526878 time elapsed: 124.4141 learning rate: 0.000856, scenario: 0, slope: -9.544320048276532e-05, fluctuations: 0.0\n",
      "step: 93200 loss: 0.526102 time elapsed: 124.4277 learning rate: 0.000856, scenario: 0, slope: -9.203354828201304e-05, fluctuations: 0.0\n",
      "step: 93210 loss: 0.525349 time elapsed: 124.4424 learning rate: 0.000856, scenario: 0, slope: -8.819515848980184e-05, fluctuations: 0.0\n",
      "step: 93220 loss: 0.524620 time elapsed: 124.4561 learning rate: 0.000856, scenario: 0, slope: -8.498683366531536e-05, fluctuations: 0.0\n",
      "step: 93230 loss: 0.523912 time elapsed: 124.4706 learning rate: 0.000856, scenario: 0, slope: -8.201811679478199e-05, fluctuations: 0.0\n",
      "step: 93240 loss: 0.523224 time elapsed: 124.4838 learning rate: 0.000856, scenario: 0, slope: -7.926534988067092e-05, fluctuations: 0.0\n",
      "step: 93250 loss: 0.522554 time elapsed: 124.4962 learning rate: 0.000856, scenario: 0, slope: -7.67078185038122e-05, fluctuations: 0.0\n",
      "step: 93260 loss: 0.521901 time elapsed: 124.5085 learning rate: 0.000856, scenario: 0, slope: -7.432732217857582e-05, fluctuations: 0.0\n",
      "step: 93270 loss: 0.521265 time elapsed: 124.5210 learning rate: 0.000856, scenario: 0, slope: -7.210781566067026e-05, fluctuations: 0.0\n",
      "step: 93280 loss: 0.520645 time elapsed: 124.5336 learning rate: 0.000856, scenario: 0, slope: -7.00351078721384e-05, fluctuations: 0.0\n",
      "step: 93290 loss: 0.520038 time elapsed: 124.5473 learning rate: 0.000856, scenario: 0, slope: -6.80966080246334e-05, fluctuations: 0.0\n",
      "step: 93300 loss: 0.519446 time elapsed: 124.5608 learning rate: 0.000856, scenario: 0, slope: -6.645741906281696e-05, fluctuations: 0.0\n",
      "step: 93310 loss: 0.518866 time elapsed: 124.5752 learning rate: 0.000856, scenario: 0, slope: -6.457861317957343e-05, fluctuations: 0.0\n",
      "step: 93320 loss: 0.518298 time elapsed: 124.5889 learning rate: 0.000856, scenario: 0, slope: -6.298015999682568e-05, fluctuations: 0.0\n",
      "step: 93330 loss: 0.517742 time elapsed: 124.6026 learning rate: 0.000856, scenario: 0, slope: -6.147770983363433e-05, fluctuations: 0.0\n",
      "step: 93340 loss: 0.517197 time elapsed: 124.6159 learning rate: 0.000856, scenario: 0, slope: -6.006402170825367e-05, fluctuations: 0.0\n",
      "step: 93350 loss: 0.516661 time elapsed: 124.6292 learning rate: 0.000856, scenario: 0, slope: -5.8732557181757634e-05, fluctuations: 0.0\n",
      "step: 93360 loss: 0.516136 time elapsed: 124.6428 learning rate: 0.000856, scenario: 0, slope: -5.747739622682786e-05, fluctuations: 0.0\n",
      "step: 93370 loss: 0.515619 time elapsed: 124.6562 learning rate: 0.000856, scenario: 0, slope: -5.629316468670598e-05, fluctuations: 0.0\n",
      "step: 93380 loss: 0.515111 time elapsed: 124.6698 learning rate: 0.000856, scenario: 0, slope: -5.517497159727873e-05, fluctuations: 0.0\n",
      "step: 93390 loss: 0.514612 time elapsed: 124.6823 learning rate: 0.000856, scenario: 0, slope: -5.411835491749062e-05, fluctuations: 0.0\n",
      "step: 93400 loss: 0.514120 time elapsed: 124.6944 learning rate: 0.000856, scenario: 0, slope: -5.321666822927951e-05, fluctuations: 0.0\n",
      "step: 93410 loss: 0.513636 time elapsed: 124.7072 learning rate: 0.000856, scenario: 0, slope: -5.21738708252446e-05, fluctuations: 0.0\n",
      "step: 93420 loss: 0.513151 time elapsed: 124.7196 learning rate: 0.000918, scenario: 1, slope: -5.128860295896629e-05, fluctuations: 0.0\n",
      "step: 93430 loss: 0.512629 time elapsed: 124.7318 learning rate: 0.001014, scenario: 1, slope: -5.061376903028559e-05, fluctuations: 0.0\n",
      "step: 93440 loss: 0.512061 time elapsed: 124.7451 learning rate: 0.001120, scenario: 1, slope: -5.038225769361462e-05, fluctuations: 0.0\n",
      "step: 93450 loss: 0.511444 time elapsed: 124.7585 learning rate: 0.001237, scenario: 1, slope: -5.078452027722292e-05, fluctuations: 0.0\n",
      "step: 93460 loss: 0.510776 time elapsed: 124.7723 learning rate: 0.001326, scenario: 0, slope: -5.196220127401161e-05, fluctuations: 0.0\n",
      "step: 93470 loss: 0.510097 time elapsed: 124.7859 learning rate: 0.001326, scenario: 0, slope: -5.389537897222338e-05, fluctuations: 0.0\n",
      "step: 93480 loss: 0.509432 time elapsed: 124.7992 learning rate: 0.001326, scenario: 0, slope: -5.6291946831607225e-05, fluctuations: 0.0\n",
      "step: 93490 loss: 0.508778 time elapsed: 124.8129 learning rate: 0.001326, scenario: 0, slope: -5.883636309403895e-05, fluctuations: 0.0\n",
      "step: 93500 loss: 0.508136 time elapsed: 124.8264 learning rate: 0.001326, scenario: 0, slope: -6.100187906588646e-05, fluctuations: 0.0\n",
      "step: 93510 loss: 0.507505 time elapsed: 124.8409 learning rate: 0.001326, scenario: 0, slope: -6.316746158593652e-05, fluctuations: 0.0\n",
      "step: 93520 loss: 0.506884 time elapsed: 124.8555 learning rate: 0.001326, scenario: 0, slope: -6.439216336302008e-05, fluctuations: 0.0\n",
      "step: 93530 loss: 0.506272 time elapsed: 124.8709 learning rate: 0.001326, scenario: 0, slope: -6.481193458221306e-05, fluctuations: 0.0\n",
      "step: 93540 loss: 0.505668 time elapsed: 124.8848 learning rate: 0.001326, scenario: 0, slope: -6.453135536475429e-05, fluctuations: 0.0\n",
      "step: 93550 loss: 0.505073 time elapsed: 124.8979 learning rate: 0.001326, scenario: 0, slope: -6.374547890795784e-05, fluctuations: 0.0\n",
      "step: 93560 loss: 0.504485 time elapsed: 124.9113 learning rate: 0.001326, scenario: 0, slope: -6.27397237551128e-05, fluctuations: 0.0\n",
      "step: 93570 loss: 0.503905 time elapsed: 124.9257 learning rate: 0.001326, scenario: 0, slope: -6.176933158244398e-05, fluctuations: 0.0\n",
      "step: 93580 loss: 0.503331 time elapsed: 124.9400 learning rate: 0.001326, scenario: 0, slope: -6.086357839509505e-05, fluctuations: 0.0\n",
      "step: 93590 loss: 0.502763 time elapsed: 124.9552 learning rate: 0.001326, scenario: 0, slope: -6.001793325311693e-05, fluctuations: 0.0\n",
      "step: 93600 loss: 0.502201 time elapsed: 124.9710 learning rate: 0.001326, scenario: 0, slope: -5.930446013439903e-05, fluctuations: 0.0\n",
      "step: 93610 loss: 0.501645 time elapsed: 124.9866 learning rate: 0.001326, scenario: 0, slope: -5.8488942105999095e-05, fluctuations: 0.0\n",
      "step: 93620 loss: 0.501093 time elapsed: 125.0012 learning rate: 0.001326, scenario: 0, slope: -5.7797332107786925e-05, fluctuations: 0.0\n",
      "step: 93630 loss: 0.500547 time elapsed: 125.0159 learning rate: 0.001326, scenario: 0, slope: -5.714937976694118e-05, fluctuations: 0.0\n",
      "step: 93640 loss: 0.500005 time elapsed: 125.0300 learning rate: 0.001326, scenario: 0, slope: -5.654179405634115e-05, fluctuations: 0.0\n",
      "step: 93650 loss: 0.499467 time elapsed: 125.0450 learning rate: 0.001326, scenario: 0, slope: -5.597157664815175e-05, fluctuations: 0.0\n",
      "step: 93660 loss: 0.498934 time elapsed: 125.0595 learning rate: 0.001326, scenario: 0, slope: -5.543599318951378e-05, fluctuations: 0.0\n",
      "step: 93670 loss: 0.498404 time elapsed: 125.0740 learning rate: 0.001326, scenario: 0, slope: -5.493254696158545e-05, fluctuations: 0.0\n",
      "step: 93680 loss: 0.497878 time elapsed: 125.0884 learning rate: 0.001326, scenario: 0, slope: -5.445895525238499e-05, fluctuations: 0.0\n",
      "step: 93690 loss: 0.497355 time elapsed: 125.1018 learning rate: 0.001326, scenario: 0, slope: -5.401312831076366e-05, fluctuations: 0.0\n",
      "step: 93700 loss: 0.496836 time elapsed: 125.1148 learning rate: 0.001326, scenario: 0, slope: -5.363403644368433e-05, fluctuations: 0.0\n",
      "step: 93710 loss: 0.496319 time elapsed: 125.1278 learning rate: 0.001326, scenario: 0, slope: -5.319726426059097e-05, fluctuations: 0.0\n",
      "step: 93720 loss: 0.495806 time elapsed: 125.1402 learning rate: 0.001326, scenario: 0, slope: -5.282385399267173e-05, fluctuations: 0.0\n",
      "step: 93730 loss: 0.495295 time elapsed: 125.1526 learning rate: 0.001326, scenario: 0, slope: -5.2471434040265846e-05, fluctuations: 0.0\n",
      "step: 93740 loss: 0.494786 time elapsed: 125.1664 learning rate: 0.001326, scenario: 0, slope: -5.213863618402915e-05, fluctuations: 0.0\n",
      "step: 93750 loss: 0.494281 time elapsed: 125.1802 learning rate: 0.001326, scenario: 0, slope: -5.182419911813279e-05, fluctuations: 0.0\n",
      "step: 93760 loss: 0.493777 time elapsed: 125.1940 learning rate: 0.001326, scenario: 0, slope: -5.1526958887035015e-05, fluctuations: 0.0\n",
      "step: 93770 loss: 0.493276 time elapsed: 125.2079 learning rate: 0.001326, scenario: 0, slope: -5.124584028293706e-05, fluctuations: 0.0\n",
      "step: 93780 loss: 0.492776 time elapsed: 125.2215 learning rate: 0.001326, scenario: 0, slope: -5.0979849096215126e-05, fluctuations: 0.0\n",
      "step: 93790 loss: 0.492279 time elapsed: 125.2354 learning rate: 0.001326, scenario: 0, slope: -5.072806512271286e-05, fluctuations: 0.0\n",
      "step: 93800 loss: 0.491784 time elapsed: 125.2491 learning rate: 0.001326, scenario: 0, slope: -5.051290080459271e-05, fluctuations: 0.0\n",
      "step: 93810 loss: 0.491290 time elapsed: 125.2632 learning rate: 0.001326, scenario: 0, slope: -5.026377072434854e-05, fluctuations: 0.0\n",
      "step: 93820 loss: 0.490798 time elapsed: 125.2767 learning rate: 0.001326, scenario: 0, slope: -5.0049736008944805e-05, fluctuations: 0.0\n",
      "step: 93830 loss: 0.490307 time elapsed: 125.2919 learning rate: 0.001326, scenario: 0, slope: -4.984685005389594e-05, fluctuations: 0.0\n",
      "step: 93840 loss: 0.489818 time elapsed: 125.3060 learning rate: 0.001326, scenario: 0, slope: -4.965447905123303e-05, fluctuations: 0.0\n",
      "step: 93850 loss: 0.489331 time elapsed: 125.3187 learning rate: 0.001326, scenario: 0, slope: -4.947203315754951e-05, fluctuations: 0.0\n",
      "step: 93860 loss: 0.488837 time elapsed: 125.3312 learning rate: 0.001422, scenario: 1, slope: -4.9308953349120394e-05, fluctuations: 0.0\n",
      "step: 93870 loss: 0.488299 time elapsed: 125.3436 learning rate: 0.001555, scenario: 0, slope: -4.932314302097206e-05, fluctuations: 0.0\n",
      "step: 93880 loss: 0.487733 time elapsed: 125.3557 learning rate: 0.001555, scenario: 0, slope: -4.970931845291079e-05, fluctuations: 0.0\n",
      "step: 93890 loss: 0.487179 time elapsed: 125.3684 learning rate: 0.001555, scenario: 0, slope: -5.041212247002444e-05, fluctuations: 0.0\n",
      "step: 93900 loss: 33.954203 time elapsed: 125.3821 learning rate: 0.001563, scenario: -1, slope: 0.005446594791808577, fluctuations: 0.0\n",
      "step: 93910 loss: 62.529404 time elapsed: 125.3965 learning rate: 0.001413, scenario: -1, slope: 0.39983277357074654, fluctuations: 0.04\n",
      "step: 93920 loss: 23.775249 time elapsed: 125.4104 learning rate: 0.001278, scenario: -1, slope: 0.41159565252815705, fluctuations: 0.08\n",
      "step: 93930 loss: 5.938417 time elapsed: 125.4239 learning rate: 0.001156, scenario: -1, slope: 0.35424780802457806, fluctuations: 0.13\n",
      "step: 93940 loss: 1.781339 time elapsed: 125.4378 learning rate: 0.001045, scenario: -1, slope: 0.27306237352259227, fluctuations: 0.18\n",
      "step: 93950 loss: 0.696618 time elapsed: 125.4512 learning rate: 0.000946, scenario: -1, slope: 0.15451195484890748, fluctuations: 0.22\n",
      "step: 93960 loss: 0.608539 time elapsed: 125.4648 learning rate: 0.000855, scenario: -1, slope: 0.02967038188532043, fluctuations: 0.26\n",
      "step: 93970 loss: 0.580568 time elapsed: 125.4781 learning rate: 0.000847, scenario: 0, slope: -0.10368065080964005, fluctuations: 0.29\n",
      "step: 93980 loss: 0.526926 time elapsed: 125.4917 learning rate: 0.000847, scenario: 0, slope: -0.2417104141328806, fluctuations: 0.33\n",
      "step: 93990 loss: 0.509954 time elapsed: 125.5067 learning rate: 0.000847, scenario: 0, slope: -0.4179812244889811, fluctuations: 0.37\n",
      "step: 94000 loss: 0.501598 time elapsed: 125.5198 learning rate: 0.000847, scenario: 0, slope: -0.6563378180957036, fluctuations: 0.41\n",
      "step: 94010 loss: 0.499276 time elapsed: 125.5330 learning rate: 0.000847, scenario: 0, slope: -0.08648104736259804, fluctuations: 0.41\n",
      "step: 94020 loss: 0.498528 time elapsed: 125.5457 learning rate: 0.000847, scenario: 0, slope: -0.031548013705038104, fluctuations: 0.4\n",
      "step: 94030 loss: 0.497814 time elapsed: 125.5582 learning rate: 0.000847, scenario: 0, slope: -0.014004979120760612, fluctuations: 0.39\n",
      "step: 94040 loss: 0.497172 time elapsed: 125.5710 learning rate: 0.000847, scenario: 0, slope: -0.004567575135884024, fluctuations: 0.37\n",
      "step: 94050 loss: 0.496593 time elapsed: 125.5853 learning rate: 0.000847, scenario: 0, slope: -0.0017746791655672484, fluctuations: 0.32\n",
      "step: 94060 loss: 0.496075 time elapsed: 125.5996 learning rate: 0.000847, scenario: 0, slope: -0.0005680615794235629, fluctuations: 0.29\n",
      "step: 94070 loss: 0.495591 time elapsed: 125.6133 learning rate: 0.000847, scenario: 0, slope: -0.000200168330923232, fluctuations: 0.25\n",
      "step: 94080 loss: 0.495137 time elapsed: 125.6270 learning rate: 0.000847, scenario: 0, slope: -0.00012069203569981233, fluctuations: 0.21\n",
      "step: 94090 loss: 0.494706 time elapsed: 125.6405 learning rate: 0.000847, scenario: 0, slope: -7.810595342107813e-05, fluctuations: 0.17\n",
      "step: 94100 loss: 0.494293 time elapsed: 125.6541 learning rate: 0.000847, scenario: 0, slope: -6.0805555319357763e-05, fluctuations: 0.14\n",
      "step: 94110 loss: 0.493897 time elapsed: 125.6686 learning rate: 0.000847, scenario: 0, slope: -5.2818741387512344e-05, fluctuations: 0.09\n",
      "step: 94120 loss: 0.493517 time elapsed: 125.6825 learning rate: 0.000805, scenario: -1, slope: -4.764631310549269e-05, fluctuations: 0.06\n",
      "step: 94130 loss: 0.493176 time elapsed: 125.6960 learning rate: 0.000728, scenario: -1, slope: -4.493956697842896e-05, fluctuations: 0.02\n",
      "step: 94140 loss: 0.492873 time elapsed: 125.7098 learning rate: 0.000724, scenario: 1, slope: -4.248183069806617e-05, fluctuations: 0.0\n",
      "step: 94150 loss: 0.492561 time elapsed: 125.7223 learning rate: 0.000800, scenario: 1, slope: -4.003861017464843e-05, fluctuations: 0.0\n",
      "step: 94160 loss: 0.492223 time elapsed: 125.7350 learning rate: 0.000884, scenario: 1, slope: -3.794263130776448e-05, fluctuations: 0.0\n",
      "step: 94170 loss: 0.491857 time elapsed: 125.7474 learning rate: 0.000976, scenario: 1, slope: -3.6323884136839056e-05, fluctuations: 0.0\n",
      "step: 94180 loss: 0.491462 time elapsed: 125.7602 learning rate: 0.001078, scenario: 1, slope: -3.530567918947946e-05, fluctuations: 0.0\n",
      "step: 94190 loss: 0.491035 time elapsed: 125.7729 learning rate: 0.001191, scenario: 1, slope: -3.499541876523506e-05, fluctuations: 0.0\n",
      "step: 94200 loss: 0.490573 time elapsed: 125.7865 learning rate: 0.001303, scenario: 1, slope: -3.5387628872484366e-05, fluctuations: 0.0\n",
      "step: 94210 loss: 0.490079 time elapsed: 125.8009 learning rate: 0.001439, scenario: 1, slope: -3.6774610879765654e-05, fluctuations: 0.0\n",
      "step: 94220 loss: 0.489545 time elapsed: 125.8144 learning rate: 0.001589, scenario: 1, slope: -3.8905840209672786e-05, fluctuations: 0.0\n",
      "step: 94230 loss: 0.488967 time elapsed: 125.8280 learning rate: 0.001756, scenario: 1, slope: -4.175840852930577e-05, fluctuations: 0.0\n",
      "step: 94240 loss: 0.488340 time elapsed: 125.8418 learning rate: 0.001939, scenario: 1, slope: -4.50672443443216e-05, fluctuations: 0.0\n",
      "step: 94250 loss: 0.487662 time elapsed: 125.8557 learning rate: 0.002142, scenario: 1, slope: -4.866687161638795e-05, fluctuations: 0.0\n",
      "step: 94260 loss: 0.503139 time elapsed: 125.8694 learning rate: 0.002185, scenario: 1, slope: -4.05111903285596e-05, fluctuations: 0.0\n",
      "step: 94270 loss: 160.350548 time elapsed: 125.8833 learning rate: 0.001986, scenario: -1, slope: 0.7849964989291099, fluctuations: 0.01\n",
      "step: 94280 loss: 177.963780 time elapsed: 125.8969 learning rate: 0.001796, scenario: -1, slope: 1.091496681931496, fluctuations: 0.05\n",
      "step: 94290 loss: 53.423398 time elapsed: 125.9105 learning rate: 0.001625, scenario: -1, slope: 0.9338158888412104, fluctuations: 0.1\n",
      "step: 94300 loss: 1.232637 time elapsed: 125.9228 learning rate: 0.001484, scenario: -1, slope: 0.7667612029851332, fluctuations: 0.14\n",
      "step: 94310 loss: 4.610677 time elapsed: 125.9361 learning rate: 0.001342, scenario: -1, slope: 0.4936099718136285, fluctuations: 0.19\n",
      "step: 94320 loss: 2.265671 time elapsed: 125.9487 learning rate: 0.001214, scenario: -1, slope: 0.20186377227475774, fluctuations: 0.23\n",
      "step: 94330 loss: 0.926628 time elapsed: 125.9608 learning rate: 0.001143, scenario: 0, slope: -0.099821015830073, fluctuations: 0.27\n",
      "step: 94340 loss: 0.645130 time elapsed: 125.9730 learning rate: 0.001143, scenario: 0, slope: -0.41994543689359776, fluctuations: 0.31\n",
      "step: 94350 loss: 0.634026 time elapsed: 125.9873 learning rate: 0.001143, scenario: 0, slope: -0.794900575887752, fluctuations: 0.35\n",
      "step: 94360 loss: 0.625028 time elapsed: 126.0010 learning rate: 0.001143, scenario: 0, slope: -1.311821145854468, fluctuations: 0.39\n",
      "step: 94370 loss: 0.607703 time elapsed: 126.0149 learning rate: 0.001143, scenario: 0, slope: -1.0116487526893088, fluctuations: 0.4\n",
      "step: 94380 loss: 0.594463 time elapsed: 126.0303 learning rate: 0.001143, scenario: 0, slope: -0.0959064051551051, fluctuations: 0.4\n",
      "step: 94390 loss: 0.586324 time elapsed: 126.0441 learning rate: 0.001143, scenario: 0, slope: -0.06355150381812635, fluctuations: 0.39\n",
      "step: 94400 loss: 0.580609 time elapsed: 126.0578 learning rate: 0.001143, scenario: 0, slope: -0.01987037066223, fluctuations: 0.36\n",
      "step: 94410 loss: 0.575985 time elapsed: 126.0727 learning rate: 0.001143, scenario: 0, slope: -0.008341180530532843, fluctuations: 0.31\n",
      "step: 94420 loss: 0.571937 time elapsed: 126.0870 learning rate: 0.001143, scenario: 0, slope: -0.003208249460169099, fluctuations: 0.27\n",
      "step: 94430 loss: 0.568326 time elapsed: 126.1017 learning rate: 0.001143, scenario: 0, slope: -0.0016960912635256892, fluctuations: 0.23\n",
      "step: 94440 loss: 0.565077 time elapsed: 126.1160 learning rate: 0.001143, scenario: 0, slope: -0.0009755167580435154, fluctuations: 0.19\n",
      "step: 94450 loss: 0.562137 time elapsed: 126.1303 learning rate: 0.001143, scenario: 0, slope: -0.000592922963179768, fluctuations: 0.16\n",
      "step: 94460 loss: 0.559464 time elapsed: 126.1445 learning rate: 0.001143, scenario: 0, slope: -0.0004660556024613864, fluctuations: 0.12\n",
      "step: 94470 loss: 0.557019 time elapsed: 126.1583 learning rate: 0.001143, scenario: 0, slope: -0.00039991244653400594, fluctuations: 0.08\n",
      "step: 94480 loss: 0.554772 time elapsed: 126.1710 learning rate: 0.001143, scenario: 0, slope: -0.0003548534115933773, fluctuations: 0.04\n",
      "step: 94490 loss: 0.552697 time elapsed: 126.1837 learning rate: 0.001143, scenario: 0, slope: -0.0003201741687950176, fluctuations: 0.01\n",
      "step: 94500 loss: 0.550773 time elapsed: 126.1968 learning rate: 0.001143, scenario: 0, slope: -0.0002927173377563028, fluctuations: 0.0\n",
      "step: 94510 loss: 0.548980 time elapsed: 126.2114 learning rate: 0.001143, scenario: 0, slope: -0.0002630849308718618, fluctuations: 0.0\n",
      "step: 94520 loss: 0.547304 time elapsed: 126.2252 learning rate: 0.001143, scenario: 0, slope: -0.00024038770111249025, fluctuations: 0.0\n",
      "step: 94530 loss: 0.545730 time elapsed: 126.2388 learning rate: 0.001143, scenario: 0, slope: -0.00022088108343559128, fluctuations: 0.0\n",
      "step: 94540 loss: 0.544247 time elapsed: 126.2525 learning rate: 0.001143, scenario: 0, slope: -0.00020397715697251709, fluctuations: 0.0\n",
      "step: 94550 loss: 0.542846 time elapsed: 126.2661 learning rate: 0.001143, scenario: 0, slope: -0.00018923810258787208, fluctuations: 0.0\n",
      "step: 94560 loss: 0.541517 time elapsed: 126.2796 learning rate: 0.001143, scenario: 0, slope: -0.0001763210431573146, fluctuations: 0.0\n",
      "step: 94570 loss: 0.540254 time elapsed: 126.2934 learning rate: 0.001143, scenario: 0, slope: -0.00016494861932757826, fluctuations: 0.0\n",
      "step: 94580 loss: 0.539050 time elapsed: 126.3071 learning rate: 0.001143, scenario: 0, slope: -0.0001548929712113405, fluctuations: 0.0\n",
      "step: 94590 loss: 0.537899 time elapsed: 126.3209 learning rate: 0.001143, scenario: 0, slope: -0.00014596522710954443, fluctuations: 0.0\n",
      "step: 94600 loss: 0.536796 time elapsed: 126.3338 learning rate: 0.001143, scenario: 0, slope: -0.00013876385220026223, fluctuations: 0.0\n",
      "step: 94610 loss: 0.535738 time elapsed: 126.3474 learning rate: 0.001143, scenario: 0, slope: -0.0001308887233091399, fluctuations: 0.0\n",
      "step: 94620 loss: 0.534719 time elapsed: 126.3598 learning rate: 0.001143, scenario: 0, slope: -0.000124496685368817, fluctuations: 0.0\n",
      "step: 94630 loss: 0.533737 time elapsed: 126.3721 learning rate: 0.001143, scenario: 0, slope: -0.0001187376917216885, fluctuations: 0.0\n",
      "step: 94640 loss: 0.532789 time elapsed: 126.3845 learning rate: 0.001143, scenario: 0, slope: -0.00011353191695962346, fluctuations: 0.0\n",
      "step: 94650 loss: 0.531871 time elapsed: 126.3969 learning rate: 0.001143, scenario: 0, slope: -0.0001088113309291482, fluctuations: 0.0\n",
      "step: 94660 loss: 0.530982 time elapsed: 126.4105 learning rate: 0.001143, scenario: 0, slope: -0.00010451773860271305, fluctuations: 0.0\n",
      "step: 94670 loss: 0.530120 time elapsed: 126.4242 learning rate: 0.001143, scenario: 0, slope: -0.0001006011783924058, fluctuations: 0.0\n",
      "step: 94680 loss: 0.529282 time elapsed: 126.4376 learning rate: 0.001143, scenario: 0, slope: -9.701860803784669e-05, fluctuations: 0.0\n",
      "step: 94690 loss: 0.528466 time elapsed: 126.4511 learning rate: 0.001143, scenario: 0, slope: -9.373282200633666e-05, fluctuations: 0.0\n",
      "step: 94700 loss: 0.527671 time elapsed: 126.4644 learning rate: 0.001143, scenario: 0, slope: -9.100260899971168e-05, fluctuations: 0.0\n",
      "step: 94710 loss: 0.526896 time elapsed: 126.4785 learning rate: 0.001143, scenario: 0, slope: -8.792674332357706e-05, fluctuations: 0.0\n",
      "step: 94720 loss: 0.526140 time elapsed: 126.4923 learning rate: 0.001143, scenario: 0, slope: -8.535389516278542e-05, fluctuations: 0.0\n",
      "step: 94730 loss: 0.525400 time elapsed: 126.5062 learning rate: 0.001143, scenario: 0, slope: -8.297158205390434e-05, fluctuations: 0.0\n",
      "step: 94740 loss: 0.524676 time elapsed: 126.5197 learning rate: 0.001143, scenario: 0, slope: -8.076099872876944e-05, fluctuations: 0.0\n",
      "step: 94750 loss: 0.523968 time elapsed: 126.5364 learning rate: 0.001143, scenario: 0, slope: -7.870559760748661e-05, fluctuations: 0.0\n",
      "step: 94760 loss: 0.523273 time elapsed: 126.5505 learning rate: 0.001143, scenario: 0, slope: -7.679077917144907e-05, fluctuations: 0.0\n",
      "step: 94770 loss: 0.522592 time elapsed: 126.5633 learning rate: 0.001143, scenario: 0, slope: -7.500362958508062e-05, fluctuations: 0.0\n",
      "step: 94780 loss: 0.521923 time elapsed: 126.5764 learning rate: 0.001143, scenario: 0, slope: -7.333269769657445e-05, fluctuations: 0.0\n",
      "step: 94790 loss: 0.521266 time elapsed: 126.5889 learning rate: 0.001143, scenario: 0, slope: -7.176780495842342e-05, fluctuations: 0.0\n",
      "step: 94800 loss: 0.520620 time elapsed: 126.6015 learning rate: 0.001143, scenario: 0, slope: -7.044254928313654e-05, fluctuations: 0.0\n",
      "step: 94810 loss: 0.519984 time elapsed: 126.6153 learning rate: 0.001143, scenario: 0, slope: -6.892083408497504e-05, fluctuations: 0.0\n",
      "step: 94820 loss: 0.519359 time elapsed: 126.6291 learning rate: 0.001143, scenario: 0, slope: -6.762341191930841e-05, fluctuations: 0.0\n",
      "step: 94830 loss: 0.518742 time elapsed: 126.6432 learning rate: 0.001143, scenario: 0, slope: -6.640111793232092e-05, fluctuations: 0.0\n",
      "step: 94840 loss: 0.518135 time elapsed: 126.6571 learning rate: 0.001143, scenario: 0, slope: -6.52481123559451e-05, fluctuations: 0.0\n",
      "step: 94850 loss: 0.517536 time elapsed: 126.6709 learning rate: 0.001143, scenario: 0, slope: -6.41591368862626e-05, fluctuations: 0.0\n",
      "step: 94860 loss: 0.516946 time elapsed: 126.6849 learning rate: 0.001143, scenario: 0, slope: -6.312944753392088e-05, fluctuations: 0.0\n",
      "step: 94870 loss: 0.516363 time elapsed: 126.6984 learning rate: 0.001143, scenario: 0, slope: -6.21547561305089e-05, fluctuations: 0.0\n",
      "step: 94880 loss: 0.515787 time elapsed: 126.7120 learning rate: 0.001143, scenario: 0, slope: -6.123117924915013e-05, fluctuations: 0.0\n",
      "step: 94890 loss: 0.515218 time elapsed: 126.7260 learning rate: 0.001143, scenario: 0, slope: -6.0355193490320595e-05, fluctuations: 0.0\n",
      "step: 94900 loss: 0.514656 time elapsed: 126.7396 learning rate: 0.001143, scenario: 0, slope: -5.960484370523564e-05, fluctuations: 0.0\n",
      "step: 94910 loss: 0.514100 time elapsed: 126.7536 learning rate: 0.001143, scenario: 0, slope: -5.8733471222076694e-05, fluctuations: 0.0\n",
      "step: 94920 loss: 0.513551 time elapsed: 126.7659 learning rate: 0.001143, scenario: 0, slope: -5.798215799748086e-05, fluctuations: 0.0\n",
      "step: 94930 loss: 0.513006 time elapsed: 126.7782 learning rate: 0.001143, scenario: 0, slope: -5.726722522643616e-05, fluctuations: 0.0\n",
      "step: 94940 loss: 0.512468 time elapsed: 126.7906 learning rate: 0.001143, scenario: 0, slope: -5.6586446877553276e-05, fluctuations: 0.0\n",
      "step: 94950 loss: 0.511935 time elapsed: 126.8036 learning rate: 0.001143, scenario: 0, slope: -5.5937781204147964e-05, fluctuations: 0.0\n",
      "step: 94960 loss: 0.511406 time elapsed: 126.8160 learning rate: 0.001143, scenario: 0, slope: -5.5319352080415797e-05, fluctuations: 0.0\n",
      "step: 94970 loss: 0.510883 time elapsed: 126.8298 learning rate: 0.001143, scenario: 0, slope: -5.4729432416348744e-05, fluctuations: 0.0\n",
      "step: 94980 loss: 0.510364 time elapsed: 126.8435 learning rate: 0.001143, scenario: 0, slope: -5.416642940322538e-05, fluctuations: 0.0\n",
      "step: 94990 loss: 0.509849 time elapsed: 126.8571 learning rate: 0.001143, scenario: 0, slope: -5.3628871372082686e-05, fluctuations: 0.0\n",
      "step: 95000 loss: 0.509338 time elapsed: 126.8708 learning rate: 0.001143, scenario: 0, slope: -5.31656966192164e-05, fluctuations: 0.0\n",
      "step: 95010 loss: 0.508832 time elapsed: 126.8851 learning rate: 0.001143, scenario: 0, slope: -5.262474024761708e-05, fluctuations: 0.0\n",
      "step: 95020 loss: 0.508329 time elapsed: 126.8987 learning rate: 0.001143, scenario: 0, slope: -5.21557302445255e-05, fluctuations: 0.0\n",
      "step: 95030 loss: 0.507830 time elapsed: 126.9124 learning rate: 0.001143, scenario: 0, slope: -5.1707273731259764e-05, fluctuations: 0.0\n",
      "step: 95040 loss: 0.507335 time elapsed: 126.9260 learning rate: 0.001143, scenario: 0, slope: -5.1278352214004415e-05, fluctuations: 0.0\n",
      "step: 95050 loss: 0.506829 time elapsed: 126.9393 learning rate: 0.001250, scenario: 1, slope: -5.089213238940294e-05, fluctuations: 0.0\n",
      "step: 95060 loss: 0.506275 time elapsed: 126.9535 learning rate: 0.001381, scenario: 1, slope: -5.074451501575389e-05, fluctuations: 0.0\n",
      "step: 95070 loss: 0.505669 time elapsed: 126.9675 learning rate: 0.001525, scenario: 1, slope: -5.1078872036706396e-05, fluctuations: 0.0\n",
      "step: 95080 loss: 0.505028 time elapsed: 126.9805 learning rate: 0.001525, scenario: 0, slope: -5.205185625124594e-05, fluctuations: 0.0\n",
      "step: 95090 loss: 0.504393 time elapsed: 126.9944 learning rate: 0.001525, scenario: 0, slope: -5.3542735397343915e-05, fluctuations: 0.0\n",
      "step: 95100 loss: 0.503763 time elapsed: 127.0079 learning rate: 0.001525, scenario: 0, slope: -5.515815847354181e-05, fluctuations: 0.0\n",
      "step: 95110 loss: 0.503139 time elapsed: 127.0214 learning rate: 0.001525, scenario: 0, slope: -5.7264024176132985e-05, fluctuations: 0.0\n",
      "step: 95120 loss: 0.502520 time elapsed: 127.0344 learning rate: 0.001525, scenario: 0, slope: -5.909780354920389e-05, fluctuations: 0.0\n",
      "step: 95130 loss: 0.501905 time elapsed: 127.0486 learning rate: 0.001525, scenario: 0, slope: -6.065612731518778e-05, fluctuations: 0.0\n",
      "step: 95140 loss: 0.501295 time elapsed: 127.0625 learning rate: 0.001525, scenario: 0, slope: -6.175070873567923e-05, fluctuations: 0.0\n",
      "step: 95150 loss: 0.500690 time elapsed: 127.0762 learning rate: 0.001525, scenario: 0, slope: -6.2223019810695e-05, fluctuations: 0.0\n",
      "step: 95160 loss: 0.500089 time elapsed: 127.0899 learning rate: 0.001525, scenario: 0, slope: -6.213544538529359e-05, fluctuations: 0.0\n",
      "step: 95170 loss: 0.499491 time elapsed: 127.1036 learning rate: 0.001525, scenario: 0, slope: -6.171652758635242e-05, fluctuations: 0.0\n",
      "step: 95180 loss: 0.498898 time elapsed: 127.1171 learning rate: 0.001525, scenario: 0, slope: -6.124215443187677e-05, fluctuations: 0.0\n",
      "step: 95190 loss: 0.498308 time elapsed: 127.1308 learning rate: 0.001525, scenario: 0, slope: -6.0790644861606685e-05, fluctuations: 0.0\n",
      "step: 95200 loss: 0.497721 time elapsed: 127.1443 learning rate: 0.001525, scenario: 0, slope: -6.04031758646535e-05, fluctuations: 0.0\n",
      "step: 95210 loss: 0.497138 time elapsed: 127.1581 learning rate: 0.001525, scenario: 0, slope: -5.99527171534945e-05, fluctuations: 0.0\n",
      "step: 95220 loss: 0.496558 time elapsed: 127.1713 learning rate: 0.001525, scenario: 0, slope: -5.956408979570499e-05, fluctuations: 0.0\n",
      "step: 95230 loss: 0.495981 time elapsed: 127.1836 learning rate: 0.001525, scenario: 0, slope: -5.9194217067976233e-05, fluctuations: 0.0\n",
      "step: 95240 loss: 0.495407 time elapsed: 127.1959 learning rate: 0.001525, scenario: 0, slope: -5.88420611455315e-05, fluctuations: 0.0\n",
      "step: 95250 loss: 0.494835 time elapsed: 127.2081 learning rate: 0.001525, scenario: 0, slope: -5.8506645371455894e-05, fluctuations: 0.0\n",
      "step: 95260 loss: 0.494266 time elapsed: 127.2202 learning rate: 0.001525, scenario: 0, slope: -5.818705297849932e-05, fluctuations: 0.0\n",
      "step: 95270 loss: 0.493699 time elapsed: 127.2323 learning rate: 0.001525, scenario: 0, slope: -5.788242390528917e-05, fluctuations: 0.0\n",
      "step: 95280 loss: 0.493135 time elapsed: 127.2458 learning rate: 0.001525, scenario: 0, slope: -5.759195117781209e-05, fluctuations: 0.0\n",
      "step: 95290 loss: 0.492573 time elapsed: 127.2591 learning rate: 0.001525, scenario: 0, slope: -5.731487734554327e-05, fluctuations: 0.0\n",
      "step: 95300 loss: 0.492013 time elapsed: 127.2722 learning rate: 0.001525, scenario: 0, slope: -5.707637816996618e-05, fluctuations: 0.0\n",
      "step: 95310 loss: 0.491455 time elapsed: 127.2866 learning rate: 0.001525, scenario: 0, slope: -5.679812425518393e-05, fluctuations: 0.0\n",
      "step: 95320 loss: 0.490899 time elapsed: 127.3029 learning rate: 0.001525, scenario: 0, slope: -5.655714864576534e-05, fluctuations: 0.0\n",
      "step: 95330 loss: 0.490345 time elapsed: 127.3180 learning rate: 0.001525, scenario: 0, slope: -5.6326973657037085e-05, fluctuations: 0.0\n",
      "step: 95340 loss: 0.489793 time elapsed: 127.3332 learning rate: 0.001525, scenario: 0, slope: -5.610704162460876e-05, fluctuations: 0.0\n",
      "step: 95350 loss: 0.489245 time elapsed: 127.3497 learning rate: 0.001525, scenario: 0, slope: -5.589458053498158e-05, fluctuations: 0.0\n",
      "step: 95360 loss: 0.492539 time elapsed: 127.3658 learning rate: 0.001525, scenario: 0, slope: -5.153044806816369e-05, fluctuations: 0.0\n",
      "step: 95370 loss: 23.686196 time elapsed: 127.3850 learning rate: 0.001472, scenario: -1, slope: 0.02489091495962966, fluctuations: 0.0\n",
      "step: 95380 loss: 8.229757 time elapsed: 127.4004 learning rate: 0.001331, scenario: -1, slope: 0.09698298659323025, fluctuations: 0.02\n",
      "step: 95390 loss: 4.519286 time elapsed: 127.4149 learning rate: 0.001204, scenario: -1, slope: 0.09292399783468994, fluctuations: 0.05\n",
      "step: 95400 loss: 0.672530 time elapsed: 127.4284 learning rate: 0.001100, scenario: -1, slope: 0.08025685333127547, fluctuations: 0.09\n",
      "step: 95410 loss: 0.691541 time elapsed: 127.4420 learning rate: 0.000995, scenario: -1, slope: 0.05454591015451689, fluctuations: 0.13\n",
      "step: 95420 loss: 0.553816 time elapsed: 127.4560 learning rate: 0.000900, scenario: -1, slope: 0.031180429835189923, fluctuations: 0.18\n",
      "step: 95430 loss: 0.526840 time elapsed: 127.4701 learning rate: 0.000814, scenario: -1, slope: 0.0005296425032292148, fluctuations: 0.22\n",
      "step: 95440 loss: 0.501334 time elapsed: 127.4842 learning rate: 0.000814, scenario: 0, slope: -0.030726289741355564, fluctuations: 0.27\n",
      "step: 95450 loss: 0.493116 time elapsed: 127.4979 learning rate: 0.000814, scenario: 0, slope: -0.06611850136954152, fluctuations: 0.32\n",
      "step: 95460 loss: 0.490230 time elapsed: 127.5117 learning rate: 0.000814, scenario: 0, slope: -0.10843428038946723, fluctuations: 0.37\n",
      "step: 95470 loss: 0.489035 time elapsed: 127.5259 learning rate: 0.000814, scenario: 0, slope: -0.12647290278513584, fluctuations: 0.42\n",
      "step: 95480 loss: 0.488396 time elapsed: 127.5398 learning rate: 0.000814, scenario: 0, slope: -0.02845462516926457, fluctuations: 0.44\n",
      "step: 95490 loss: 0.487953 time elapsed: 127.5535 learning rate: 0.000814, scenario: 0, slope: -0.005551971144714507, fluctuations: 0.46\n",
      "step: 95500 loss: 0.487585 time elapsed: 127.5669 learning rate: 0.000814, scenario: 0, slope: -0.0013399372519711855, fluctuations: 0.47\n",
      "step: 95510 loss: 0.487250 time elapsed: 127.5826 learning rate: 0.000814, scenario: 0, slope: -0.0005070896701101361, fluctuations: 0.42\n",
      "step: 95520 loss: 0.486929 time elapsed: 127.5958 learning rate: 0.000814, scenario: 0, slope: -0.00014048798668851546, fluctuations: 0.38\n",
      "step: 95530 loss: 0.486617 time elapsed: 127.6091 learning rate: 0.000814, scenario: 0, slope: -5.552261173419719e-05, fluctuations: 0.33\n",
      "step: 95540 loss: 0.486310 time elapsed: 127.6222 learning rate: 0.000789, scenario: -1, slope: -4.3792756464929336e-05, fluctuations: 0.28\n",
      "step: 95550 loss: 0.486021 time elapsed: 127.6359 learning rate: 0.000721, scenario: -1, slope: -3.6456910348911066e-05, fluctuations: 0.23\n",
      "step: 95560 loss: 0.485761 time elapsed: 127.6491 learning rate: 0.000652, scenario: -1, slope: -3.2780960124094786e-05, fluctuations: 0.18\n",
      "step: 95570 loss: 0.485527 time elapsed: 127.6623 learning rate: 0.000590, scenario: -1, slope: -3.082800441326236e-05, fluctuations: 0.13\n",
      "step: 95580 loss: 0.485315 time elapsed: 127.6778 learning rate: 0.000533, scenario: -1, slope: -2.9406130546734822e-05, fluctuations: 0.08\n",
      "step: 95590 loss: 0.485124 time elapsed: 127.6922 learning rate: 0.000482, scenario: -1, slope: -2.8057485157946663e-05, fluctuations: 0.03\n",
      "step: 95600 loss: 0.484948 time elapsed: 127.7062 learning rate: 0.000485, scenario: 1, slope: -2.6716449740411873e-05, fluctuations: 0.0\n",
      "step: 95610 loss: 0.484762 time elapsed: 127.7209 learning rate: 0.000535, scenario: 1, slope: -2.4716364433884016e-05, fluctuations: 0.0\n",
      "step: 95620 loss: 0.484556 time elapsed: 127.7350 learning rate: 0.000591, scenario: 1, slope: -2.3032848999479527e-05, fluctuations: 0.0\n",
      "step: 95630 loss: 0.484329 time elapsed: 127.7486 learning rate: 0.000653, scenario: 1, slope: -2.1691643032973182e-05, fluctuations: 0.0\n",
      "step: 95640 loss: 0.484078 time elapsed: 127.7625 learning rate: 0.000722, scenario: 1, slope: -2.090229703680373e-05, fluctuations: 0.0\n",
      "step: 95650 loss: 0.483801 time elapsed: 127.7762 learning rate: 0.000797, scenario: 1, slope: -2.0813378366564997e-05, fluctuations: 0.0\n",
      "step: 95660 loss: 0.483496 time elapsed: 127.7902 learning rate: 0.000881, scenario: 1, slope: -2.14462033693913e-05, fluctuations: 0.0\n",
      "step: 95670 loss: 0.483159 time elapsed: 127.8040 learning rate: 0.000973, scenario: 1, slope: -2.2760939664975522e-05, fluctuations: 0.0\n",
      "step: 95680 loss: 0.482787 time elapsed: 127.8169 learning rate: 0.001074, scenario: 1, slope: -2.4685213454172647e-05, fluctuations: 0.0\n",
      "step: 95690 loss: 0.482377 time elapsed: 127.8296 learning rate: 0.001187, scenario: 1, slope: -2.711296740440857e-05, fluctuations: 0.0\n",
      "step: 95700 loss: 0.481926 time elapsed: 127.8418 learning rate: 0.001298, scenario: 1, slope: -2.9614190735344797e-05, fluctuations: 0.0\n",
      "step: 95710 loss: 0.481433 time elapsed: 127.8546 learning rate: 0.001434, scenario: 1, slope: -3.2977210974438216e-05, fluctuations: 0.0\n",
      "step: 95720 loss: 0.480891 time elapsed: 127.8671 learning rate: 0.001584, scenario: 1, slope: -3.632773274937014e-05, fluctuations: 0.0\n",
      "step: 95730 loss: 0.480295 time elapsed: 127.8811 learning rate: 0.001750, scenario: 1, slope: -3.998972436281627e-05, fluctuations: 0.0\n",
      "step: 95740 loss: 67.456123 time elapsed: 127.8955 learning rate: 0.001793, scenario: -1, slope: 0.04593180575256676, fluctuations: 0.0\n",
      "step: 95750 loss: 34.073831 time elapsed: 127.9096 learning rate: 0.001622, scenario: -1, slope: 1.107160964527433, fluctuations: 0.04\n",
      "step: 95760 loss: 147.410922 time elapsed: 127.9236 learning rate: 0.001467, scenario: -1, slope: 1.3302901438681434, fluctuations: 0.08\n",
      "step: 95770 loss: 45.704477 time elapsed: 127.9377 learning rate: 0.001326, scenario: -1, slope: 1.149811141892384, fluctuations: 0.12\n",
      "step: 95780 loss: 3.807442 time elapsed: 127.9516 learning rate: 0.001200, scenario: -1, slope: 0.8552562854771014, fluctuations: 0.16\n",
      "step: 95790 loss: 6.017218 time elapsed: 127.9654 learning rate: 0.001085, scenario: -1, slope: 0.4700481034439154, fluctuations: 0.19\n",
      "step: 95800 loss: 1.083799 time elapsed: 127.9788 learning rate: 0.000991, scenario: -1, slope: 0.12433631044954654, fluctuations: 0.22\n",
      "step: 95810 loss: 1.079401 time elapsed: 127.9944 learning rate: 0.000971, scenario: 0, slope: -0.30996870349527306, fluctuations: 0.26\n",
      "step: 95820 loss: 0.991971 time elapsed: 128.0077 learning rate: 0.000971, scenario: 0, slope: -0.755914002837367, fluctuations: 0.29\n",
      "step: 95830 loss: 0.801842 time elapsed: 128.0204 learning rate: 0.000971, scenario: 0, slope: -1.3202646383078875, fluctuations: 0.33\n",
      "step: 95840 loss: 0.715364 time elapsed: 128.0325 learning rate: 0.000971, scenario: 0, slope: -2.0724036899067775, fluctuations: 0.36\n",
      "step: 95850 loss: 0.692938 time elapsed: 128.0447 learning rate: 0.000971, scenario: 0, slope: -0.9342194379992842, fluctuations: 0.34\n",
      "step: 95860 loss: 0.677097 time elapsed: 128.0569 learning rate: 0.000971, scenario: 0, slope: -0.22577915167838297, fluctuations: 0.33\n",
      "step: 95870 loss: 0.661508 time elapsed: 128.0697 learning rate: 0.000971, scenario: 0, slope: -0.08020200667774668, fluctuations: 0.31\n",
      "step: 95880 loss: 0.649848 time elapsed: 128.0839 learning rate: 0.000971, scenario: 0, slope: -0.03619899811239829, fluctuations: 0.27\n",
      "step: 95890 loss: 0.640724 time elapsed: 128.0979 learning rate: 0.000971, scenario: 0, slope: -0.011088951524808599, fluctuations: 0.24\n",
      "step: 95900 loss: 0.632816 time elapsed: 128.1115 learning rate: 0.000971, scenario: 0, slope: -0.005108329371249647, fluctuations: 0.21\n",
      "step: 95910 loss: 0.625946 time elapsed: 128.1262 learning rate: 0.000971, scenario: 0, slope: -0.002907639818189881, fluctuations: 0.17\n",
      "step: 95920 loss: 0.619946 time elapsed: 128.1407 learning rate: 0.000971, scenario: 0, slope: -0.0017288438103438624, fluctuations: 0.14\n",
      "step: 95930 loss: 0.614609 time elapsed: 128.1548 learning rate: 0.000971, scenario: 0, slope: -0.0012083023311273036, fluctuations: 0.11\n",
      "step: 95940 loss: 0.609816 time elapsed: 128.1689 learning rate: 0.000971, scenario: 0, slope: -0.0009418292118572297, fluctuations: 0.08\n",
      "step: 95950 loss: 0.605478 time elapsed: 128.1828 learning rate: 0.000971, scenario: 0, slope: -0.000796197318782461, fluctuations: 0.04\n",
      "step: 95960 loss: 0.601520 time elapsed: 128.1966 learning rate: 0.000971, scenario: 0, slope: -0.0006854470325579641, fluctuations: 0.01\n",
      "step: 95970 loss: 0.597885 time elapsed: 128.2101 learning rate: 0.000971, scenario: 0, slope: -0.0006005904523755332, fluctuations: 0.0\n",
      "step: 95980 loss: 0.594529 time elapsed: 128.2233 learning rate: 0.000971, scenario: 0, slope: -0.0005306864429527225, fluctuations: 0.0\n",
      "step: 95990 loss: 0.591414 time elapsed: 128.2364 learning rate: 0.000971, scenario: 0, slope: -0.00047457545537328463, fluctuations: 0.0\n",
      "step: 96000 loss: 0.588511 time elapsed: 128.2489 learning rate: 0.000971, scenario: 0, slope: -0.0004328697687578108, fluctuations: 0.0\n",
      "step: 96010 loss: 0.585795 time elapsed: 128.2618 learning rate: 0.000971, scenario: 0, slope: -0.00039038444802310225, fluctuations: 0.0\n",
      "step: 96020 loss: 0.583244 time elapsed: 128.2747 learning rate: 0.000971, scenario: 0, slope: -0.00035802883720525974, fluctuations: 0.0\n",
      "step: 96030 loss: 0.580842 time elapsed: 128.2887 learning rate: 0.000971, scenario: 0, slope: -0.0003303384383051619, fluctuations: 0.0\n",
      "step: 96040 loss: 0.578574 time elapsed: 128.3025 learning rate: 0.000971, scenario: 0, slope: -0.00030638780600611887, fluctuations: 0.0\n",
      "step: 96050 loss: 0.576427 time elapsed: 128.3163 learning rate: 0.000971, scenario: 0, slope: -0.00028547310629327795, fluctuations: 0.0\n",
      "step: 96060 loss: 0.574389 time elapsed: 128.3298 learning rate: 0.000971, scenario: 0, slope: -0.00026705448694312213, fluctuations: 0.0\n",
      "step: 96070 loss: 0.572452 time elapsed: 128.3434 learning rate: 0.000971, scenario: 0, slope: -0.00025071063368522784, fluctuations: 0.0\n",
      "step: 96080 loss: 0.570607 time elapsed: 128.3573 learning rate: 0.000971, scenario: 0, slope: -0.00023610893921306974, fluctuations: 0.0\n",
      "step: 96090 loss: 0.568846 time elapsed: 128.3711 learning rate: 0.000971, scenario: 0, slope: -0.00022298411856379034, fluctuations: 0.0\n",
      "step: 96100 loss: 0.567163 time elapsed: 128.3849 learning rate: 0.000971, scenario: 0, slope: -0.00021225690133595747, fluctuations: 0.0\n",
      "step: 96110 loss: 0.565552 time elapsed: 128.3991 learning rate: 0.000971, scenario: 0, slope: -0.00020034957765549134, fluctuations: 0.0\n",
      "step: 96120 loss: 0.564008 time elapsed: 128.4130 learning rate: 0.000971, scenario: 0, slope: -0.00019052298558094128, fluctuations: 0.0\n",
      "step: 96130 loss: 0.562526 time elapsed: 128.4259 learning rate: 0.000971, scenario: 0, slope: -0.00018152422668493712, fluctuations: 0.0\n",
      "step: 96140 loss: 0.561102 time elapsed: 128.4381 learning rate: 0.000971, scenario: 0, slope: -0.00017325449311170104, fluctuations: 0.0\n",
      "step: 96150 loss: 0.559732 time elapsed: 128.4502 learning rate: 0.000971, scenario: 0, slope: -0.00016563062842024252, fluctuations: 0.0\n",
      "step: 96160 loss: 0.558412 time elapsed: 128.4624 learning rate: 0.000971, scenario: 0, slope: -0.0001585821246511817, fluctuations: 0.0\n",
      "step: 96170 loss: 0.557139 time elapsed: 128.4745 learning rate: 0.000971, scenario: 0, slope: -0.00015204877285764643, fluctuations: 0.0\n",
      "step: 96180 loss: 0.555910 time elapsed: 128.4878 learning rate: 0.000971, scenario: 0, slope: -0.00014597881161262629, fluctuations: 0.0\n",
      "step: 96190 loss: 0.554723 time elapsed: 128.5018 learning rate: 0.000971, scenario: 0, slope: -0.00014032745762840553, fluctuations: 0.0\n",
      "step: 96200 loss: 0.553575 time elapsed: 128.5155 learning rate: 0.000971, scenario: 0, slope: -0.00013556682777834156, fluctuations: 0.0\n",
      "step: 96210 loss: 0.552463 time elapsed: 128.5300 learning rate: 0.000971, scenario: 0, slope: -0.00013012951423943542, fluctuations: 0.0\n",
      "step: 96220 loss: 0.551386 time elapsed: 128.5439 learning rate: 0.000971, scenario: 0, slope: -0.00012551878106036173, fluctuations: 0.0\n",
      "step: 96230 loss: 0.550342 time elapsed: 128.5577 learning rate: 0.000971, scenario: 0, slope: -0.00012119698032144748, fluctuations: 0.0\n",
      "step: 96240 loss: 0.549328 time elapsed: 128.5717 learning rate: 0.000971, scenario: 0, slope: -0.00011714052212489216, fluctuations: 0.0\n",
      "step: 96250 loss: 0.548343 time elapsed: 128.5858 learning rate: 0.000971, scenario: 0, slope: -0.00011332835743315364, fluctuations: 0.0\n",
      "step: 96260 loss: 0.547385 time elapsed: 128.6003 learning rate: 0.000971, scenario: 0, slope: -0.00010974162873988385, fluctuations: 0.0\n",
      "step: 96270 loss: 0.546454 time elapsed: 128.6144 learning rate: 0.000971, scenario: 0, slope: -0.00010636337856687404, fluctuations: 0.0\n",
      "step: 96280 loss: 0.545547 time elapsed: 128.6291 learning rate: 0.000971, scenario: 0, slope: -0.00010317830484509598, fluctuations: 0.0\n",
      "step: 96290 loss: 0.544663 time elapsed: 128.6433 learning rate: 0.000971, scenario: 0, slope: -0.00010017255452102034, fluctuations: 0.0\n",
      "step: 96300 loss: 0.543802 time elapsed: 128.6564 learning rate: 0.000971, scenario: 0, slope: -9.761028026136426e-05, fluctuations: 0.0\n",
      "step: 96310 loss: 0.542961 time elapsed: 128.6694 learning rate: 0.000971, scenario: 0, slope: -9.464983242058564e-05, fluctuations: 0.0\n",
      "step: 96320 loss: 0.542140 time elapsed: 128.6819 learning rate: 0.000971, scenario: 0, slope: -9.2110948828111e-05, fluctuations: 0.0\n",
      "step: 96330 loss: 0.541338 time elapsed: 128.6947 learning rate: 0.000971, scenario: 0, slope: -8.970732717782514e-05, fluctuations: 0.0\n",
      "step: 96340 loss: 0.540554 time elapsed: 128.7093 learning rate: 0.000971, scenario: 0, slope: -8.743018875440967e-05, fluctuations: 0.0\n",
      "step: 96350 loss: 0.539787 time elapsed: 128.7232 learning rate: 0.000971, scenario: 0, slope: -8.527146414011059e-05, fluctuations: 0.0\n",
      "step: 96360 loss: 0.539036 time elapsed: 128.7369 learning rate: 0.000971, scenario: 0, slope: -8.322372128354934e-05, fluctuations: 0.0\n",
      "step: 96370 loss: 0.538300 time elapsed: 128.7507 learning rate: 0.000971, scenario: 0, slope: -8.128010256470632e-05, fluctuations: 0.0\n",
      "step: 96380 loss: 0.537579 time elapsed: 128.7643 learning rate: 0.000971, scenario: 0, slope: -7.943426952597483e-05, fluctuations: 0.0\n",
      "step: 96390 loss: 0.536872 time elapsed: 128.7780 learning rate: 0.000971, scenario: 0, slope: -7.768035416249207e-05, fluctuations: 0.0\n",
      "step: 96400 loss: 0.536179 time elapsed: 128.7916 learning rate: 0.000971, scenario: 0, slope: -7.61759150676335e-05, fluctuations: 0.0\n",
      "step: 96410 loss: 0.535498 time elapsed: 128.8057 learning rate: 0.000971, scenario: 0, slope: -7.44269031133744e-05, fluctuations: 0.0\n",
      "step: 96420 loss: 0.534829 time elapsed: 128.8200 learning rate: 0.000971, scenario: 0, slope: -7.291761964042188e-05, fluctuations: 0.0\n",
      "step: 96430 loss: 0.534172 time elapsed: 128.8347 learning rate: 0.000971, scenario: 0, slope: -7.148069388930412e-05, fluctuations: 0.0\n",
      "step: 96440 loss: 0.533525 time elapsed: 128.8481 learning rate: 0.000971, scenario: 0, slope: -7.011205191790304e-05, fluctuations: 0.0\n",
      "step: 96450 loss: 0.532889 time elapsed: 128.8611 learning rate: 0.000971, scenario: 0, slope: -6.880789297503764e-05, fluctuations: 0.0\n",
      "step: 96460 loss: 0.532264 time elapsed: 128.8736 learning rate: 0.000971, scenario: 0, slope: -6.756466753173303e-05, fluctuations: 0.0\n",
      "step: 96470 loss: 0.531647 time elapsed: 128.8858 learning rate: 0.000971, scenario: 0, slope: -6.63790574559443e-05, fluctuations: 0.0\n",
      "step: 96480 loss: 0.531040 time elapsed: 128.8993 learning rate: 0.000971, scenario: 0, slope: -6.524795807880881e-05, fluctuations: 0.0\n",
      "step: 96490 loss: 0.530442 time elapsed: 128.9136 learning rate: 0.000971, scenario: 0, slope: -6.416846193380834e-05, fluctuations: 0.0\n",
      "step: 96500 loss: 0.529852 time elapsed: 128.9275 learning rate: 0.000971, scenario: 0, slope: -6.323878037267625e-05, fluctuations: 0.0\n",
      "step: 96510 loss: 0.529270 time elapsed: 128.9424 learning rate: 0.000971, scenario: 0, slope: -6.215354815580476e-05, fluctuations: 0.0\n",
      "step: 96520 loss: 0.528696 time elapsed: 128.9563 learning rate: 0.000971, scenario: 0, slope: -6.121317509038575e-05, fluctuations: 0.0\n",
      "step: 96530 loss: 0.528129 time elapsed: 128.9700 learning rate: 0.000971, scenario: 0, slope: -6.031447089947308e-05, fluctuations: 0.0\n",
      "step: 96540 loss: 0.527569 time elapsed: 128.9835 learning rate: 0.000971, scenario: 0, slope: -5.945531690663023e-05, fluctuations: 0.0\n",
      "step: 96550 loss: 0.527016 time elapsed: 128.9978 learning rate: 0.000971, scenario: 0, slope: -5.863372022433704e-05, fluctuations: 0.0\n",
      "step: 96560 loss: 0.526469 time elapsed: 129.0116 learning rate: 0.000971, scenario: 0, slope: -5.784780510153019e-05, fluctuations: 0.0\n",
      "step: 96570 loss: 0.525929 time elapsed: 129.0253 learning rate: 0.000971, scenario: 0, slope: -5.7095804964376894e-05, fluctuations: 0.0\n",
      "step: 96580 loss: 0.525394 time elapsed: 129.0392 learning rate: 0.000971, scenario: 0, slope: -5.6376055085580835e-05, fluctuations: 0.0\n",
      "step: 96590 loss: 0.524865 time elapsed: 129.0527 learning rate: 0.000971, scenario: 0, slope: -5.568698582288711e-05, fluctuations: 0.0\n",
      "step: 96600 loss: 0.524342 time elapsed: 129.0654 learning rate: 0.000971, scenario: 0, slope: -5.509182979041262e-05, fluctuations: 0.0\n",
      "step: 96610 loss: 0.523823 time elapsed: 129.0788 learning rate: 0.000971, scenario: 0, slope: -5.4395049015251076e-05, fluctuations: 0.0\n",
      "step: 96620 loss: 0.523310 time elapsed: 129.0921 learning rate: 0.000971, scenario: 0, slope: -5.3789463740376994e-05, fluctuations: 0.0\n",
      "step: 96630 loss: 0.522801 time elapsed: 129.1053 learning rate: 0.000971, scenario: 0, slope: -5.3209113333212476e-05, fluctuations: 0.0\n",
      "step: 96640 loss: 0.522297 time elapsed: 129.1193 learning rate: 0.001001, scenario: 1, slope: -5.265311684793268e-05, fluctuations: 0.0\n",
      "step: 96650 loss: 0.521764 time elapsed: 129.1336 learning rate: 0.001105, scenario: 1, slope: -5.220194098462018e-05, fluctuations: 0.0\n",
      "step: 96660 loss: 0.521181 time elapsed: 129.1473 learning rate: 0.001221, scenario: 1, slope: -5.2100794756356266e-05, fluctuations: 0.0\n",
      "step: 96670 loss: 0.520544 time elapsed: 129.1611 learning rate: 0.001349, scenario: 1, slope: -5.257826083542408e-05, fluctuations: 0.0\n",
      "step: 96680 loss: 0.519873 time elapsed: 129.1747 learning rate: 0.001349, scenario: 0, slope: -5.376985268067482e-05, fluctuations: 0.0\n",
      "step: 96690 loss: 0.519209 time elapsed: 129.1883 learning rate: 0.001349, scenario: 0, slope: -5.551986570423653e-05, fluctuations: 0.0\n",
      "step: 96700 loss: 0.518553 time elapsed: 129.2015 learning rate: 0.001349, scenario: 0, slope: -5.737206959053951e-05, fluctuations: 0.0\n",
      "step: 96710 loss: 0.517905 time elapsed: 129.2155 learning rate: 0.001349, scenario: 0, slope: -5.9732703996489607e-05, fluctuations: 0.0\n",
      "step: 96720 loss: 0.517264 time elapsed: 129.2292 learning rate: 0.001349, scenario: 0, slope: -6.172963240218577e-05, fluctuations: 0.0\n",
      "step: 96730 loss: 0.516629 time elapsed: 129.2425 learning rate: 0.001349, scenario: 0, slope: -6.335295505715959e-05, fluctuations: 0.0\n",
      "step: 96740 loss: 0.516001 time elapsed: 129.2555 learning rate: 0.001349, scenario: 0, slope: -6.438455867987156e-05, fluctuations: 0.0\n",
      "step: 96750 loss: 0.515379 time elapsed: 129.2681 learning rate: 0.001349, scenario: 0, slope: -6.470330759600428e-05, fluctuations: 0.0\n",
      "step: 96760 loss: 0.514763 time elapsed: 129.2808 learning rate: 0.001349, scenario: 0, slope: -6.443651447507618e-05, fluctuations: 0.0\n",
      "step: 96770 loss: 0.514153 time elapsed: 129.2933 learning rate: 0.001349, scenario: 0, slope: -6.382829676802091e-05, fluctuations: 0.0\n",
      "step: 96780 loss: 0.513547 time elapsed: 129.3061 learning rate: 0.001349, scenario: 0, slope: -6.316978621307922e-05, fluctuations: 0.0\n",
      "step: 96790 loss: 0.512947 time elapsed: 129.3205 learning rate: 0.001349, scenario: 0, slope: -6.254308111750554e-05, fluctuations: 0.0\n",
      "step: 96800 loss: 0.512351 time elapsed: 129.3345 learning rate: 0.001349, scenario: 0, slope: -6.200521293260884e-05, fluctuations: 0.0\n",
      "step: 96810 loss: 0.511760 time elapsed: 129.3490 learning rate: 0.001349, scenario: 0, slope: -6.137973412272657e-05, fluctuations: 0.0\n",
      "step: 96820 loss: 0.511173 time elapsed: 129.3627 learning rate: 0.001349, scenario: 0, slope: -6.083987008469218e-05, fluctuations: 0.0\n",
      "step: 96830 loss: 0.510590 time elapsed: 129.3761 learning rate: 0.001349, scenario: 0, slope: -6.0325767720571695e-05, fluctuations: 0.0\n",
      "step: 96840 loss: 0.510012 time elapsed: 129.3896 learning rate: 0.001349, scenario: 0, slope: -5.983594948108958e-05, fluctuations: 0.0\n",
      "step: 96850 loss: 0.509437 time elapsed: 129.4031 learning rate: 0.001349, scenario: 0, slope: -5.93690329710477e-05, fluctuations: 0.0\n",
      "step: 96860 loss: 0.508865 time elapsed: 129.4164 learning rate: 0.001349, scenario: 0, slope: -5.892372763151839e-05, fluctuations: 0.0\n",
      "step: 96870 loss: 0.508297 time elapsed: 129.4302 learning rate: 0.001349, scenario: 0, slope: -5.84988290578611e-05, fluctuations: 0.0\n",
      "step: 96880 loss: 0.507733 time elapsed: 129.4452 learning rate: 0.001349, scenario: 0, slope: -5.809321287845125e-05, fluctuations: 0.0\n",
      "step: 96890 loss: 0.507171 time elapsed: 129.4590 learning rate: 0.001349, scenario: 0, slope: -5.770582883176072e-05, fluctuations: 0.0\n",
      "step: 96900 loss: 0.506613 time elapsed: 129.4715 learning rate: 0.001349, scenario: 0, slope: -5.737195896648817e-05, fluctuations: 0.0\n",
      "step: 96910 loss: 0.506057 time elapsed: 129.4845 learning rate: 0.001349, scenario: 0, slope: -5.698189385914226e-05, fluctuations: 0.0\n",
      "step: 96920 loss: 0.505505 time elapsed: 129.4974 learning rate: 0.001349, scenario: 0, slope: -5.664356526184055e-05, fluctuations: 0.0\n",
      "step: 96930 loss: 0.504954 time elapsed: 129.5100 learning rate: 0.001349, scenario: 0, slope: -5.631990444075993e-05, fluctuations: 0.0\n",
      "step: 96940 loss: 0.504407 time elapsed: 129.5238 learning rate: 0.001349, scenario: 0, slope: -5.601015688434758e-05, fluctuations: 0.0\n",
      "step: 96950 loss: 0.503862 time elapsed: 129.5386 learning rate: 0.001349, scenario: 0, slope: -5.5713614923910694e-05, fluctuations: 0.0\n",
      "step: 96960 loss: 0.503319 time elapsed: 129.5532 learning rate: 0.001349, scenario: 0, slope: -5.542961438127653e-05, fluctuations: 0.0\n",
      "step: 96970 loss: 0.502778 time elapsed: 129.5676 learning rate: 0.001349, scenario: 0, slope: -5.5157531483002445e-05, fluctuations: 0.0\n",
      "step: 96980 loss: 0.502240 time elapsed: 129.5818 learning rate: 0.001349, scenario: 0, slope: -5.4896780017546014e-05, fluctuations: 0.0\n",
      "step: 96990 loss: 0.501704 time elapsed: 129.5955 learning rate: 0.001349, scenario: 0, slope: -5.464680871498493e-05, fluctuations: 0.0\n",
      "step: 97000 loss: 0.501169 time elapsed: 129.6093 learning rate: 0.001349, scenario: 0, slope: -5.443062220341748e-05, fluctuations: 0.0\n",
      "step: 97010 loss: 0.500637 time elapsed: 129.6234 learning rate: 0.001349, scenario: 0, slope: -5.4177161910818554e-05, fluctuations: 0.0\n",
      "step: 97020 loss: 0.500106 time elapsed: 129.6369 learning rate: 0.001349, scenario: 0, slope: -5.395653773852856e-05, fluctuations: 0.0\n",
      "step: 97030 loss: 0.499578 time elapsed: 129.6507 learning rate: 0.001349, scenario: 0, slope: -5.374479242284955e-05, fluctuations: 0.0\n",
      "step: 97040 loss: 0.499050 time elapsed: 129.6648 learning rate: 0.001349, scenario: 0, slope: -5.354151664106185e-05, fluctuations: 0.0\n",
      "step: 97050 loss: 0.498525 time elapsed: 129.6784 learning rate: 0.001349, scenario: 0, slope: -5.3346324008405596e-05, fluctuations: 0.0\n",
      "step: 97060 loss: 0.498001 time elapsed: 129.6916 learning rate: 0.001349, scenario: 0, slope: -5.31588495696772e-05, fluctuations: 0.0\n",
      "step: 97070 loss: 0.497478 time elapsed: 129.7047 learning rate: 0.001349, scenario: 0, slope: -5.297874840184003e-05, fluctuations: 0.0\n",
      "step: 97080 loss: 0.496957 time elapsed: 129.7177 learning rate: 0.001349, scenario: 0, slope: -5.280569431881072e-05, fluctuations: 0.0\n",
      "step: 97090 loss: 0.496438 time elapsed: 129.7312 learning rate: 0.001349, scenario: 0, slope: -5.263937867058588e-05, fluctuations: 0.0\n",
      "step: 97100 loss: 0.495920 time elapsed: 129.7450 learning rate: 0.001349, scenario: 0, slope: -5.249521409987323e-05, fluctuations: 0.0\n",
      "step: 97110 loss: 0.495403 time elapsed: 129.7596 learning rate: 0.001349, scenario: 0, slope: -5.232580915299796e-05, fluctuations: 0.0\n",
      "step: 97120 loss: 0.494887 time elapsed: 129.7730 learning rate: 0.001349, scenario: 0, slope: -5.2178016028855144e-05, fluctuations: 0.0\n",
      "step: 97130 loss: 0.494372 time elapsed: 129.7865 learning rate: 0.001349, scenario: 0, slope: -5.203588097682663e-05, fluctuations: 0.0\n",
      "step: 97140 loss: 0.493859 time elapsed: 129.7998 learning rate: 0.001349, scenario: 0, slope: -5.1899167821565224e-05, fluctuations: 0.0\n",
      "step: 97150 loss: 0.493347 time elapsed: 129.8134 learning rate: 0.001349, scenario: 0, slope: -5.1767652320268244e-05, fluctuations: 0.0\n",
      "step: 97160 loss: 0.492835 time elapsed: 129.8273 learning rate: 0.001349, scenario: 0, slope: -5.16411214445106e-05, fluctuations: 0.0\n",
      "step: 97170 loss: 0.492325 time elapsed: 129.8409 learning rate: 0.001349, scenario: 0, slope: -5.151937271154597e-05, fluctuations: 0.0\n",
      "step: 97180 loss: 0.491816 time elapsed: 129.8547 learning rate: 0.001349, scenario: 0, slope: -5.140221356124901e-05, fluctuations: 0.0\n",
      "step: 97190 loss: 0.491308 time elapsed: 129.8684 learning rate: 0.001349, scenario: 0, slope: -5.1289460775946833e-05, fluctuations: 0.0\n",
      "step: 97200 loss: 0.490800 time elapsed: 129.8811 learning rate: 0.001349, scenario: 0, slope: -5.119160638219011e-05, fluctuations: 0.0\n",
      "step: 97210 loss: 0.490294 time elapsed: 129.8946 learning rate: 0.001349, scenario: 0, slope: -5.107648492977078e-05, fluctuations: 0.0\n",
      "step: 97220 loss: 0.489788 time elapsed: 129.9077 learning rate: 0.001349, scenario: 0, slope: -5.097593745326578e-05, fluctuations: 0.0\n",
      "step: 97230 loss: 0.489283 time elapsed: 129.9211 learning rate: 0.001349, scenario: 0, slope: -5.0879146599497426e-05, fluctuations: 0.0\n",
      "step: 97240 loss: 0.488779 time elapsed: 129.9345 learning rate: 0.001349, scenario: 0, slope: -5.078596843293624e-05, fluctuations: 0.0\n",
      "step: 97250 loss: 0.488276 time elapsed: 129.9486 learning rate: 0.001349, scenario: 0, slope: -5.069626559376023e-05, fluctuations: 0.0\n",
      "step: 97260 loss: 0.487773 time elapsed: 129.9628 learning rate: 0.001349, scenario: 0, slope: -5.060988208182343e-05, fluctuations: 0.0\n",
      "step: 97270 loss: 0.487337 time elapsed: 129.9769 learning rate: 0.001349, scenario: 0, slope: -5.045802802378577e-05, fluctuations: 0.0\n",
      "step: 97280 loss: 1.142199 time elapsed: 129.9906 learning rate: 0.001369, scenario: -1, slope: 0.000527336650978925, fluctuations: 0.0\n",
      "step: 97290 loss: 30.431312 time elapsed: 130.0049 learning rate: 0.001238, scenario: -1, slope: 0.10524205470015932, fluctuations: 0.01\n",
      "step: 97300 loss: 5.140812 time elapsed: 130.0190 learning rate: 0.001131, scenario: -1, slope: 0.10800963005921396, fluctuations: 0.04\n",
      "step: 97310 loss: 1.197189 time elapsed: 130.0337 learning rate: 0.001023, scenario: -1, slope: 0.09751011901278898, fluctuations: 0.08\n",
      "step: 97320 loss: 0.976038 time elapsed: 130.0476 learning rate: 0.000925, scenario: -1, slope: 0.07597806492060771, fluctuations: 0.12\n",
      "step: 97330 loss: 0.501101 time elapsed: 130.0614 learning rate: 0.000837, scenario: -1, slope: 0.05123079659396948, fluctuations: 0.17\n",
      "step: 97340 loss: 0.502353 time elapsed: 130.0747 learning rate: 0.000757, scenario: -1, slope: 0.020412202362503025, fluctuations: 0.22\n",
      "step: 97350 loss: 0.499344 time elapsed: 130.0877 learning rate: 0.000719, scenario: 0, slope: -0.015062180526618821, fluctuations: 0.27\n",
      "step: 97360 loss: 0.491687 time elapsed: 130.1004 learning rate: 0.000719, scenario: 0, slope: -0.05503178316953308, fluctuations: 0.31\n",
      "step: 97370 loss: 0.488072 time elapsed: 130.1132 learning rate: 0.000719, scenario: 0, slope: -0.10016890504309423, fluctuations: 0.36\n",
      "step: 97380 loss: 0.487769 time elapsed: 130.1257 learning rate: 0.000719, scenario: 0, slope: -0.1552795136159428, fluctuations: 0.4\n",
      "step: 97390 loss: 0.486708 time elapsed: 130.1393 learning rate: 0.000719, scenario: 0, slope: -0.0542259673566315, fluctuations: 0.43\n",
      "step: 97400 loss: 0.486154 time elapsed: 130.1532 learning rate: 0.000719, scenario: 0, slope: -0.019584874606992233, fluctuations: 0.44\n",
      "step: 97410 loss: 0.485864 time elapsed: 130.1678 learning rate: 0.000719, scenario: 0, slope: -0.004524190803325351, fluctuations: 0.45\n",
      "step: 97420 loss: 0.485571 time elapsed: 130.1817 learning rate: 0.000719, scenario: 0, slope: -0.0007827820531055044, fluctuations: 0.41\n",
      "step: 97430 loss: 0.485285 time elapsed: 130.1955 learning rate: 0.000719, scenario: 0, slope: -0.0003706005163431153, fluctuations: 0.36\n",
      "step: 97440 loss: 0.485009 time elapsed: 130.2095 learning rate: 0.000719, scenario: 0, slope: -0.00016227071853630888, fluctuations: 0.31\n",
      "step: 97450 loss: 0.484738 time elapsed: 130.2231 learning rate: 0.000719, scenario: 0, slope: -8.168688701950078e-05, fluctuations: 0.26\n",
      "step: 97460 loss: 0.484468 time elapsed: 130.2368 learning rate: 0.000698, scenario: -1, slope: -4.305720768265698e-05, fluctuations: 0.22\n",
      "step: 97470 loss: 0.484217 time elapsed: 130.2507 learning rate: 0.000631, scenario: -1, slope: -3.32632658570341e-05, fluctuations: 0.17\n",
      "step: 97480 loss: 0.483991 time elapsed: 130.2645 learning rate: 0.000571, scenario: -1, slope: -2.8108003180170275e-05, fluctuations: 0.13\n",
      "step: 97490 loss: 0.483788 time elapsed: 130.2781 learning rate: 0.000516, scenario: -1, slope: -2.7153701694936008e-05, fluctuations: 0.08\n",
      "step: 97500 loss: 0.483604 time elapsed: 130.2909 learning rate: 0.000472, scenario: -1, slope: -2.594303924688012e-05, fluctuations: 0.04\n",
      "step: 97510 loss: 0.483434 time elapsed: 130.3041 learning rate: 0.000469, scenario: 1, slope: -2.4655920487325492e-05, fluctuations: 0.0\n",
      "step: 97520 loss: 0.483255 time elapsed: 130.3168 learning rate: 0.000518, scenario: 1, slope: -2.3213866437799138e-05, fluctuations: 0.0\n",
      "step: 97530 loss: 0.483056 time elapsed: 130.3293 learning rate: 0.000572, scenario: 1, slope: -2.1872925057874082e-05, fluctuations: 0.0\n",
      "step: 97540 loss: 0.482837 time elapsed: 130.3426 learning rate: 0.000632, scenario: 1, slope: -2.0789571457033827e-05, fluctuations: 0.0\n",
      "step: 97550 loss: 0.482595 time elapsed: 130.3570 learning rate: 0.000699, scenario: 1, slope: -2.0131999397260238e-05, fluctuations: 0.0\n",
      "step: 97560 loss: 0.482328 time elapsed: 130.3713 learning rate: 0.000772, scenario: 1, slope: -2.006046024730936e-05, fluctuations: 0.0\n",
      "step: 97570 loss: 0.482033 time elapsed: 130.3850 learning rate: 0.000852, scenario: 1, slope: -2.067561423776224e-05, fluctuations: 0.0\n",
      "step: 97580 loss: 0.481707 time elapsed: 130.3985 learning rate: 0.000942, scenario: 1, slope: -2.1951889669245796e-05, fluctuations: 0.0\n",
      "step: 97590 loss: 0.481347 time elapsed: 130.4122 learning rate: 0.001040, scenario: 1, slope: -2.3820091762174342e-05, fluctuations: 0.0\n",
      "step: 97600 loss: 0.480950 time elapsed: 130.4257 learning rate: 0.001137, scenario: 1, slope: -2.592464673873814e-05, fluctuations: 0.0\n",
      "step: 97610 loss: 0.480516 time elapsed: 130.4399 learning rate: 0.001256, scenario: 1, slope: -2.8889745700905997e-05, fluctuations: 0.0\n",
      "step: 97620 loss: 0.480038 time elapsed: 130.4537 learning rate: 0.001388, scenario: 1, slope: -3.186352718412208e-05, fluctuations: 0.0\n",
      "step: 97630 loss: 0.479511 time elapsed: 130.4670 learning rate: 0.001533, scenario: 1, slope: -3.5122447722776165e-05, fluctuations: 0.0\n",
      "step: 97640 loss: 0.479143 time elapsed: 130.4809 learning rate: 0.001694, scenario: 1, slope: -3.854708805475645e-05, fluctuations: 0.0\n",
      "step: 97650 loss: 187.806512 time elapsed: 130.4942 learning rate: 0.001602, scenario: -1, slope: 0.5913464217932742, fluctuations: 0.01\n",
      "step: 97660 loss: 34.251517 time elapsed: 130.5067 learning rate: 0.001449, scenario: -1, slope: 1.3758924215102322, fluctuations: 0.04\n",
      "step: 97670 loss: 43.906240 time elapsed: 130.5191 learning rate: 0.001310, scenario: -1, slope: 1.3771435733196151, fluctuations: 0.08\n",
      "step: 97680 loss: 8.690589 time elapsed: 130.5313 learning rate: 0.001185, scenario: -1, slope: 1.1426174109384226, fluctuations: 0.12\n",
      "step: 97690 loss: 3.950260 time elapsed: 130.5435 learning rate: 0.001072, scenario: -1, slope: 0.8105942874102393, fluctuations: 0.16\n",
      "step: 97700 loss: 1.780562 time elapsed: 130.5570 learning rate: 0.000979, scenario: -1, slope: 0.4339395811169132, fluctuations: 0.19\n",
      "step: 97710 loss: 1.097121 time elapsed: 130.5713 learning rate: 0.000904, scenario: 0, slope: -0.058197995368030074, fluctuations: 0.22\n",
      "step: 97720 loss: 0.652799 time elapsed: 130.5849 learning rate: 0.000904, scenario: 0, slope: -0.49305236037381794, fluctuations: 0.26\n",
      "step: 97730 loss: 0.575750 time elapsed: 130.5987 learning rate: 0.000904, scenario: 0, slope: -0.9948991217298567, fluctuations: 0.29\n",
      "step: 97740 loss: 0.574197 time elapsed: 130.6123 learning rate: 0.000904, scenario: 0, slope: -1.617726073979647, fluctuations: 0.32\n",
      "step: 97750 loss: 0.564341 time elapsed: 130.6260 learning rate: 0.000904, scenario: 0, slope: -1.7775241276131302, fluctuations: 0.33\n",
      "step: 97760 loss: 0.553430 time elapsed: 130.6396 learning rate: 0.000904, scenario: 0, slope: -0.45503794236652617, fluctuations: 0.34\n",
      "step: 97770 loss: 0.547874 time elapsed: 130.6531 learning rate: 0.000904, scenario: 0, slope: -0.1197351195742014, fluctuations: 0.33\n",
      "step: 97780 loss: 0.544713 time elapsed: 130.6669 learning rate: 0.000904, scenario: 0, slope: -0.046187669827512615, fluctuations: 0.32\n",
      "step: 97790 loss: 0.541845 time elapsed: 130.6814 learning rate: 0.000904, scenario: 0, slope: -0.011329109117688156, fluctuations: 0.29\n",
      "step: 97800 loss: 0.539224 time elapsed: 130.6945 learning rate: 0.000904, scenario: 0, slope: -0.004868075088422739, fluctuations: 0.26\n",
      "step: 97810 loss: 0.536918 time elapsed: 130.7075 learning rate: 0.000904, scenario: 0, slope: -0.0018357972778682, fluctuations: 0.22\n",
      "step: 97820 loss: 0.534827 time elapsed: 130.7201 learning rate: 0.000904, scenario: 0, slope: -0.0007754134431284202, fluctuations: 0.19\n",
      "step: 97830 loss: 0.532902 time elapsed: 130.7323 learning rate: 0.000904, scenario: 0, slope: -0.00046602302573887157, fluctuations: 0.16\n",
      "step: 97840 loss: 0.531128 time elapsed: 130.7443 learning rate: 0.000904, scenario: 0, slope: -0.00034355343945437265, fluctuations: 0.12\n",
      "step: 97850 loss: 0.529484 time elapsed: 130.7564 learning rate: 0.000904, scenario: 0, slope: -0.0002665071705824395, fluctuations: 0.09\n",
      "step: 97860 loss: 0.527956 time elapsed: 130.7700 learning rate: 0.000904, scenario: 0, slope: -0.0002286284396474053, fluctuations: 0.06\n",
      "step: 97870 loss: 0.526528 time elapsed: 130.7838 learning rate: 0.000904, scenario: 0, slope: -0.00020749608282647319, fluctuations: 0.02\n",
      "step: 97880 loss: 0.525192 time elapsed: 130.7980 learning rate: 0.000904, scenario: 0, slope: -0.0001899795229748574, fluctuations: 0.0\n",
      "step: 97890 loss: 0.523936 time elapsed: 130.8114 learning rate: 0.000904, scenario: 0, slope: -0.00017452359498383468, fluctuations: 0.0\n",
      "step: 97900 loss: 0.522754 time elapsed: 130.8248 learning rate: 0.000904, scenario: 0, slope: -0.00016270623155110689, fluctuations: 0.0\n",
      "step: 97910 loss: 0.521637 time elapsed: 130.8393 learning rate: 0.000904, scenario: 0, slope: -0.00015015826278248027, fluctuations: 0.0\n",
      "step: 97920 loss: 0.520580 time elapsed: 130.8531 learning rate: 0.000904, scenario: 0, slope: -0.00014016625167584873, fluctuations: 0.0\n",
      "step: 97930 loss: 0.519576 time elapsed: 130.8666 learning rate: 0.000904, scenario: 0, slope: -0.00013128726259633663, fluctuations: 0.0\n",
      "step: 97940 loss: 0.518622 time elapsed: 130.8800 learning rate: 0.000904, scenario: 0, slope: -0.00012334961627506326, fluctuations: 0.0\n",
      "step: 97950 loss: 0.517712 time elapsed: 130.8940 learning rate: 0.000904, scenario: 0, slope: -0.00011622127791162021, fluctuations: 0.0\n",
      "step: 97960 loss: 0.516844 time elapsed: 130.9087 learning rate: 0.000904, scenario: 0, slope: -0.00010979633595865948, fluctuations: 0.0\n",
      "step: 97970 loss: 0.516013 time elapsed: 130.9218 learning rate: 0.000904, scenario: 0, slope: -0.00010398660700112462, fluctuations: 0.0\n",
      "step: 97980 loss: 0.515216 time elapsed: 130.9344 learning rate: 0.000904, scenario: 0, slope: -9.871792046816417e-05, fluctuations: 0.0\n",
      "step: 97990 loss: 0.514450 time elapsed: 130.9468 learning rate: 0.000904, scenario: 0, slope: -9.392724988965291e-05, fluctuations: 0.0\n",
      "step: 98000 loss: 0.513714 time elapsed: 130.9599 learning rate: 0.000904, scenario: 0, slope: -8.997952448827424e-05, fluctuations: 0.0\n",
      "step: 98010 loss: 0.513005 time elapsed: 130.9728 learning rate: 0.000904, scenario: 0, slope: -8.557122375379412e-05, fluctuations: 0.0\n",
      "step: 98020 loss: 0.512320 time elapsed: 130.9864 learning rate: 0.000904, scenario: 0, slope: -8.191884951117934e-05, fluctuations: 0.0\n",
      "step: 98030 loss: 0.511658 time elapsed: 131.0008 learning rate: 0.000904, scenario: 0, slope: -7.856822947413672e-05, fluctuations: 0.0\n",
      "step: 98040 loss: 0.511018 time elapsed: 131.0147 learning rate: 0.000904, scenario: 0, slope: -7.548857872935932e-05, fluctuations: 0.0\n",
      "step: 98050 loss: 0.510397 time elapsed: 131.0283 learning rate: 0.000904, scenario: 0, slope: -7.265287686449659e-05, fluctuations: 0.0\n",
      "step: 98060 loss: 0.509794 time elapsed: 131.0419 learning rate: 0.000904, scenario: 0, slope: -7.003732985263154e-05, fluctuations: 0.0\n",
      "step: 98070 loss: 0.509209 time elapsed: 131.0554 learning rate: 0.000904, scenario: 0, slope: -6.762092144749211e-05, fluctuations: 0.0\n",
      "step: 98080 loss: 0.508639 time elapsed: 131.0688 learning rate: 0.000904, scenario: 0, slope: -6.538503665052501e-05, fluctuations: 0.0\n",
      "step: 98090 loss: 0.508084 time elapsed: 131.0825 learning rate: 0.000904, scenario: 0, slope: -6.331314377821298e-05, fluctuations: 0.0\n",
      "step: 98100 loss: 0.507543 time elapsed: 131.0959 learning rate: 0.000904, scenario: 0, slope: -6.157645497361456e-05, fluctuations: 0.0\n",
      "step: 98110 loss: 0.507015 time elapsed: 131.1104 learning rate: 0.000904, scenario: 0, slope: -5.960404390444216e-05, fluctuations: 0.0\n",
      "step: 98120 loss: 0.506499 time elapsed: 131.1227 learning rate: 0.000904, scenario: 0, slope: -5.794195286874223e-05, fluctuations: 0.0\n",
      "step: 98130 loss: 0.505994 time elapsed: 131.1349 learning rate: 0.000904, scenario: 0, slope: -5.639371914435057e-05, fluctuations: 0.0\n",
      "step: 98140 loss: 0.505500 time elapsed: 131.1469 learning rate: 0.000904, scenario: 0, slope: -5.4949881017413685e-05, fluctuations: 0.0\n",
      "step: 98150 loss: 0.505016 time elapsed: 131.1587 learning rate: 0.000904, scenario: 0, slope: -5.3601921024478395e-05, fluctuations: 0.0\n",
      "step: 98160 loss: 0.504541 time elapsed: 131.1709 learning rate: 0.000904, scenario: 0, slope: -5.2342156404980955e-05, fluctuations: 0.0\n",
      "step: 98170 loss: 0.504074 time elapsed: 131.1848 learning rate: 0.000904, scenario: 0, slope: -5.1163643875932956e-05, fluctuations: 0.0\n",
      "step: 98180 loss: 0.503607 time elapsed: 131.1987 learning rate: 0.000978, scenario: 1, slope: -5.007504969278654e-05, fluctuations: 0.0\n",
      "step: 98190 loss: 0.503102 time elapsed: 131.2125 learning rate: 0.001081, scenario: 1, slope: -4.92350843895747e-05, fluctuations: 0.0\n",
      "step: 98200 loss: 0.502556 time elapsed: 131.2258 learning rate: 0.001182, scenario: 1, slope: -4.88714123665836e-05, fluctuations: 0.0\n",
      "step: 98210 loss: 0.501969 time elapsed: 131.2401 learning rate: 0.001306, scenario: 1, slope: -4.9107526914008826e-05, fluctuations: 0.0\n",
      "step: 98220 loss: 0.501336 time elapsed: 131.2535 learning rate: 0.001442, scenario: 1, slope: -5.0086750832933047e-05, fluctuations: 0.0\n",
      "step: 98230 loss: 0.500663 time elapsed: 131.2672 learning rate: 0.001486, scenario: 0, slope: -5.1863218300996645e-05, fluctuations: 0.0\n",
      "step: 98240 loss: 0.499999 time elapsed: 131.2807 learning rate: 0.001486, scenario: 0, slope: -5.4248088704915076e-05, fluctuations: 0.0\n",
      "step: 98250 loss: 0.499350 time elapsed: 131.2944 learning rate: 0.001486, scenario: 0, slope: -5.688905724111511e-05, fluctuations: 0.0\n",
      "step: 98260 loss: 0.498715 time elapsed: 131.3080 learning rate: 0.001486, scenario: 0, slope: -5.944590394276593e-05, fluctuations: 0.0\n",
      "step: 98270 loss: 0.498092 time elapsed: 131.3213 learning rate: 0.001486, scenario: 0, slope: -6.159386770578952e-05, fluctuations: 0.0\n",
      "step: 98280 loss: 0.497481 time elapsed: 131.3339 learning rate: 0.001486, scenario: 0, slope: -6.303837744893727e-05, fluctuations: 0.0\n",
      "step: 98290 loss: 0.496880 time elapsed: 131.3467 learning rate: 0.001486, scenario: 0, slope: -6.368148110847548e-05, fluctuations: 0.0\n",
      "step: 98300 loss: 0.496289 time elapsed: 131.3588 learning rate: 0.001486, scenario: 0, slope: -6.363249566606005e-05, fluctuations: 0.0\n",
      "step: 98310 loss: 0.495708 time elapsed: 131.3713 learning rate: 0.001486, scenario: 0, slope: -6.29178105014566e-05, fluctuations: 0.0\n",
      "step: 98320 loss: 0.495134 time elapsed: 131.3832 learning rate: 0.001486, scenario: 0, slope: -6.186618757532423e-05, fluctuations: 0.0\n",
      "step: 98330 loss: 0.494569 time elapsed: 131.3964 learning rate: 0.001486, scenario: 0, slope: -6.07455332548507e-05, fluctuations: 0.0\n",
      "step: 98340 loss: 0.494010 time elapsed: 131.4104 learning rate: 0.001486, scenario: 0, slope: -5.970860332448406e-05, fluctuations: 0.0\n",
      "step: 98350 loss: 0.493458 time elapsed: 131.4241 learning rate: 0.001486, scenario: 0, slope: -5.875224616682688e-05, fluctuations: 0.0\n",
      "step: 98360 loss: 0.492913 time elapsed: 131.4378 learning rate: 0.001486, scenario: 0, slope: -5.7869152024853146e-05, fluctuations: 0.0\n",
      "step: 98370 loss: 0.492373 time elapsed: 131.4516 learning rate: 0.001486, scenario: 0, slope: -5.7052506770406016e-05, fluctuations: 0.0\n",
      "step: 98380 loss: 0.491838 time elapsed: 131.4649 learning rate: 0.001486, scenario: 0, slope: -5.6296156432398405e-05, fluctuations: 0.0\n",
      "step: 98390 loss: 0.491309 time elapsed: 131.4785 learning rate: 0.001486, scenario: 0, slope: -5.5594600240147235e-05, fluctuations: 0.0\n",
      "step: 98400 loss: 0.490784 time elapsed: 131.4918 learning rate: 0.001486, scenario: 0, slope: -5.5005983512289404e-05, fluctuations: 0.0\n",
      "step: 98410 loss: 0.490264 time elapsed: 131.5056 learning rate: 0.001486, scenario: 0, slope: -5.433677950678113e-05, fluctuations: 0.0\n",
      "step: 98420 loss: 0.489748 time elapsed: 131.5193 learning rate: 0.001486, scenario: 0, slope: -5.3772228365087284e-05, fluctuations: 0.0\n",
      "step: 98430 loss: 0.489236 time elapsed: 131.5348 learning rate: 0.001486, scenario: 0, slope: -5.3245778027711636e-05, fluctuations: 0.0\n",
      "step: 98440 loss: 0.488727 time elapsed: 131.5476 learning rate: 0.001486, scenario: 0, slope: -5.275428602088539e-05, fluctuations: 0.0\n",
      "step: 98450 loss: 0.488222 time elapsed: 131.5597 learning rate: 0.001486, scenario: 0, slope: -5.2294926332491404e-05, fluctuations: 0.0\n",
      "step: 98460 loss: 0.487720 time elapsed: 131.5721 learning rate: 0.001486, scenario: 0, slope: -5.186515225890494e-05, fluctuations: 0.0\n",
      "step: 98470 loss: 0.487220 time elapsed: 131.5845 learning rate: 0.001486, scenario: 0, slope: -5.146266417111755e-05, fluctuations: 0.0\n",
      "step: 98480 loss: 0.486724 time elapsed: 131.5965 learning rate: 0.001486, scenario: 0, slope: -5.108538149115545e-05, fluctuations: 0.0\n",
      "step: 98490 loss: 0.486231 time elapsed: 131.6104 learning rate: 0.001486, scenario: 0, slope: -5.073141827035759e-05, fluctuations: 0.0\n",
      "step: 98500 loss: 0.485740 time elapsed: 131.6238 learning rate: 0.001486, scenario: 0, slope: -5.043137072882808e-05, fluctuations: 0.0\n",
      "step: 98510 loss: 0.485251 time elapsed: 131.6383 learning rate: 0.001486, scenario: 0, slope: -5.008675420836108e-05, fluctuations: 0.0\n",
      "step: 98520 loss: 0.484764 time elapsed: 131.6521 learning rate: 0.001486, scenario: 0, slope: -4.979307550906303e-05, fluctuations: 0.0\n",
      "step: 98530 loss: 0.484280 time elapsed: 131.6657 learning rate: 0.001486, scenario: 0, slope: -4.9516729735128307e-05, fluctuations: 0.0\n",
      "step: 98540 loss: 0.483798 time elapsed: 131.6790 learning rate: 0.001486, scenario: 0, slope: -4.925653194834125e-05, fluctuations: 0.0\n",
      "step: 98550 loss: 0.483317 time elapsed: 131.6926 learning rate: 0.001486, scenario: 0, slope: -4.901139706550827e-05, fluctuations: 0.0\n",
      "step: 98560 loss: 0.482838 time elapsed: 131.7062 learning rate: 0.001486, scenario: 0, slope: -4.878032992069832e-05, fluctuations: 0.0\n",
      "step: 98570 loss: 0.482344 time elapsed: 131.7197 learning rate: 0.001641, scenario: 1, slope: -4.8595761521404676e-05, fluctuations: 0.0\n",
      "step: 98580 loss: 0.481800 time elapsed: 131.7339 learning rate: 0.001813, scenario: 1, slope: -4.866390785090304e-05, fluctuations: 0.0\n",
      "step: 98590 loss: 0.481223 time elapsed: 131.7469 learning rate: 0.001813, scenario: 0, slope: -4.918010867837404e-05, fluctuations: 0.0\n",
      "step: 98600 loss: 0.483366 time elapsed: 131.7600 learning rate: 0.001813, scenario: 0, slope: -4.9567361737135195e-05, fluctuations: 0.0\n",
      "step: 98610 loss: 58.985683 time elapsed: 131.7729 learning rate: 0.001715, scenario: -1, slope: 0.5414887430449171, fluctuations: 0.01\n",
      "step: 98620 loss: 86.576029 time elapsed: 131.7852 learning rate: 0.001551, scenario: -1, slope: 0.8913237557037247, fluctuations: 0.05\n",
      "step: 98630 loss: 21.802762 time elapsed: 131.7977 learning rate: 0.001403, scenario: -1, slope: 0.9566562150163067, fluctuations: 0.09\n",
      "step: 98640 loss: 15.096679 time elapsed: 131.8102 learning rate: 0.001269, scenario: -1, slope: 0.8035693304391401, fluctuations: 0.13\n",
      "step: 98650 loss: 4.175763 time elapsed: 131.8250 learning rate: 0.001148, scenario: -1, slope: 0.5914295910419892, fluctuations: 0.18\n",
      "step: 98660 loss: 1.785944 time elapsed: 131.8394 learning rate: 0.001038, scenario: -1, slope: 0.2980496728701538, fluctuations: 0.22\n",
      "step: 98670 loss: 0.992843 time elapsed: 131.8534 learning rate: 0.000948, scenario: 0, slope: -0.030124907550071835, fluctuations: 0.25\n",
      "step: 98680 loss: 0.668589 time elapsed: 131.8671 learning rate: 0.000948, scenario: 0, slope: -0.34351484517939207, fluctuations: 0.29\n",
      "step: 98690 loss: 0.597222 time elapsed: 131.8807 learning rate: 0.000948, scenario: 0, slope: -0.7004901358795587, fluctuations: 0.32\n",
      "step: 98700 loss: 0.551991 time elapsed: 131.8940 learning rate: 0.000948, scenario: 0, slope: -1.1016840943045327, fluctuations: 0.35\n",
      "step: 98710 loss: 0.551867 time elapsed: 131.9083 learning rate: 0.000948, scenario: 0, slope: -0.8286239755298794, fluctuations: 0.39\n",
      "step: 98720 loss: 0.544024 time elapsed: 131.9220 learning rate: 0.000948, scenario: 0, slope: -0.30714135827956096, fluctuations: 0.38\n",
      "step: 98730 loss: 0.538997 time elapsed: 131.9355 learning rate: 0.000948, scenario: 0, slope: -0.09180901652270683, fluctuations: 0.36\n",
      "step: 98740 loss: 0.536171 time elapsed: 131.9501 learning rate: 0.000948, scenario: 0, slope: -0.018980580024230056, fluctuations: 0.32\n",
      "step: 98750 loss: 0.533743 time elapsed: 131.9647 learning rate: 0.000948, scenario: 0, slope: -0.005967705269421715, fluctuations: 0.28\n",
      "step: 98760 loss: 0.531784 time elapsed: 131.9776 learning rate: 0.000948, scenario: 0, slope: -0.0027445799359428744, fluctuations: 0.24\n",
      "step: 98770 loss: 0.530055 time elapsed: 131.9911 learning rate: 0.000948, scenario: 0, slope: -0.0014581052084085624, fluctuations: 0.2\n",
      "step: 98780 loss: 0.528492 time elapsed: 132.0057 learning rate: 0.000948, scenario: 0, slope: -0.0006547122709590868, fluctuations: 0.17\n",
      "step: 98790 loss: 0.527080 time elapsed: 132.0186 learning rate: 0.000948, scenario: 0, slope: -0.0003695730327574132, fluctuations: 0.13\n",
      "step: 98800 loss: 0.525788 time elapsed: 132.0318 learning rate: 0.000948, scenario: 0, slope: -0.00025536790706805825, fluctuations: 0.1\n",
      "step: 98810 loss: 0.524595 time elapsed: 132.0463 learning rate: 0.000948, scenario: 0, slope: -0.0002021381326588525, fluctuations: 0.06\n",
      "step: 98820 loss: 0.523484 time elapsed: 132.0599 learning rate: 0.000948, scenario: 0, slope: -0.00017716128519995887, fluctuations: 0.02\n",
      "step: 98830 loss: 0.522446 time elapsed: 132.0735 learning rate: 0.000948, scenario: 0, slope: -0.00015709322285254024, fluctuations: 0.0\n",
      "step: 98840 loss: 0.521469 time elapsed: 132.0870 learning rate: 0.000948, scenario: 0, slope: -0.00014076306482850473, fluctuations: 0.0\n",
      "step: 98850 loss: 0.520547 time elapsed: 132.1005 learning rate: 0.000948, scenario: 0, slope: -0.00012824212139383653, fluctuations: 0.0\n",
      "step: 98860 loss: 0.519673 time elapsed: 132.1140 learning rate: 0.000948, scenario: 0, slope: -0.00011810182513800897, fluctuations: 0.0\n",
      "step: 98870 loss: 0.518842 time elapsed: 132.1272 learning rate: 0.000948, scenario: 0, slope: -0.00010967288203958607, fluctuations: 0.0\n",
      "step: 98880 loss: 0.518048 time elapsed: 132.1408 learning rate: 0.000948, scenario: 0, slope: -0.00010251794346721141, fluctuations: 0.0\n",
      "step: 98890 loss: 0.517289 time elapsed: 132.1542 learning rate: 0.000948, scenario: 0, slope: -9.634934412599339e-05, fluctuations: 0.0\n",
      "step: 98900 loss: 0.516561 time elapsed: 132.1688 learning rate: 0.000948, scenario: 0, slope: -9.147794322732389e-05, fluctuations: 0.0\n",
      "step: 98910 loss: 0.515861 time elapsed: 132.1824 learning rate: 0.000948, scenario: 0, slope: -8.623816019450062e-05, fluctuations: 0.0\n",
      "step: 98920 loss: 0.515187 time elapsed: 132.1949 learning rate: 0.000948, scenario: 0, slope: -8.204022501071993e-05, fluctuations: 0.0\n",
      "step: 98930 loss: 0.514536 time elapsed: 132.2073 learning rate: 0.000948, scenario: 0, slope: -7.829208502977231e-05, fluctuations: 0.0\n",
      "step: 98940 loss: 0.513907 time elapsed: 132.2194 learning rate: 0.000948, scenario: 0, slope: -7.492642127286579e-05, fluctuations: 0.0\n",
      "step: 98950 loss: 0.513297 time elapsed: 132.2318 learning rate: 0.000948, scenario: 0, slope: -7.18892434184094e-05, fluctuations: 0.0\n",
      "step: 98960 loss: 0.512705 time elapsed: 132.2458 learning rate: 0.000948, scenario: 0, slope: -6.91366794257128e-05, fluctuations: 0.0\n",
      "step: 98970 loss: 0.512130 time elapsed: 132.2595 learning rate: 0.000948, scenario: 0, slope: -6.663264052392395e-05, fluctuations: 0.0\n",
      "step: 98980 loss: 0.511571 time elapsed: 132.2734 learning rate: 0.000948, scenario: 0, slope: -6.434710899430773e-05, fluctuations: 0.0\n",
      "step: 98990 loss: 0.511025 time elapsed: 132.2870 learning rate: 0.000948, scenario: 0, slope: -6.225486576598839e-05, fluctuations: 0.0\n",
      "step: 99000 loss: 0.510493 time elapsed: 132.3006 learning rate: 0.000948, scenario: 0, slope: -6.051936995858154e-05, fluctuations: 0.0\n",
      "step: 99010 loss: 0.509973 time elapsed: 132.3144 learning rate: 0.000948, scenario: 0, slope: -5.85678232415307e-05, fluctuations: 0.0\n",
      "step: 99020 loss: 0.509465 time elapsed: 132.3276 learning rate: 0.000948, scenario: 0, slope: -5.693900380425826e-05, fluctuations: 0.0\n",
      "step: 99030 loss: 0.508967 time elapsed: 132.3412 learning rate: 0.000948, scenario: 0, slope: -5.5434420170346336e-05, fluctuations: 0.0\n",
      "step: 99040 loss: 0.508479 time elapsed: 132.3550 learning rate: 0.000948, scenario: 0, slope: -5.404216139480647e-05, fluctuations: 0.0\n",
      "step: 99050 loss: 0.508000 time elapsed: 132.3689 learning rate: 0.000948, scenario: 0, slope: -5.2751776941779416e-05, fluctuations: 0.0\n",
      "step: 99060 loss: 0.507530 time elapsed: 132.3818 learning rate: 0.000948, scenario: 0, slope: -5.155405125480593e-05, fluctuations: 0.0\n",
      "step: 99070 loss: 0.507058 time elapsed: 132.3941 learning rate: 0.001027, scenario: 1, slope: -5.04559100988323e-05, fluctuations: 0.0\n",
      "step: 99080 loss: 0.506549 time elapsed: 132.4061 learning rate: 0.001134, scenario: 1, slope: -4.9616105286183764e-05, fluctuations: 0.0\n",
      "step: 99090 loss: 0.505997 time elapsed: 132.4187 learning rate: 0.001253, scenario: 1, slope: -4.925226431215614e-05, fluctuations: 0.0\n",
      "step: 99100 loss: 0.505398 time elapsed: 132.4309 learning rate: 0.001370, scenario: 1, slope: -4.947632556631286e-05, fluctuations: 0.0\n",
      "step: 99110 loss: 0.504757 time elapsed: 132.4435 learning rate: 0.001513, scenario: 1, slope: -5.058393703767105e-05, fluctuations: 0.0\n",
      "step: 99120 loss: 0.504078 time elapsed: 132.4569 learning rate: 0.001544, scenario: 0, slope: -5.242785280577211e-05, fluctuations: 0.0\n",
      "step: 99130 loss: 0.503411 time elapsed: 132.4707 learning rate: 0.001544, scenario: 0, slope: -5.485677585367508e-05, fluctuations: 0.0\n",
      "step: 99140 loss: 0.502757 time elapsed: 132.4841 learning rate: 0.001544, scenario: 0, slope: -5.7522648478918014e-05, fluctuations: 0.0\n",
      "step: 99150 loss: 0.502116 time elapsed: 132.4974 learning rate: 0.001544, scenario: 0, slope: -6.0091066297858016e-05, fluctuations: 0.0\n",
      "step: 99160 loss: 0.501486 time elapsed: 132.5110 learning rate: 0.001544, scenario: 0, slope: -6.22420157624609e-05, fluctuations: 0.0\n",
      "step: 99170 loss: 0.500867 time elapsed: 132.5246 learning rate: 0.001544, scenario: 0, slope: -6.368472365212255e-05, fluctuations: 0.0\n",
      "step: 99180 loss: 0.500257 time elapsed: 132.5380 learning rate: 0.001544, scenario: 0, slope: -6.432575294890322e-05, fluctuations: 0.0\n",
      "step: 99190 loss: 0.499656 time elapsed: 132.5514 learning rate: 0.001544, scenario: 0, slope: -6.424220071176432e-05, fluctuations: 0.0\n",
      "step: 99200 loss: 0.499063 time elapsed: 132.5648 learning rate: 0.001544, scenario: 0, slope: -6.367994867023105e-05, fluctuations: 0.0\n",
      "step: 99210 loss: 0.498478 time elapsed: 132.5788 learning rate: 0.001544, scenario: 0, slope: -6.26196760212586e-05, fluctuations: 0.0\n",
      "step: 99220 loss: 0.497900 time elapsed: 132.5925 learning rate: 0.001544, scenario: 0, slope: -6.159855146312038e-05, fluctuations: 0.0\n",
      "step: 99230 loss: 0.497328 time elapsed: 132.6052 learning rate: 0.001544, scenario: 0, slope: -6.065964485728503e-05, fluctuations: 0.0\n",
      "step: 99240 loss: 0.496762 time elapsed: 132.6177 learning rate: 0.001544, scenario: 0, slope: -5.9797094426438996e-05, fluctuations: 0.0\n",
      "step: 99250 loss: 0.496201 time elapsed: 132.6300 learning rate: 0.001544, scenario: 0, slope: -5.9003500530956096e-05, fluctuations: 0.0\n",
      "step: 99260 loss: 0.495646 time elapsed: 132.6424 learning rate: 0.001544, scenario: 0, slope: -5.827207386189995e-05, fluctuations: 0.0\n",
      "step: 99270 loss: 0.495095 time elapsed: 132.6544 learning rate: 0.001544, scenario: 0, slope: -5.759674201695354e-05, fluctuations: 0.0\n",
      "step: 99280 loss: 0.494549 time elapsed: 132.6680 learning rate: 0.001544, scenario: 0, slope: -5.697211668281924e-05, fluctuations: 0.0\n",
      "step: 99290 loss: 0.494007 time elapsed: 132.6818 learning rate: 0.001544, scenario: 0, slope: -5.639342307851087e-05, fluctuations: 0.0\n",
      "step: 99300 loss: 0.493469 time elapsed: 132.6954 learning rate: 0.001544, scenario: 0, slope: -5.590835950637497e-05, fluctuations: 0.0\n",
      "step: 99310 loss: 0.492934 time elapsed: 132.7096 learning rate: 0.001544, scenario: 0, slope: -5.535735798928684e-05, fluctuations: 0.0\n",
      "step: 99320 loss: 0.492403 time elapsed: 132.7232 learning rate: 0.001544, scenario: 0, slope: -5.48928660126552e-05, fluctuations: 0.0\n",
      "step: 99330 loss: 0.491875 time elapsed: 132.7366 learning rate: 0.001544, scenario: 0, slope: -5.4459952936544305e-05, fluctuations: 0.0\n",
      "step: 99340 loss: 0.491350 time elapsed: 132.7498 learning rate: 0.001544, scenario: 0, slope: -5.405593634278589e-05, fluctuations: 0.0\n",
      "step: 99350 loss: 0.490828 time elapsed: 132.7635 learning rate: 0.001544, scenario: 0, slope: -5.367840956841018e-05, fluctuations: 0.0\n",
      "step: 99360 loss: 0.490308 time elapsed: 132.7771 learning rate: 0.001544, scenario: 0, slope: -5.332520872600576e-05, fluctuations: 0.0\n",
      "step: 99370 loss: 0.489791 time elapsed: 132.7904 learning rate: 0.001544, scenario: 0, slope: -5.299438422494139e-05, fluctuations: 0.0\n",
      "step: 99380 loss: 0.489276 time elapsed: 132.8043 learning rate: 0.001544, scenario: 0, slope: -5.2684176110265855e-05, fluctuations: 0.0\n",
      "step: 99390 loss: 0.488763 time elapsed: 132.8175 learning rate: 0.001544, scenario: 0, slope: -5.23929926468485e-05, fluctuations: 0.0\n",
      "step: 99400 loss: 0.488253 time elapsed: 132.8298 learning rate: 0.001544, scenario: 0, slope: -5.2145998834652246e-05, fluctuations: 0.0\n",
      "step: 99410 loss: 0.487744 time elapsed: 132.8428 learning rate: 0.001544, scenario: 0, slope: -5.1862064291016845e-05, fluctuations: 0.0\n",
      "step: 99420 loss: 0.487237 time elapsed: 132.8553 learning rate: 0.001544, scenario: 0, slope: -5.161982064776167e-05, fluctuations: 0.0\n",
      "step: 99430 loss: 0.486732 time elapsed: 132.8677 learning rate: 0.001544, scenario: 0, slope: -5.139157736791264e-05, fluctuations: 0.0\n",
      "step: 99440 loss: 0.486229 time elapsed: 132.8812 learning rate: 0.001544, scenario: 0, slope: -5.117634655237503e-05, fluctuations: 0.0\n",
      "step: 99450 loss: 0.485727 time elapsed: 132.8950 learning rate: 0.001544, scenario: 0, slope: -5.097322604636284e-05, fluctuations: 0.0\n",
      "step: 99460 loss: 0.485227 time elapsed: 132.9086 learning rate: 0.001544, scenario: 0, slope: -5.0781390835024663e-05, fluctuations: 0.0\n",
      "step: 99470 loss: 0.484728 time elapsed: 132.9224 learning rate: 0.001544, scenario: 0, slope: -5.060008541552535e-05, fluctuations: 0.0\n",
      "step: 99480 loss: 0.484231 time elapsed: 132.9359 learning rate: 0.001544, scenario: 0, slope: -5.042861701960507e-05, fluctuations: 0.0\n",
      "step: 99490 loss: 0.483734 time elapsed: 132.9493 learning rate: 0.001544, scenario: 0, slope: -5.0266349580071734e-05, fluctuations: 0.0\n",
      "step: 99500 loss: 0.483239 time elapsed: 132.9626 learning rate: 0.001544, scenario: 0, slope: -5.012769142648967e-05, fluctuations: 0.0\n",
      "step: 99510 loss: 0.482745 time elapsed: 132.9771 learning rate: 0.001544, scenario: 0, slope: -4.9967125086625056e-05, fluctuations: 0.0\n",
      "step: 99520 loss: 0.482253 time elapsed: 132.9906 learning rate: 0.001544, scenario: 0, slope: -4.9829133758675014e-05, fluctuations: 0.0\n",
      "step: 99530 loss: 0.481761 time elapsed: 133.0041 learning rate: 0.001544, scenario: 0, slope: -4.969826667296279e-05, fluctuations: 0.0\n",
      "step: 99540 loss: 0.481270 time elapsed: 133.0170 learning rate: 0.001544, scenario: 0, slope: -4.9574101014167025e-05, fluctuations: 0.0\n",
      "step: 99550 loss: 0.480780 time elapsed: 133.0293 learning rate: 0.001544, scenario: 0, slope: -4.9456245725735375e-05, fluctuations: 0.0\n",
      "step: 99560 loss: 0.480291 time elapsed: 133.0417 learning rate: 0.001544, scenario: 0, slope: -4.9344338702762005e-05, fluctuations: 0.0\n",
      "step: 99570 loss: 0.479803 time elapsed: 133.0541 learning rate: 0.001544, scenario: 0, slope: -4.923804426092347e-05, fluctuations: 0.0\n",
      "step: 99580 loss: 0.479316 time elapsed: 133.0663 learning rate: 0.001544, scenario: 0, slope: -4.913705085148958e-05, fluctuations: 0.0\n",
      "step: 99590 loss: 0.478829 time elapsed: 133.0784 learning rate: 0.001544, scenario: 0, slope: -4.9041068996934136e-05, fluctuations: 0.0\n",
      "step: 99600 loss: 0.478343 time elapsed: 133.0913 learning rate: 0.001544, scenario: 0, slope: -4.895874726236931e-05, fluctuations: 0.0\n",
      "step: 99610 loss: 0.477858 time elapsed: 133.1058 learning rate: 0.001544, scenario: 0, slope: -4.886308136773644e-05, fluctuations: 0.0\n",
      "step: 99620 loss: 0.477373 time elapsed: 133.1195 learning rate: 0.001544, scenario: 0, slope: -4.878059104840093e-05, fluctuations: 0.0\n",
      "step: 99630 loss: 0.476890 time elapsed: 133.1332 learning rate: 0.001544, scenario: 0, slope: -4.8702140271189204e-05, fluctuations: 0.0\n",
      "step: 99640 loss: 0.476406 time elapsed: 133.1468 learning rate: 0.001544, scenario: 0, slope: -4.862752516870377e-05, fluctuations: 0.0\n",
      "step: 99650 loss: 0.475924 time elapsed: 133.1601 learning rate: 0.001544, scenario: 0, slope: -4.855655505226315e-05, fluctuations: 0.0\n",
      "step: 99660 loss: 0.475441 time elapsed: 133.1735 learning rate: 0.001544, scenario: 0, slope: -4.8489051225783675e-05, fluctuations: 0.0\n",
      "step: 99670 loss: 0.474960 time elapsed: 133.1868 learning rate: 0.001544, scenario: 0, slope: -4.8424731318964163e-05, fluctuations: 0.0\n",
      "step: 99680 loss: 0.474619 time elapsed: 133.2001 learning rate: 0.001544, scenario: 0, slope: -4.820742316614126e-05, fluctuations: 0.0\n",
      "step: 99690 loss: 1.997825 time elapsed: 133.2136 learning rate: 0.001567, scenario: -1, slope: 0.0012888082329273731, fluctuations: 0.0\n",
      "step: 99700 loss: 44.241906 time elapsed: 133.2269 learning rate: 0.001431, scenario: -1, slope: 0.1398095371971723, fluctuations: 0.01\n",
      "step: 99710 loss: 9.699929 time elapsed: 133.2398 learning rate: 0.001294, scenario: -1, slope: 0.16030386185773804, fluctuations: 0.05\n",
      "step: 99720 loss: 4.153673 time elapsed: 133.2521 learning rate: 0.001170, scenario: -1, slope: 0.13698442091930935, fluctuations: 0.08\n",
      "step: 99730 loss: 1.253740 time elapsed: 133.2643 learning rate: 0.001059, scenario: -1, slope: 0.10307580120105822, fluctuations: 0.12\n",
      "step: 99740 loss: 0.719906 time elapsed: 133.2766 learning rate: 0.000957, scenario: -1, slope: 0.06691317295901339, fluctuations: 0.17\n",
      "step: 99750 loss: 0.532611 time elapsed: 133.2889 learning rate: 0.000866, scenario: -1, slope: 0.023370892363465203, fluctuations: 0.22\n",
      "step: 99760 loss: 0.513862 time elapsed: 133.3024 learning rate: 0.000832, scenario: 0, slope: -0.0291194043292984, fluctuations: 0.26\n",
      "step: 99770 loss: 0.486257 time elapsed: 133.3165 learning rate: 0.000832, scenario: 0, slope: -0.08360938737607031, fluctuations: 0.31\n",
      "step: 99780 loss: 0.476935 time elapsed: 133.3301 learning rate: 0.000832, scenario: 0, slope: -0.14840579035575627, fluctuations: 0.36\n",
      "step: 99790 loss: 0.476260 time elapsed: 133.3436 learning rate: 0.000832, scenario: 0, slope: -0.23318321833222427, fluctuations: 0.41\n",
      "step: 99800 loss: 0.476093 time elapsed: 133.3573 learning rate: 0.000832, scenario: 0, slope: -0.09741448203113796, fluctuations: 0.43\n",
      "step: 99810 loss: 0.475588 time elapsed: 133.3715 learning rate: 0.000832, scenario: 0, slope: -0.012134285208893649, fluctuations: 0.45\n",
      "step: 99820 loss: 0.475126 time elapsed: 133.3853 learning rate: 0.000832, scenario: 0, slope: -0.004256336724872507, fluctuations: 0.46\n",
      "step: 99830 loss: 0.474767 time elapsed: 133.3990 learning rate: 0.000832, scenario: 0, slope: -0.0008817090558330944, fluctuations: 0.43\n",
      "step: 99840 loss: 0.474455 time elapsed: 133.4123 learning rate: 0.000832, scenario: 0, slope: -0.000617338003050866, fluctuations: 0.38\n",
      "step: 99850 loss: 0.474160 time elapsed: 133.4258 learning rate: 0.000832, scenario: 0, slope: -0.00015510318524445448, fluctuations: 0.34\n",
      "step: 99860 loss: 0.473872 time elapsed: 133.4391 learning rate: 0.000832, scenario: 0, slope: -5.3691314850959145e-05, fluctuations: 0.29\n",
      "step: 99870 loss: 0.473588 time elapsed: 133.4514 learning rate: 0.000832, scenario: 0, slope: -4.848294863365232e-05, fluctuations: 0.24\n",
      "step: 99880 loss: 0.473316 time elapsed: 133.4637 learning rate: 0.000760, scenario: -1, slope: -3.6185483291395334e-05, fluctuations: 0.19\n",
      "step: 99890 loss: 0.473072 time elapsed: 133.4757 learning rate: 0.000687, scenario: -1, slope: -3.245425575421495e-05, fluctuations: 0.14\n",
      "step: 99900 loss: 0.472853 time elapsed: 133.4876 learning rate: 0.000628, scenario: -1, slope: -2.9490761808382196e-05, fluctuations: 0.1\n",
      "step: 99910 loss: 0.472654 time elapsed: 133.5003 learning rate: 0.000568, scenario: -1, slope: -2.7675381448478034e-05, fluctuations: 0.05\n",
      "step: 99920 loss: 0.472475 time elapsed: 133.5142 learning rate: 0.000532, scenario: 1, slope: -2.6480885168676106e-05, fluctuations: 0.0\n",
      "step: 99930 loss: 0.472295 time elapsed: 133.5282 learning rate: 0.000587, scenario: 1, slope: -2.4852186572994215e-05, fluctuations: 0.0\n",
      "step: 99940 loss: 0.472097 time elapsed: 133.5417 learning rate: 0.000649, scenario: 1, slope: -2.330696658089964e-05, fluctuations: 0.0\n",
      "step: 99950 loss: 0.471878 time elapsed: 133.5552 learning rate: 0.000717, scenario: 1, slope: -2.199844407641736e-05, fluctuations: 0.0\n",
      "step: 99960 loss: 0.471638 time elapsed: 133.5688 learning rate: 0.000792, scenario: 1, slope: -2.1090917755752782e-05, fluctuations: 0.0\n",
      "step: 99970 loss: 0.471372 time elapsed: 133.5823 learning rate: 0.000874, scenario: 1, slope: -2.0743582003083688e-05, fluctuations: 0.0\n",
      "step: 99980 loss: 0.471079 time elapsed: 133.5959 learning rate: 0.000966, scenario: 1, slope: -2.1089477210694017e-05, fluctuations: 0.0\n",
      "step: 99990 loss: 0.470757 time elapsed: 133.6093 learning rate: 0.001067, scenario: 1, slope: -2.213524611972107e-05, fluctuations: 0.0\n",
      "Time taken by learn is 133.8056 seconds\n",
      "Time taken by compute_tau_f is 1.2465 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.962416"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTYPE = 'float64'\n",
    "L63_data_path = '../data/L63-trajectories'\n",
    "save_folder='../data/adaptive-rate'\n",
    "log_interval = 100\n",
    "milestones = [10*2**n for n in range(15)]\n",
    "learning_rate = 1e-3\n",
    "drop = 0.7\n",
    "steps = int(1e5)\n",
    "save_interval = 100\n",
    "N = 2000\n",
    "L0 = 0.4\n",
    "L1 = 3.5\n",
    "beta = 4e-5\n",
    "train = np.load(f'{L63_data_path}/train.npy').astype(DTYPE)\n",
    "test = np.load(f'{L63_data_path}/test.npy')[:, :, :1000].astype(DTYPE)\n",
    "\n",
    "model = srnn.SurrogateModel_NN(3, 100, name='nn', save_folder=save_folder)\n",
    "model.learn(train[:, :N], steps, 1e-4, batch_size='GD', log_interval=10, save_interval=100,\\\n",
    "            update_interval=100, del_loss=0.01, fluc_lim=0.01, del_rate=0.01)\n",
    "tau_f_rmse, tau_f_se, rmse, se = model.compute_tau_f(test[:100], error_threshold=0.05)\n",
    "tau_f_1 = tau_f_se.mean()\n",
    "tau_f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5664b0-49e2-4a05-986d-acbcf7319486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x = [15217365, 1918240., 2192992, 1542356.749509, 1e6, 1195660.183959, 854033.622230]\n",
    "y = np.arange(len(x))\n",
    "res = stats.linregress(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdc5137-5f2d-4c17-ba78-d2f16186c1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1633148.0630497143"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67548c09-c9c1-4f2f-96a5-ee608a2a926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "argrelextrema(np.array(x), np.greater)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1a655-5e17-4326-8089-3113abfaeadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
