{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f91fe87-e4cf-4d90-bf34-2e9dac62ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.integrate import odeint\n",
    "import os, sys, warnings\n",
    "from pathlib import Path\n",
    "from os.path import dirname, realpath\n",
    "script_dir = Path(dirname(realpath('.')))\n",
    "module_dir = str(script_dir)\n",
    "sys.path.insert(0, module_dir + '/modules')\n",
    "import utility as ut\n",
    "import surrogate_nn as srnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sample as sm\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8fafbc-83f6-4c5d-8b8e-5bfa3e65dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by compute_tau_f is 1.4708 seconds\n",
      "step: 0    loss: 2.443649     time elapsed=0.0407\n",
      "step: 10    loss: 8606.196460     time elapsed=0.3675\n",
      "step: 20    loss: 905.939144     time elapsed=0.6854\n",
      "step: 30    loss: 174.658216     time elapsed=1.0003\n",
      "step: 40    loss: 330.290570     time elapsed=1.3231\n",
      "step: 50    loss: 149.628210     time elapsed=1.6388\n",
      "step: 60    loss: 55.895526     time elapsed=1.9592\n",
      "step: 70    loss: 21.561350     time elapsed=2.2778\n",
      "step: 80    loss: 7.756402     time elapsed=2.5981\n",
      "step: 90    loss: 2.918559     time elapsed=2.9187\n",
      "step: 100    loss: 2.711548     time elapsed=3.2393\n",
      "step: 110    loss: 2.815166     time elapsed=3.5562\n",
      "step: 120    loss: 2.493772     time elapsed=3.8802\n",
      "step: 130    loss: 2.495909     time elapsed=4.1968\n",
      "step: 140    loss: 2.461304     time elapsed=4.5173\n",
      "step: 150    loss: 2.454576     time elapsed=4.8377\n",
      "step: 160    loss: 2.443749     time elapsed=5.1603\n",
      "step: 170    loss: 2.438769     time elapsed=5.4776\n",
      "step: 180    loss: 2.433501     time elapsed=5.8084\n",
      "step: 190    loss: 2.428976     time elapsed=6.1287\n",
      "step: 200    loss: 2.425111     time elapsed=6.4607\n",
      "step: 210    loss: 2.421678     time elapsed=6.7809\n",
      "step: 220    loss: 2.418619     time elapsed=7.1022\n",
      "step: 230    loss: 2.415901     time elapsed=7.4205\n",
      "step: 240    loss: 2.413489     time elapsed=7.7460\n",
      "step: 250    loss: 2.411350     time elapsed=8.0633\n",
      "step: 260    loss: 2.409455     time elapsed=8.3846\n",
      "step: 270    loss: 2.407778     time elapsed=8.7070\n",
      "step: 280    loss: 2.406296     time elapsed=9.0307\n",
      "step: 290    loss: 2.404988     time elapsed=9.3482\n",
      "step: 300    loss: 2.403833     time elapsed=9.6710\n",
      "step: 310    loss: 2.402815     time elapsed=9.9908\n",
      "step: 320    loss: 2.401917     time elapsed=10.3202\n",
      "step: 330    loss: 2.401125     time elapsed=10.6409\n",
      "step: 340    loss: 2.400426     time elapsed=10.9635\n",
      "step: 350    loss: 2.399808     time elapsed=11.2787\n",
      "step: 360    loss: 2.399261     time elapsed=11.5972\n",
      "step: 370    loss: 2.398832     time elapsed=11.9223\n",
      "step: 380    loss: 3.157046     time elapsed=12.2505\n",
      "step: 390    loss: 96.502634     time elapsed=12.5722\n",
      "step: 400    loss: 39.866653     time elapsed=12.8994\n",
      "step: 410    loss: 14.722635     time elapsed=13.2181\n",
      "step: 420    loss: 3.124124     time elapsed=13.5389\n",
      "step: 430    loss: 2.470254     time elapsed=13.8579\n",
      "step: 440    loss: 2.576953     time elapsed=14.1770\n",
      "step: 450    loss: 2.478176     time elapsed=14.4929\n",
      "step: 460    loss: 2.411510     time elapsed=14.8159\n",
      "step: 470    loss: 2.396242     time elapsed=15.1323\n",
      "step: 480    loss: 2.401927     time elapsed=15.4487\n",
      "step: 490    loss: 2.399746     time elapsed=15.7635\n",
      "step: 500    loss: 2.395612     time elapsed=16.0818\n",
      "step: 510    loss: 2.396016     time elapsed=16.3979\n",
      "step: 520    loss: 2.395354     time elapsed=16.7244\n",
      "step: 530    loss: 2.395189     time elapsed=17.0417\n",
      "step: 540    loss: 2.395067     time elapsed=17.3606\n",
      "step: 550    loss: 2.394934     time elapsed=17.6788\n",
      "step: 560    loss: 2.394811     time elapsed=18.0050\n",
      "step: 570    loss: 2.394697     time elapsed=18.3243\n",
      "step: 580    loss: 2.394589     time elapsed=18.6636\n",
      "step: 590    loss: 2.394485     time elapsed=18.9928\n",
      "step: 600    loss: 2.394385     time elapsed=19.3194\n",
      "step: 610    loss: 2.394289     time elapsed=19.6423\n",
      "step: 620    loss: 2.394196     time elapsed=19.9667\n",
      "step: 630    loss: 2.394106     time elapsed=20.2881\n",
      "step: 640    loss: 2.394019     time elapsed=20.6102\n",
      "step: 650    loss: 2.393934     time elapsed=20.9365\n",
      "step: 660    loss: 2.393853     time elapsed=21.2623\n",
      "step: 670    loss: 2.393848     time elapsed=21.5875\n",
      "step: 680    loss: 2.439703     time elapsed=21.9090\n",
      "step: 690    loss: 73.053148     time elapsed=22.2241\n",
      "step: 700    loss: 83.781103     time elapsed=22.5464\n",
      "step: 710    loss: 8.745530     time elapsed=22.8709\n",
      "step: 720    loss: 13.566266     time elapsed=23.1942\n",
      "step: 730    loss: 4.429245     time elapsed=23.5146\n",
      "step: 740    loss: 3.718494     time elapsed=23.8337\n",
      "step: 750    loss: 2.872388     time elapsed=24.1572\n",
      "step: 760    loss: 2.629397     time elapsed=24.4899\n",
      "step: 770    loss: 2.441771     time elapsed=24.8230\n",
      "step: 780    loss: 2.420545     time elapsed=25.1568\n",
      "step: 790    loss: 2.400443     time elapsed=25.4877\n",
      "step: 800    loss: 2.397206     time elapsed=25.8235\n",
      "step: 810    loss: 2.393947     time elapsed=26.1389\n",
      "step: 820    loss: 2.393443     time elapsed=26.4615\n",
      "step: 830    loss: 2.393052     time elapsed=26.7769\n",
      "step: 840    loss: 2.392954     time elapsed=27.1079\n",
      "step: 850    loss: 2.392835     time elapsed=27.4368\n",
      "step: 860    loss: 2.392766     time elapsed=27.7668\n",
      "step: 870    loss: 2.392712     time elapsed=28.1066\n",
      "step: 880    loss: 2.392663     time elapsed=28.4393\n",
      "step: 890    loss: 2.392613     time elapsed=28.7578\n",
      "step: 900    loss: 2.392565     time elapsed=29.0881\n",
      "step: 910    loss: 2.392527     time elapsed=29.4217\n",
      "step: 920    loss: 2.393529     time elapsed=29.7545\n",
      "step: 930    loss: 2.769969     time elapsed=30.0725\n",
      "step: 940    loss: 238.011112     time elapsed=30.3942\n",
      "step: 950    loss: 32.825108     time elapsed=30.7122\n",
      "step: 960    loss: 8.904227     time elapsed=31.0345\n",
      "step: 970    loss: 19.861913     time elapsed=31.3515\n",
      "step: 980    loss: 7.937048     time elapsed=31.6751\n",
      "step: 990    loss: 3.341444     time elapsed=31.9983\n",
      "step: 1000    loss: 2.797740     time elapsed=32.3207\n",
      "step: 1010    loss: 2.551499     time elapsed=32.6400\n",
      "step: 1020    loss: 2.451910     time elapsed=32.9648\n",
      "step: 1030    loss: 2.424426     time elapsed=33.2789\n",
      "step: 1040    loss: 2.406797     time elapsed=33.5980\n",
      "step: 1050    loss: 2.394945     time elapsed=33.9141\n",
      "step: 1060    loss: 2.393661     time elapsed=34.2351\n",
      "step: 1070    loss: 2.392448     time elapsed=34.5591\n",
      "step: 1080    loss: 2.392340     time elapsed=34.8814\n",
      "step: 1090    loss: 2.392102     time elapsed=35.2103\n",
      "step: 1100    loss: 2.392043     time elapsed=35.5368\n",
      "step: 1110    loss: 2.391996     time elapsed=35.8719\n",
      "step: 1120    loss: 2.391959     time elapsed=36.1997\n",
      "step: 1130    loss: 2.391924     time elapsed=36.5280\n",
      "step: 1140    loss: 2.391886     time elapsed=36.8444\n",
      "step: 1150    loss: 2.391853     time elapsed=37.1652\n",
      "step: 1160    loss: 2.391842     time elapsed=37.4805\n",
      "step: 1170    loss: 2.393350     time elapsed=37.7994\n",
      "step: 1180    loss: 2.678344     time elapsed=38.1149\n",
      "step: 1190    loss: 113.247789     time elapsed=38.4339\n",
      "step: 1200    loss: 198.298389     time elapsed=38.7520\n",
      "step: 1210    loss: 75.695577     time elapsed=39.0794\n",
      "step: 1220    loss: 17.310748     time elapsed=39.3998\n",
      "step: 1230    loss: 3.224206     time elapsed=39.7193\n",
      "step: 1240    loss: 2.574207     time elapsed=40.0330\n",
      "step: 1250    loss: 2.573044     time elapsed=40.3486\n",
      "step: 1260    loss: 2.445827     time elapsed=40.6649\n",
      "step: 1270    loss: 2.400985     time elapsed=40.9927\n",
      "step: 1280    loss: 2.400673     time elapsed=41.3071\n",
      "step: 1290    loss: 2.407242     time elapsed=41.6284\n",
      "step: 1300    loss: 2.399215     time elapsed=41.9418\n",
      "step: 1310    loss: 2.391885     time elapsed=42.2549\n",
      "step: 1320    loss: 2.392721     time elapsed=42.5733\n",
      "step: 1330    loss: 2.391610     time elapsed=42.8887\n",
      "step: 1340    loss: 2.391723     time elapsed=43.2126\n",
      "step: 1350    loss: 2.391714     time elapsed=43.5281\n",
      "step: 1360    loss: 2.394992     time elapsed=43.8448\n",
      "step: 1370    loss: 2.697512     time elapsed=44.1588\n",
      "step: 1380    loss: 64.941379     time elapsed=44.4774\n",
      "step: 1390    loss: 39.817284     time elapsed=44.7904\n",
      "step: 1400    loss: 15.832773     time elapsed=45.1142\n",
      "step: 1410    loss: 21.320143     time elapsed=45.4297\n",
      "step: 1420    loss: 11.943287     time elapsed=45.7480\n",
      "step: 1430    loss: 4.797913     time elapsed=46.0701\n",
      "step: 1440    loss: 2.522499     time elapsed=46.4050\n",
      "step: 1450    loss: 2.755286     time elapsed=46.7313\n",
      "step: 1460    loss: 2.500516     time elapsed=47.0499\n",
      "step: 1470    loss: 2.433160     time elapsed=47.3726\n",
      "step: 1480    loss: 2.398187     time elapsed=47.6992\n",
      "step: 1490    loss: 2.400469     time elapsed=48.0158\n",
      "step: 1500    loss: 2.394716     time elapsed=48.3493\n",
      "step: 1510    loss: 2.392367     time elapsed=48.6784\n",
      "step: 1520    loss: 2.391906     time elapsed=49.0101\n",
      "step: 1530    loss: 2.393009     time elapsed=49.3264\n",
      "step: 1540    loss: 2.492435     time elapsed=49.6537\n",
      "step: 1550    loss: 19.535598     time elapsed=49.9675\n",
      "step: 1560    loss: 40.428369     time elapsed=50.2927\n",
      "step: 1570    loss: 16.599702     time elapsed=50.6110\n",
      "step: 1580    loss: 9.015884     time elapsed=50.9335\n",
      "step: 1590    loss: 4.046661     time elapsed=51.2550\n",
      "step: 1600    loss: 5.579856     time elapsed=51.5926\n",
      "step: 1610    loss: 13.314211     time elapsed=51.9287\n",
      "step: 1620    loss: 91.137232     time elapsed=52.2534\n",
      "step: 1630    loss: 4.211273     time elapsed=52.5702\n",
      "step: 1640    loss: 3.890037     time elapsed=52.9025\n",
      "step: 1650    loss: 7.422840     time elapsed=53.2205\n",
      "step: 1660    loss: 4.911094     time elapsed=53.5399\n",
      "step: 1670    loss: 3.075321     time elapsed=53.8543\n",
      "step: 1680    loss: 2.402327     time elapsed=54.1727\n",
      "step: 1690    loss: 2.856137     time elapsed=54.4909\n",
      "step: 1700    loss: 14.879364     time elapsed=54.8134\n",
      "step: 1710    loss: 272.694963     time elapsed=55.1326\n",
      "step: 1720    loss: 77.079011     time elapsed=55.4614\n",
      "step: 1730    loss: 3.027645     time elapsed=55.7805\n",
      "step: 1740    loss: 12.005568     time elapsed=56.1024\n",
      "step: 1750    loss: 5.676211     time elapsed=56.4206\n",
      "step: 1760    loss: 2.643924     time elapsed=56.7354\n",
      "step: 1770    loss: 2.406906     time elapsed=57.0528\n",
      "step: 1780    loss: 2.395493     time elapsed=57.3688\n",
      "step: 1790    loss: 2.412413     time elapsed=57.6872\n",
      "step: 1800    loss: 2.433503     time elapsed=58.0006\n",
      "step: 1810    loss: 2.401074     time elapsed=58.3202\n",
      "step: 1820    loss: 2.606377     time elapsed=58.6343\n",
      "step: 1830    loss: 18.203971     time elapsed=58.9593\n",
      "step: 1840    loss: 18.340208     time elapsed=59.2749\n",
      "step: 1850    loss: 43.017389     time elapsed=59.5931\n",
      "step: 1860    loss: 130.838886     time elapsed=59.9218\n",
      "step: 1870    loss: 34.081889     time elapsed=60.2389\n",
      "step: 1880    loss: 9.913313     time elapsed=60.5542\n",
      "step: 1890    loss: 4.034924     time elapsed=60.8733\n",
      "step: 1900    loss: 3.047452     time elapsed=61.1887\n",
      "step: 1910    loss: 3.601173     time elapsed=61.5077\n",
      "step: 1920    loss: 2.637297     time elapsed=61.8218\n",
      "step: 1930    loss: 2.397747     time elapsed=62.1364\n",
      "step: 1940    loss: 2.395326     time elapsed=62.4583\n",
      "step: 1950    loss: 2.394446     time elapsed=62.7756\n",
      "step: 1960    loss: 2.551452     time elapsed=63.0896\n",
      "step: 1970    loss: 34.643404     time elapsed=63.4060\n",
      "step: 1980    loss: 99.147617     time elapsed=63.7245\n",
      "step: 1990    loss: 116.413228     time elapsed=64.0392\n",
      "step: 2000    loss: 44.815536     time elapsed=64.3559\n",
      "step: 2010    loss: 16.910266     time elapsed=64.6700\n",
      "step: 2020    loss: 8.063908     time elapsed=64.9896\n",
      "step: 2030    loss: 4.738705     time elapsed=65.3044\n",
      "step: 2040    loss: 3.152307     time elapsed=65.6258\n",
      "step: 2050    loss: 2.492139     time elapsed=65.9428\n",
      "step: 2060    loss: 2.402987     time elapsed=66.2629\n",
      "step: 2070    loss: 2.429739     time elapsed=66.5832\n",
      "step: 2080    loss: 2.395781     time elapsed=66.9064\n",
      "step: 2090    loss: 2.395204     time elapsed=67.2216\n",
      "step: 2100    loss: 2.391993     time elapsed=67.5402\n",
      "step: 2110    loss: 2.392137     time elapsed=67.8561\n",
      "step: 2120    loss: 2.391461     time elapsed=68.1733\n",
      "step: 2130    loss: 2.391427     time elapsed=68.4899\n",
      "step: 2140    loss: 2.406423     time elapsed=68.8112\n",
      "step: 2150    loss: 6.596470     time elapsed=69.1246\n",
      "step: 2160    loss: 218.094474     time elapsed=69.4430\n",
      "step: 2170    loss: 40.656550     time elapsed=69.7633\n",
      "step: 2180    loss: 22.467131     time elapsed=70.0784\n",
      "step: 2190    loss: 2.981574     time elapsed=70.3969\n",
      "step: 2200    loss: 4.077178     time elapsed=70.7125\n",
      "step: 2210    loss: 2.700250     time elapsed=71.0319\n",
      "step: 2220    loss: 2.506562     time elapsed=71.3470\n",
      "step: 2230    loss: 2.944289     time elapsed=71.6723\n",
      "step: 2240    loss: 21.668017     time elapsed=71.9878\n",
      "step: 2250    loss: 211.610976     time elapsed=72.3060\n",
      "step: 2260    loss: 22.718909     time elapsed=72.6234\n",
      "step: 2270    loss: 26.014085     time elapsed=72.9506\n",
      "step: 2280    loss: 7.066532     time elapsed=73.2734\n",
      "step: 2290    loss: 3.980609     time elapsed=73.5948\n",
      "step: 2300    loss: 3.699068     time elapsed=73.9182\n",
      "step: 2310    loss: 2.728657     time elapsed=74.2593\n",
      "step: 2320    loss: 2.587703     time elapsed=74.5893\n",
      "step: 2330    loss: 2.537990     time elapsed=74.9126\n",
      "step: 2340    loss: 2.908973     time elapsed=75.2316\n",
      "step: 2350    loss: 12.718334     time elapsed=75.5549\n",
      "step: 2360    loss: 247.314928     time elapsed=75.8731\n",
      "step: 2370    loss: 92.601591     time elapsed=76.1899\n",
      "step: 2380    loss: 30.329474     time elapsed=76.5128\n",
      "step: 2390    loss: 4.462503     time elapsed=76.8356\n",
      "step: 2400    loss: 2.621010     time elapsed=77.1596\n",
      "step: 2410    loss: 2.775885     time elapsed=77.4792\n",
      "step: 2420    loss: 2.767354     time elapsed=77.8071\n",
      "step: 2430    loss: 6.868347     time elapsed=78.1298\n",
      "step: 2440    loss: 137.587342     time elapsed=78.4521\n",
      "step: 2450    loss: 38.054201     time elapsed=78.7791\n",
      "step: 2460    loss: 25.907323     time elapsed=79.0962\n",
      "step: 2470    loss: 4.455431     time elapsed=79.4202\n",
      "step: 2480    loss: 3.105858     time elapsed=79.7392\n",
      "step: 2490    loss: 3.232396     time elapsed=80.0635\n",
      "step: 2500    loss: 2.831526     time elapsed=80.3820\n",
      "step: 2510    loss: 2.603296     time elapsed=80.7051\n",
      "step: 2520    loss: 2.492295     time elapsed=81.0290\n",
      "step: 2530    loss: 10.941762     time elapsed=81.3556\n",
      "step: 2540    loss: 634.430302     time elapsed=81.6764\n",
      "step: 2550    loss: 12.333827     time elapsed=81.9952\n",
      "step: 2560    loss: 46.652619     time elapsed=82.3171\n",
      "step: 2570    loss: 27.411037     time elapsed=82.6333\n",
      "step: 2580    loss: 8.835539     time elapsed=82.9570\n",
      "step: 2590    loss: 2.534410     time elapsed=83.2782\n",
      "step: 2600    loss: 2.985312     time elapsed=83.6013\n",
      "step: 2610    loss: 2.758185     time elapsed=83.9199\n",
      "step: 2620    loss: 2.400186     time elapsed=84.2401\n",
      "step: 2630    loss: 2.446158     time elapsed=84.5594\n",
      "step: 2640    loss: 2.415537     time elapsed=84.8881\n",
      "step: 2650    loss: 3.716281     time elapsed=85.2069\n",
      "step: 2660    loss: 183.765668     time elapsed=85.5336\n",
      "step: 2670    loss: 250.136158     time elapsed=85.8609\n",
      "step: 2680    loss: 47.361625     time elapsed=86.1898\n",
      "step: 2690    loss: 10.768780     time elapsed=86.5110\n",
      "step: 2700    loss: 4.364697     time elapsed=86.8327\n",
      "step: 2710    loss: 3.886163     time elapsed=87.1535\n",
      "step: 2720    loss: 3.518761     time elapsed=87.4870\n",
      "step: 2730    loss: 2.766563     time elapsed=87.8066\n",
      "step: 2740    loss: 2.452078     time elapsed=88.1259\n",
      "step: 2750    loss: 2.498615     time elapsed=88.4448\n",
      "step: 2760    loss: 2.554471     time elapsed=88.7756\n",
      "step: 2770    loss: 4.614567     time elapsed=89.1008\n",
      "step: 2780    loss: 56.502009     time elapsed=89.4214\n",
      "step: 2790    loss: 26.127812     time elapsed=89.7370\n",
      "step: 2800    loss: 17.818670     time elapsed=90.0743\n",
      "step: 2810    loss: 12.515481     time elapsed=90.4336\n",
      "step: 2820    loss: 33.335754     time elapsed=90.7903\n",
      "step: 2830    loss: 106.155670     time elapsed=91.1098\n",
      "step: 2840    loss: 10.104582     time elapsed=91.4331\n",
      "step: 2850    loss: 6.397200     time elapsed=91.7509\n",
      "step: 2860    loss: 9.101679     time elapsed=92.0740\n",
      "step: 2870    loss: 2.945222     time elapsed=92.3939\n",
      "step: 2880    loss: 2.881550     time elapsed=92.7247\n",
      "step: 2890    loss: 6.108063     time elapsed=93.0424\n",
      "step: 2900    loss: 54.294982     time elapsed=93.3730\n",
      "step: 2910    loss: 124.057714     time elapsed=93.6909\n",
      "step: 2920    loss: 43.982533     time elapsed=94.0098\n",
      "step: 2930    loss: 15.766577     time elapsed=94.3244\n",
      "step: 2940    loss: 7.884887     time elapsed=94.6442\n",
      "step: 2950    loss: 5.324012     time elapsed=94.9708\n",
      "step: 2960    loss: 4.972776     time elapsed=95.3010\n",
      "step: 2970    loss: 165.214667     time elapsed=95.6192\n",
      "step: 2980    loss: 136.483643     time elapsed=95.9408\n",
      "step: 2990    loss: 11.254459     time elapsed=96.2615\n",
      "step: 3000    loss: 24.134629     time elapsed=96.5874\n",
      "step: 3010    loss: 10.555782     time elapsed=96.9093\n",
      "step: 3020    loss: 3.879814     time elapsed=97.2393\n",
      "step: 3030    loss: 2.882633     time elapsed=97.5721\n",
      "step: 3040    loss: 2.831798     time elapsed=97.8934\n",
      "step: 3050    loss: 2.402614     time elapsed=98.2105\n",
      "step: 3060    loss: 2.463700     time elapsed=98.5356\n",
      "step: 3070    loss: 2.478144     time elapsed=98.8533\n",
      "step: 3080    loss: 5.628566     time elapsed=99.1822\n",
      "step: 3090    loss: 242.324856     time elapsed=99.5064\n",
      "step: 3100    loss: 167.770608     time elapsed=99.8420\n",
      "step: 3110    loss: 13.713316     time elapsed=100.1603\n",
      "step: 3120    loss: 31.055367     time elapsed=100.4820\n",
      "step: 3130    loss: 3.370374     time elapsed=100.7975\n",
      "step: 3140    loss: 5.386462     time elapsed=101.1293\n",
      "step: 3150    loss: 2.631228     time elapsed=101.4487\n",
      "step: 3160    loss: 2.883635     time elapsed=101.7687\n",
      "step: 3170    loss: 2.411173     time elapsed=102.0827\n",
      "step: 3180    loss: 2.408839     time elapsed=102.3984\n",
      "step: 3190    loss: 2.412461     time elapsed=102.7160\n",
      "step: 3200    loss: 2.401718     time elapsed=103.0302\n",
      "step: 3210    loss: 2.395762     time elapsed=103.3516\n",
      "step: 3220    loss: 2.393041     time elapsed=103.6673\n",
      "step: 3230    loss: 2.392071     time elapsed=103.9881\n",
      "step: 3240    loss: 2.393316     time elapsed=104.3023\n",
      "step: 3250    loss: 2.540399     time elapsed=104.6212\n",
      "step: 3260    loss: 42.795972     time elapsed=104.9362\n",
      "step: 3270    loss: 52.942983     time elapsed=105.2567\n",
      "step: 3280    loss: 32.290991     time elapsed=105.5839\n",
      "step: 3290    loss: 13.104456     time elapsed=105.9127\n",
      "step: 3300    loss: 10.964031     time elapsed=106.2314\n",
      "step: 3310    loss: 7.133647     time elapsed=106.5507\n",
      "step: 3320    loss: 4.317957     time elapsed=106.8683\n",
      "step: 3330    loss: 3.126765     time elapsed=107.1891\n",
      "step: 3340    loss: 2.667995     time elapsed=107.5076\n",
      "step: 3350    loss: 2.471812     time elapsed=107.8294\n",
      "step: 3360    loss: 2.412728     time elapsed=108.1433\n",
      "step: 3370    loss: 2.914426     time elapsed=108.4619\n",
      "step: 3380    loss: 79.413545     time elapsed=108.7801\n",
      "step: 3390    loss: 3.346013     time elapsed=109.1024\n",
      "step: 3400    loss: 83.439873     time elapsed=109.4180\n",
      "step: 3410    loss: 48.788468     time elapsed=109.7361\n",
      "step: 3420    loss: 19.980303     time elapsed=110.0550\n",
      "step: 3430    loss: 6.505640     time elapsed=110.3715\n",
      "step: 3440    loss: 2.567139     time elapsed=110.6903\n",
      "step: 3450    loss: 2.655610     time elapsed=111.0026\n",
      "step: 3460    loss: 2.693073     time elapsed=111.3291\n",
      "step: 3470    loss: 2.404042     time elapsed=111.6479\n",
      "step: 3480    loss: 2.425787     time elapsed=111.9652\n",
      "step: 3490    loss: 2.394900     time elapsed=112.2803\n",
      "step: 3500    loss: 2.398048     time elapsed=112.5989\n",
      "step: 3510    loss: 2.393043     time elapsed=112.9221\n",
      "step: 3520    loss: 2.392629     time elapsed=113.2429\n",
      "step: 3530    loss: 2.392723     time elapsed=113.5613\n",
      "step: 3540    loss: 2.396761     time elapsed=113.8798\n",
      "step: 3550    loss: 2.799402     time elapsed=114.2015\n",
      "step: 3560    loss: 67.491298     time elapsed=114.5230\n",
      "step: 3570    loss: 81.772365     time elapsed=114.8369\n",
      "step: 3580    loss: 13.886558     time elapsed=115.1781\n",
      "step: 3590    loss: 3.566375     time elapsed=115.4915\n",
      "step: 3600    loss: 4.090822     time elapsed=115.8174\n",
      "step: 3610    loss: 3.536892     time elapsed=116.1380\n",
      "step: 3620    loss: 3.017948     time elapsed=116.4603\n",
      "step: 3630    loss: 7.149564     time elapsed=116.7804\n",
      "step: 3640    loss: 141.359770     time elapsed=117.1050\n",
      "step: 3650    loss: 10.824818     time elapsed=117.4234\n",
      "step: 3660    loss: 31.688280     time elapsed=117.7423\n",
      "step: 3670    loss: 21.679494     time elapsed=118.0640\n",
      "step: 3680    loss: 7.293635     time elapsed=118.3898\n",
      "step: 3690    loss: 4.508734     time elapsed=118.7178\n",
      "step: 3700    loss: 26.101790     time elapsed=119.0383\n",
      "step: 3710    loss: 226.405186     time elapsed=119.3552\n",
      "step: 3720    loss: 75.365454     time elapsed=119.6766\n",
      "step: 3730    loss: 25.391562     time elapsed=119.9911\n",
      "step: 3740    loss: 9.558205     time elapsed=120.3055\n",
      "step: 3750    loss: 5.243955     time elapsed=120.6287\n",
      "step: 3760    loss: 3.531595     time elapsed=120.9685\n",
      "step: 3770    loss: 2.552211     time elapsed=121.3016\n",
      "step: 3780    loss: 2.548611     time elapsed=121.6246\n",
      "step: 3790    loss: 2.440907     time elapsed=121.9498\n",
      "step: 3800    loss: 2.546738     time elapsed=122.2645\n",
      "step: 3810    loss: 3.266801     time elapsed=122.5909\n",
      "step: 3820    loss: 20.041651     time elapsed=122.9103\n",
      "step: 3830    loss: 313.419205     time elapsed=123.2338\n",
      "step: 3840    loss: 113.093523     time elapsed=123.5493\n",
      "step: 3850    loss: 22.930026     time elapsed=123.8726\n",
      "step: 3860    loss: 3.520615     time elapsed=124.1884\n",
      "step: 3870    loss: 12.393496     time elapsed=124.5082\n",
      "step: 3880    loss: 113.770013     time elapsed=124.8222\n",
      "step: 3890    loss: 24.111718     time elapsed=125.1418\n",
      "step: 3900    loss: 12.022894     time elapsed=125.4583\n",
      "step: 3910    loss: 5.895958     time elapsed=125.7792\n",
      "step: 3920    loss: 3.617686     time elapsed=126.0995\n",
      "step: 3930    loss: 3.615590     time elapsed=126.4289\n",
      "step: 3940    loss: 3.078431     time elapsed=126.7512\n",
      "step: 3950    loss: 2.607106     time elapsed=127.0773\n",
      "step: 3960    loss: 2.549949     time elapsed=127.4011\n",
      "step: 3970    loss: 3.712129     time elapsed=127.7263\n",
      "step: 3980    loss: 30.740358     time elapsed=128.0497\n",
      "step: 3990    loss: 24.966512     time elapsed=128.3691\n",
      "step: 4000    loss: 183.932715     time elapsed=128.6871\n",
      "step: 4010    loss: 118.691909     time elapsed=129.0087\n",
      "step: 4020    loss: 47.486823     time elapsed=129.3226\n",
      "step: 4030    loss: 12.256540     time elapsed=129.6425\n",
      "step: 4040    loss: 5.524032     time elapsed=129.9664\n",
      "step: 4050    loss: 3.567601     time elapsed=130.2831\n",
      "step: 4060    loss: 6.287919     time elapsed=130.6030\n",
      "step: 4070    loss: 30.051113     time elapsed=130.9190\n",
      "step: 4080    loss: 157.549429     time elapsed=131.2386\n",
      "step: 4090    loss: 31.419239     time elapsed=131.5537\n",
      "step: 4100    loss: 5.797097     time elapsed=131.8737\n",
      "step: 4110    loss: 2.611395     time elapsed=132.1882\n",
      "step: 4120    loss: 5.056158     time elapsed=132.5078\n",
      "step: 4130    loss: 3.142087     time elapsed=132.8225\n",
      "step: 4140    loss: 3.260949     time elapsed=133.1493\n",
      "step: 4150    loss: 2.938792     time elapsed=133.4647\n",
      "step: 4160    loss: 5.291636     time elapsed=133.7858\n",
      "step: 4170    loss: 65.051306     time elapsed=134.1049\n",
      "step: 4180    loss: 132.634090     time elapsed=134.4245\n",
      "step: 4190    loss: 27.032373     time elapsed=134.7416\n",
      "step: 4200    loss: 14.895092     time elapsed=135.0620\n",
      "step: 4210    loss: 12.441683     time elapsed=135.3771\n",
      "step: 4220    loss: 5.487163     time elapsed=135.6977\n",
      "step: 4230    loss: 2.955970     time elapsed=136.0139\n",
      "step: 4240    loss: 2.595271     time elapsed=136.3306\n",
      "step: 4250    loss: 7.316342     time elapsed=136.6490\n",
      "step: 4260    loss: 454.418052     time elapsed=136.9623\n",
      "step: 4270    loss: 169.928354     time elapsed=137.2848\n",
      "step: 4280    loss: 59.676212     time elapsed=137.6040\n",
      "step: 4290    loss: 7.083713     time elapsed=137.9227\n",
      "step: 4300    loss: 6.334380     time elapsed=138.2377\n",
      "step: 4310    loss: 5.605557     time elapsed=138.5589\n",
      "step: 4320    loss: 2.580811     time elapsed=138.8757\n",
      "step: 4330    loss: 2.752376     time elapsed=139.1956\n",
      "step: 4340    loss: 2.425945     time elapsed=139.5165\n",
      "step: 4350    loss: 2.447892     time elapsed=139.8373\n",
      "step: 4360    loss: 2.401515     time elapsed=140.1550\n",
      "step: 4370    loss: 2.401232     time elapsed=140.4746\n",
      "step: 4380    loss: 2.436263     time elapsed=140.7939\n",
      "step: 4390    loss: 3.346049     time elapsed=141.1144\n",
      "step: 4400    loss: 54.453427     time elapsed=141.4311\n",
      "step: 4410    loss: 162.073281     time elapsed=141.7542\n",
      "step: 4420    loss: 25.190345     time elapsed=142.0731\n",
      "step: 4430    loss: 32.153628     time elapsed=142.3951\n",
      "step: 4440    loss: 5.497411     time elapsed=142.7107\n",
      "step: 4450    loss: 6.929085     time elapsed=143.0298\n",
      "step: 4460    loss: 2.461068     time elapsed=143.3496\n",
      "step: 4470    loss: 2.878891     time elapsed=143.6659\n",
      "step: 4480    loss: 2.637140     time elapsed=143.9873\n",
      "step: 4490    loss: 2.454448     time elapsed=144.3036\n",
      "step: 4500    loss: 2.407993     time elapsed=144.6323\n",
      "step: 4510    loss: 2.400346     time elapsed=144.9507\n",
      "step: 4520    loss: 2.398415     time elapsed=145.2708\n",
      "step: 4530    loss: 2.402581     time elapsed=145.5880\n",
      "step: 4540    loss: 2.860381     time elapsed=145.9143\n",
      "step: 4550    loss: 58.043712     time elapsed=146.2352\n",
      "step: 4560    loss: 261.455427     time elapsed=146.5550\n",
      "step: 4570    loss: 47.883612     time elapsed=146.8694\n",
      "step: 4580    loss: 41.824228     time elapsed=147.1872\n",
      "step: 4590    loss: 11.819017     time elapsed=147.4995\n",
      "step: 4600    loss: 6.401841     time elapsed=147.8252\n",
      "step: 4610    loss: 3.123195     time elapsed=148.1425\n",
      "step: 4620    loss: 3.118105     time elapsed=148.4641\n",
      "step: 4630    loss: 2.603101     time elapsed=148.7845\n",
      "step: 4640    loss: 2.591863     time elapsed=149.1024\n",
      "step: 4650    loss: 4.470154     time elapsed=149.4201\n",
      "step: 4660    loss: 68.729642     time elapsed=149.7364\n",
      "step: 4670    loss: 27.596575     time elapsed=150.0555\n",
      "step: 4680    loss: 16.620028     time elapsed=150.3737\n",
      "step: 4690    loss: 16.706434     time elapsed=150.7028\n",
      "step: 4700    loss: 5.096399     time elapsed=151.0383\n",
      "step: 4710    loss: 3.015784     time elapsed=151.3864\n",
      "step: 4720    loss: 2.877315     time elapsed=151.7237\n",
      "step: 4730    loss: 2.492373     time elapsed=152.0468\n",
      "step: 4740    loss: 2.504792     time elapsed=152.3679\n",
      "step: 4750    loss: 2.464039     time elapsed=152.6898\n",
      "step: 4760    loss: 2.675746     time elapsed=153.0065\n",
      "step: 4770    loss: 7.162811     time elapsed=153.3255\n",
      "step: 4780    loss: 167.745606     time elapsed=153.6436\n",
      "step: 4790    loss: 21.575560     time elapsed=153.9688\n",
      "step: 4800    loss: 54.438646     time elapsed=154.2848\n",
      "step: 4810    loss: 16.904988     time elapsed=154.6041\n",
      "step: 4820    loss: 2.564607     time elapsed=154.9202\n",
      "step: 4830    loss: 3.332221     time elapsed=155.2432\n",
      "step: 4840    loss: 3.282943     time elapsed=155.5588\n",
      "step: 4850    loss: 4.109437     time elapsed=155.8862\n",
      "step: 4860    loss: 53.537369     time elapsed=156.2005\n",
      "step: 4870    loss: 76.769488     time elapsed=156.5274\n",
      "step: 4880    loss: 35.500462     time elapsed=156.8441\n",
      "step: 4890    loss: 28.013400     time elapsed=157.1715\n",
      "step: 4900    loss: 10.893566     time elapsed=157.4880\n",
      "step: 4910    loss: 2.915534     time elapsed=157.8092\n",
      "step: 4920    loss: 4.153321     time elapsed=158.1365\n",
      "step: 4930    loss: 2.611321     time elapsed=158.4615\n",
      "step: 4940    loss: 2.631389     time elapsed=158.7833\n",
      "step: 4950    loss: 2.449870     time elapsed=159.1073\n",
      "step: 4960    loss: 2.423053     time elapsed=159.4233\n",
      "step: 4970    loss: 2.776369     time elapsed=159.7426\n",
      "step: 4980    loss: 20.670792     time elapsed=160.0597\n",
      "step: 4990    loss: 478.061559     time elapsed=160.3731\n",
      "step: 5000    loss: 47.751651     time elapsed=160.7001\n",
      "step: 5010    loss: 41.997077     time elapsed=161.0148\n",
      "step: 5020    loss: 5.005338     time elapsed=161.3424\n",
      "step: 5030    loss: 8.234304     time elapsed=161.6596\n",
      "step: 5040    loss: 3.098205     time elapsed=161.9831\n",
      "step: 5050    loss: 2.713336     time elapsed=162.3089\n",
      "step: 5060    loss: 2.730165     time elapsed=162.6321\n",
      "step: 5070    loss: 2.470296     time elapsed=162.9466\n",
      "step: 5080    loss: 2.551242     time elapsed=163.2670\n",
      "step: 5090    loss: 13.785293     time elapsed=163.5843\n",
      "step: 5100    loss: 382.273513     time elapsed=163.9027\n",
      "step: 5110    loss: 11.021721     time elapsed=164.2276\n",
      "step: 5120    loss: 36.861447     time elapsed=164.5455\n",
      "step: 5130    loss: 11.165590     time elapsed=164.8605\n",
      "step: 5140    loss: 5.130753     time elapsed=165.1861\n",
      "step: 5150    loss: 3.058540     time elapsed=165.5020\n",
      "step: 5160    loss: 3.211085     time elapsed=165.8406\n",
      "step: 5170    loss: 4.416115     time elapsed=166.1645\n",
      "step: 5180    loss: 32.225822     time elapsed=166.4820\n",
      "step: 5190    loss: 4.449302     time elapsed=166.8028\n",
      "step: 5200    loss: 3.267985     time elapsed=167.1190\n",
      "step: 5210    loss: 3.154715     time elapsed=167.4383\n",
      "step: 5220    loss: 7.533543     time elapsed=167.7552\n",
      "step: 5230    loss: 108.996860     time elapsed=168.0847\n",
      "step: 5240    loss: 13.963550     time elapsed=168.4000\n",
      "step: 5250    loss: 3.455877     time elapsed=168.7208\n",
      "step: 5260    loss: 8.935536     time elapsed=169.0402\n",
      "step: 5270    loss: 7.307074     time elapsed=169.3633\n",
      "step: 5280    loss: 4.518522     time elapsed=169.6813\n",
      "step: 5290    loss: 3.027375     time elapsed=170.0044\n",
      "step: 5300    loss: 2.435829     time elapsed=170.3191\n",
      "step: 5310    loss: 2.465828     time elapsed=170.6426\n",
      "step: 5320    loss: 2.484698     time elapsed=170.9591\n",
      "step: 5330    loss: 2.958174     time elapsed=171.2807\n",
      "step: 5340    loss: 29.340643     time elapsed=171.6206\n",
      "step: 5350    loss: 174.037521     time elapsed=171.9494\n",
      "step: 5360    loss: 90.924850     time elapsed=172.2659\n",
      "step: 5370    loss: 32.490286     time elapsed=172.5837\n",
      "step: 5380    loss: 9.258631     time elapsed=172.9057\n",
      "step: 5390    loss: 3.654975     time elapsed=173.2230\n",
      "step: 5400    loss: 7.554452     time elapsed=173.5512\n",
      "step: 5410    loss: 72.724212     time elapsed=173.8682\n",
      "step: 5420    loss: 80.267041     time elapsed=174.1885\n",
      "step: 5430    loss: 11.565594     time elapsed=174.5066\n",
      "step: 5440    loss: 2.500820     time elapsed=174.8276\n",
      "step: 5450    loss: 3.621101     time elapsed=175.1471\n",
      "step: 5460    loss: 3.095321     time elapsed=175.4680\n",
      "step: 5470    loss: 2.498498     time elapsed=175.7819\n",
      "step: 5480    loss: 2.428331     time elapsed=176.1037\n",
      "step: 5490    loss: 2.578874     time elapsed=176.4247\n",
      "step: 5500    loss: 2.416326     time elapsed=176.7425\n",
      "step: 5510    loss: 2.464197     time elapsed=177.0653\n",
      "step: 5520    loss: 2.511843     time elapsed=177.3839\n",
      "step: 5530    loss: 3.373239     time elapsed=177.7006\n",
      "step: 5540    loss: 28.469432     time elapsed=178.0251\n",
      "step: 5550    loss: 336.347735     time elapsed=178.3443\n",
      "step: 5560    loss: 87.282245     time elapsed=178.6680\n",
      "step: 5570    loss: 35.626466     time elapsed=178.9841\n",
      "step: 5580    loss: 29.732214     time elapsed=179.3045\n",
      "step: 5590    loss: 11.183786     time elapsed=179.6400\n",
      "step: 5600    loss: 5.966458     time elapsed=179.9562\n",
      "step: 5610    loss: 9.340719     time elapsed=180.3035\n",
      "step: 5620    loss: 47.122360     time elapsed=180.6451\n",
      "step: 5630    loss: 64.279394     time elapsed=180.9669\n",
      "step: 5640    loss: 20.701688     time elapsed=181.2849\n",
      "step: 5650    loss: 7.920828     time elapsed=181.6047\n",
      "step: 5660    loss: 6.073246     time elapsed=181.9221\n",
      "step: 5670    loss: 3.592468     time elapsed=182.2526\n",
      "step: 5680    loss: 3.127617     time elapsed=182.5681\n",
      "step: 5690    loss: 4.079313     time elapsed=182.8892\n",
      "step: 5700    loss: 32.023347     time elapsed=183.2081\n",
      "step: 5710    loss: 148.380273     time elapsed=183.5272\n",
      "step: 5720    loss: 59.088941     time elapsed=183.8474\n",
      "step: 5730    loss: 15.070672     time elapsed=184.1708\n",
      "step: 5740    loss: 7.502151     time elapsed=184.4907\n",
      "step: 5750    loss: 11.380282     time elapsed=184.8049\n",
      "step: 5760    loss: 69.085505     time elapsed=185.1277\n",
      "step: 5770    loss: 67.131623     time elapsed=185.4422\n",
      "step: 5780    loss: 38.453236     time elapsed=185.7703\n",
      "step: 5790    loss: 16.061646     time elapsed=186.0881\n",
      "step: 5800    loss: 4.702611     time elapsed=186.4142\n",
      "step: 5810    loss: 3.230874     time elapsed=186.7332\n",
      "step: 5820    loss: 10.936626     time elapsed=187.0581\n",
      "step: 5830    loss: 79.553241     time elapsed=187.3777\n",
      "step: 5840    loss: 18.069767     time elapsed=187.7010\n",
      "step: 5850    loss: 4.623819     time elapsed=188.0169\n",
      "step: 5860    loss: 33.653294     time elapsed=188.3395\n",
      "step: 5870    loss: 139.190845     time elapsed=188.6586\n",
      "step: 5880    loss: 85.299295     time elapsed=188.9779\n",
      "step: 5890    loss: 72.632945     time elapsed=189.2994\n",
      "step: 5900    loss: 8.883525     time elapsed=189.6191\n",
      "step: 5910    loss: 8.945518     time elapsed=189.9388\n",
      "step: 5920    loss: 4.521255     time elapsed=190.2565\n",
      "step: 5930    loss: 7.329797     time elapsed=190.5758\n",
      "step: 5940    loss: 16.386899     time elapsed=190.8904\n",
      "step: 5950    loss: 81.245561     time elapsed=191.2173\n",
      "step: 5960    loss: 23.696750     time elapsed=191.5345\n",
      "step: 5970    loss: 26.720059     time elapsed=191.8565\n",
      "step: 5980    loss: 4.872104     time elapsed=192.1718\n",
      "step: 5990    loss: 5.750624     time elapsed=192.4897\n",
      "step: 6000    loss: 2.462682     time elapsed=192.8061\n",
      "step: 6010    loss: 3.969725     time elapsed=193.1304\n",
      "step: 6020    loss: 15.234052     time elapsed=193.4460\n",
      "step: 6030    loss: 172.075434     time elapsed=193.7709\n",
      "step: 6040    loss: 18.581609     time elapsed=194.0849\n",
      "step: 6050    loss: 29.500494     time elapsed=194.4053\n",
      "step: 6060    loss: 52.324481     time elapsed=194.7240\n",
      "step: 6070    loss: 5.165681     time elapsed=195.0437\n",
      "step: 6080    loss: 11.934026     time elapsed=195.3604\n",
      "step: 6090    loss: 4.537425     time elapsed=195.6806\n",
      "step: 6100    loss: 4.732531     time elapsed=195.9997\n",
      "step: 6110    loss: 35.473561     time elapsed=196.3176\n",
      "step: 6120    loss: 50.465067     time elapsed=196.6373\n",
      "step: 6130    loss: 22.752638     time elapsed=196.9559\n",
      "step: 6140    loss: 188.063596     time elapsed=197.2971\n",
      "step: 6150    loss: 58.552476     time elapsed=197.6274\n",
      "step: 6160    loss: 20.000184     time elapsed=197.9532\n",
      "step: 6170    loss: 5.279939     time elapsed=198.2750\n",
      "step: 6180    loss: 3.342875     time elapsed=198.5960\n",
      "step: 6190    loss: 2.647998     time elapsed=198.9151\n",
      "step: 6200    loss: 2.821785     time elapsed=199.2374\n",
      "step: 6210    loss: 4.108107     time elapsed=199.5553\n",
      "step: 6220    loss: 49.238802     time elapsed=199.8770\n",
      "step: 6230    loss: 209.654620     time elapsed=200.2011\n",
      "step: 6240    loss: 10.820552     time elapsed=200.5215\n",
      "step: 6250    loss: 16.576829     time elapsed=200.8479\n",
      "step: 6260    loss: 14.087476     time elapsed=201.1689\n",
      "step: 6270    loss: 4.071912     time elapsed=201.5049\n",
      "step: 6280    loss: 2.416617     time elapsed=201.8270\n",
      "step: 6290    loss: 2.419639     time elapsed=202.1481\n",
      "step: 6300    loss: 2.418802     time elapsed=202.4644\n",
      "step: 6310    loss: 2.791805     time elapsed=202.7860\n",
      "step: 6320    loss: 31.560451     time elapsed=203.1019\n",
      "step: 6330    loss: 260.373551     time elapsed=203.4215\n",
      "step: 6340    loss: 123.233686     time elapsed=203.7386\n",
      "step: 6350    loss: 15.687281     time elapsed=204.0622\n",
      "step: 6360    loss: 14.973370     time elapsed=204.3810\n",
      "step: 6370    loss: 5.135676     time elapsed=204.7029\n",
      "step: 6380    loss: 2.608763     time elapsed=205.0260\n",
      "step: 6390    loss: 2.847755     time elapsed=205.3510\n",
      "step: 6400    loss: 2.609719     time elapsed=205.6728\n",
      "step: 6410    loss: 2.480572     time elapsed=205.9965\n",
      "step: 6420    loss: 2.426585     time elapsed=206.3307\n",
      "step: 6430    loss: 2.409616     time elapsed=206.6516\n",
      "step: 6440    loss: 2.965218     time elapsed=206.9704\n",
      "step: 6450    loss: 71.124951     time elapsed=207.2866\n",
      "step: 6460    loss: 32.957354     time elapsed=207.6070\n",
      "step: 6470    loss: 134.817113     time elapsed=207.9217\n",
      "step: 6480    loss: 40.053702     time elapsed=208.2448\n",
      "step: 6490    loss: 5.613129     time elapsed=208.5615\n",
      "step: 6500    loss: 4.212612     time elapsed=208.8824\n",
      "step: 6510    loss: 4.631778     time elapsed=209.1969\n",
      "step: 6520    loss: 2.637161     time elapsed=209.5180\n",
      "step: 6530    loss: 2.574748     time elapsed=209.8344\n",
      "step: 6540    loss: 2.444861     time elapsed=210.1566\n",
      "step: 6550    loss: 2.422684     time elapsed=210.4768\n",
      "step: 6560    loss: 2.396100     time elapsed=210.8076\n",
      "step: 6570    loss: 2.421040     time elapsed=211.1251\n",
      "step: 6580    loss: 4.218173     time elapsed=211.4412\n",
      "step: 6590    loss: 229.502746     time elapsed=211.7710\n",
      "step: 6600    loss: 193.696523     time elapsed=212.0983\n",
      "step: 6610    loss: 30.109021     time elapsed=212.4290\n",
      "step: 6620    loss: 7.782536     time elapsed=212.7446\n",
      "step: 6630    loss: 3.874593     time elapsed=213.0636\n",
      "step: 6640    loss: 4.824132     time elapsed=213.3841\n",
      "step: 6650    loss: 8.684519     time elapsed=213.7042\n",
      "step: 6660    loss: 100.979533     time elapsed=214.0193\n",
      "step: 6670    loss: 19.998375     time elapsed=214.3407\n",
      "step: 6680    loss: 4.015257     time elapsed=214.6597\n",
      "step: 6690    loss: 6.226960     time elapsed=214.9824\n",
      "step: 6700    loss: 2.925101     time elapsed=215.2969\n",
      "step: 6710    loss: 2.440573     time elapsed=215.6197\n",
      "step: 6720    loss: 2.497966     time elapsed=215.9402\n",
      "step: 6730    loss: 2.479578     time elapsed=216.2602\n",
      "step: 6740    loss: 2.847383     time elapsed=216.5830\n",
      "step: 6750    loss: 19.632616     time elapsed=216.9020\n",
      "step: 6760    loss: 441.384651     time elapsed=217.2237\n",
      "step: 6770    loss: 84.563256     time elapsed=217.5693\n",
      "step: 6780    loss: 17.771485     time elapsed=217.9183\n",
      "step: 6790    loss: 15.789538     time elapsed=218.2436\n",
      "step: 6800    loss: 3.010781     time elapsed=218.5665\n",
      "step: 6810    loss: 4.747856     time elapsed=218.8815\n"
     ]
    }
   ],
   "source": [
    "DTYPE = 'float64'\n",
    "L63_data_path = '../data/L63-trajectories'\n",
    "save_folder='../data/rf-start-aggresive'\n",
    "log_interval = 100\n",
    "milestones = [10*2**n for n in range(15)]\n",
    "learning_rate = 1e-3\n",
    "drop = 0.7\n",
    "steps = int(1e4)\n",
    "save_interval = 100\n",
    "N = 20000\n",
    "L0 = 0.4\n",
    "L1 = 3.5\n",
    "beta = 4e-5\n",
    "partition = [100, 100, 100]\n",
    "train = np.load(f'{L63_data_path}/train.npy').astype(DTYPE)\n",
    "test = np.load(f'{L63_data_path}/test.npy')[:, :, :1000].astype(DTYPE)\n",
    "\n",
    "model = srnn.SurrogateModel_NN(3, 300, name='nn', save_folder=save_folder)\n",
    "model.init_with_rf(L0, L1, beta, train, partition)\n",
    "model.init_with_rf(L0, L1, beta, train, partition=[300, 0, 0])\n",
    "tau_f_rmse, tau_f_se, rmse, se = model.compute_tau_f(test[:100], error_threshold=0.05)\n",
    "tau_f_1 = tau_f_se.mean()\n",
    "model.learn(train[:, :N], 10000, 1e-4, drop=1., batch_size='GD', log_interval=10, save_interval=10)\n",
    "tau_f_rmse, tau_f_se, rmse, se = model.compute_tau_f(test[:100], error_threshold=0.05)\n",
    "tau_f_2 = tau_f_se.mean()\n",
    "\n",
    "tau_f_1 , tau_f_2, tau_f_1 < tau_f_2\n",
    "model.count_row_types(L0, L1, train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce187ac8-a1f1-480b-bae0-3e16ff18d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{model.save_folder}/train_log.csv')\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.plot(df['iteration'], df['good_rows_W_in'], c='green', label='good rows')\n",
    "ax1.plot(df['iteration'], df['linear_rows_W_in'], c='blue', label = 'linear rows')\n",
    "ax1.plot(df['iteration'], df['extreme_rows_W_in'], c='red', label = 'extreme rows')\n",
    "ax2.semilogy(df['iteration'], df['loss'])\n",
    "ax1.set_xlabel('iteration')\n",
    "ax2.set_xlabel('iteration')\n",
    "ax2.set_xlabel('log loss')\n",
    "plt.savefig(f'{model.save_folder}/row_types.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7af086-ed7a-4123-8f78-3453b7e96f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
