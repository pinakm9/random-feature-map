{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646653a5-f2b6-47ef-af57-e22da1bd6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.integrate import odeint\n",
    "import os, sys, warnings\n",
    "from pathlib import Path\n",
    "from os.path import dirname, realpath\n",
    "script_dir = Path(dirname(realpath('.')))\n",
    "module_dir = str(script_dir)\n",
    "sys.path.insert(0, module_dir + '/modules')\n",
    "import utility as ut\n",
    "import surrogate_nn as srnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02818a22-e0ab-4f51-8f27-dae86c82ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 1520022.820261 time elapsed: 0.0022 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 10 loss: 723597.972213 time elapsed: 0.0162 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 20 loss: 333868.029524 time elapsed: 0.0272 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 30 loss: 202380.277214 time elapsed: 0.0375 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 40 loss: 175957.627949 time elapsed: 0.0477 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 50 loss: 157845.885146 time elapsed: 0.0579 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 60 loss: 138307.531560 time elapsed: 0.0678 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 70 loss: 114201.961104 time elapsed: 0.0779 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 80 loss: 90419.577131 time elapsed: 0.0878 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 90 loss: 69946.960015 time elapsed: 0.0977 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 100 loss: 51565.333745 time elapsed: 0.1074 learning rate: 0.010000, scenario: 0, slope: None, fluctuations: 0\n",
      "step: 110 loss: 36805.734209 time elapsed: 0.1202 learning rate: 0.010000, scenario: 0, slope: -3665.0350756442785, fluctuations: 0.0\n",
      "step: 120 loss: 26717.900570 time elapsed: 0.1325 learning rate: 0.010000, scenario: 0, slope: -2245.570936022343, fluctuations: 0.0\n",
      "step: 130 loss: 19537.176331 time elapsed: 0.1446 learning rate: 0.010000, scenario: 0, slope: -1905.538211254762, fluctuations: 0.0\n",
      "step: 140 loss: 14689.950021 time elapsed: 0.1568 learning rate: 0.010000, scenario: 0, slope: -1744.4321654217663, fluctuations: 0.0\n",
      "step: 150 loss: 11329.435563 time elapsed: 0.1691 learning rate: 0.010000, scenario: 0, slope: -1528.8587390400721, fluctuations: 0.0\n",
      "step: 160 loss: 9027.031508 time elapsed: 0.1813 learning rate: 0.010000, scenario: 0, slope: -1266.0441776162377, fluctuations: 0.0\n",
      "step: 170 loss: 7470.788568 time elapsed: 0.1933 learning rate: 0.010000, scenario: 0, slope: -995.547028855646, fluctuations: 0.0\n",
      "step: 180 loss: 6376.078839 time elapsed: 0.2058 learning rate: 0.010000, scenario: 0, slope: -750.2849959325758, fluctuations: 0.0\n",
      "step: 190 loss: 5545.363670 time elapsed: 0.2184 learning rate: 0.010000, scenario: 0, slope: -542.2019273413531, fluctuations: 0.0\n",
      "step: 200 loss: 4840.212242 time elapsed: 0.2310 learning rate: 0.010000, scenario: 0, slope: -392.54726181860207, fluctuations: 0.0\n",
      "step: 210 loss: 4270.195404 time elapsed: 0.2438 learning rate: 0.010000, scenario: 0, slope: -262.8490888265063, fluctuations: 0.0\n",
      "step: 220 loss: 3798.164468 time elapsed: 0.2560 learning rate: 0.010000, scenario: 0, slope: -184.38624367780415, fluctuations: 0.0\n",
      "step: 230 loss: 3428.953427 time elapsed: 0.2682 learning rate: 0.010000, scenario: 0, slope: -131.42766456247634, fluctuations: 0.0\n",
      "step: 240 loss: 3114.480460 time elapsed: 0.2799 learning rate: 0.010000, scenario: 0, slope: -96.2406728970032, fluctuations: 0.0\n",
      "step: 250 loss: 2844.434840 time elapsed: 0.2921 learning rate: 0.010000, scenario: 0, slope: -72.836199350704, fluctuations: 0.0\n",
      "step: 260 loss: 2610.610025 time elapsed: 0.3040 learning rate: 0.010000, scenario: 0, slope: -57.29701006217842, fluctuations: 0.0\n",
      "step: 270 loss: 2405.518308 time elapsed: 0.3167 learning rate: 0.010000, scenario: 0, slope: -46.62736796594672, fluctuations: 0.0\n",
      "step: 280 loss: 2346.231445 time elapsed: 0.3290 learning rate: 0.010000, scenario: 0, slope: -38.62133013741614, fluctuations: 0.01\n",
      "step: 290 loss: 2080.483178 time elapsed: 0.3419 learning rate: 0.010000, scenario: 0, slope: -32.28015196092152, fluctuations: 0.02\n",
      "step: 300 loss: 1907.410129 time elapsed: 0.3544 learning rate: 0.010000, scenario: 0, slope: -27.305792458031757, fluctuations: 0.02\n",
      "step: 310 loss: 1762.995831 time elapsed: 0.3672 learning rate: 0.010000, scenario: 0, slope: -23.13758146304782, fluctuations: 0.02\n",
      "step: 320 loss: 1645.023825 time elapsed: 0.3792 learning rate: 0.010000, scenario: 0, slope: -20.382998615153813, fluctuations: 0.02\n",
      "step: 330 loss: 1538.001690 time elapsed: 0.3916 learning rate: 0.010000, scenario: 0, slope: -18.184783720325036, fluctuations: 0.02\n",
      "step: 340 loss: 1443.104275 time elapsed: 0.4041 learning rate: 0.010000, scenario: 0, slope: -16.35820926894449, fluctuations: 0.02\n",
      "step: 350 loss: 1358.395905 time elapsed: 0.4164 learning rate: 0.010000, scenario: 0, slope: -14.785657735409938, fluctuations: 0.02\n",
      "step: 360 loss: 1291.062626 time elapsed: 0.4284 learning rate: 0.010000, scenario: 0, slope: -13.383755148799613, fluctuations: 0.02\n",
      "step: 370 loss: 1347.979539 time elapsed: 0.4408 learning rate: 0.010000, scenario: 0, slope: -11.433483451870233, fluctuations: 0.03\n",
      "step: 380 loss: 1195.556394 time elapsed: 0.4528 learning rate: 0.010000, scenario: 0, slope: -9.89145954352329, fluctuations: 0.05\n",
      "step: 390 loss: 1120.444620 time elapsed: 0.4650 learning rate: 0.010000, scenario: 0, slope: -8.722085641022078, fluctuations: 0.06\n",
      "step: 400 loss: 1046.537175 time elapsed: 0.4767 learning rate: 0.010000, scenario: 0, slope: -7.892274531219587, fluctuations: 0.09\n",
      "step: 410 loss: 995.658559 time elapsed: 0.4893 learning rate: 0.010000, scenario: 0, slope: -7.149984432685241, fluctuations: 0.1\n",
      "step: 420 loss: 951.397494 time elapsed: 0.5016 learning rate: 0.010000, scenario: 0, slope: -6.670046694392256, fluctuations: 0.1\n",
      "step: 430 loss: 909.482677 time elapsed: 0.5137 learning rate: 0.010000, scenario: 0, slope: -6.320475501953578, fluctuations: 0.1\n",
      "step: 440 loss: 870.545635 time elapsed: 0.5263 learning rate: 0.010000, scenario: 0, slope: -6.064454048866671, fluctuations: 0.1\n",
      "step: 450 loss: 834.211851 time elapsed: 0.5390 learning rate: 0.010000, scenario: 0, slope: -5.892027595256137, fluctuations: 0.1\n",
      "step: 460 loss: 799.806372 time elapsed: 0.5517 learning rate: 0.010000, scenario: 0, slope: -5.795695434583483, fluctuations: 0.1\n",
      "step: 470 loss: 769.201859 time elapsed: 0.5639 learning rate: 0.010000, scenario: 0, slope: -4.88003647952766, fluctuations: 0.08\n",
      "step: 480 loss: 865.778625 time elapsed: 0.5759 learning rate: 0.010000, scenario: 0, slope: -3.821262716644576, fluctuations: 0.07\n",
      "step: 490 loss: 751.257143 time elapsed: 0.5879 learning rate: 0.010000, scenario: 0, slope: -3.4154943527957538, fluctuations: 0.06\n",
      "step: 500 loss: 689.623220 time elapsed: 0.5998 learning rate: 0.010000, scenario: 0, slope: -3.2126658150622993, fluctuations: 0.05\n",
      "step: 510 loss: 728.816290 time elapsed: 0.6125 learning rate: 0.010000, scenario: 0, slope: -2.9746556020568895, fluctuations: 0.06\n",
      "step: 520 loss: 735.539546 time elapsed: 0.6250 learning rate: 0.010000, scenario: 0, slope: -2.379832252869513, fluctuations: 0.07\n",
      "step: 530 loss: 632.098026 time elapsed: 0.6374 learning rate: 0.010000, scenario: 0, slope: -2.3806982631519906, fluctuations: 0.09\n",
      "step: 540 loss: 583.190879 time elapsed: 0.6496 learning rate: 0.010000, scenario: 0, slope: -2.4722519955313746, fluctuations: 0.11\n",
      "step: 550 loss: 562.660283 time elapsed: 0.6617 learning rate: 0.010000, scenario: 0, slope: -2.6180278843114118, fluctuations: 0.13\n",
      "step: 560 loss: 538.075726 time elapsed: 0.6737 learning rate: 0.010000, scenario: 0, slope: -2.8260249805633606, fluctuations: 0.13\n",
      "step: 570 loss: 518.191044 time elapsed: 0.6858 learning rate: 0.010000, scenario: 0, slope: -3.0526941435667907, fluctuations: 0.13\n",
      "step: 580 loss: 498.215275 time elapsed: 0.6978 learning rate: 0.010000, scenario: 0, slope: -2.8537746523963663, fluctuations: 0.12\n",
      "step: 590 loss: 480.401268 time elapsed: 0.7100 learning rate: 0.010000, scenario: 0, slope: -2.783686807948023, fluctuations: 0.1\n",
      "step: 600 loss: 484.998278 time elapsed: 0.7212 learning rate: 0.010000, scenario: 0, slope: -2.7768148818184835, fluctuations: 0.08\n",
      "step: 610 loss: 489.569630 time elapsed: 0.7339 learning rate: 0.010000, scenario: 0, slope: -2.1591969228861103, fluctuations: 0.08\n",
      "step: 620 loss: 483.071750 time elapsed: 0.7459 learning rate: 0.010000, scenario: 0, slope: -1.4607048089677717, fluctuations: 0.08\n",
      "step: 630 loss: 425.546006 time elapsed: 0.7581 learning rate: 0.010000, scenario: 0, slope: -1.3725773467373232, fluctuations: 0.07\n",
      "step: 640 loss: 404.042989 time elapsed: 0.7700 learning rate: 0.010000, scenario: 0, slope: -1.3742039951866687, fluctuations: 0.08\n",
      "step: 650 loss: 388.544306 time elapsed: 0.7819 learning rate: 0.010000, scenario: 0, slope: -1.479397312883283, fluctuations: 0.07\n",
      "step: 660 loss: 422.196874 time elapsed: 0.7939 learning rate: 0.010000, scenario: 0, slope: -1.5075652182086856, fluctuations: 0.08\n",
      "step: 670 loss: 370.824444 time elapsed: 0.8060 learning rate: 0.010000, scenario: 0, slope: -1.4032832854237265, fluctuations: 0.09\n",
      "step: 680 loss: 367.982109 time elapsed: 0.8178 learning rate: 0.010000, scenario: 0, slope: -1.545398741445201, fluctuations: 0.1\n",
      "step: 690 loss: 342.083522 time elapsed: 0.8301 learning rate: 0.010000, scenario: 0, slope: -1.784497665182784, fluctuations: 0.12\n",
      "step: 700 loss: 327.630979 time elapsed: 0.8421 learning rate: 0.010000, scenario: 0, slope: -2.0228386757341346, fluctuations: 0.13\n",
      "step: 710 loss: 426.239913 time elapsed: 0.8546 learning rate: 0.010000, scenario: 0, slope: -1.4134192206914342, fluctuations: 0.12\n",
      "step: 720 loss: 548.088160 time elapsed: 0.8669 learning rate: 0.013310, scenario: 1, slope: -0.14630473059028176, fluctuations: 0.11\n",
      "step: 730 loss: 1450.861201 time elapsed: 0.8790 learning rate: 0.004922, scenario: -1, slope: 4.044628072523606, fluctuations: 0.12\n",
      "step: 740 loss: 512.651371 time elapsed: 0.8909 learning rate: 0.001716, scenario: -1, slope: 3.4277111031853935, fluctuations: 0.14\n",
      "step: 750 loss: 329.291466 time elapsed: 0.9030 learning rate: 0.000598, scenario: -1, slope: 2.2041900425933956, fluctuations: 0.15\n",
      "step: 760 loss: 309.053088 time elapsed: 0.9153 learning rate: 0.000209, scenario: -1, slope: 0.9086259419928645, fluctuations: 0.15\n",
      "step: 770 loss: 306.207921 time elapsed: 0.9271 learning rate: 0.000085, scenario: 1, slope: -0.06941105362675684, fluctuations: 0.15\n",
      "step: 780 loss: 304.675292 time elapsed: 0.9389 learning rate: 0.000114, scenario: 0, slope: -1.176491754931142, fluctuations: 0.13\n",
      "step: 790 loss: 304.095032 time elapsed: 0.9510 learning rate: 0.000114, scenario: 0, slope: -2.3285291064983946, fluctuations: 0.13\n",
      "step: 800 loss: 303.458964 time elapsed: 0.9627 learning rate: 0.000114, scenario: 0, slope: -3.422555173699295, fluctuations: 0.12\n",
      "step: 810 loss: 303.009005 time elapsed: 0.9753 learning rate: 0.000114, scenario: 0, slope: -5.031347337256249, fluctuations: 0.12\n",
      "step: 820 loss: 302.573643 time elapsed: 0.9876 learning rate: 0.000114, scenario: 0, slope: -5.0730775873022695, fluctuations: 0.11\n",
      "step: 830 loss: 302.170357 time elapsed: 0.9995 learning rate: 0.000114, scenario: 0, slope: -1.129610224299139, fluctuations: 0.07\n",
      "step: 840 loss: 301.779458 time elapsed: 1.0116 learning rate: 0.000151, scenario: 1, slope: -0.31732676160822865, fluctuations: 0.04\n",
      "step: 850 loss: 301.069652 time elapsed: 1.0238 learning rate: 0.000392, scenario: 1, slope: -0.1313469318007193, fluctuations: 0.02\n",
      "step: 860 loss: 299.403554 time elapsed: 1.0364 learning rate: 0.001017, scenario: 1, slope: -0.06809560980939844, fluctuations: 0.02\n",
      "step: 870 loss: 295.944043 time elapsed: 1.0487 learning rate: 0.002638, scenario: 1, slope: -0.06744028778447604, fluctuations: 0.01\n",
      "step: 880 loss: 289.859304 time elapsed: 1.0613 learning rate: 0.006842, scenario: 1, slope: -0.10182862360626972, fluctuations: 0.01\n",
      "step: 890 loss: 279.177760 time elapsed: 1.0736 learning rate: 0.017745, scenario: 1, slope: -0.16869435358955606, fluctuations: 0.0\n",
      "step: 900 loss: 119226.788867 time elapsed: 1.0858 learning rate: 0.024307, scenario: -1, slope: 38.43749844079464, fluctuations: 0.01\n",
      "step: 910 loss: 21861.088091 time elapsed: 1.0986 learning rate: 0.008475, scenario: -1, slope: 235.14237873966397, fluctuations: 0.04\n",
      "step: 920 loss: 10159.992544 time elapsed: 1.1107 learning rate: 0.002955, scenario: -1, slope: 255.7300227081755, fluctuations: 0.05\n",
      "step: 930 loss: 6045.837935 time elapsed: 1.1232 learning rate: 0.001030, scenario: -1, slope: 217.77780754121014, fluctuations: 0.05\n",
      "step: 940 loss: 5035.424260 time elapsed: 1.1353 learning rate: 0.000359, scenario: -1, slope: 164.9769735331082, fluctuations: 0.05\n",
      "step: 950 loss: 4773.844498 time elapsed: 1.1472 learning rate: 0.000125, scenario: -1, slope: 106.4376586247096, fluctuations: 0.05\n",
      "step: 960 loss: 4698.769842 time elapsed: 1.1596 learning rate: 0.000044, scenario: -1, slope: 42.990761725004724, fluctuations: 0.05\n",
      "step: 970 loss: 4674.022174 time elapsed: 1.1716 learning rate: 0.000023, scenario: 0, slope: -27.153603239160436, fluctuations: 0.05\n",
      "step: 980 loss: 4656.980258 time elapsed: 1.1837 learning rate: 0.000023, scenario: 0, slope: -106.91269737758824, fluctuations: 0.05\n",
      "step: 990 loss: 4640.440882 time elapsed: 1.1957 learning rate: 0.000023, scenario: 0, slope: -200.4330191092779, fluctuations: 0.05\n",
      "step: 1000 loss: 4624.312741 time elapsed: 1.2075 learning rate: 0.000023, scenario: 0, slope: -302.7304518959814, fluctuations: 0.03\n",
      "step: 1010 loss: 4608.531564 time elapsed: 1.2204 learning rate: 0.000023, scenario: 0, slope: -64.71108447464462, fluctuations: 0.01\n",
      "step: 1020 loss: 4593.048296 time elapsed: 1.2322 learning rate: 0.000023, scenario: 0, slope: -19.489056982682786, fluctuations: 0.0\n",
      "step: 1030 loss: 4577.824113 time elapsed: 1.2446 learning rate: 0.000023, scenario: 0, slope: -6.1027818151160735, fluctuations: 0.0\n",
      "step: 1040 loss: 4556.021643 time elapsed: 1.2570 learning rate: 0.000060, scenario: 1, slope: -2.5798775931714597, fluctuations: 0.0\n",
      "step: 1050 loss: 4501.625170 time elapsed: 1.2693 learning rate: 0.000156, scenario: 1, slope: -1.903781971391835, fluctuations: 0.0\n",
      "step: 1060 loss: 4370.570660 time elapsed: 1.2812 learning rate: 0.000405, scenario: 1, slope: -2.2536405403263844, fluctuations: 0.0\n",
      "step: 1070 loss: 4077.920448 time elapsed: 1.2932 learning rate: 0.001051, scenario: 1, slope: -3.699415398877631, fluctuations: 0.0\n",
      "step: 1080 loss: 3592.689550 time elapsed: 1.3051 learning rate: 0.001398, scenario: 0, slope: -6.984146883835444, fluctuations: 0.0\n",
      "step: 1090 loss: 3219.146712 time elapsed: 1.3173 learning rate: 0.001398, scenario: 0, slope: -11.696629392144116, fluctuations: 0.0\n",
      "step: 1100 loss: 2951.352772 time elapsed: 1.3296 learning rate: 0.001398, scenario: 0, slope: -16.181740176867645, fluctuations: 0.0\n",
      "step: 1110 loss: 2752.368976 time elapsed: 1.3422 learning rate: 0.001398, scenario: 0, slope: -21.079389889459645, fluctuations: 0.0\n",
      "step: 1120 loss: 2597.016719 time elapsed: 1.3544 learning rate: 0.001398, scenario: 0, slope: -24.394734075683523, fluctuations: 0.0\n",
      "step: 1130 loss: 2469.337085 time elapsed: 1.3663 learning rate: 0.001398, scenario: 0, slope: -26.23730022251495, fluctuations: 0.0\n",
      "step: 1140 loss: 2361.135728 time elapsed: 1.3782 learning rate: 0.001398, scenario: 0, slope: -26.34804226010095, fluctuations: 0.0\n",
      "step: 1150 loss: 2267.282657 time elapsed: 1.3903 learning rate: 0.001398, scenario: 0, slope: -24.64910945046077, fluctuations: 0.0\n",
      "step: 1160 loss: 2184.405009 time elapsed: 1.4021 learning rate: 0.001398, scenario: 0, slope: -21.351865768639083, fluctuations: 0.0\n",
      "step: 1170 loss: 2110.135360 time elapsed: 1.4139 learning rate: 0.001398, scenario: 0, slope: -17.189723167990145, fluctuations: 0.0\n",
      "step: 1180 loss: 2042.624627 time elapsed: 1.4258 learning rate: 0.001398, scenario: 0, slope: -13.626673374360376, fluctuations: 0.0\n",
      "step: 1190 loss: 1980.442866 time elapsed: 1.4375 learning rate: 0.001398, scenario: 0, slope: -11.212676935083639, fluctuations: 0.0\n",
      "step: 1200 loss: 1922.513353 time elapsed: 1.4491 learning rate: 0.001398, scenario: 0, slope: -9.688397367714488, fluctuations: 0.0\n",
      "step: 1210 loss: 1868.054555 time elapsed: 1.4622 learning rate: 0.001398, scenario: 0, slope: -8.350425897294537, fluctuations: 0.0\n",
      "step: 1220 loss: 1816.514143 time elapsed: 1.4752 learning rate: 0.001398, scenario: 0, slope: -7.452841495986289, fluctuations: 0.0\n",
      "step: 1230 loss: 1767.507597 time elapsed: 1.4874 learning rate: 0.001398, scenario: 0, slope: -6.757640295185816, fluctuations: 0.0\n",
      "step: 1240 loss: 1720.772430 time elapsed: 1.5000 learning rate: 0.001398, scenario: 0, slope: -6.207855999890771, fluctuations: 0.0\n",
      "step: 1250 loss: 1676.133883 time elapsed: 1.5125 learning rate: 0.001398, scenario: 0, slope: -5.764941145567957, fluctuations: 0.0\n",
      "step: 1260 loss: 1633.476337 time elapsed: 1.5246 learning rate: 0.001398, scenario: 0, slope: -5.401212579622843, fluctuations: 0.0\n",
      "step: 1270 loss: 1592.717652 time elapsed: 1.5365 learning rate: 0.001398, scenario: 0, slope: -5.095369075960582, fluctuations: 0.0\n",
      "step: 1280 loss: 1553.786815 time elapsed: 1.5489 learning rate: 0.001398, scenario: 0, slope: -4.830976125192079, fluctuations: 0.0\n",
      "step: 1290 loss: 1516.607376 time elapsed: 1.5613 learning rate: 0.001398, scenario: 0, slope: -4.595847343699761, fluctuations: 0.0\n",
      "step: 1300 loss: 1481.088699 time elapsed: 1.5734 learning rate: 0.001398, scenario: 0, slope: -4.402114812060962, fluctuations: 0.0\n",
      "step: 1310 loss: 1447.124819 time elapsed: 1.5861 learning rate: 0.001398, scenario: 0, slope: -4.182109573921631, fluctuations: 0.0\n",
      "step: 1320 loss: 1414.598502 time elapsed: 1.5978 learning rate: 0.001398, scenario: 0, slope: -3.994587862958353, fluctuations: 0.0\n",
      "step: 1330 loss: 1383.387366 time elapsed: 1.6121 learning rate: 0.001398, scenario: 0, slope: -3.8172290307605987, fluctuations: 0.0\n",
      "step: 1340 loss: 1353.369703 time elapsed: 1.6248 learning rate: 0.001398, scenario: 0, slope: -3.649559601622702, fluctuations: 0.0\n",
      "step: 1350 loss: 1324.428962 time elapsed: 1.6373 learning rate: 0.001398, scenario: 0, slope: -3.491795453527153, fluctuations: 0.0\n",
      "step: 1360 loss: 1296.456961 time elapsed: 1.6496 learning rate: 0.001398, scenario: 0, slope: -3.344438623313202, fluctuations: 0.0\n",
      "step: 1370 loss: 1269.356345 time elapsed: 1.6641 learning rate: 0.001398, scenario: 0, slope: -3.207959447494696, fluctuations: 0.0\n",
      "step: 1380 loss: 1243.042974 time elapsed: 1.6775 learning rate: 0.001398, scenario: 0, slope: -3.0825825567659217, fluctuations: 0.0\n",
      "step: 1390 loss: 1217.448716 time elapsed: 1.6910 learning rate: 0.001398, scenario: 0, slope: -2.968177023074307, fluctuations: 0.0\n",
      "step: 1400 loss: 1192.524713 time elapsed: 1.7030 learning rate: 0.001398, scenario: 0, slope: -2.8741748046250093, fluctuations: 0.0\n",
      "step: 1410 loss: 1168.244290 time elapsed: 1.7158 learning rate: 0.001398, scenario: 0, slope: -2.769856296724713, fluctuations: 0.0\n",
      "step: 1420 loss: 1144.603561 time elapsed: 1.7286 learning rate: 0.001398, scenario: 0, slope: -2.6838579780690375, fluctuations: 0.0\n",
      "step: 1430 loss: 1121.617372 time elapsed: 1.7411 learning rate: 0.001398, scenario: 0, slope: -2.6047635985223985, fluctuations: 0.0\n",
      "step: 1440 loss: 1099.309622 time elapsed: 1.7534 learning rate: 0.001398, scenario: 0, slope: -2.530929990592487, fluctuations: 0.0\n",
      "step: 1450 loss: 1077.700318 time elapsed: 1.7654 learning rate: 0.001398, scenario: 0, slope: -2.460681644083857, fluctuations: 0.0\n",
      "step: 1460 loss: 1056.794507 time elapsed: 1.7777 learning rate: 0.001398, scenario: 0, slope: -2.3924922016641355, fluctuations: 0.0\n",
      "step: 1470 loss: 1036.577516 time elapsed: 1.7901 learning rate: 0.001398, scenario: 0, slope: -2.3251685818852192, fluctuations: 0.0\n",
      "step: 1480 loss: 1017.017097 time elapsed: 1.8020 learning rate: 0.001398, scenario: 0, slope: -2.2579897492298353, fluctuations: 0.0\n",
      "step: 1490 loss: 998.069539 time elapsed: 1.8140 learning rate: 0.001398, scenario: 0, slope: -2.1907656322637044, fluctuations: 0.0\n",
      "step: 1500 loss: 979.686027 time elapsed: 1.8261 learning rate: 0.001398, scenario: 0, slope: -2.1304722453772516, fluctuations: 0.0\n",
      "step: 1510 loss: 961.817034 time elapsed: 1.8389 learning rate: 0.001398, scenario: 0, slope: -2.057806670227319, fluctuations: 0.0\n",
      "step: 1520 loss: 944.414340 time elapsed: 1.8503 learning rate: 0.001398, scenario: 0, slope: -1.9936839331013727, fluctuations: 0.0\n",
      "step: 1530 loss: 927.431328 time elapsed: 1.8620 learning rate: 0.001398, scenario: 0, slope: -1.9323816240882339, fluctuations: 0.0\n",
      "step: 1540 loss: 910.822388 time elapsed: 1.8742 learning rate: 0.001398, scenario: 0, slope: -1.8747082834306712, fluctuations: 0.0\n",
      "step: 1550 loss: 894.542027 time elapsed: 1.8865 learning rate: 0.001398, scenario: 0, slope: -1.8212393089486776, fluctuations: 0.0\n",
      "step: 1560 loss: 878.544011 time elapsed: 1.8985 learning rate: 0.001398, scenario: 0, slope: -1.7723043334422068, fluctuations: 0.0\n",
      "step: 1570 loss: 862.780671 time elapsed: 1.9104 learning rate: 0.001398, scenario: 0, slope: -1.7280431215528276, fluctuations: 0.0\n",
      "step: 1580 loss: 847.202526 time elapsed: 1.9226 learning rate: 0.001398, scenario: 0, slope: -1.6884909578324723, fluctuations: 0.0\n",
      "step: 1590 loss: 831.758596 time elapsed: 1.9352 learning rate: 0.001398, scenario: 0, slope: -1.653654734250159, fluctuations: 0.0\n",
      "step: 1600 loss: 816.398299 time elapsed: 1.9471 learning rate: 0.001398, scenario: 0, slope: -1.6263510860282833, fluctuations: 0.0\n",
      "step: 1610 loss: 801.076808 time elapsed: 1.9599 learning rate: 0.001398, scenario: 0, slope: -1.598222765294498, fluctuations: 0.0\n",
      "step: 1620 loss: 785.766817 time elapsed: 1.9718 learning rate: 0.001398, scenario: 0, slope: -1.5776313257725418, fluctuations: 0.0\n",
      "step: 1630 loss: 770.479723 time elapsed: 1.9862 learning rate: 0.001398, scenario: 0, slope: -1.561548243553767, fluctuations: 0.0\n",
      "step: 1640 loss: 755.295373 time elapsed: 2.0005 learning rate: 0.001398, scenario: 0, slope: -1.5492742705869464, fluctuations: 0.0\n",
      "step: 1650 loss: 740.388827 time elapsed: 2.0136 learning rate: 0.001398, scenario: 0, slope: -1.5392931480588103, fluctuations: 0.0\n",
      "step: 1660 loss: 726.028491 time elapsed: 2.0266 learning rate: 0.001398, scenario: 0, slope: -1.5289573509312882, fluctuations: 0.0\n",
      "step: 1670 loss: 712.518709 time elapsed: 2.0395 learning rate: 0.001398, scenario: 0, slope: -1.5144693030626626, fluctuations: 0.0\n",
      "step: 1680 loss: 700.090718 time elapsed: 2.0521 learning rate: 0.001398, scenario: 0, slope: -1.491430085251334, fluctuations: 0.0\n",
      "step: 1690 loss: 688.803567 time elapsed: 2.0648 learning rate: 0.001398, scenario: 0, slope: -1.4559621345989868, fluctuations: 0.0\n",
      "step: 1700 loss: 678.537912 time elapsed: 2.0778 learning rate: 0.001398, scenario: 0, slope: -1.4116114494165208, fluctuations: 0.0\n",
      "step: 1710 loss: 669.083503 time elapsed: 2.0940 learning rate: 0.001398, scenario: 0, slope: -1.3417232574625562, fluctuations: 0.0\n",
      "step: 1720 loss: 660.233942 time elapsed: 2.1069 learning rate: 0.001398, scenario: 0, slope: -1.2660632418224271, fluctuations: 0.0\n",
      "step: 1730 loss: 651.831272 time elapsed: 2.1201 learning rate: 0.001398, scenario: 0, slope: -1.1835695963427288, fluctuations: 0.0\n",
      "step: 1740 loss: 643.768215 time elapsed: 2.1326 learning rate: 0.001398, scenario: 0, slope: -1.0998409914462892, fluctuations: 0.0\n",
      "step: 1750 loss: 635.974485 time elapsed: 2.1450 learning rate: 0.001398, scenario: 0, slope: -1.0204712101230897, fluctuations: 0.0\n",
      "step: 1760 loss: 628.403257 time elapsed: 2.1574 learning rate: 0.001398, scenario: 0, slope: -0.9499867904563074, fluctuations: 0.0\n",
      "step: 1770 loss: 621.022190 time elapsed: 2.1698 learning rate: 0.001398, scenario: 0, slope: -0.8909599437924091, fluctuations: 0.0\n",
      "step: 1780 loss: 613.808130 time elapsed: 2.1819 learning rate: 0.001398, scenario: 0, slope: -0.8436774964855607, fluctuations: 0.0\n",
      "step: 1790 loss: 606.743970 time elapsed: 2.1937 learning rate: 0.001398, scenario: 0, slope: -0.8066036194090981, fluctuations: 0.0\n",
      "step: 1800 loss: 599.816719 time elapsed: 2.2054 learning rate: 0.001398, scenario: 0, slope: -0.7800188964656573, fluctuations: 0.0\n",
      "step: 1810 loss: 593.016284 time elapsed: 2.2182 learning rate: 0.001398, scenario: 0, slope: -0.7537523815696897, fluctuations: 0.0\n",
      "step: 1820 loss: 586.334680 time elapsed: 2.2301 learning rate: 0.001398, scenario: 0, slope: -0.7339824321868301, fluctuations: 0.0\n",
      "step: 1830 loss: 579.765495 time elapsed: 2.2420 learning rate: 0.001398, scenario: 0, slope: -0.7168929382150626, fluctuations: 0.0\n",
      "step: 1840 loss: 573.303529 time elapsed: 2.2540 learning rate: 0.001398, scenario: 0, slope: -0.7017155315138709, fluctuations: 0.0\n",
      "step: 1850 loss: 566.944517 time elapsed: 2.2660 learning rate: 0.001398, scenario: 0, slope: -0.6879463395080908, fluctuations: 0.0\n",
      "step: 1860 loss: 560.684933 time elapsed: 2.2781 learning rate: 0.001398, scenario: 0, slope: -0.6752445377829219, fluctuations: 0.0\n",
      "step: 1870 loss: 554.521824 time elapsed: 2.2901 learning rate: 0.001398, scenario: 0, slope: -0.663371338295462, fluctuations: 0.0\n",
      "step: 1880 loss: 548.452683 time elapsed: 2.3027 learning rate: 0.001398, scenario: 0, slope: -0.6521540844478059, fluctuations: 0.0\n",
      "step: 1890 loss: 542.475338 time elapsed: 2.3151 learning rate: 0.001398, scenario: 0, slope: -0.6414646205219791, fluctuations: 0.0\n",
      "step: 1900 loss: 536.587855 time elapsed: 2.3271 learning rate: 0.001398, scenario: 0, slope: -0.6322147023876268, fluctuations: 0.0\n",
      "step: 1910 loss: 530.788461 time elapsed: 2.3397 learning rate: 0.001398, scenario: 0, slope: -0.6213038870695387, fluctuations: 0.0\n",
      "step: 1920 loss: 525.075465 time elapsed: 2.3517 learning rate: 0.001398, scenario: 0, slope: -0.6117015907110158, fluctuations: 0.0\n",
      "step: 1930 loss: 519.447203 time elapsed: 2.3635 learning rate: 0.001398, scenario: 0, slope: -0.6023560324199164, fluctuations: 0.0\n",
      "step: 1940 loss: 513.901980 time elapsed: 2.3755 learning rate: 0.001398, scenario: 0, slope: -0.5932354577568654, fluctuations: 0.0\n",
      "step: 1950 loss: 508.438034 time elapsed: 2.3876 learning rate: 0.001398, scenario: 0, slope: -0.5843175479103178, fluctuations: 0.0\n",
      "step: 1960 loss: 503.053503 time elapsed: 2.3996 learning rate: 0.001398, scenario: 0, slope: -0.5755879629682485, fluctuations: 0.0\n",
      "step: 1970 loss: 497.746409 time elapsed: 2.4117 learning rate: 0.001398, scenario: 0, slope: -0.5670390987996318, fluctuations: 0.0\n",
      "step: 1980 loss: 492.514652 time elapsed: 2.4236 learning rate: 0.001398, scenario: 0, slope: -0.5586689469982551, fluctuations: 0.0\n",
      "step: 1990 loss: 487.356014 time elapsed: 2.4356 learning rate: 0.001398, scenario: 0, slope: -0.5504799933668267, fluctuations: 0.0\n",
      "step: 2000 loss: 482.268172 time elapsed: 2.4478 learning rate: 0.001398, scenario: 0, slope: -0.5432696724335262, fluctuations: 0.0\n",
      "step: 2010 loss: 477.248723 time elapsed: 2.4606 learning rate: 0.001398, scenario: 0, slope: -0.5346715405376639, fluctuations: 0.0\n",
      "step: 2020 loss: 472.295217 time elapsed: 2.4723 learning rate: 0.001398, scenario: 0, slope: -0.5270696784898417, fluctuations: 0.0\n",
      "step: 2030 loss: 467.405187 time elapsed: 2.4845 learning rate: 0.001398, scenario: 0, slope: -0.5196821818030113, fluctuations: 0.0\n",
      "step: 2040 loss: 462.045949 time elapsed: 2.4972 learning rate: 0.002477, scenario: 1, slope: -0.5131241123791482, fluctuations: 0.0\n",
      "step: 2050 loss: 453.720106 time elapsed: 2.5093 learning rate: 0.002477, scenario: 0, slope: -0.519719378534285, fluctuations: 0.0\n",
      "step: 2060 loss: 445.509363 time elapsed: 2.5215 learning rate: 0.002477, scenario: 0, slope: -0.5428023941600789, fluctuations: 0.0\n",
      "step: 2070 loss: 437.492530 time elapsed: 2.5339 learning rate: 0.002477, scenario: 0, slope: -0.5773526615246698, fluctuations: 0.0\n",
      "step: 2080 loss: 429.664075 time elapsed: 2.5460 learning rate: 0.002477, scenario: 0, slope: -0.6185204178004193, fluctuations: 0.0\n",
      "step: 2090 loss: 422.018436 time elapsed: 2.5579 learning rate: 0.002477, scenario: 0, slope: -0.6616396591065726, fluctuations: 0.0\n",
      "step: 2100 loss: 414.552514 time elapsed: 2.5693 learning rate: 0.002477, scenario: 0, slope: -0.6983973967991592, fluctuations: 0.0\n",
      "step: 2110 loss: 407.266251 time elapsed: 2.5820 learning rate: 0.002477, scenario: 0, slope: -0.7359077486757843, fluctuations: 0.0\n",
      "step: 2120 loss: 400.162162 time elapsed: 2.5935 learning rate: 0.002477, scenario: 0, slope: -0.7585149965576397, fluctuations: 0.0\n",
      "step: 2130 loss: 393.244191 time elapsed: 2.6050 learning rate: 0.002477, scenario: 0, slope: -0.7659709502825844, fluctuations: 0.0\n",
      "step: 2140 loss: 386.516178 time elapsed: 2.6170 learning rate: 0.002477, scenario: 0, slope: -0.7549838951447904, fluctuations: 0.0\n",
      "step: 2150 loss: 379.980296 time elapsed: 2.6292 learning rate: 0.002477, scenario: 0, slope: -0.7365169638940214, fluctuations: 0.0\n",
      "step: 2160 loss: 373.635850 time elapsed: 2.6413 learning rate: 0.002477, scenario: 0, slope: -0.7180448587759112, fluctuations: 0.0\n",
      "step: 2170 loss: 367.478729 time elapsed: 2.6534 learning rate: 0.002477, scenario: 0, slope: -0.6995038500981431, fluctuations: 0.0\n",
      "step: 2180 loss: 361.501561 time elapsed: 2.6652 learning rate: 0.002477, scenario: 0, slope: -0.6808775386942147, fluctuations: 0.0\n",
      "step: 2190 loss: 355.694445 time elapsed: 2.6771 learning rate: 0.002477, scenario: 0, slope: -0.662227296832405, fluctuations: 0.0\n",
      "step: 2200 loss: 350.045944 time elapsed: 2.6892 learning rate: 0.002477, scenario: 0, slope: -0.6455328981226475, fluctuations: 0.0\n",
      "step: 2210 loss: 344.544070 time elapsed: 2.7019 learning rate: 0.002477, scenario: 0, slope: -0.6254526589736877, fluctuations: 0.0\n",
      "step: 2220 loss: 339.177051 time elapsed: 2.7139 learning rate: 0.002477, scenario: 0, slope: -0.607729061875332, fluctuations: 0.0\n",
      "step: 2230 loss: 333.933823 time elapsed: 2.7259 learning rate: 0.002477, scenario: 0, slope: -0.5907202358369308, fluctuations: 0.0\n",
      "step: 2240 loss: 328.804261 time elapsed: 2.7374 learning rate: 0.002477, scenario: 0, slope: -0.5745916127107313, fluctuations: 0.0\n",
      "step: 2250 loss: 323.779233 time elapsed: 2.7494 learning rate: 0.002477, scenario: 0, slope: -0.5594557857803021, fluctuations: 0.0\n",
      "step: 2260 loss: 318.850546 time elapsed: 2.7613 learning rate: 0.002477, scenario: 0, slope: -0.545367673828486, fluctuations: 0.0\n",
      "step: 2270 loss: 314.010853 time elapsed: 2.7733 learning rate: 0.002477, scenario: 0, slope: -0.532329867993904, fluctuations: 0.0\n",
      "step: 2280 loss: 309.253582 time elapsed: 2.7852 learning rate: 0.002477, scenario: 0, slope: -0.5203044766000731, fluctuations: 0.0\n",
      "step: 2290 loss: 304.572920 time elapsed: 2.7972 learning rate: 0.002477, scenario: 0, slope: -0.5092270251365697, fluctuations: 0.0\n",
      "step: 2300 loss: 299.963900 time elapsed: 2.8088 learning rate: 0.002477, scenario: 0, slope: -0.5000027459242646, fluctuations: 0.0\n",
      "step: 2310 loss: 295.422589 time elapsed: 2.8212 learning rate: 0.002477, scenario: 0, slope: -0.4895939473462764, fluctuations: 0.0\n",
      "step: 2320 loss: 290.946370 time elapsed: 2.8328 learning rate: 0.002477, scenario: 0, slope: -0.4808646290476951, fluctuations: 0.0\n",
      "step: 2330 loss: 286.534230 time elapsed: 2.8449 learning rate: 0.002477, scenario: 0, slope: -0.4727381957760075, fluctuations: 0.0\n",
      "step: 2340 loss: 282.186905 time elapsed: 2.8570 learning rate: 0.002477, scenario: 0, slope: -0.4651149083477849, fluctuations: 0.0\n",
      "step: 2350 loss: 277.906704 time elapsed: 2.8689 learning rate: 0.002477, scenario: 0, slope: -0.45788480279006133, fluctuations: 0.0\n",
      "step: 2360 loss: 273.696948 time elapsed: 2.8807 learning rate: 0.002477, scenario: 0, slope: -0.4509278161883142, fluctuations: 0.0\n",
      "step: 2370 loss: 269.561146 time elapsed: 2.8931 learning rate: 0.002477, scenario: 0, slope: -0.44411877118807735, fluctuations: 0.0\n",
      "step: 2380 loss: 265.502232 time elapsed: 2.9052 learning rate: 0.002477, scenario: 0, slope: -0.43733695423444874, fluctuations: 0.0\n",
      "step: 2390 loss: 261.522145 time elapsed: 2.9172 learning rate: 0.002477, scenario: 0, slope: -0.43047787209971766, fluctuations: 0.0\n",
      "step: 2400 loss: 257.621846 time elapsed: 2.9288 learning rate: 0.002477, scenario: 0, slope: -0.4241736937938167, fluctuations: 0.0\n",
      "step: 2410 loss: 253.801616 time elapsed: 2.9413 learning rate: 0.002477, scenario: 0, slope: -0.41624994086012534, fluctuations: 0.0\n",
      "step: 2420 loss: 250.061431 time elapsed: 2.9533 learning rate: 0.002477, scenario: 0, slope: -0.40882550468307266, fluctuations: 0.0\n",
      "step: 2430 loss: 246.401241 time elapsed: 2.9654 learning rate: 0.002477, scenario: 0, slope: -0.40120829906047695, fluctuations: 0.0\n",
      "step: 2440 loss: 242.821106 time elapsed: 2.9776 learning rate: 0.002477, scenario: 0, slope: -0.3934353947150132, fluctuations: 0.0\n",
      "step: 2450 loss: 239.321199 time elapsed: 2.9900 learning rate: 0.002477, scenario: 0, slope: -0.38555136327122747, fluctuations: 0.0\n",
      "step: 2460 loss: 235.901729 time elapsed: 3.0022 learning rate: 0.002477, scenario: 0, slope: -0.37759747078168276, fluctuations: 0.0\n",
      "step: 2470 loss: 232.562813 time elapsed: 3.0147 learning rate: 0.002477, scenario: 0, slope: -0.36960476016987265, fluctuations: 0.0\n",
      "step: 2480 loss: 229.304347 time elapsed: 3.0271 learning rate: 0.002477, scenario: 0, slope: -0.3615925312749368, fluctuations: 0.0\n",
      "step: 2490 loss: 226.125891 time elapsed: 3.0398 learning rate: 0.002477, scenario: 0, slope: -0.3535715323374649, fluctuations: 0.0\n",
      "step: 2500 loss: 223.026587 time elapsed: 3.0516 learning rate: 0.002477, scenario: 0, slope: -0.3463515348441738, fluctuations: 0.0\n",
      "step: 2510 loss: 220.005129 time elapsed: 3.0640 learning rate: 0.002477, scenario: 0, slope: -0.3375367937404673, fluctuations: 0.0\n",
      "step: 2520 loss: 217.059769 time elapsed: 3.0761 learning rate: 0.002477, scenario: 0, slope: -0.3295495306410699, fluctuations: 0.0\n",
      "step: 2530 loss: 214.188373 time elapsed: 3.0877 learning rate: 0.002477, scenario: 0, slope: -0.3216112466150244, fluctuations: 0.0\n",
      "step: 2540 loss: 211.388490 time elapsed: 3.1002 learning rate: 0.002477, scenario: 0, slope: -0.31375187192453274, fluctuations: 0.0\n",
      "step: 2550 loss: 208.657447 time elapsed: 3.1122 learning rate: 0.002477, scenario: 0, slope: -0.3060056157884969, fluctuations: 0.0\n",
      "step: 2560 loss: 205.992430 time elapsed: 3.1241 learning rate: 0.002477, scenario: 0, slope: -0.29840816434222583, fluctuations: 0.0\n",
      "step: 2570 loss: 203.390564 time elapsed: 3.1361 learning rate: 0.002477, scenario: 0, slope: -0.29099383963821074, fluctuations: 0.0\n",
      "step: 2580 loss: 200.848972 time elapsed: 3.1481 learning rate: 0.002477, scenario: 0, slope: -0.2837931698393983, fluctuations: 0.0\n",
      "step: 2590 loss: 198.364830 time elapsed: 3.1600 learning rate: 0.002477, scenario: 0, slope: -0.2768311516849153, fluctuations: 0.0\n",
      "step: 2600 loss: 195.935393 time elapsed: 3.1721 learning rate: 0.002477, scenario: 0, slope: -0.27078483610432724, fluctuations: 0.0\n",
      "step: 2610 loss: 193.558027 time elapsed: 3.1849 learning rate: 0.002477, scenario: 0, slope: -0.263690548487804, fluctuations: 0.0\n",
      "step: 2620 loss: 191.230221 time elapsed: 3.1973 learning rate: 0.002477, scenario: 0, slope: -0.2575295243161152, fluctuations: 0.0\n",
      "step: 2630 loss: 188.949595 time elapsed: 3.2093 learning rate: 0.002477, scenario: 0, slope: -0.2516435232380772, fluctuations: 0.0\n",
      "step: 2640 loss: 186.713909 time elapsed: 3.2215 learning rate: 0.002477, scenario: 0, slope: -0.24602843822654005, fluctuations: 0.0\n",
      "step: 2650 loss: 184.521063 time elapsed: 3.2336 learning rate: 0.002477, scenario: 0, slope: -0.24067679489612395, fluctuations: 0.0\n",
      "step: 2660 loss: 182.369096 time elapsed: 3.2456 learning rate: 0.002477, scenario: 0, slope: -0.235578676294192, fluctuations: 0.0\n",
      "step: 2670 loss: 180.256185 time elapsed: 3.2578 learning rate: 0.002477, scenario: 0, slope: -0.23072249999162933, fluctuations: 0.0\n",
      "step: 2680 loss: 178.180637 time elapsed: 3.2700 learning rate: 0.002477, scenario: 0, slope: -0.22609563888970927, fluctuations: 0.0\n",
      "step: 2690 loss: 176.140883 time elapsed: 3.2821 learning rate: 0.002477, scenario: 0, slope: -0.22168490317983308, fluctuations: 0.0\n",
      "step: 2700 loss: 174.135474 time elapsed: 3.2941 learning rate: 0.002477, scenario: 0, slope: -0.21788896735488084, fluctuations: 0.0\n",
      "step: 2710 loss: 172.163066 time elapsed: 3.3066 learning rate: 0.002477, scenario: 0, slope: -0.21345837530219247, fluctuations: 0.0\n",
      "step: 2720 loss: 170.222420 time elapsed: 3.3187 learning rate: 0.002477, scenario: 0, slope: -0.20961632617105666, fluctuations: 0.0\n",
      "step: 2730 loss: 168.312384 time elapsed: 3.3310 learning rate: 0.002477, scenario: 0, slope: -0.20593828498499633, fluctuations: 0.0\n",
      "step: 2740 loss: 166.431892 time elapsed: 3.3433 learning rate: 0.002477, scenario: 0, slope: -0.2024123954745075, fluctuations: 0.0\n",
      "step: 2750 loss: 164.579951 time elapsed: 3.3555 learning rate: 0.002477, scenario: 0, slope: -0.19902751760127987, fluctuations: 0.0\n",
      "step: 2760 loss: 162.755638 time elapsed: 3.3674 learning rate: 0.002477, scenario: 0, slope: -0.19577328664723812, fluctuations: 0.0\n",
      "step: 2770 loss: 160.958089 time elapsed: 3.3795 learning rate: 0.002477, scenario: 0, slope: -0.19264014066422083, fluctuations: 0.0\n",
      "step: 2780 loss: 159.186495 time elapsed: 3.3915 learning rate: 0.002477, scenario: 0, slope: -0.18961932016253044, fluctuations: 0.0\n",
      "step: 2790 loss: 157.440098 time elapsed: 3.4036 learning rate: 0.002477, scenario: 0, slope: -0.1867028447556863, fluctuations: 0.0\n",
      "step: 2800 loss: 155.718185 time elapsed: 3.4155 learning rate: 0.002477, scenario: 0, slope: -0.18416123163990178, fluctuations: 0.0\n",
      "step: 2810 loss: 154.020084 time elapsed: 3.4280 learning rate: 0.002477, scenario: 0, slope: -0.1811546435221718, fluctuations: 0.0\n",
      "step: 2820 loss: 152.345161 time elapsed: 3.4397 learning rate: 0.002477, scenario: 0, slope: -0.17851042376267123, fluctuations: 0.0\n",
      "step: 2830 loss: 150.692818 time elapsed: 3.4516 learning rate: 0.002477, scenario: 0, slope: -0.17594543473692845, fluctuations: 0.0\n",
      "step: 2840 loss: 149.062490 time elapsed: 3.4635 learning rate: 0.002477, scenario: 0, slope: -0.17345479095489064, fluctuations: 0.0\n",
      "step: 2850 loss: 147.453641 time elapsed: 3.4757 learning rate: 0.002477, scenario: 0, slope: -0.17103403655945762, fluctuations: 0.0\n",
      "step: 2860 loss: 145.865767 time elapsed: 3.4878 learning rate: 0.002477, scenario: 0, slope: -0.16867908660795328, fluctuations: 0.0\n",
      "step: 2870 loss: 144.298390 time elapsed: 3.5002 learning rate: 0.002477, scenario: 0, slope: -0.16638617360385763, fluctuations: 0.0\n",
      "step: 2880 loss: 142.751060 time elapsed: 3.5123 learning rate: 0.002477, scenario: 0, slope: -0.1641517999052471, fluctuations: 0.0\n",
      "step: 2890 loss: 141.223353 time elapsed: 3.5247 learning rate: 0.002477, scenario: 0, slope: -0.16197269626200309, fluctuations: 0.0\n",
      "step: 2900 loss: 139.714869 time elapsed: 3.5369 learning rate: 0.002477, scenario: 0, slope: -0.16005621289950922, fluctuations: 0.0\n",
      "step: 2910 loss: 138.225232 time elapsed: 3.5497 learning rate: 0.002477, scenario: 0, slope: -0.15776815794726357, fluctuations: 0.0\n",
      "step: 2920 loss: 136.754088 time elapsed: 3.5617 learning rate: 0.002477, scenario: 0, slope: -0.15573703793786703, fluctuations: 0.0\n",
      "step: 2930 loss: 135.301108 time elapsed: 3.5735 learning rate: 0.002477, scenario: 0, slope: -0.15374977494198494, fluctuations: 0.0\n",
      "step: 2940 loss: 133.865983 time elapsed: 3.5856 learning rate: 0.002477, scenario: 0, slope: -0.15180382496330505, fluctuations: 0.0\n",
      "step: 2950 loss: 132.448424 time elapsed: 3.5979 learning rate: 0.002477, scenario: 0, slope: -0.14989674209612652, fluctuations: 0.0\n",
      "step: 2960 loss: 131.048162 time elapsed: 3.6100 learning rate: 0.002477, scenario: 0, slope: -0.14802617296100642, fluctuations: 0.0\n",
      "step: 2970 loss: 129.664947 time elapsed: 3.6219 learning rate: 0.002477, scenario: 0, slope: -0.14618985446449148, fluctuations: 0.0\n",
      "step: 2980 loss: 128.298545 time elapsed: 3.6335 learning rate: 0.002477, scenario: 0, slope: -0.14438561433151345, fluctuations: 0.0\n",
      "step: 2990 loss: 126.948739 time elapsed: 3.6459 learning rate: 0.002477, scenario: 0, slope: -0.1426113738341826, fluctuations: 0.0\n",
      "step: 3000 loss: 125.615327 time elapsed: 3.6577 learning rate: 0.002477, scenario: 0, slope: -0.14103856779581767, fluctuations: 0.0\n",
      "step: 3010 loss: 124.298121 time elapsed: 3.6698 learning rate: 0.002477, scenario: 0, slope: -0.13914507155369074, fluctuations: 0.0\n",
      "step: 3020 loss: 122.996944 time elapsed: 3.6814 learning rate: 0.002477, scenario: 0, slope: -0.137449363429283, fluctuations: 0.0\n",
      "step: 3030 loss: 121.711630 time elapsed: 3.6931 learning rate: 0.002477, scenario: 0, slope: -0.13577637356732492, fluctuations: 0.0\n",
      "step: 3040 loss: 120.442025 time elapsed: 3.7048 learning rate: 0.002477, scenario: 0, slope: -0.13412456719725568, fluctuations: 0.0\n",
      "step: 3050 loss: 119.187981 time elapsed: 3.7167 learning rate: 0.002477, scenario: 0, slope: -0.1324925327270352, fluctuations: 0.0\n",
      "step: 3060 loss: 117.949357 time elapsed: 3.7290 learning rate: 0.002997, scenario: 1, slope: -0.13087898402811604, fluctuations: 0.0\n",
      "step: 3070 loss: 116.147524 time elapsed: 3.7408 learning rate: 0.003990, scenario: 0, slope: -0.1307692202817652, fluctuations: 0.0\n",
      "step: 3080 loss: 114.221528 time elapsed: 3.7537 learning rate: 0.003990, scenario: 0, slope: -0.13421287686693814, fluctuations: 0.0\n",
      "step: 3090 loss: 112.338521 time elapsed: 3.7662 learning rate: 0.003990, scenario: 0, slope: -0.14029245186967443, fluctuations: 0.0\n",
      "step: 3100 loss: 110.498813 time elapsed: 3.7785 learning rate: 0.003990, scenario: 0, slope: -0.14719324585273888, fluctuations: 0.0\n",
      "step: 3110 loss: 108.701920 time elapsed: 3.7911 learning rate: 0.003990, scenario: 0, slope: -0.1564068076060877, fluctuations: 0.0\n",
      "step: 3120 loss: 106.947101 time elapsed: 3.8030 learning rate: 0.003990, scenario: 0, slope: -0.16455401431215966, fluctuations: 0.0\n",
      "step: 3130 loss: 105.233535 time elapsed: 3.8151 learning rate: 0.003990, scenario: 0, slope: -0.1715644346140068, fluctuations: 0.0\n",
      "step: 3140 loss: 103.560381 time elapsed: 3.8270 learning rate: 0.003990, scenario: 0, slope: -0.1765861013783051, fluctuations: 0.0\n",
      "step: 3150 loss: 101.926805 time elapsed: 3.8393 learning rate: 0.003990, scenario: 0, slope: -0.17880258913691588, fluctuations: 0.0\n",
      "step: 3160 loss: 100.331977 time elapsed: 3.8512 learning rate: 0.003990, scenario: 0, slope: -0.17743210573228682, fluctuations: 0.0\n",
      "step: 3170 loss: 98.775076 time elapsed: 3.8631 learning rate: 0.003990, scenario: 0, slope: -0.1733983636435859, fluctuations: 0.0\n",
      "step: 3180 loss: 97.255286 time elapsed: 3.8749 learning rate: 0.003990, scenario: 0, slope: -0.1693266805023879, fluctuations: 0.0\n",
      "step: 3190 loss: 95.771793 time elapsed: 3.8863 learning rate: 0.003990, scenario: 0, slope: -0.16533218295442612, fluctuations: 0.0\n",
      "step: 3200 loss: 94.323783 time elapsed: 3.8981 learning rate: 0.003990, scenario: 0, slope: -0.16180666983012007, fluctuations: 0.0\n",
      "step: 3210 loss: 92.910442 time elapsed: 3.9103 learning rate: 0.003990, scenario: 0, slope: -0.15758820474986676, fluctuations: 0.0\n",
      "step: 3220 loss: 91.530950 time elapsed: 3.9223 learning rate: 0.003990, scenario: 0, slope: -0.1538395678157799, fluctuations: 0.0\n",
      "step: 3230 loss: 90.184484 time elapsed: 3.9346 learning rate: 0.003990, scenario: 0, slope: -0.150172916620521, fluctuations: 0.0\n",
      "step: 3240 loss: 88.870214 time elapsed: 3.9466 learning rate: 0.003990, scenario: 0, slope: -0.1465881144549362, fluctuations: 0.0\n",
      "step: 3250 loss: 87.587306 time elapsed: 3.9589 learning rate: 0.003990, scenario: 0, slope: -0.14308518469762443, fluctuations: 0.0\n",
      "step: 3260 loss: 86.334921 time elapsed: 3.9720 learning rate: 0.003990, scenario: 0, slope: -0.13966430550018305, fluctuations: 0.0\n",
      "step: 3270 loss: 85.112217 time elapsed: 3.9847 learning rate: 0.003990, scenario: 0, slope: -0.13632577393835485, fluctuations: 0.0\n",
      "step: 3280 loss: 83.918352 time elapsed: 3.9980 learning rate: 0.003990, scenario: 0, slope: -0.13306995643789576, fluctuations: 0.0\n",
      "step: 3290 loss: 82.752482 time elapsed: 4.0105 learning rate: 0.003990, scenario: 0, slope: -0.12989723350123844, fluctuations: 0.0\n",
      "step: 3300 loss: 81.613772 time elapsed: 4.0230 learning rate: 0.003990, scenario: 0, slope: -0.12711311030398656, fluctuations: 0.0\n",
      "step: 3310 loss: 80.501387 time elapsed: 4.0366 learning rate: 0.003990, scenario: 0, slope: -0.12380233089941221, fluctuations: 0.0\n",
      "step: 3320 loss: 79.414505 time elapsed: 4.0490 learning rate: 0.003990, scenario: 0, slope: -0.12088049681061445, fluctuations: 0.0\n",
      "step: 3330 loss: 78.352314 time elapsed: 4.0611 learning rate: 0.003990, scenario: 0, slope: -0.11804236217998325, fluctuations: 0.0\n",
      "step: 3340 loss: 77.314016 time elapsed: 4.0737 learning rate: 0.003990, scenario: 0, slope: -0.11528763655851319, fluctuations: 0.0\n",
      "step: 3350 loss: 76.298830 time elapsed: 4.0864 learning rate: 0.003990, scenario: 0, slope: -0.11261579784837951, fluctuations: 0.0\n",
      "step: 3360 loss: 75.305992 time elapsed: 4.0988 learning rate: 0.003990, scenario: 0, slope: -0.11002608116843511, fluctuations: 0.0\n",
      "step: 3370 loss: 74.334760 time elapsed: 4.1111 learning rate: 0.003990, scenario: 0, slope: -0.10751747637538571, fluctuations: 0.0\n",
      "step: 3380 loss: 73.384412 time elapsed: 4.1235 learning rate: 0.003990, scenario: 0, slope: -0.10508873321038488, fluctuations: 0.0\n",
      "step: 3390 loss: 72.454250 time elapsed: 4.1354 learning rate: 0.003990, scenario: 0, slope: -0.10273837285699924, fluctuations: 0.0\n",
      "step: 3400 loss: 71.543600 time elapsed: 4.1488 learning rate: 0.003990, scenario: 0, slope: -0.10068867254914031, fluctuations: 0.0\n",
      "step: 3410 loss: 70.651815 time elapsed: 4.1633 learning rate: 0.003990, scenario: 0, slope: -0.09826584656539154, fluctuations: 0.0\n",
      "step: 3420 loss: 69.778269 time elapsed: 4.1762 learning rate: 0.003990, scenario: 0, slope: -0.09613974868469767, fluctuations: 0.0\n",
      "step: 3430 loss: 68.922367 time elapsed: 4.1888 learning rate: 0.003990, scenario: 0, slope: -0.09408421800132132, fluctuations: 0.0\n",
      "step: 3440 loss: 68.083535 time elapsed: 4.2013 learning rate: 0.003990, scenario: 0, slope: -0.09209694426572243, fluctuations: 0.0\n",
      "step: 3450 loss: 67.261228 time elapsed: 4.2130 learning rate: 0.003990, scenario: 0, slope: -0.09017552576379671, fluctuations: 0.0\n",
      "step: 3460 loss: 66.454926 time elapsed: 4.2245 learning rate: 0.003990, scenario: 0, slope: -0.08831749454173843, fluctuations: 0.0\n",
      "step: 3470 loss: 65.664132 time elapsed: 4.2364 learning rate: 0.003990, scenario: 0, slope: -0.08652034059151872, fluctuations: 0.0\n",
      "step: 3480 loss: 64.888374 time elapsed: 4.2484 learning rate: 0.003990, scenario: 0, slope: -0.08478153465393896, fluctuations: 0.0\n",
      "step: 3490 loss: 64.127204 time elapsed: 4.2605 learning rate: 0.003990, scenario: 0, slope: -0.08309854940968182, fluctuations: 0.0\n",
      "step: 3500 loss: 63.380196 time elapsed: 4.2722 learning rate: 0.003990, scenario: 0, slope: -0.08162951749901336, fluctuations: 0.0\n",
      "step: 3510 loss: 62.646946 time elapsed: 4.2851 learning rate: 0.003990, scenario: 0, slope: -0.07989005632545781, fluctuations: 0.0\n",
      "step: 3520 loss: 61.927069 time elapsed: 4.2969 learning rate: 0.003990, scenario: 0, slope: -0.0783596696538217, fluctuations: 0.0\n",
      "step: 3530 loss: 61.220199 time elapsed: 4.3090 learning rate: 0.003990, scenario: 0, slope: -0.07687537607924387, fluctuations: 0.0\n",
      "step: 3540 loss: 60.525992 time elapsed: 4.3211 learning rate: 0.003990, scenario: 0, slope: -0.07543491444351189, fluctuations: 0.0\n",
      "step: 3550 loss: 59.844115 time elapsed: 4.3332 learning rate: 0.003990, scenario: 0, slope: -0.07403611629986931, fluctuations: 0.0\n",
      "step: 3560 loss: 59.174254 time elapsed: 4.3451 learning rate: 0.003990, scenario: 0, slope: -0.07267691550219244, fluctuations: 0.0\n",
      "step: 3570 loss: 58.516107 time elapsed: 4.3568 learning rate: 0.003990, scenario: 0, slope: -0.07135535639413086, fluctuations: 0.0\n",
      "step: 3580 loss: 57.869387 time elapsed: 4.3686 learning rate: 0.003990, scenario: 0, slope: -0.070069600600861, fluctuations: 0.0\n",
      "step: 3590 loss: 57.233817 time elapsed: 4.3805 learning rate: 0.003990, scenario: 0, slope: -0.0688179323763594, fluctuations: 0.0\n",
      "step: 3600 loss: 56.609130 time elapsed: 4.3924 learning rate: 0.003990, scenario: 0, slope: -0.06771925962918555, fluctuations: 0.0\n",
      "step: 3610 loss: 55.995068 time elapsed: 4.4046 learning rate: 0.003990, scenario: 0, slope: -0.06641062996833234, fluctuations: 0.0\n",
      "step: 3620 loss: 55.391380 time elapsed: 4.4168 learning rate: 0.003990, scenario: 0, slope: -0.06525220319584707, fluctuations: 0.0\n",
      "step: 3630 loss: 54.797824 time elapsed: 4.4287 learning rate: 0.003990, scenario: 0, slope: -0.06412227747212348, fluctuations: 0.0\n",
      "step: 3640 loss: 54.214162 time elapsed: 4.4407 learning rate: 0.003990, scenario: 0, slope: -0.06301977167651784, fluctuations: 0.0\n",
      "step: 3650 loss: 53.640162 time elapsed: 4.4526 learning rate: 0.003990, scenario: 0, slope: -0.061943722345847205, fluctuations: 0.0\n",
      "step: 3660 loss: 53.075596 time elapsed: 4.4645 learning rate: 0.003990, scenario: 0, slope: -0.06089327577168857, fluctuations: 0.0\n",
      "step: 3670 loss: 52.520242 time elapsed: 4.4764 learning rate: 0.003990, scenario: 0, slope: -0.05986767820629286, fluctuations: 0.0\n",
      "step: 3680 loss: 51.973880 time elapsed: 4.4885 learning rate: 0.003990, scenario: 0, slope: -0.05886626446403879, fluctuations: 0.0\n",
      "step: 3690 loss: 51.436295 time elapsed: 4.5006 learning rate: 0.003990, scenario: 0, slope: -0.057888445316318, fluctuations: 0.0\n",
      "step: 3700 loss: 50.907275 time elapsed: 4.5118 learning rate: 0.003990, scenario: 0, slope: -0.057028145153406866, fluctuations: 0.0\n",
      "step: 3710 loss: 50.386613 time elapsed: 4.5241 learning rate: 0.003990, scenario: 0, slope: -0.05600153354183444, fluctuations: 0.0\n",
      "step: 3720 loss: 49.747721 time elapsed: 4.5362 learning rate: 0.008552, scenario: 1, slope: -0.05527925744515246, fluctuations: 0.0\n",
      "step: 3730 loss: 48.778664 time elapsed: 4.5480 learning rate: 0.008552, scenario: 0, slope: -0.05667010712634883, fluctuations: 0.0\n",
      "step: 3740 loss: 5517.153445 time elapsed: 4.5601 learning rate: 0.004295, scenario: -1, slope: 12.52220750948687, fluctuations: 0.03\n",
      "step: 3750 loss: 947.864658 time elapsed: 4.5721 learning rate: 0.001498, scenario: -1, slope: 19.88544698623282, fluctuations: 0.05\n",
      "step: 3760 loss: 674.921802 time elapsed: 4.5844 learning rate: 0.000522, scenario: -1, slope: 19.34170485138021, fluctuations: 0.07\n",
      "step: 3770 loss: 422.229188 time elapsed: 4.5965 learning rate: 0.000182, scenario: -1, slope: 15.087265441531418, fluctuations: 0.07\n",
      "step: 3780 loss: 383.592157 time elapsed: 4.6085 learning rate: 0.000063, scenario: -1, slope: 10.470459496612992, fluctuations: 0.07\n",
      "step: 3790 loss: 368.793837 time elapsed: 4.6202 learning rate: 0.000022, scenario: -1, slope: 5.644096237842143, fluctuations: 0.07\n",
      "step: 3800 loss: 364.983462 time elapsed: 4.6317 learning rate: 0.000009, scenario: -1, slope: 1.033743829000622, fluctuations: 0.07\n",
      "step: 3810 loss: 363.122781 time elapsed: 4.6443 learning rate: 0.000009, scenario: 0, slope: -5.218481291252085, fluctuations: 0.07\n",
      "step: 3820 loss: 361.383359 time elapsed: 4.6565 learning rate: 0.000009, scenario: 0, slope: -11.869717913775474, fluctuations: 0.07\n",
      "step: 3830 loss: 359.745705 time elapsed: 4.6688 learning rate: 0.000009, scenario: 0, slope: -19.98786384769526, fluctuations: 0.07\n",
      "step: 3840 loss: 358.180340 time elapsed: 4.6811 learning rate: 0.000009, scenario: 0, slope: -10.516897964559275, fluctuations: 0.04\n",
      "step: 3850 loss: 356.665552 time elapsed: 4.6936 learning rate: 0.000009, scenario: 0, slope: -2.8583717980607344, fluctuations: 0.02\n",
      "step: 3860 loss: 355.185702 time elapsed: 4.7062 learning rate: 0.000009, scenario: 0, slope: -0.9214858154908758, fluctuations: 0.0\n",
      "step: 3870 loss: 353.715228 time elapsed: 4.7185 learning rate: 0.000012, scenario: 1, slope: -0.37019483670894904, fluctuations: 0.0\n",
      "step: 3880 loss: 350.947881 time elapsed: 4.7307 learning rate: 0.000031, scenario: 1, slope: -0.20240253368320765, fluctuations: 0.0\n",
      "step: 3890 loss: 343.904204 time elapsed: 4.7429 learning rate: 0.000081, scenario: 1, slope: -0.18461092066358636, fluctuations: 0.0\n",
      "step: 3900 loss: 325.958299 time elapsed: 4.7548 learning rate: 0.000191, scenario: 1, slope: -0.24097163627606677, fluctuations: 0.0\n",
      "step: 3910 loss: 285.850702 time elapsed: 4.7679 learning rate: 0.000339, scenario: 0, slope: -0.45893424667306026, fluctuations: 0.0\n",
      "step: 3920 loss: 238.839488 time elapsed: 4.7815 learning rate: 0.000339, scenario: 0, slope: -0.8605558780076221, fluctuations: 0.0\n",
      "step: 3930 loss: 198.987754 time elapsed: 4.7942 learning rate: 0.000339, scenario: 0, slope: -1.3833486513642816, fluctuations: 0.0\n",
      "step: 3940 loss: 167.997238 time elapsed: 4.8067 learning rate: 0.000339, scenario: 0, slope: -1.9317126268854055, fluctuations: 0.0\n",
      "step: 3950 loss: 145.104103 time elapsed: 4.8190 learning rate: 0.000339, scenario: 0, slope: -2.418805416432807, fluctuations: 0.0\n",
      "step: 3960 loss: 128.469633 time elapsed: 4.8307 learning rate: 0.000339, scenario: 0, slope: -2.7762821366947112, fluctuations: 0.0\n",
      "step: 3970 loss: 116.331913 time elapsed: 4.8427 learning rate: 0.000339, scenario: 0, slope: -2.9543607705834187, fluctuations: 0.0\n",
      "step: 3980 loss: 107.331633 time elapsed: 4.8550 learning rate: 0.000339, scenario: 0, slope: -2.9212670263675475, fluctuations: 0.0\n",
      "step: 3990 loss: 100.504563 time elapsed: 4.8669 learning rate: 0.000339, scenario: 0, slope: -2.67147055585363, fluctuations: 0.0\n",
      "step: 4000 loss: 95.189490 time elapsed: 4.8787 learning rate: 0.000339, scenario: 0, slope: -2.2889166577708595, fluctuations: 0.0\n",
      "step: 4010 loss: 90.938620 time elapsed: 4.8910 learning rate: 0.000339, scenario: 0, slope: -1.7387840189806052, fluctuations: 0.0\n",
      "step: 4020 loss: 87.450534 time elapsed: 4.9029 learning rate: 0.000339, scenario: 0, slope: -1.309441309082762, fluctuations: 0.0\n",
      "step: 4030 loss: 84.522540 time elapsed: 4.9146 learning rate: 0.000339, scenario: 0, slope: -0.9791030147698726, fluctuations: 0.0\n",
      "step: 4040 loss: 82.017487 time elapsed: 4.9268 learning rate: 0.000339, scenario: 0, slope: -0.7386450959956097, fluctuations: 0.0\n",
      "step: 4050 loss: 79.841160 time elapsed: 4.9384 learning rate: 0.000339, scenario: 0, slope: -0.568025290205662, fluctuations: 0.0\n",
      "step: 4060 loss: 77.927293 time elapsed: 4.9504 learning rate: 0.000339, scenario: 0, slope: -0.4474588378498629, fluctuations: 0.0\n",
      "step: 4070 loss: 76.227887 time elapsed: 4.9624 learning rate: 0.000339, scenario: 0, slope: -0.36150051309804043, fluctuations: 0.0\n",
      "step: 4080 loss: 74.707038 time elapsed: 4.9743 learning rate: 0.000339, scenario: 0, slope: -0.29914825402097894, fluctuations: 0.0\n",
      "step: 4090 loss: 73.337016 time elapsed: 4.9863 learning rate: 0.000339, scenario: 0, slope: -0.25287967943736367, fluctuations: 0.0\n",
      "step: 4100 loss: 72.095782 time elapsed: 4.9981 learning rate: 0.000339, scenario: 0, slope: -0.22077825142955587, fluctuations: 0.0\n",
      "step: 4110 loss: 70.965410 time elapsed: 5.0110 learning rate: 0.000339, scenario: 0, slope: -0.19008850428445062, fluctuations: 0.0\n",
      "step: 4120 loss: 69.931063 time elapsed: 5.0235 learning rate: 0.000339, scenario: 0, slope: -0.16798848591697224, fluctuations: 0.0\n",
      "step: 4130 loss: 68.980304 time elapsed: 5.0353 learning rate: 0.000339, scenario: 0, slope: -0.14987255317080098, fluctuations: 0.0\n",
      "step: 4140 loss: 68.102623 time elapsed: 5.0474 learning rate: 0.000339, scenario: 0, slope: -0.1347536501386917, fluctuations: 0.0\n",
      "step: 4150 loss: 67.289091 time elapsed: 5.0594 learning rate: 0.000339, scenario: 0, slope: -0.12195685011818963, fluctuations: 0.0\n",
      "step: 4160 loss: 66.532087 time elapsed: 5.0713 learning rate: 0.000339, scenario: 0, slope: -0.11100760515897252, fluctuations: 0.0\n",
      "step: 4170 loss: 65.825089 time elapsed: 5.0833 learning rate: 0.000339, scenario: 0, slope: -0.10156101779456586, fluctuations: 0.0\n",
      "step: 4180 loss: 65.162500 time elapsed: 5.0952 learning rate: 0.000339, scenario: 0, slope: -0.09335753809126703, fluctuations: 0.0\n",
      "step: 4190 loss: 64.539500 time elapsed: 5.1073 learning rate: 0.000339, scenario: 0, slope: -0.08619535987062327, fluctuations: 0.0\n",
      "step: 4200 loss: 63.951931 time elapsed: 5.1189 learning rate: 0.000339, scenario: 0, slope: -0.08050576961498737, fluctuations: 0.0\n",
      "step: 4210 loss: 63.396194 time elapsed: 5.1315 learning rate: 0.000339, scenario: 0, slope: -0.07437948133793443, fluctuations: 0.0\n",
      "step: 4220 loss: 62.869166 time elapsed: 5.1434 learning rate: 0.000372, scenario: 1, slope: -0.0694851980299044, fluctuations: 0.0\n",
      "step: 4230 loss: 62.079292 time elapsed: 5.1552 learning rate: 0.000966, scenario: 1, slope: -0.06571051326836001, fluctuations: 0.0\n",
      "step: 4240 loss: 60.250934 time elapsed: 5.1671 learning rate: 0.002506, scenario: 1, slope: -0.06678079380290912, fluctuations: 0.0\n",
      "step: 4250 loss: 57.454241 time elapsed: 5.1793 learning rate: 0.002756, scenario: 0, slope: -0.07832814748500799, fluctuations: 0.0\n",
      "step: 4260 loss: 55.314585 time elapsed: 5.1912 learning rate: 0.002756, scenario: 0, slope: -0.0974272068164498, fluctuations: 0.0\n",
      "step: 4270 loss: 53.668351 time elapsed: 5.2037 learning rate: 0.002756, scenario: 0, slope: -0.11852287971940599, fluctuations: 0.0\n",
      "step: 4280 loss: 52.355395 time elapsed: 5.2158 learning rate: 0.002756, scenario: 0, slope: -0.13782304068159956, fluctuations: 0.0\n",
      "step: 4290 loss: 51.278061 time elapsed: 5.2277 learning rate: 0.002756, scenario: 0, slope: -0.1526453498834901, fluctuations: 0.0\n",
      "step: 4300 loss: 50.370880 time elapsed: 5.2395 learning rate: 0.002756, scenario: 0, slope: -0.1605332688670581, fluctuations: 0.0\n",
      "step: 4310 loss: 49.588645 time elapsed: 5.2521 learning rate: 0.002756, scenario: 0, slope: -0.16155869519858937, fluctuations: 0.0\n",
      "step: 4320 loss: 48.899669 time elapsed: 5.2642 learning rate: 0.002756, scenario: 0, slope: -0.1531142416126163, fluctuations: 0.0\n",
      "step: 4330 loss: 48.281350 time elapsed: 5.2761 learning rate: 0.002756, scenario: 0, slope: -0.1354968678462523, fluctuations: 0.0\n",
      "step: 4340 loss: 47.717455 time elapsed: 5.2883 learning rate: 0.002756, scenario: 0, slope: -0.11279320806057185, fluctuations: 0.0\n",
      "step: 4350 loss: 47.196297 time elapsed: 5.3004 learning rate: 0.002756, scenario: 0, slope: -0.09363782863852381, fluctuations: 0.0\n",
      "step: 4360 loss: 46.709421 time elapsed: 5.3123 learning rate: 0.002756, scenario: 0, slope: -0.07992415836717445, fluctuations: 0.0\n",
      "step: 4370 loss: 46.250645 time elapsed: 5.3244 learning rate: 0.002756, scenario: 0, slope: -0.06985525630626867, fluctuations: 0.0\n",
      "step: 4380 loss: 45.815379 time elapsed: 5.3364 learning rate: 0.002756, scenario: 0, slope: -0.062306360879034704, fluctuations: 0.0\n",
      "step: 4390 loss: 45.400137 time elapsed: 5.3490 learning rate: 0.002756, scenario: 0, slope: -0.056538977715224016, fluctuations: 0.0\n",
      "step: 4400 loss: 45.002216 time elapsed: 5.3605 learning rate: 0.002756, scenario: 0, slope: -0.052449417307965006, fluctuations: 0.0\n",
      "step: 4410 loss: 44.595357 time elapsed: 5.3729 learning rate: 0.004439, scenario: 1, slope: -0.04849981135677481, fluctuations: 0.0\n",
      "step: 4420 loss: 43.749440 time elapsed: 5.3851 learning rate: 0.011513, scenario: 1, slope: -0.04686037176345909, fluctuations: 0.0\n",
      "step: 4430 loss: 41.854407 time elapsed: 5.3978 learning rate: 0.020397, scenario: 0, slope: -0.0510823501342697, fluctuations: 0.0\n",
      "step: 4440 loss: 28649.941390 time elapsed: 5.4104 learning rate: 0.011382, scenario: -1, slope: 97.92238762174512, fluctuations: 0.02\n",
      "step: 4450 loss: 16156.717852 time elapsed: 5.4227 learning rate: 0.003969, scenario: -1, slope: 213.45767210234013, fluctuations: 0.03\n",
      "step: 4460 loss: 8867.403784 time elapsed: 5.4352 learning rate: 0.001384, scenario: -1, slope: 217.32937563280285, fluctuations: 0.03\n",
      "step: 4470 loss: 6746.304397 time elapsed: 5.4473 learning rate: 0.000482, scenario: -1, slope: 192.07851423701078, fluctuations: 0.03\n",
      "step: 4480 loss: 6294.529721 time elapsed: 5.4588 learning rate: 0.000168, scenario: -1, slope: 154.0083175499592, fluctuations: 0.03\n",
      "step: 4490 loss: 6146.245841 time elapsed: 5.4709 learning rate: 0.000059, scenario: -1, slope: 108.045583991875, fluctuations: 0.03\n",
      "step: 4500 loss: 6095.658001 time elapsed: 5.4828 learning rate: 0.000023, scenario: -1, slope: 60.14690100914626, fluctuations: 0.03\n",
      "step: 4510 loss: 6076.843602 time elapsed: 5.4955 learning rate: 0.000010, scenario: 0, slope: -7.793032991355914, fluctuations: 0.03\n",
      "step: 4520 loss: 6065.661017 time elapsed: 5.5074 learning rate: 0.000010, scenario: 0, slope: -80.23536448822705, fluctuations: 0.03\n",
      "step: 4530 loss: 6054.597526 time elapsed: 5.5193 learning rate: 0.000010, scenario: 0, slope: -165.26498070601278, fluctuations: 0.03\n",
      "step: 4540 loss: 6043.636226 time elapsed: 5.5313 learning rate: 0.000010, scenario: 0, slope: -128.21976209233245, fluctuations: 0.01\n",
      "step: 4550 loss: 6032.767711 time elapsed: 5.5433 learning rate: 0.000010, scenario: 0, slope: -32.75690555477132, fluctuations: 0.0\n",
      "step: 4560 loss: 6021.985477 time elapsed: 5.5554 learning rate: 0.000010, scenario: 0, slope: -10.40094050355486, fluctuations: 0.0\n",
      "step: 4570 loss: 6008.632803 time elapsed: 5.5669 learning rate: 0.000021, scenario: 1, slope: -3.4498799407098257, fluctuations: 0.0\n",
      "step: 4580 loss: 5975.872368 time elapsed: 5.5781 learning rate: 0.000054, scenario: 1, slope: -1.799814537369756, fluctuations: 0.0\n",
      "step: 4590 loss: 5893.583915 time elapsed: 5.5897 learning rate: 0.000141, scenario: 1, slope: -1.5946243323176803, fluctuations: 0.0\n",
      "step: 4600 loss: 5694.862556 time elapsed: 5.6015 learning rate: 0.000333, scenario: 1, slope: -2.274931291417724, fluctuations: 0.0\n",
      "step: 4610 loss: 5281.178020 time elapsed: 5.6147 learning rate: 0.000863, scenario: 1, slope: -4.723412226017079, fluctuations: 0.0\n",
      "step: 4620 loss: 4559.405835 time elapsed: 5.6268 learning rate: 0.001148, scenario: 0, slope: -9.69992318472854, fluctuations: 0.0\n",
      "step: 4630 loss: 3990.012482 time elapsed: 5.6389 learning rate: 0.001148, scenario: 0, slope: -16.876540765050272, fluctuations: 0.0\n",
      "step: 4640 loss: 3563.021658 time elapsed: 5.6506 learning rate: 0.001148, scenario: 0, slope: -24.524332984538518, fluctuations: 0.0\n",
      "step: 4650 loss: 3201.999430 time elapsed: 5.6625 learning rate: 0.001148, scenario: 0, slope: -31.56536114109944, fluctuations: 0.0\n",
      "step: 4660 loss: 2886.539359 time elapsed: 5.6745 learning rate: 0.001148, scenario: 0, slope: -37.27528605670444, fluctuations: 0.0\n",
      "step: 4670 loss: 2622.403901 time elapsed: 5.6865 learning rate: 0.001148, scenario: 0, slope: -41.00560155084392, fluctuations: 0.0\n",
      "step: 4680 loss: 2405.505881 time elapsed: 5.6985 learning rate: 0.001148, scenario: 0, slope: -42.222394362436994, fluctuations: 0.0\n",
      "step: 4690 loss: 2221.480232 time elapsed: 5.7106 learning rate: 0.001148, scenario: 0, slope: -40.678875802602946, fluctuations: 0.0\n",
      "step: 4700 loss: 2062.655861 time elapsed: 5.7224 learning rate: 0.001148, scenario: 0, slope: -37.094004678477674, fluctuations: 0.0\n",
      "step: 4710 loss: 1927.958680 time elapsed: 5.7345 learning rate: 0.001148, scenario: 0, slope: -30.882380328160703, fluctuations: 0.0\n",
      "step: 4720 loss: 1814.807698 time elapsed: 5.7463 learning rate: 0.001148, scenario: 0, slope: -25.571514333835413, fluctuations: 0.0\n",
      "step: 4730 loss: 1718.258888 time elapsed: 5.7580 learning rate: 0.001148, scenario: 0, slope: -21.56022899805403, fluctuations: 0.0\n",
      "step: 4740 loss: 1633.992443 time elapsed: 5.7704 learning rate: 0.001148, scenario: 0, slope: -18.267715784126462, fluctuations: 0.0\n",
      "step: 4750 loss: 1559.277702 time elapsed: 5.7838 learning rate: 0.001148, scenario: 0, slope: -15.467626202982645, fluctuations: 0.0\n",
      "step: 4760 loss: 1492.381280 time elapsed: 5.7972 learning rate: 0.001148, scenario: 0, slope: -13.146593163929493, fluctuations: 0.0\n",
      "step: 4770 loss: 1432.040449 time elapsed: 5.8108 learning rate: 0.001148, scenario: 0, slope: -11.266709602559276, fluctuations: 0.0\n",
      "step: 4780 loss: 1377.246403 time elapsed: 5.8229 learning rate: 0.001148, scenario: 0, slope: -9.730907010436049, fluctuations: 0.0\n",
      "step: 4790 loss: 1327.159284 time elapsed: 5.8361 learning rate: 0.001148, scenario: 0, slope: -8.466557249944085, fluctuations: 0.0\n",
      "step: 4800 loss: 1281.065287 time elapsed: 5.8477 learning rate: 0.001148, scenario: 0, slope: -7.533635854884153, fluctuations: 0.0\n",
      "step: 4810 loss: 1238.343978 time elapsed: 5.8601 learning rate: 0.001148, scenario: 0, slope: -6.619581189294517, fluctuations: 0.0\n",
      "step: 4820 loss: 1198.434571 time elapsed: 5.8719 learning rate: 0.001148, scenario: 0, slope: -5.9554004010542725, fluctuations: 0.0\n",
      "step: 4830 loss: 1160.803329 time elapsed: 5.8836 learning rate: 0.001148, scenario: 0, slope: -5.407357306550587, fluctuations: 0.0\n",
      "step: 4840 loss: 1124.969818 time elapsed: 5.8958 learning rate: 0.001148, scenario: 0, slope: -4.949219742751215, fluctuations: 0.0\n",
      "step: 4850 loss: 1090.804359 time elapsed: 5.9081 learning rate: 0.001148, scenario: 0, slope: -4.564762577632171, fluctuations: 0.0\n",
      "step: 4860 loss: 1058.751234 time elapsed: 5.9202 learning rate: 0.001148, scenario: 0, slope: -4.239823534774847, fluctuations: 0.0\n",
      "step: 4870 loss: 1028.699465 time elapsed: 5.9321 learning rate: 0.001148, scenario: 0, slope: -3.9606848610935175, fluctuations: 0.0\n",
      "step: 4880 loss: 1000.348137 time elapsed: 5.9440 learning rate: 0.001148, scenario: 0, slope: -3.71677975664072, fluctuations: 0.0\n",
      "step: 4890 loss: 973.535361 time elapsed: 5.9559 learning rate: 0.001148, scenario: 0, slope: -3.4995316147023297, fluctuations: 0.0\n",
      "step: 4900 loss: 948.062677 time elapsed: 5.9680 learning rate: 0.001148, scenario: 0, slope: -3.3213401845641597, fluctuations: 0.0\n",
      "step: 4910 loss: 923.792258 time elapsed: 5.9805 learning rate: 0.001148, scenario: 0, slope: -3.120522031833913, fluctuations: 0.0\n",
      "step: 4920 loss: 900.605670 time elapsed: 5.9925 learning rate: 0.001148, scenario: 0, slope: -2.950855322062351, fluctuations: 0.0\n",
      "step: 4930 loss: 878.406826 time elapsed: 6.0048 learning rate: 0.001148, scenario: 0, slope: -2.792120320589458, fluctuations: 0.0\n",
      "step: 4940 loss: 857.114795 time elapsed: 6.0172 learning rate: 0.001148, scenario: 0, slope: -2.645140056512307, fluctuations: 0.0\n",
      "step: 4950 loss: 836.662450 time elapsed: 6.0295 learning rate: 0.001148, scenario: 0, slope: -2.5120622913443396, fluctuations: 0.0\n",
      "step: 4960 loss: 816.993874 time elapsed: 6.0416 learning rate: 0.001148, scenario: 0, slope: -2.3929886364515554, fluctuations: 0.0\n",
      "step: 4970 loss: 798.062638 time elapsed: 6.0535 learning rate: 0.001148, scenario: 0, slope: -2.2854098971120385, fluctuations: 0.0\n",
      "step: 4980 loss: 779.830347 time elapsed: 6.0652 learning rate: 0.001148, scenario: 0, slope: -2.1875118081929745, fluctuations: 0.0\n",
      "step: 4990 loss: 762.265372 time elapsed: 6.0770 learning rate: 0.001148, scenario: 0, slope: -2.097577144189526, fluctuations: 0.0\n",
      "step: 5000 loss: 745.341702 time elapsed: 6.0888 learning rate: 0.001148, scenario: 0, slope: -2.0222966355001484, fluctuations: 0.0\n",
      "step: 5010 loss: 729.037804 time elapsed: 6.1010 learning rate: 0.001148, scenario: 0, slope: -1.9363315916777664, fluctuations: 0.0\n",
      "step: 5020 loss: 713.335465 time elapsed: 6.1127 learning rate: 0.001148, scenario: 0, slope: -1.862972734383434, fluctuations: 0.0\n",
      "step: 5030 loss: 698.218524 time elapsed: 6.1249 learning rate: 0.001148, scenario: 0, slope: -1.7933760466137174, fluctuations: 0.0\n",
      "step: 5040 loss: 683.671571 time elapsed: 6.1367 learning rate: 0.001148, scenario: 0, slope: -1.7269113195218995, fluctuations: 0.0\n",
      "step: 5050 loss: 669.678728 time elapsed: 6.1485 learning rate: 0.001148, scenario: 0, slope: -1.6630777577443572, fluctuations: 0.0\n",
      "step: 5060 loss: 656.222688 time elapsed: 6.1606 learning rate: 0.001148, scenario: 0, slope: -1.6014978579514478, fluctuations: 0.0\n",
      "step: 5070 loss: 643.284164 time elapsed: 6.1724 learning rate: 0.001148, scenario: 0, slope: -1.5419121368047441, fluctuations: 0.0\n",
      "step: 5080 loss: 630.841808 time elapsed: 6.1846 learning rate: 0.001148, scenario: 0, slope: -1.4841716300816106, fluctuations: 0.0\n",
      "step: 5090 loss: 618.872524 time elapsed: 6.1966 learning rate: 0.001148, scenario: 0, slope: -1.4282254819679465, fluctuations: 0.0\n",
      "step: 5100 loss: 607.352052 time elapsed: 6.2085 learning rate: 0.001148, scenario: 0, slope: -1.379430873499243, fluctuations: 0.0\n",
      "step: 5110 loss: 596.255644 time elapsed: 6.2205 learning rate: 0.001148, scenario: 0, slope: -1.3218892781886393, fluctuations: 0.0\n",
      "step: 5120 loss: 585.558712 time elapsed: 6.2325 learning rate: 0.001148, scenario: 0, slope: -1.2717011966046587, fluctuations: 0.0\n",
      "step: 5130 loss: 575.237352 time elapsed: 6.2448 learning rate: 0.001148, scenario: 0, slope: -1.223659028389, fluctuations: 0.0\n",
      "step: 5140 loss: 565.268752 time elapsed: 6.2569 learning rate: 0.001148, scenario: 0, slope: -1.1778648821794584, fluctuations: 0.0\n",
      "step: 5150 loss: 555.631463 time elapsed: 6.2685 learning rate: 0.001148, scenario: 0, slope: -1.1343859198856967, fluctuations: 0.0\n",
      "step: 5160 loss: 546.305587 time elapsed: 6.2805 learning rate: 0.001148, scenario: 0, slope: -1.0932452472408078, fluctuations: 0.0\n",
      "step: 5170 loss: 537.272896 time elapsed: 6.2921 learning rate: 0.001148, scenario: 0, slope: -1.054419966766388, fluctuations: 0.0\n",
      "step: 5180 loss: 528.516918 time elapsed: 6.3043 learning rate: 0.001148, scenario: 0, slope: -1.0178447521264335, fluctuations: 0.0\n",
      "step: 5190 loss: 520.023006 time elapsed: 6.3161 learning rate: 0.001148, scenario: 0, slope: -0.9834186301369946, fluctuations: 0.0\n",
      "step: 5200 loss: 511.778401 time elapsed: 6.3279 learning rate: 0.001148, scenario: 0, slope: -0.9541665576059917, fluctuations: 0.0\n",
      "step: 5210 loss: 503.772279 time elapsed: 6.3403 learning rate: 0.001148, scenario: 0, slope: -0.9204775689238507, fluctuations: 0.0\n",
      "step: 5220 loss: 495.995769 time elapsed: 6.3521 learning rate: 0.001148, scenario: 0, slope: -0.8916486620189339, fluctuations: 0.0\n",
      "step: 5230 loss: 488.441917 time elapsed: 6.3642 learning rate: 0.001148, scenario: 0, slope: -0.8643512443057663, fluctuations: 0.0\n",
      "step: 5240 loss: 481.105531 time elapsed: 6.3763 learning rate: 0.001148, scenario: 0, slope: -0.8384039228760147, fluctuations: 0.0\n",
      "step: 5250 loss: 473.982875 time elapsed: 6.3883 learning rate: 0.001148, scenario: 0, slope: -0.8136226539391829, fluctuations: 0.0\n",
      "step: 5260 loss: 467.071157 time elapsed: 6.4003 learning rate: 0.001148, scenario: 0, slope: -0.7898259299448639, fluctuations: 0.0\n",
      "step: 5270 loss: 460.367810 time elapsed: 6.4123 learning rate: 0.001148, scenario: 0, slope: -0.7668420552221851, fluctuations: 0.0\n",
      "step: 5280 loss: 453.869646 time elapsed: 6.4242 learning rate: 0.001148, scenario: 0, slope: -0.7445188149461596, fluctuations: 0.0\n",
      "step: 5290 loss: 447.572030 time elapsed: 6.4369 learning rate: 0.001148, scenario: 0, slope: -0.7227349038487136, fluctuations: 0.0\n",
      "step: 5300 loss: 441.468276 time elapsed: 6.4493 learning rate: 0.001148, scenario: 0, slope: -0.7035241951701677, fluctuations: 0.0\n",
      "step: 5310 loss: 435.549470 time elapsed: 6.4622 learning rate: 0.001148, scenario: 0, slope: -0.6805200720225011, fluctuations: 0.0\n",
      "step: 5320 loss: 429.804762 time elapsed: 6.4740 learning rate: 0.001148, scenario: 0, slope: -0.6600870955661047, fluctuations: 0.0\n",
      "step: 5330 loss: 424.222032 time elapsed: 6.4859 learning rate: 0.001148, scenario: 0, slope: -0.6401877446225934, fluctuations: 0.0\n",
      "step: 5340 loss: 418.788708 time elapsed: 6.4981 learning rate: 0.001148, scenario: 0, slope: -0.6209343543167718, fluctuations: 0.0\n",
      "step: 5350 loss: 413.492479 time elapsed: 6.5100 learning rate: 0.001148, scenario: 0, slope: -0.602458000417057, fluctuations: 0.0\n",
      "step: 5360 loss: 408.321760 time elapsed: 6.5218 learning rate: 0.001148, scenario: 0, slope: -0.5848878740050117, fluctuations: 0.0\n",
      "step: 5370 loss: 403.265870 time elapsed: 6.5334 learning rate: 0.001148, scenario: 0, slope: -0.5683323721176867, fluctuations: 0.0\n",
      "step: 5380 loss: 398.314983 time elapsed: 6.5451 learning rate: 0.001148, scenario: 0, slope: -0.5528655539275005, fluctuations: 0.0\n",
      "step: 5390 loss: 393.459949 time elapsed: 6.5572 learning rate: 0.001148, scenario: 0, slope: -0.538521294457713, fluctuations: 0.0\n",
      "step: 5400 loss: 388.692064 time elapsed: 6.5687 learning rate: 0.001148, scenario: 0, slope: -0.5265685826138496, fluctuations: 0.0\n",
      "step: 5410 loss: 384.002866 time elapsed: 6.5813 learning rate: 0.001148, scenario: 0, slope: -0.5131553628797599, fluctuations: 0.0\n",
      "step: 5420 loss: 379.383982 time elapsed: 6.5932 learning rate: 0.001148, scenario: 0, slope: -0.5020509521515741, fluctuations: 0.0\n",
      "step: 5430 loss: 374.827066 time elapsed: 6.6048 learning rate: 0.001148, scenario: 0, slope: -0.49192825050951833, fluctuations: 0.0\n",
      "step: 5440 loss: 370.323854 time elapsed: 6.6170 learning rate: 0.001148, scenario: 0, slope: -0.4827379276587019, fluctuations: 0.0\n",
      "step: 5450 loss: 365.866371 time elapsed: 6.6295 learning rate: 0.001148, scenario: 0, slope: -0.4744397276676372, fluctuations: 0.0\n",
      "step: 5460 loss: 361.447337 time elapsed: 6.6420 learning rate: 0.001148, scenario: 0, slope: -0.4670014438201021, fluctuations: 0.0\n",
      "step: 5470 loss: 357.060822 time elapsed: 6.6541 learning rate: 0.001148, scenario: 0, slope: -0.46039287088902103, fluctuations: 0.0\n",
      "step: 5480 loss: 352.703132 time elapsed: 6.6661 learning rate: 0.001148, scenario: 0, slope: -0.45457538805711845, fluctuations: 0.0\n",
      "step: 5490 loss: 348.373871 time elapsed: 6.6780 learning rate: 0.001148, scenario: 0, slope: -0.4494881017986301, fluctuations: 0.0\n",
      "step: 5500 loss: 344.076929 time elapsed: 6.6896 learning rate: 0.001148, scenario: 0, slope: -0.4454532200045689, fluctuations: 0.0\n",
      "step: 5510 loss: 339.821015 time elapsed: 6.7021 learning rate: 0.001148, scenario: 0, slope: -0.44105658349502425, fluctuations: 0.0\n",
      "step: 5520 loss: 335.619250 time elapsed: 6.7140 learning rate: 0.001148, scenario: 0, slope: -0.437349704136326, fluctuations: 0.0\n",
      "step: 5530 loss: 331.487558 time elapsed: 6.7262 learning rate: 0.001148, scenario: 0, slope: -0.4336433501010658, fluctuations: 0.0\n",
      "step: 5540 loss: 327.442069 time elapsed: 6.7380 learning rate: 0.001148, scenario: 0, slope: -0.4296327456816142, fluctuations: 0.0\n",
      "step: 5550 loss: 323.496319 time elapsed: 6.7495 learning rate: 0.001148, scenario: 0, slope: -0.4250124908634501, fluctuations: 0.0\n",
      "step: 5560 loss: 319.659240 time elapsed: 6.7609 learning rate: 0.001148, scenario: 0, slope: -0.41952170229746555, fluctuations: 0.0\n",
      "step: 5570 loss: 315.934497 time elapsed: 6.7730 learning rate: 0.001148, scenario: 0, slope: -0.41298729854238314, fluctuations: 0.0\n",
      "step: 5580 loss: 312.321120 time elapsed: 6.7849 learning rate: 0.001148, scenario: 0, slope: -0.4053542075336699, fluctuations: 0.0\n",
      "step: 5590 loss: 308.814857 time elapsed: 6.7970 learning rate: 0.001148, scenario: 0, slope: -0.3966949289643123, fluctuations: 0.0\n",
      "step: 5600 loss: 305.409645 time elapsed: 6.8088 learning rate: 0.001148, scenario: 0, slope: -0.38817664035329313, fluctuations: 0.0\n",
      "step: 5610 loss: 302.098817 time elapsed: 6.8214 learning rate: 0.001148, scenario: 0, slope: -0.37712553470707766, fluctuations: 0.0\n",
      "step: 5620 loss: 298.875905 time elapsed: 6.8337 learning rate: 0.001148, scenario: 0, slope: -0.3667854579436662, fluctuations: 0.0\n",
      "step: 5630 loss: 295.735066 time elapsed: 6.8468 learning rate: 0.001148, scenario: 0, slope: -0.3564652663711726, fluctuations: 0.0\n",
      "step: 5640 loss: 292.671240 time elapsed: 6.8590 learning rate: 0.001148, scenario: 0, slope: -0.346402276135196, fluctuations: 0.0\n",
      "step: 5650 loss: 289.680141 time elapsed: 6.8713 learning rate: 0.001148, scenario: 0, slope: -0.3367602095108182, fluctuations: 0.0\n",
      "step: 5660 loss: 286.758145 time elapsed: 6.8833 learning rate: 0.001148, scenario: 0, slope: -0.32762675300888233, fluctuations: 0.0\n",
      "step: 5670 loss: 283.902158 time elapsed: 6.8950 learning rate: 0.001148, scenario: 0, slope: -0.31902587402190513, fluctuations: 0.0\n",
      "step: 5680 loss: 281.081835 time elapsed: 6.9062 learning rate: 0.001528, scenario: 1, slope: -0.31095372020332956, fluctuations: 0.0\n",
      "step: 5690 loss: 275.857601 time elapsed: 6.9178 learning rate: 0.003604, scenario: 0, slope: -0.30901958077192937, fluctuations: 0.0\n",
      "step: 5700 loss: 267.783584 time elapsed: 6.9296 learning rate: 0.003604, scenario: 0, slope: -0.3276919603533267, fluctuations: 0.0\n",
      "step: 5710 loss: 260.255048 time elapsed: 6.9418 learning rate: 0.003604, scenario: 0, slope: -0.37437441100489866, fluctuations: 0.0\n",
      "step: 5720 loss: 253.234185 time elapsed: 6.9537 learning rate: 0.003604, scenario: 0, slope: -0.4303764011467367, fluctuations: 0.0\n",
      "step: 5730 loss: 246.663395 time elapsed: 6.9657 learning rate: 0.003604, scenario: 0, slope: -0.49110532156896686, fluctuations: 0.0\n",
      "step: 5740 loss: 240.489708 time elapsed: 6.9777 learning rate: 0.003604, scenario: 0, slope: -0.5494635188979294, fluctuations: 0.0\n",
      "step: 5750 loss: 234.669034 time elapsed: 6.9893 learning rate: 0.003604, scenario: 0, slope: -0.5990519745621518, fluctuations: 0.0\n",
      "step: 5760 loss: 229.164903 time elapsed: 7.0015 learning rate: 0.003604, scenario: 0, slope: -0.6340685451572717, fluctuations: 0.0\n",
      "step: 5770 loss: 223.946674 time elapsed: 7.0135 learning rate: 0.003604, scenario: 0, slope: -0.6492255152371252, fluctuations: 0.0\n",
      "step: 5780 loss: 218.988171 time elapsed: 7.0257 learning rate: 0.003604, scenario: 0, slope: -0.6397005129814556, fluctuations: 0.0\n",
      "step: 5790 loss: 214.266737 time elapsed: 7.0373 learning rate: 0.003604, scenario: 0, slope: -0.6073877987448382, fluctuations: 0.0\n",
      "step: 5800 loss: 209.762571 time elapsed: 7.0491 learning rate: 0.003604, scenario: 0, slope: -0.5759583089089072, fluctuations: 0.0\n",
      "step: 5810 loss: 205.458252 time elapsed: 7.0617 learning rate: 0.003604, scenario: 0, slope: -0.5414529851307797, fluctuations: 0.0\n",
      "step: 5820 loss: 201.338350 time elapsed: 7.0737 learning rate: 0.003604, scenario: 0, slope: -0.5133285198407356, fluctuations: 0.0\n",
      "step: 5830 loss: 197.389127 time elapsed: 7.0857 learning rate: 0.003604, scenario: 0, slope: -0.4878277173073915, fluctuations: 0.0\n",
      "step: 5840 loss: 193.598301 time elapsed: 7.0976 learning rate: 0.003604, scenario: 0, slope: -0.4645808451854739, fluctuations: 0.0\n",
      "step: 5850 loss: 189.954862 time elapsed: 7.1092 learning rate: 0.003604, scenario: 0, slope: -0.44329102755636857, fluctuations: 0.0\n",
      "step: 5860 loss: 186.448919 time elapsed: 7.1209 learning rate: 0.003604, scenario: 0, slope: -0.4237165255642476, fluctuations: 0.0\n",
      "step: 5870 loss: 183.071569 time elapsed: 7.1326 learning rate: 0.003604, scenario: 0, slope: -0.40565651408954817, fluctuations: 0.0\n",
      "step: 5880 loss: 179.814783 time elapsed: 7.1450 learning rate: 0.003604, scenario: 0, slope: -0.3889412488209734, fluctuations: 0.0\n",
      "step: 5890 loss: 176.671297 time elapsed: 7.1572 learning rate: 0.003604, scenario: 0, slope: -0.37342538421293203, fluctuations: 0.0\n",
      "step: 5900 loss: 173.634514 time elapsed: 7.1687 learning rate: 0.003604, scenario: 0, slope: -0.36038238607407386, fluctuations: 0.0\n",
      "step: 5910 loss: 170.698418 time elapsed: 7.1811 learning rate: 0.003604, scenario: 0, slope: -0.34550544959987173, fluctuations: 0.0\n",
      "step: 5920 loss: 167.857492 time elapsed: 7.1930 learning rate: 0.003604, scenario: 0, slope: -0.3328962380561704, fluctuations: 0.0\n",
      "step: 5930 loss: 165.106649 time elapsed: 7.2047 learning rate: 0.003604, scenario: 0, slope: -0.32107175267236165, fluctuations: 0.0\n",
      "step: 5940 loss: 162.441174 time elapsed: 7.2167 learning rate: 0.003604, scenario: 0, slope: -0.30995856890503476, fluctuations: 0.0\n",
      "step: 5950 loss: 159.856675 time elapsed: 7.2291 learning rate: 0.003604, scenario: 0, slope: -0.2994925710593261, fluctuations: 0.0\n",
      "step: 5960 loss: 157.349042 time elapsed: 7.2406 learning rate: 0.003604, scenario: 0, slope: -0.2896179404580184, fluctuations: 0.0\n",
      "step: 5970 loss: 154.914416 time elapsed: 7.2532 learning rate: 0.003604, scenario: 0, slope: -0.28028619439896574, fluctuations: 0.0\n",
      "step: 5980 loss: 152.549161 time elapsed: 7.2659 learning rate: 0.003604, scenario: 0, slope: -0.2714552497369805, fluctuations: 0.0\n",
      "step: 5990 loss: 150.249845 time elapsed: 7.2780 learning rate: 0.003604, scenario: 0, slope: -0.26308851700945923, fluctuations: 0.0\n",
      "step: 6000 loss: 148.013221 time elapsed: 7.2897 learning rate: 0.003604, scenario: 0, slope: -0.2559288690076214, fluctuations: 0.0\n",
      "step: 6010 loss: 145.836214 time elapsed: 7.3020 learning rate: 0.003604, scenario: 0, slope: -0.24762374718909227, fluctuations: 0.0\n",
      "step: 6020 loss: 143.715913 time elapsed: 7.3142 learning rate: 0.003604, scenario: 0, slope: -0.24047270314965807, fluctuations: 0.0\n",
      "step: 6030 loss: 141.649555 time elapsed: 7.3263 learning rate: 0.003604, scenario: 0, slope: -0.2336785845653068, fluctuations: 0.0\n",
      "step: 6040 loss: 139.634521 time elapsed: 7.3382 learning rate: 0.003604, scenario: 0, slope: -0.2272211646542458, fluctuations: 0.0\n",
      "step: 6050 loss: 137.668328 time elapsed: 7.3500 learning rate: 0.003604, scenario: 0, slope: -0.22108192837208374, fluctuations: 0.0\n",
      "step: 6060 loss: 135.748624 time elapsed: 7.3620 learning rate: 0.003604, scenario: 0, slope: -0.21524376374798476, fluctuations: 0.0\n",
      "step: 6070 loss: 133.873177 time elapsed: 7.3739 learning rate: 0.003604, scenario: 0, slope: -0.20969072236920974, fluctuations: 0.0\n",
      "step: 6080 loss: 132.039876 time elapsed: 7.3861 learning rate: 0.003604, scenario: 0, slope: -0.20440783602406856, fluctuations: 0.0\n",
      "step: 6090 loss: 130.246722 time elapsed: 7.3981 learning rate: 0.003604, scenario: 0, slope: -0.1993809777764, fluctuations: 0.0\n",
      "step: 6100 loss: 128.491823 time elapsed: 7.4095 learning rate: 0.003604, scenario: 0, slope: -0.1950646285127979, fluctuations: 0.0\n",
      "step: 6110 loss: 126.773391 time elapsed: 7.4219 learning rate: 0.003604, scenario: 0, slope: -0.19004244341932192, fluctuations: 0.0\n",
      "step: 6120 loss: 125.089735 time elapsed: 7.4341 learning rate: 0.003604, scenario: 0, slope: -0.18570590368341455, fluctuations: 0.0\n",
      "step: 6130 loss: 123.439259 time elapsed: 7.4455 learning rate: 0.003604, scenario: 0, slope: -0.18157556161242788, fluctuations: 0.0\n",
      "step: 6140 loss: 121.820458 time elapsed: 7.4576 learning rate: 0.003604, scenario: 0, slope: -0.17764036159236107, fluctuations: 0.0\n",
      "step: 6150 loss: 120.231909 time elapsed: 7.4697 learning rate: 0.003604, scenario: 0, slope: -0.17388974178382485, fluctuations: 0.0\n",
      "step: 6160 loss: 118.672275 time elapsed: 7.4819 learning rate: 0.003604, scenario: 0, slope: -0.1703136114727238, fluctuations: 0.0\n",
      "step: 6170 loss: 117.140295 time elapsed: 7.4938 learning rate: 0.003604, scenario: 0, slope: -0.16690233128615542, fluctuations: 0.0\n",
      "step: 6180 loss: 115.634782 time elapsed: 7.5062 learning rate: 0.003604, scenario: 0, slope: -0.16364669505358292, fluctuations: 0.0\n",
      "step: 6190 loss: 114.154620 time elapsed: 7.5177 learning rate: 0.003604, scenario: 0, slope: -0.1605379125109639, fluctuations: 0.0\n",
      "step: 6200 loss: 112.698762 time elapsed: 7.5295 learning rate: 0.003604, scenario: 0, slope: -0.15785862475316917, fluctuations: 0.0\n",
      "step: 6210 loss: 111.266226 time elapsed: 7.5416 learning rate: 0.003604, scenario: 0, slope: -0.15472772549626987, fluctuations: 0.0\n",
      "step: 6220 loss: 109.856089 time elapsed: 7.5536 learning rate: 0.003604, scenario: 0, slope: -0.15201066817293354, fluctuations: 0.0\n",
      "step: 6230 loss: 108.467489 time elapsed: 7.5655 learning rate: 0.003604, scenario: 0, slope: -0.1494091254138291, fluctuations: 0.0\n",
      "step: 6240 loss: 107.099622 time elapsed: 7.5774 learning rate: 0.003604, scenario: 0, slope: -0.1469161345140068, fluctuations: 0.0\n",
      "step: 6250 loss: 105.751735 time elapsed: 7.5892 learning rate: 0.003604, scenario: 0, slope: -0.1445250489304608, fluctuations: 0.0\n",
      "step: 6260 loss: 104.423128 time elapsed: 7.6010 learning rate: 0.003604, scenario: 0, slope: -0.14222952265039507, fluctuations: 0.0\n",
      "step: 6270 loss: 103.113149 time elapsed: 7.6126 learning rate: 0.003604, scenario: 0, slope: -0.14002349516602106, fluctuations: 0.0\n",
      "step: 6280 loss: 101.821196 time elapsed: 7.6245 learning rate: 0.003604, scenario: 0, slope: -0.13790117714043312, fluctuations: 0.0\n",
      "step: 6290 loss: 100.546708 time elapsed: 7.6366 learning rate: 0.003604, scenario: 0, slope: -0.1358570368020448, fluctuations: 0.0\n",
      "step: 6300 loss: 99.289171 time elapsed: 7.6477 learning rate: 0.003604, scenario: 0, slope: -0.13407977777774138, fluctuations: 0.0\n",
      "step: 6310 loss: 98.048109 time elapsed: 7.6599 learning rate: 0.003604, scenario: 0, slope: -0.13198237323490314, fluctuations: 0.0\n",
      "step: 6320 loss: 96.823089 time elapsed: 7.6723 learning rate: 0.003604, scenario: 0, slope: -0.13014196143340806, fluctuations: 0.0\n",
      "step: 6330 loss: 95.613712 time elapsed: 7.6844 learning rate: 0.003604, scenario: 0, slope: -0.12835992722794792, fluctuations: 0.0\n",
      "step: 6340 loss: 94.419619 time elapsed: 7.6966 learning rate: 0.003604, scenario: 0, slope: -0.12663184473863984, fluctuations: 0.0\n",
      "step: 6350 loss: 93.240485 time elapsed: 7.7085 learning rate: 0.003604, scenario: 0, slope: -0.12495347590250197, fluctuations: 0.0\n",
      "step: 6360 loss: 92.076017 time elapsed: 7.7206 learning rate: 0.003604, scenario: 0, slope: -0.12332075995665853, fluctuations: 0.0\n",
      "step: 6370 loss: 90.925957 time elapsed: 7.7324 learning rate: 0.003604, scenario: 0, slope: -0.12172980321645804, fluctuations: 0.0\n",
      "step: 6380 loss: 89.790077 time elapsed: 7.7443 learning rate: 0.003604, scenario: 0, slope: -0.12017686937007517, fluctuations: 0.0\n",
      "step: 6390 loss: 88.668178 time elapsed: 7.7561 learning rate: 0.003604, scenario: 0, slope: -0.118658370666388, fluctuations: 0.0\n",
      "step: 6400 loss: 87.560091 time elapsed: 7.7681 learning rate: 0.003604, scenario: 0, slope: -0.11731831246280149, fluctuations: 0.0\n",
      "step: 6410 loss: 86.465674 time elapsed: 7.7802 learning rate: 0.003604, scenario: 0, slope: -0.11571102829041005, fluctuations: 0.0\n",
      "step: 6420 loss: 85.384811 time elapsed: 7.7921 learning rate: 0.003604, scenario: 0, slope: -0.11427569674881595, fluctuations: 0.0\n",
      "step: 6430 loss: 84.317409 time elapsed: 7.8041 learning rate: 0.003604, scenario: 0, slope: -0.11286182341077701, fluctuations: 0.0\n",
      "step: 6440 loss: 83.263396 time elapsed: 7.8158 learning rate: 0.003604, scenario: 0, slope: -0.11146650596943246, fluctuations: 0.0\n",
      "step: 6450 loss: 82.222721 time elapsed: 7.8276 learning rate: 0.003604, scenario: 0, slope: -0.11008699246749207, fluctuations: 0.0\n",
      "step: 6460 loss: 81.195349 time elapsed: 7.8397 learning rate: 0.003604, scenario: 0, slope: -0.10872069610605677, fluctuations: 0.0\n",
      "step: 6470 loss: 80.181257 time elapsed: 7.8511 learning rate: 0.003604, scenario: 0, slope: -0.1073652141430132, fluctuations: 0.0\n",
      "step: 6480 loss: 79.180435 time elapsed: 7.8635 learning rate: 0.003604, scenario: 0, slope: -0.10601834970376366, fluctuations: 0.0\n",
      "step: 6490 loss: 78.192877 time elapsed: 7.8757 learning rate: 0.003604, scenario: 0, slope: -0.10467813471699795, fluctuations: 0.0\n",
      "step: 6500 loss: 77.218586 time elapsed: 7.8872 learning rate: 0.003604, scenario: 0, slope: -0.10347620152352442, fluctuations: 0.0\n",
      "step: 6510 loss: 76.257561 time elapsed: 7.8998 learning rate: 0.003604, scenario: 0, slope: -0.102011051840809, fluctuations: 0.0\n",
      "step: 6520 loss: 75.309805 time elapsed: 7.9119 learning rate: 0.003604, scenario: 0, slope: -0.10068156697700494, fluctuations: 0.0\n",
      "step: 6530 loss: 74.375314 time elapsed: 7.9236 learning rate: 0.003604, scenario: 0, slope: -0.0993535133696966, fluctuations: 0.0\n",
      "step: 6540 loss: 73.454083 time elapsed: 7.9356 learning rate: 0.003604, scenario: 0, slope: -0.09802628616640885, fluctuations: 0.0\n",
      "step: 6550 loss: 72.546098 time elapsed: 7.9473 learning rate: 0.003604, scenario: 0, slope: -0.09669954434529389, fluctuations: 0.0\n",
      "step: 6560 loss: 71.651342 time elapsed: 7.9592 learning rate: 0.003604, scenario: 0, slope: -0.09537318714575556, fluctuations: 0.0\n",
      "step: 6570 loss: 70.769788 time elapsed: 7.9715 learning rate: 0.003604, scenario: 0, slope: -0.09404732399318769, fluctuations: 0.0\n",
      "step: 6580 loss: 69.901404 time elapsed: 7.9834 learning rate: 0.003604, scenario: 0, slope: -0.09272224065894477, fluctuations: 0.0\n",
      "step: 6590 loss: 69.046149 time elapsed: 7.9955 learning rate: 0.003604, scenario: 0, slope: -0.09139836470625748, fluctuations: 0.0\n",
      "step: 6600 loss: 68.203975 time elapsed: 8.0070 learning rate: 0.003604, scenario: 0, slope: -0.09020835068192055, fluctuations: 0.0\n",
      "step: 6610 loss: 67.374826 time elapsed: 8.0188 learning rate: 0.003604, scenario: 0, slope: -0.08875646443858236, fluctuations: 0.0\n",
      "step: 6620 loss: 66.558638 time elapsed: 8.0309 learning rate: 0.003604, scenario: 0, slope: -0.08743973689166086, fluctuations: 0.0\n",
      "step: 6630 loss: 65.755338 time elapsed: 8.0430 learning rate: 0.003604, scenario: 0, slope: -0.08612677299851978, fluctuations: 0.0\n",
      "step: 6640 loss: 64.964845 time elapsed: 8.0546 learning rate: 0.003604, scenario: 0, slope: -0.08481833027364256, fluctuations: 0.0\n",
      "step: 6650 loss: 64.187066 time elapsed: 8.0673 learning rate: 0.003604, scenario: 0, slope: -0.08351519689823157, fluctuations: 0.0\n",
      "step: 6660 loss: 63.421902 time elapsed: 8.0796 learning rate: 0.003604, scenario: 0, slope: -0.08221819082951501, fluctuations: 0.0\n",
      "step: 6670 loss: 62.669242 time elapsed: 8.0919 learning rate: 0.003604, scenario: 0, slope: -0.08092816067681936, fluctuations: 0.0\n",
      "step: 6680 loss: 61.928965 time elapsed: 8.1038 learning rate: 0.003604, scenario: 0, slope: -0.0796459867978597, fluctuations: 0.0\n",
      "step: 6690 loss: 61.200939 time elapsed: 8.1156 learning rate: 0.003604, scenario: 0, slope: -0.07837258137059733, fluctuations: 0.0\n",
      "step: 6700 loss: 60.485023 time elapsed: 8.1274 learning rate: 0.003604, scenario: 0, slope: -0.07723479167561106, fluctuations: 0.0\n",
      "step: 6710 loss: 59.781066 time elapsed: 8.1399 learning rate: 0.003604, scenario: 0, slope: -0.07585587054982156, fluctuations: 0.0\n",
      "step: 6720 loss: 59.088906 time elapsed: 8.1517 learning rate: 0.003604, scenario: 0, slope: -0.07461452072764665, fluctuations: 0.0\n",
      "step: 6730 loss: 58.408372 time elapsed: 8.1639 learning rate: 0.003604, scenario: 0, slope: -0.07338583529585588, fluctuations: 0.0\n",
      "step: 6740 loss: 57.739285 time elapsed: 8.1764 learning rate: 0.003604, scenario: 0, slope: -0.07217081266379603, fluctuations: 0.0\n",
      "step: 6750 loss: 57.081457 time elapsed: 8.1887 learning rate: 0.003604, scenario: 0, slope: -0.07097043978885934, fluctuations: 0.0\n",
      "step: 6760 loss: 56.434693 time elapsed: 8.2011 learning rate: 0.003604, scenario: 0, slope: -0.0697856798329698, fluctuations: 0.0\n",
      "step: 6770 loss: 55.798791 time elapsed: 8.2137 learning rate: 0.003604, scenario: 0, slope: -0.06861745973471417, fluctuations: 0.0\n",
      "step: 6780 loss: 55.173544 time elapsed: 8.2269 learning rate: 0.003604, scenario: 0, slope: -0.06746665822006583, fluctuations: 0.0\n",
      "step: 6790 loss: 54.558741 time elapsed: 8.2399 learning rate: 0.003604, scenario: 0, slope: -0.06633409470280445, fluctuations: 0.0\n",
      "step: 6800 loss: 53.954166 time elapsed: 8.2537 learning rate: 0.003604, scenario: 0, slope: -0.06533100268043707, fluctuations: 0.0\n",
      "step: 6810 loss: 53.359600 time elapsed: 8.2681 learning rate: 0.003604, scenario: 0, slope: -0.06412660521674883, fluctuations: 0.0\n",
      "step: 6820 loss: 52.774824 time elapsed: 8.2812 learning rate: 0.003604, scenario: 0, slope: -0.06305294074256233, fluctuations: 0.0\n",
      "step: 6830 loss: 52.199618 time elapsed: 8.2946 learning rate: 0.003604, scenario: 0, slope: -0.062000025856976934, fluctuations: 0.0\n",
      "step: 6840 loss: 51.633760 time elapsed: 8.3071 learning rate: 0.003604, scenario: 0, slope: -0.06096826856161265, fluctuations: 0.0\n",
      "step: 6850 loss: 51.077031 time elapsed: 8.3201 learning rate: 0.003604, scenario: 0, slope: -0.05995798382895722, fluctuations: 0.0\n",
      "step: 6860 loss: 50.529212 time elapsed: 8.3330 learning rate: 0.003604, scenario: 0, slope: -0.05896939408045515, fluctuations: 0.0\n",
      "step: 6870 loss: 49.990088 time elapsed: 8.3457 learning rate: 0.003604, scenario: 0, slope: -0.058002631181917695, fluctuations: 0.0\n",
      "step: 6880 loss: 49.459446 time elapsed: 8.3582 learning rate: 0.003604, scenario: 0, slope: -0.057057739771726325, fluctuations: 0.0\n",
      "step: 6890 loss: 48.937077 time elapsed: 8.3713 learning rate: 0.003604, scenario: 0, slope: -0.056134681716600486, fluctuations: 0.0\n",
      "step: 6900 loss: 48.422776 time elapsed: 8.3838 learning rate: 0.003604, scenario: 0, slope: -0.055322502997686256, fluctuations: 0.0\n",
      "step: 6910 loss: 47.916341 time elapsed: 8.3968 learning rate: 0.003604, scenario: 0, slope: -0.05435353218640973, fluctuations: 0.0\n",
      "step: 6920 loss: 47.417576 time elapsed: 8.4086 learning rate: 0.003604, scenario: 0, slope: -0.05349500217749672, fluctuations: 0.0\n",
      "step: 6930 loss: 46.926290 time elapsed: 8.4212 learning rate: 0.003604, scenario: 0, slope: -0.05265744185973564, fluctuations: 0.0\n",
      "step: 6940 loss: 46.442297 time elapsed: 8.4336 learning rate: 0.003604, scenario: 0, slope: -0.05184049067446581, fluctuations: 0.0\n",
      "step: 6950 loss: 45.965417 time elapsed: 8.4464 learning rate: 0.003604, scenario: 0, slope: -0.05104374403260465, fluctuations: 0.0\n",
      "step: 6960 loss: 45.291276 time elapsed: 8.4591 learning rate: 0.007023, scenario: 0, slope: -0.050646952328884325, fluctuations: 0.0\n",
      "step: 6970 loss: 313.064217 time elapsed: 8.4730 learning rate: 0.007375, scenario: -1, slope: 0.11808591316249369, fluctuations: 0.0\n",
      "step: 6980 loss: 1700.792286 time elapsed: 8.4865 learning rate: 0.002571, scenario: -1, slope: 6.699290625777879, fluctuations: 0.04\n",
      "step: 6990 loss: 203.745798 time elapsed: 8.4994 learning rate: 0.000897, scenario: -1, slope: 7.704811005003743, fluctuations: 0.06\n",
      "step: 7000 loss: 111.644652 time elapsed: 8.5126 learning rate: 0.000347, scenario: -1, slope: 6.504705471553975, fluctuations: 0.07\n",
      "step: 7010 loss: 95.482225 time elapsed: 8.5261 learning rate: 0.000121, scenario: -1, slope: 4.4485201399519685, fluctuations: 0.08\n",
      "step: 7020 loss: 88.544579 time elapsed: 8.5395 learning rate: 0.000042, scenario: -1, slope: 2.3032442232936177, fluctuations: 0.08\n",
      "step: 7030 loss: 87.858612 time elapsed: 8.5526 learning rate: 0.000015, scenario: -1, slope: 0.3413326521937244, fluctuations: 0.09\n",
      "step: 7040 loss: 86.978416 time elapsed: 8.5650 learning rate: 0.000013, scenario: 0, slope: -1.7797610502999148, fluctuations: 0.09\n",
      "step: 7050 loss: 86.239315 time elapsed: 8.5770 learning rate: 0.000013, scenario: 0, slope: -4.042757181712895, fluctuations: 0.09\n",
      "step: 7060 loss: 85.718950 time elapsed: 8.5895 learning rate: 0.000013, scenario: 0, slope: -6.631236486034467, fluctuations: 0.09\n",
      "step: 7070 loss: 85.355464 time elapsed: 8.6023 learning rate: 0.000013, scenario: 0, slope: -9.593628605441854, fluctuations: 0.09\n",
      "step: 7080 loss: 85.076328 time elapsed: 8.6148 learning rate: 0.000013, scenario: 0, slope: -2.6053273006876387, fluctuations: 0.05\n",
      "step: 7090 loss: 84.836296 time elapsed: 8.6272 learning rate: 0.000013, scenario: 0, slope: -0.6863665601491242, fluctuations: 0.03\n",
      "step: 7100 loss: 84.613294 time elapsed: 8.6391 learning rate: 0.000013, scenario: 0, slope: -0.2272761329165335, fluctuations: 0.02\n",
      "step: 7110 loss: 84.374548 time elapsed: 8.6515 learning rate: 0.000023, scenario: 1, slope: -0.0486212846364925, fluctuations: 0.01\n",
      "step: 7120 loss: 83.839370 time elapsed: 8.6638 learning rate: 0.000061, scenario: 1, slope: -0.04260387794819048, fluctuations: 0.0\n",
      "step: 7130 loss: 82.525069 time elapsed: 8.6756 learning rate: 0.000158, scenario: 1, slope: -0.03913100009791812, fluctuations: 0.0\n",
      "step: 7140 loss: 79.510292 time elapsed: 8.6883 learning rate: 0.000410, scenario: 1, slope: -0.04745484328025594, fluctuations: 0.0\n",
      "step: 7150 loss: 73.492397 time elapsed: 8.7011 learning rate: 0.001063, scenario: 1, slope: -0.07854827608367863, fluctuations: 0.0\n",
      "step: 7160 loss: 66.215324 time elapsed: 8.7136 learning rate: 0.001169, scenario: 0, slope: -0.14104420157555125, fluctuations: 0.0\n",
      "step: 7170 loss: 61.979269 time elapsed: 8.7259 learning rate: 0.001169, scenario: 0, slope: -0.21579320405358957, fluctuations: 0.0\n",
      "step: 7180 loss: 59.206518 time elapsed: 8.7381 learning rate: 0.001169, scenario: 0, slope: -0.28428585393174394, fluctuations: 0.0\n",
      "step: 7190 loss: 57.110521 time elapsed: 8.7502 learning rate: 0.001169, scenario: 0, slope: -0.337382782510108, fluctuations: 0.0\n",
      "step: 7200 loss: 55.395619 time elapsed: 8.7621 learning rate: 0.001169, scenario: 0, slope: -0.36755876287321465, fluctuations: 0.0\n",
      "step: 7210 loss: 53.945783 time elapsed: 8.7747 learning rate: 0.001169, scenario: 0, slope: -0.37771693987061883, fluctuations: 0.0\n",
      "step: 7220 loss: 52.714935 time elapsed: 8.7866 learning rate: 0.001169, scenario: 0, slope: -0.3593402634860534, fluctuations: 0.0\n",
      "step: 7230 loss: 51.674279 time elapsed: 8.7984 learning rate: 0.001169, scenario: 0, slope: -0.31589406183642593, fluctuations: 0.0\n",
      "step: 7240 loss: 50.794746 time elapsed: 8.8104 learning rate: 0.001169, scenario: 0, slope: -0.2542832598104542, fluctuations: 0.0\n",
      "step: 7250 loss: 50.046575 time elapsed: 8.8225 learning rate: 0.001169, scenario: 0, slope: -0.19096731696011116, fluctuations: 0.0\n",
      "step: 7260 loss: 49.402455 time elapsed: 8.8343 learning rate: 0.001169, scenario: 0, slope: -0.14791676193752898, fluctuations: 0.0\n",
      "step: 7270 loss: 48.839731 time elapsed: 8.8462 learning rate: 0.001169, scenario: 0, slope: -0.12135233004558255, fluctuations: 0.0\n",
      "step: 7280 loss: 48.340624 time elapsed: 8.8581 learning rate: 0.001169, scenario: 0, slope: -0.10218887563677108, fluctuations: 0.0\n",
      "step: 7290 loss: 47.891514 time elapsed: 8.8694 learning rate: 0.001169, scenario: 0, slope: -0.08704883070988675, fluctuations: 0.0\n",
      "step: 7300 loss: 47.482033 time elapsed: 8.8814 learning rate: 0.001169, scenario: 0, slope: -0.07581608285942505, fluctuations: 0.0\n",
      "step: 7310 loss: 47.104292 time elapsed: 8.8945 learning rate: 0.001169, scenario: 0, slope: -0.06461016086702491, fluctuations: 0.0\n",
      "step: 7320 loss: 46.752273 time elapsed: 8.9067 learning rate: 0.001169, scenario: 0, slope: -0.05650053891633718, fluctuations: 0.0\n",
      "step: 7330 loss: 46.411366 time elapsed: 8.9188 learning rate: 0.001711, scenario: 1, slope: -0.05005807657108643, fluctuations: 0.0\n",
      "step: 7340 loss: 45.760620 time elapsed: 8.9307 learning rate: 0.004439, scenario: 1, slope: -0.04578727134505062, fluctuations: 0.0\n",
      "step: 7350 loss: 44.290274 time elapsed: 8.9429 learning rate: 0.011512, scenario: 1, slope: -0.046527661743767476, fluctuations: 0.0\n",
      "step: 7360 loss: 41.623981 time elapsed: 8.9550 learning rate: 0.016855, scenario: 0, slope: -0.05726013153030095, fluctuations: 0.0\n",
      "step: 7370 loss: 4119.234788 time elapsed: 8.9664 learning rate: 0.009953, scenario: -1, slope: 11.431938851308841, fluctuations: 0.01\n",
      "step: 7380 loss: 888.671603 time elapsed: 8.9782 learning rate: 0.003470, scenario: -1, slope: 18.95672559528794, fluctuations: 0.04\n",
      "step: 7390 loss: 586.241329 time elapsed: 8.9898 learning rate: 0.001210, scenario: -1, slope: 18.925248781879258, fluctuations: 0.06\n",
      "step: 7400 loss: 477.799161 time elapsed: 9.0017 learning rate: 0.000469, scenario: -1, slope: 16.183244802152952, fluctuations: 0.07\n",
      "step: 7410 loss: 436.012384 time elapsed: 9.0142 learning rate: 0.000163, scenario: -1, slope: 11.49340881635339, fluctuations: 0.07\n",
      "step: 7420 loss: 427.519256 time elapsed: 9.0264 learning rate: 0.000057, scenario: -1, slope: 7.027145920761041, fluctuations: 0.07\n",
      "step: 7430 loss: 424.642469 time elapsed: 9.0383 learning rate: 0.000020, scenario: -1, slope: 2.264988571854218, fluctuations: 0.07\n",
      "step: 7440 loss: 423.536841 time elapsed: 9.0503 learning rate: 0.000013, scenario: 0, slope: -3.029872317416408, fluctuations: 0.07\n",
      "step: 7450 loss: 422.675670 time elapsed: 9.0620 learning rate: 0.000013, scenario: 0, slope: -9.176887859928408, fluctuations: 0.07\n",
      "step: 7460 loss: 421.889917 time elapsed: 9.0745 learning rate: 0.000013, scenario: 0, slope: -16.63793014302035, fluctuations: 0.07\n",
      "step: 7470 loss: 421.167744 time elapsed: 9.0864 learning rate: 0.000013, scenario: 0, slope: -13.771835508902864, fluctuations: 0.05\n",
      "step: 7480 loss: 420.492916 time elapsed: 9.0995 learning rate: 0.000013, scenario: 0, slope: -2.550983505525469, fluctuations: 0.03\n",
      "step: 7490 loss: 419.851752 time elapsed: 9.1119 learning rate: 0.000013, scenario: 0, slope: -0.6926130795756137, fluctuations: 0.01\n",
      "step: 7500 loss: 419.083052 time elapsed: 9.1239 learning rate: 0.000025, scenario: 1, slope: -0.2735738891102806, fluctuations: 0.0\n",
      "step: 7510 loss: 417.410367 time elapsed: 9.1363 learning rate: 0.000066, scenario: 1, slope: -0.10888222611015118, fluctuations: 0.0\n",
      "step: 7520 loss: 413.359847 time elapsed: 9.1479 learning rate: 0.000171, scenario: 1, slope: -0.09556111216796863, fluctuations: 0.0\n",
      "step: 7530 loss: 403.605868 time elapsed: 9.1597 learning rate: 0.000443, scenario: 1, slope: -0.13242217407923745, fluctuations: 0.0\n",
      "step: 7540 loss: 381.415091 time elapsed: 9.1716 learning rate: 0.001150, scenario: 1, slope: -0.24752685689814385, fluctuations: 0.0\n",
      "step: 7550 loss: 337.199508 time elapsed: 9.1836 learning rate: 0.002241, scenario: 0, slope: -0.5154096799691507, fluctuations: 0.0\n",
      "step: 7560 loss: 293.216799 time elapsed: 9.1958 learning rate: 0.002241, scenario: 0, slope: -0.9667079224212943, fluctuations: 0.0\n",
      "step: 7570 loss: 259.716873 time elapsed: 9.2078 learning rate: 0.002241, scenario: 0, slope: -1.493991260546171, fluctuations: 0.0\n",
      "step: 7580 loss: 233.736227 time elapsed: 9.2200 learning rate: 0.002241, scenario: 0, slope: -2.0057020437113167, fluctuations: 0.0\n",
      "step: 7590 loss: 212.987007 time elapsed: 9.2318 learning rate: 0.002241, scenario: 0, slope: -2.4340424011346133, fluctuations: 0.0\n",
      "step: 7600 loss: 195.844526 time elapsed: 9.2434 learning rate: 0.002241, scenario: 0, slope: -2.706916303619579, fluctuations: 0.0\n",
      "step: 7610 loss: 181.332272 time elapsed: 9.2556 learning rate: 0.002241, scenario: 0, slope: -2.857235881391035, fluctuations: 0.0\n",
      "step: 7620 loss: 168.868141 time elapsed: 9.2675 learning rate: 0.002241, scenario: 0, slope: -2.7985070489665387, fluctuations: 0.0\n",
      "step: 7630 loss: 158.065228 time elapsed: 9.2798 learning rate: 0.002241, scenario: 0, slope: -2.5566733395649464, fluctuations: 0.0\n",
      "step: 7640 loss: 148.651573 time elapsed: 9.2913 learning rate: 0.002241, scenario: 0, slope: -2.1772478596210267, fluctuations: 0.0\n",
      "step: 7650 loss: 140.421172 time elapsed: 9.3044 learning rate: 0.002241, scenario: 0, slope: -1.7769021852641937, fluctuations: 0.0\n",
      "step: 7660 loss: 133.204021 time elapsed: 9.3170 learning rate: 0.002241, scenario: 0, slope: -1.4715485507171748, fluctuations: 0.0\n",
      "step: 7670 loss: 126.852109 time elapsed: 9.3288 learning rate: 0.002241, scenario: 0, slope: -1.242632821387605, fluctuations: 0.0\n",
      "step: 7680 loss: 121.233814 time elapsed: 9.3412 learning rate: 0.002241, scenario: 0, slope: -1.0656012788707507, fluctuations: 0.0\n",
      "step: 7690 loss: 116.233103 time elapsed: 9.3532 learning rate: 0.002241, scenario: 0, slope: -0.9234353421189129, fluctuations: 0.0\n",
      "step: 7700 loss: 111.750320 time elapsed: 9.3651 learning rate: 0.002241, scenario: 0, slope: -0.8167473525957256, fluctuations: 0.0\n",
      "step: 7710 loss: 107.702231 time elapsed: 9.3774 learning rate: 0.002241, scenario: 0, slope: -0.7071935339377594, fluctuations: 0.0\n",
      "step: 7720 loss: 104.020848 time elapsed: 9.3886 learning rate: 0.002241, scenario: 0, slope: -0.6236555285522909, fluctuations: 0.0\n",
      "step: 7730 loss: 100.651419 time elapsed: 9.4003 learning rate: 0.002241, scenario: 0, slope: -0.552829646071949, fluctuations: 0.0\n",
      "step: 7740 loss: 97.550145 time elapsed: 9.4123 learning rate: 0.002241, scenario: 0, slope: -0.4928362188528583, fluctuations: 0.0\n",
      "step: 7750 loss: 94.682017 time elapsed: 9.4242 learning rate: 0.002241, scenario: 0, slope: -0.4420834880089845, fluctuations: 0.0\n",
      "step: 7760 loss: 92.018922 time elapsed: 9.4358 learning rate: 0.002241, scenario: 0, slope: -0.39913946637988656, fluctuations: 0.0\n",
      "step: 7770 loss: 89.538061 time elapsed: 9.4477 learning rate: 0.002241, scenario: 0, slope: -0.3627021982240485, fluctuations: 0.0\n",
      "step: 7780 loss: 87.220630 time elapsed: 9.4601 learning rate: 0.002241, scenario: 0, slope: -0.3316116640214794, fluctuations: 0.0\n",
      "step: 7790 loss: 85.050750 time elapsed: 9.4718 learning rate: 0.002241, scenario: 0, slope: -0.30487005705440307, fluctuations: 0.0\n",
      "step: 7800 loss: 83.014659 time elapsed: 9.4838 learning rate: 0.002241, scenario: 0, slope: -0.28383374693565744, fluctuations: 0.0\n",
      "step: 7810 loss: 81.100143 time elapsed: 9.4962 learning rate: 0.002241, scenario: 0, slope: -0.2612873934070897, fluctuations: 0.0\n",
      "step: 7820 loss: 79.296195 time elapsed: 9.5089 learning rate: 0.002241, scenario: 0, slope: -0.24326171306173947, fluctuations: 0.0\n",
      "step: 7830 loss: 77.592827 time elapsed: 9.5212 learning rate: 0.002241, scenario: 0, slope: -0.22717297530420036, fluctuations: 0.0\n",
      "step: 7840 loss: 75.980973 time elapsed: 9.5334 learning rate: 0.002241, scenario: 0, slope: -0.21271691151225888, fluctuations: 0.0\n",
      "step: 7850 loss: 74.452434 time elapsed: 9.5451 learning rate: 0.002241, scenario: 0, slope: -0.19966261229891374, fluctuations: 0.0\n",
      "step: 7860 loss: 72.999825 time elapsed: 9.5568 learning rate: 0.002241, scenario: 0, slope: -0.18783309597992792, fluctuations: 0.0\n",
      "step: 7870 loss: 71.616521 time elapsed: 9.5689 learning rate: 0.002241, scenario: 0, slope: -0.17708929329951228, fluctuations: 0.0\n",
      "step: 7880 loss: 70.296591 time elapsed: 9.5809 learning rate: 0.002241, scenario: 0, slope: -0.16731760323666572, fluctuations: 0.0\n",
      "step: 7890 loss: 69.034741 time elapsed: 9.5933 learning rate: 0.002241, scenario: 0, slope: -0.15842098793730713, fluctuations: 0.0\n",
      "step: 7900 loss: 67.826240 time elapsed: 9.6050 learning rate: 0.002241, scenario: 0, slope: -0.1510908486482312, fluctuations: 0.0\n",
      "step: 7910 loss: 66.666866 time elapsed: 9.6171 learning rate: 0.002241, scenario: 0, slope: -0.14291616460539702, fluctuations: 0.0\n",
      "step: 7920 loss: 65.552845 time elapsed: 9.6290 learning rate: 0.002241, scenario: 0, slope: -0.13615768464297368, fluctuations: 0.0\n",
      "step: 7930 loss: 64.480800 time elapsed: 9.6408 learning rate: 0.002241, scenario: 0, slope: -0.1299719639017978, fluctuations: 0.0\n",
      "step: 7940 loss: 63.447706 time elapsed: 9.6528 learning rate: 0.002241, scenario: 0, slope: -0.12429903808045985, fluctuations: 0.0\n",
      "step: 7950 loss: 62.450848 time elapsed: 9.6647 learning rate: 0.002241, scenario: 0, slope: -0.11908477188029967, fluctuations: 0.0\n",
      "step: 7960 loss: 61.487786 time elapsed: 9.6763 learning rate: 0.002241, scenario: 0, slope: -0.1142806397488348, fluctuations: 0.0\n",
      "step: 7970 loss: 60.556324 time elapsed: 9.6883 learning rate: 0.002241, scenario: 0, slope: -0.10984338061867648, fluctuations: 0.0\n",
      "step: 7980 loss: 59.654482 time elapsed: 9.7001 learning rate: 0.002241, scenario: 0, slope: -0.10573456728395109, fluctuations: 0.0\n",
      "step: 7990 loss: 58.780471 time elapsed: 9.7129 learning rate: 0.002241, scenario: 0, slope: -0.10192013754310959, fluctuations: 0.0\n",
      "step: 8000 loss: 57.932674 time elapsed: 9.7247 learning rate: 0.002241, scenario: 0, slope: -0.09871384306215766, fluctuations: 0.0\n",
      "step: 8010 loss: 57.109622 time elapsed: 9.7371 learning rate: 0.002241, scenario: 0, slope: -0.09505721530743037, fluctuations: 0.0\n",
      "step: 8020 loss: 56.309983 time elapsed: 9.7490 learning rate: 0.002241, scenario: 0, slope: -0.09195833984891497, fluctuations: 0.0\n",
      "step: 8030 loss: 55.532545 time elapsed: 9.7610 learning rate: 0.002241, scenario: 0, slope: -0.08905231537896431, fluctuations: 0.0\n",
      "step: 8040 loss: 54.776200 time elapsed: 9.7729 learning rate: 0.002241, scenario: 0, slope: -0.08632052660326008, fluctuations: 0.0\n",
      "step: 8050 loss: 54.039935 time elapsed: 9.7848 learning rate: 0.002241, scenario: 0, slope: -0.0837464504269308, fluctuations: 0.0\n",
      "step: 8060 loss: 53.322821 time elapsed: 9.7967 learning rate: 0.002241, scenario: 0, slope: -0.08131541664115116, fluctuations: 0.0\n",
      "step: 8070 loss: 52.624001 time elapsed: 9.8083 learning rate: 0.002241, scenario: 0, slope: -0.07901440004277742, fluctuations: 0.0\n",
      "step: 8080 loss: 51.942684 time elapsed: 9.8201 learning rate: 0.002241, scenario: 0, slope: -0.07683183904273831, fluctuations: 0.0\n",
      "step: 8090 loss: 51.278137 time elapsed: 9.8321 learning rate: 0.002241, scenario: 0, slope: -0.07475747630054984, fluctuations: 0.0\n",
      "step: 8100 loss: 50.629677 time elapsed: 9.8439 learning rate: 0.002241, scenario: 0, slope: -0.07297552097480933, fluctuations: 0.0\n",
      "step: 8110 loss: 49.996667 time elapsed: 9.8560 learning rate: 0.002241, scenario: 0, slope: -0.07089800548026039, fluctuations: 0.0\n",
      "step: 8120 loss: 49.378509 time elapsed: 9.8682 learning rate: 0.002241, scenario: 0, slope: -0.06909770607484544, fluctuations: 0.0\n",
      "step: 8130 loss: 48.774641 time elapsed: 9.8800 learning rate: 0.002241, scenario: 0, slope: -0.06737500559982876, fluctuations: 0.0\n",
      "step: 8140 loss: 48.184532 time elapsed: 9.8917 learning rate: 0.002241, scenario: 0, slope: -0.06572431667085273, fluctuations: 0.0\n",
      "step: 8150 loss: 47.607680 time elapsed: 9.9031 learning rate: 0.002241, scenario: 0, slope: -0.06414069244831994, fluctuations: 0.0\n",
      "step: 8160 loss: 47.043608 time elapsed: 9.9159 learning rate: 0.002241, scenario: 0, slope: -0.06261974822729421, fluctuations: 0.0\n",
      "step: 8170 loss: 46.491863 time elapsed: 9.9284 learning rate: 0.002241, scenario: 0, slope: -0.06115758995457744, fluctuations: 0.0\n",
      "step: 8180 loss: 45.952012 time elapsed: 9.9404 learning rate: 0.002241, scenario: 0, slope: -0.059750749359673005, fluctuations: 0.0\n",
      "step: 8190 loss: 45.423642 time elapsed: 9.9522 learning rate: 0.002241, scenario: 0, slope: -0.058396125476324036, fluctuations: 0.0\n",
      "step: 8200 loss: 44.906359 time elapsed: 9.9636 learning rate: 0.002241, scenario: 0, slope: -0.057219300944885705, fluctuations: 0.0\n",
      "step: 8210 loss: 44.399785 time elapsed: 9.9762 learning rate: 0.002241, scenario: 0, slope: -0.055832652842432766, fluctuations: 0.0\n",
      "step: 8220 loss: 43.903557 time elapsed: 9.9880 learning rate: 0.002241, scenario: 0, slope: -0.05461899797305673, fluctuations: 0.0\n",
      "step: 8230 loss: 43.417330 time elapsed: 9.9998 learning rate: 0.002241, scenario: 0, slope: -0.053447872052049664, fluctuations: 0.0\n",
      "step: 8240 loss: 42.940770 time elapsed: 10.0120 learning rate: 0.002241, scenario: 0, slope: -0.0523173427163605, fluctuations: 0.0\n",
      "step: 8250 loss: 42.473559 time elapsed: 10.0236 learning rate: 0.002241, scenario: 0, slope: -0.05122561579435643, fluctuations: 0.0\n",
      "step: 8260 loss: 42.015391 time elapsed: 10.0355 learning rate: 0.002241, scenario: 0, slope: -0.05017101444754072, fluctuations: 0.0\n",
      "step: 8270 loss: 41.565973 time elapsed: 10.0476 learning rate: 0.002241, scenario: 0, slope: -0.04915196213352746, fluctuations: 0.0\n",
      "step: 8280 loss: 41.125023 time elapsed: 10.0592 learning rate: 0.002241, scenario: 0, slope: -0.048166968907134754, fluctuations: 0.0\n",
      "step: 8290 loss: 40.692273 time elapsed: 10.0713 learning rate: 0.002241, scenario: 0, slope: -0.047214620580823666, fluctuations: 0.0\n",
      "step: 8300 loss: 40.267462 time elapsed: 10.0834 learning rate: 0.002241, scenario: 0, slope: -0.046384304006886395, fluctuations: 0.0\n",
      "step: 8310 loss: 39.850344 time elapsed: 10.0962 learning rate: 0.002241, scenario: 0, slope: -0.04540253199500222, fluctuations: 0.0\n",
      "step: 8320 loss: 39.440679 time elapsed: 10.1077 learning rate: 0.002241, scenario: 0, slope: -0.044540275645273124, fluctuations: 0.0\n",
      "step: 8330 loss: 39.038241 time elapsed: 10.1207 learning rate: 0.002241, scenario: 0, slope: -0.04370562346630024, fluctuations: 0.0\n",
      "step: 8340 loss: 38.642809 time elapsed: 10.1328 learning rate: 0.002241, scenario: 0, slope: -0.042897447263273776, fluctuations: 0.0\n",
      "step: 8350 loss: 38.078064 time elapsed: 10.1448 learning rate: 0.005284, scenario: 0, slope: -0.04243324383725889, fluctuations: 0.0\n",
      "step: 8360 loss: 37.192555 time elapsed: 10.1568 learning rate: 0.005284, scenario: 0, slope: -0.044117433186713254, fluctuations: 0.0\n",
      "step: 8370 loss: 36.343564 time elapsed: 10.1685 learning rate: 0.005284, scenario: 0, slope: -0.047905648109959495, fluctuations: 0.0\n",
      "step: 8380 loss: 35.530835 time elapsed: 10.1813 learning rate: 0.005284, scenario: 0, slope: -0.05304882831293031, fluctuations: 0.0\n",
      "step: 8390 loss: 34.752529 time elapsed: 10.1940 learning rate: 0.005284, scenario: 0, slope: -0.058840438702301504, fluctuations: 0.0\n",
      "step: 8400 loss: 34.006428 time elapsed: 10.2063 learning rate: 0.005284, scenario: 0, slope: -0.06406004538875919, fluctuations: 0.0\n",
      "step: 8410 loss: 33.290338 time elapsed: 10.2188 learning rate: 0.005284, scenario: 0, slope: -0.06976872942090052, fluctuations: 0.0\n",
      "step: 8420 loss: 32.602205 time elapsed: 10.2306 learning rate: 0.005284, scenario: 0, slope: -0.07371194909793508, fluctuations: 0.0\n",
      "step: 8430 loss: 31.940142 time elapsed: 10.2426 learning rate: 0.005284, scenario: 0, slope: -0.07591051627772048, fluctuations: 0.0\n",
      "step: 8440 loss: 31.302424 time elapsed: 10.2545 learning rate: 0.005284, scenario: 0, slope: -0.07586048615505168, fluctuations: 0.0\n",
      "step: 8450 loss: 30.687479 time elapsed: 10.2666 learning rate: 0.005284, scenario: 0, slope: -0.07343709146289282, fluctuations: 0.0\n",
      "step: 8460 loss: 30.093872 time elapsed: 10.2782 learning rate: 0.005284, scenario: 0, slope: -0.07052585933030427, fluctuations: 0.0\n",
      "step: 8470 loss: 29.520301 time elapsed: 10.2896 learning rate: 0.005284, scenario: 0, slope: -0.067799650015615, fluctuations: 0.0\n",
      "step: 8480 loss: 28.965583 time elapsed: 10.3018 learning rate: 0.005284, scenario: 0, slope: -0.06525200406581197, fluctuations: 0.0\n",
      "step: 8490 loss: 28.428648 time elapsed: 10.3136 learning rate: 0.005284, scenario: 0, slope: -0.06287018578142774, fluctuations: 0.0\n",
      "step: 8500 loss: 27.908531 time elapsed: 10.3264 learning rate: 0.005284, scenario: 0, slope: -0.06085682486357416, fluctuations: 0.0\n",
      "step: 8510 loss: 27.404364 time elapsed: 10.3392 learning rate: 0.005284, scenario: 0, slope: -0.05854878123562618, fluctuations: 0.0\n",
      "step: 8520 loss: 26.915369 time elapsed: 10.3511 learning rate: 0.005284, scenario: 0, slope: -0.05658302347039863, fluctuations: 0.0\n",
      "step: 8530 loss: 26.440847 time elapsed: 10.3632 learning rate: 0.005284, scenario: 0, slope: -0.05473123845335026, fluctuations: 0.0\n",
      "step: 8540 loss: 25.980171 time elapsed: 10.3752 learning rate: 0.005284, scenario: 0, slope: -0.052982539496210956, fluctuations: 0.0\n",
      "step: 8550 loss: 25.532771 time elapsed: 10.3870 learning rate: 0.005284, scenario: 0, slope: -0.05132690472864735, fluctuations: 0.0\n",
      "step: 8560 loss: 25.098126 time elapsed: 10.3990 learning rate: 0.005284, scenario: 0, slope: -0.04975518466949917, fluctuations: 0.0\n",
      "step: 8570 loss: 24.675755 time elapsed: 10.4106 learning rate: 0.005284, scenario: 0, slope: -0.04825912958845338, fluctuations: 0.0\n",
      "step: 8580 loss: 24.265204 time elapsed: 10.4228 learning rate: 0.005284, scenario: 0, slope: -0.04683142403136285, fluctuations: 0.0\n",
      "step: 8590 loss: 23.866040 time elapsed: 10.4346 learning rate: 0.005284, scenario: 0, slope: -0.045465713873156004, fluctuations: 0.0\n",
      "step: 8600 loss: 23.477845 time elapsed: 10.4462 learning rate: 0.005284, scenario: 0, slope: -0.04428510941449395, fluctuations: 0.0\n",
      "step: 8610 loss: 23.100209 time elapsed: 10.4586 learning rate: 0.005284, scenario: 0, slope: -0.04289968038217459, fluctuations: 0.0\n",
      "step: 8620 loss: 22.732730 time elapsed: 10.4705 learning rate: 0.005284, scenario: 0, slope: -0.041691364381989324, fluctuations: 0.0\n",
      "step: 8630 loss: 22.375007 time elapsed: 10.4824 learning rate: 0.005284, scenario: 0, slope: -0.04052891464924772, fluctuations: 0.0\n",
      "step: 8640 loss: 22.026645 time elapsed: 10.4943 learning rate: 0.005284, scenario: 0, slope: -0.03941026889530414, fluctuations: 0.0\n",
      "step: 8650 loss: 21.687253 time elapsed: 10.5061 learning rate: 0.005284, scenario: 0, slope: -0.03833392307443728, fluctuations: 0.0\n",
      "step: 8660 loss: 21.356442 time elapsed: 10.5176 learning rate: 0.005284, scenario: 0, slope: -0.03729879562343407, fluctuations: 0.0\n",
      "step: 8670 loss: 21.033831 time elapsed: 10.5304 learning rate: 0.005284, scenario: 0, slope: -0.03630409550336716, fluctuations: 0.0\n",
      "step: 8680 loss: 20.719049 time elapsed: 10.5425 learning rate: 0.005284, scenario: 0, slope: -0.03534920177219525, fluctuations: 0.0\n",
      "step: 8690 loss: 20.411731 time elapsed: 10.5546 learning rate: 0.005284, scenario: 0, slope: -0.03443355961125707, fluctuations: 0.0\n",
      "step: 8700 loss: 20.111527 time elapsed: 10.5665 learning rate: 0.005284, scenario: 0, slope: -0.03364256873715638, fluctuations: 0.0\n",
      "step: 8710 loss: 19.818102 time elapsed: 10.5790 learning rate: 0.005284, scenario: 0, slope: -0.032717646545544515, fluctuations: 0.0\n",
      "step: 8720 loss: 19.531137 time elapsed: 10.5911 learning rate: 0.005284, scenario: 0, slope: -0.03191591569141285, fluctuations: 0.0\n",
      "step: 8730 loss: 19.250334 time elapsed: 10.6032 learning rate: 0.005284, scenario: 0, slope: -0.03115042690674539, fluctuations: 0.0\n",
      "step: 8740 loss: 18.975418 time elapsed: 10.6155 learning rate: 0.005284, scenario: 0, slope: -0.030420000355210933, fluctuations: 0.0\n",
      "step: 8750 loss: 18.706137 time elapsed: 10.6275 learning rate: 0.005284, scenario: 0, slope: -0.02972323126502312, fluctuations: 0.0\n",
      "step: 8760 loss: 18.442271 time elapsed: 10.6393 learning rate: 0.005284, scenario: 0, slope: -0.029058475529836154, fluctuations: 0.0\n",
      "step: 8770 loss: 18.183625 time elapsed: 10.6510 learning rate: 0.005284, scenario: 0, slope: -0.028423840967541074, fluctuations: 0.0\n",
      "step: 8780 loss: 17.930040 time elapsed: 10.6630 learning rate: 0.005284, scenario: 0, slope: -0.027817185047660748, fluctuations: 0.0\n",
      "step: 8790 loss: 17.681387 time elapsed: 10.6748 learning rate: 0.005284, scenario: 0, slope: -0.027236121033072016, fluctuations: 0.0\n",
      "step: 8800 loss: 17.437573 time elapsed: 10.6870 learning rate: 0.005284, scenario: 0, slope: -0.02673288859664577, fluctuations: 0.0\n",
      "step: 8810 loss: 17.198534 time elapsed: 10.6993 learning rate: 0.005284, scenario: 0, slope: -0.02614011930454324, fluctuations: 0.0\n",
      "step: 8820 loss: 16.964235 time elapsed: 10.7117 learning rate: 0.005284, scenario: 0, slope: -0.025619418235049166, fluctuations: 0.0\n",
      "step: 8830 loss: 16.734667 time elapsed: 10.7236 learning rate: 0.005284, scenario: 0, slope: -0.025112899543691905, fluctuations: 0.0\n",
      "step: 8840 loss: 16.509839 time elapsed: 10.7362 learning rate: 0.005284, scenario: 0, slope: -0.024617539753745452, fluctuations: 0.0\n",
      "step: 8850 loss: 16.289771 time elapsed: 10.7495 learning rate: 0.005284, scenario: 0, slope: -0.024130427545565637, fluctuations: 0.0\n",
      "step: 8860 loss: 16.074492 time elapsed: 10.7625 learning rate: 0.005284, scenario: 0, slope: -0.023648876982645083, fluctuations: 0.0\n",
      "step: 8870 loss: 15.864026 time elapsed: 10.7753 learning rate: 0.005284, scenario: 0, slope: -0.023170542245356664, fluctuations: 0.0\n",
      "step: 8880 loss: 15.658387 time elapsed: 10.7888 learning rate: 0.005284, scenario: 0, slope: -0.022693523197750014, fluctuations: 0.0\n",
      "step: 8890 loss: 15.457578 time elapsed: 10.8024 learning rate: 0.005284, scenario: 0, slope: -0.02221645049840251, fluctuations: 0.0\n",
      "step: 8900 loss: 15.261582 time elapsed: 10.8153 learning rate: 0.005284, scenario: 0, slope: -0.021786378694740063, fluctuations: 0.0\n",
      "step: 8910 loss: 15.070361 time elapsed: 10.8286 learning rate: 0.005284, scenario: 0, slope: -0.021259609814038384, fluctuations: 0.0\n",
      "step: 8920 loss: 14.883854 time elapsed: 10.8418 learning rate: 0.005284, scenario: 0, slope: -0.02078005504485042, fluctuations: 0.0\n",
      "step: 8930 loss: 14.701981 time elapsed: 10.8542 learning rate: 0.005284, scenario: 0, slope: -0.020300785382368874, fluctuations: 0.0\n",
      "step: 8940 loss: 14.524641 time elapsed: 10.8669 learning rate: 0.005284, scenario: 0, slope: -0.019823128890122246, fluctuations: 0.0\n",
      "step: 8950 loss: 14.351717 time elapsed: 10.8791 learning rate: 0.005284, scenario: 0, slope: -0.019348714246353538, fluctuations: 0.0\n",
      "step: 8960 loss: 14.183080 time elapsed: 10.8913 learning rate: 0.005284, scenario: 0, slope: -0.018879343060734202, fluctuations: 0.0\n",
      "step: 8970 loss: 14.018589 time elapsed: 10.9041 learning rate: 0.005284, scenario: 0, slope: -0.01841686471717676, fluctuations: 0.0\n",
      "step: 8980 loss: 13.858095 time elapsed: 10.9176 learning rate: 0.005284, scenario: 0, slope: -0.017963064417805166, fluctuations: 0.0\n",
      "step: 8990 loss: 13.701449 time elapsed: 10.9321 learning rate: 0.005284, scenario: 0, slope: -0.017519572154216068, fluctuations: 0.0\n",
      "step: 9000 loss: 13.548497 time elapsed: 10.9455 learning rate: 0.005284, scenario: 0, slope: -0.017130412201633194, fluctuations: 0.0\n",
      "step: 9010 loss: 13.399087 time elapsed: 10.9589 learning rate: 0.005284, scenario: 0, slope: -0.01666888598387382, fluctuations: 0.0\n",
      "step: 9020 loss: 13.253070 time elapsed: 10.9716 learning rate: 0.005284, scenario: 0, slope: -0.01626370984367895, fluctuations: 0.0\n",
      "step: 9030 loss: 13.110299 time elapsed: 10.9848 learning rate: 0.005284, scenario: 0, slope: -0.015872864294365244, fluctuations: 0.0\n",
      "step: 9040 loss: 12.970635 time elapsed: 10.9970 learning rate: 0.005284, scenario: 0, slope: -0.01549668920165844, fluctuations: 0.0\n",
      "step: 9050 loss: 12.833943 time elapsed: 11.0088 learning rate: 0.005284, scenario: 0, slope: -0.015135296218078097, fluctuations: 0.0\n",
      "step: 9060 loss: 12.700092 time elapsed: 11.0214 learning rate: 0.005284, scenario: 0, slope: -0.014788601843935316, fluctuations: 0.0\n",
      "step: 9070 loss: 12.568961 time elapsed: 11.0340 learning rate: 0.005284, scenario: 0, slope: -0.01445636207157703, fluctuations: 0.0\n",
      "step: 9080 loss: 12.440434 time elapsed: 11.0461 learning rate: 0.005284, scenario: 0, slope: -0.014138205967884133, fluctuations: 0.0\n",
      "step: 9090 loss: 12.314400 time elapsed: 11.0580 learning rate: 0.005284, scenario: 0, slope: -0.013833666481169305, fluctuations: 0.0\n",
      "step: 9100 loss: 12.190757 time elapsed: 11.0699 learning rate: 0.005284, scenario: 0, slope: -0.013570781127890471, fluctuations: 0.0\n",
      "step: 9110 loss: 12.100129 time elapsed: 11.0825 learning rate: 0.013706, scenario: 1, slope: -0.013310520380395296, fluctuations: 0.0\n",
      "step: 9120 loss: 55434.738809 time elapsed: 11.0943 learning rate: 0.006195, scenario: -1, slope: 165.40124394028064, fluctuations: 0.02\n",
      "step: 9130 loss: 17380.851710 time elapsed: 11.1062 learning rate: 0.002160, scenario: -1, slope: 261.1759610862399, fluctuations: 0.04\n",
      "step: 9140 loss: 9362.474000 time elapsed: 11.1182 learning rate: 0.000753, scenario: -1, slope: 254.38243169843096, fluctuations: 0.04\n",
      "step: 9150 loss: 7035.889620 time elapsed: 11.1305 learning rate: 0.000263, scenario: -1, slope: 216.29157619905095, fluctuations: 0.04\n",
      "step: 9160 loss: 6477.458460 time elapsed: 11.1448 learning rate: 0.000092, scenario: -1, slope: 165.58335458712097, fluctuations: 0.04\n",
      "step: 9170 loss: 6293.840346 time elapsed: 11.1582 learning rate: 0.000032, scenario: -1, slope: 107.09720067224251, fluctuations: 0.04\n",
      "step: 9180 loss: 6233.228693 time elapsed: 11.1714 learning rate: 0.000011, scenario: -1, slope: 40.70464906322596, fluctuations: 0.04\n",
      "step: 9190 loss: 6212.011091 time elapsed: 11.1849 learning rate: 0.000007, scenario: 0, slope: -35.26294354628432, fluctuations: 0.04\n",
      "step: 9200 loss: 6195.878717 time elapsed: 11.1984 learning rate: 0.000007, scenario: 0, slope: -114.05866281476224, fluctuations: 0.04\n",
      "step: 9210 loss: 6180.051057 time elapsed: 11.2107 learning rate: 0.000007, scenario: 0, slope: -228.04535297558613, fluctuations: 0.04\n",
      "step: 9220 loss: 6164.487668 time elapsed: 11.2230 learning rate: 0.000007, scenario: 0, slope: -149.57142226379764, fluctuations: 0.01\n",
      "step: 9230 loss: 6149.163088 time elapsed: 11.2351 learning rate: 0.000007, scenario: 0, slope: -37.23796567879075, fluctuations: 0.0\n",
      "step: 9240 loss: 6134.058182 time elapsed: 11.2471 learning rate: 0.000007, scenario: 0, slope: -12.196881130860708, fluctuations: 0.0\n",
      "step: 9250 loss: 6116.615311 time elapsed: 11.2591 learning rate: 0.000013, scenario: 1, slope: -4.375525598159508, fluctuations: 0.0\n",
      "step: 9260 loss: 6075.386284 time elapsed: 11.2715 learning rate: 0.000033, scenario: 1, slope: -2.3356871014587473, fluctuations: 0.0\n",
      "step: 9270 loss: 5972.470223 time elapsed: 11.2836 learning rate: 0.000086, scenario: 1, slope: -2.110278264019362, fluctuations: 0.0\n",
      "step: 9280 loss: 5725.573898 time elapsed: 11.2960 learning rate: 0.000224, scenario: 1, slope: -3.134878976565672, fluctuations: 0.0\n",
      "step: 9290 loss: 5168.861756 time elapsed: 11.3079 learning rate: 0.000580, scenario: 1, slope: -6.099445372375556, fluctuations: 0.0\n",
      "step: 9300 loss: 4383.595524 time elapsed: 11.3198 learning rate: 0.000580, scenario: 0, slope: -11.508783096988012, fluctuations: 0.0\n",
      "step: 9310 loss: 3812.625172 time elapsed: 11.3328 learning rate: 0.000580, scenario: 0, slope: -20.291325644660752, fluctuations: 0.0\n",
      "step: 9320 loss: 3393.768312 time elapsed: 11.3448 learning rate: 0.000580, scenario: 0, slope: -28.469623343236805, fluctuations: 0.0\n",
      "step: 9330 loss: 3075.463525 time elapsed: 11.3585 learning rate: 0.000580, scenario: 0, slope: -35.565814123515054, fluctuations: 0.0\n",
      "step: 9340 loss: 2822.478032 time elapsed: 11.3713 learning rate: 0.000580, scenario: 0, slope: -40.734244684975295, fluctuations: 0.0\n",
      "step: 9350 loss: 2615.245916 time elapsed: 11.3837 learning rate: 0.000580, scenario: 0, slope: -43.379933725982255, fluctuations: 0.0\n",
      "step: 9360 loss: 2442.819029 time elapsed: 11.3958 learning rate: 0.000580, scenario: 0, slope: -43.108936705064686, fluctuations: 0.0\n",
      "step: 9370 loss: 2295.672662 time elapsed: 11.4075 learning rate: 0.000580, scenario: 0, slope: -39.85211496582684, fluctuations: 0.0\n",
      "step: 9380 loss: 2167.642661 time elapsed: 11.4197 learning rate: 0.000580, scenario: 0, slope: -34.05274189276177, fluctuations: 0.0\n",
      "step: 9390 loss: 2054.336650 time elapsed: 11.4319 learning rate: 0.000580, scenario: 0, slope: -27.14441363034063, fluctuations: 0.0\n",
      "step: 9400 loss: 1952.403202 time elapsed: 11.4437 learning rate: 0.000580, scenario: 0, slope: -22.082678723358313, fluctuations: 0.0\n",
      "step: 9410 loss: 1859.325240 time elapsed: 11.4559 learning rate: 0.000580, scenario: 0, slope: -17.764349167782953, fluctuations: 0.0\n",
      "step: 9420 loss: 1773.465791 time elapsed: 11.4679 learning rate: 0.000580, scenario: 0, slope: -15.00236774670465, fluctuations: 0.0\n",
      "step: 9430 loss: 1694.457347 time elapsed: 11.4801 learning rate: 0.000580, scenario: 0, slope: -12.95739054946959, fluctuations: 0.0\n",
      "step: 9440 loss: 1623.195146 time elapsed: 11.4921 learning rate: 0.000580, scenario: 0, slope: -11.390027061265943, fluctuations: 0.0\n",
      "step: 9450 loss: 1559.909564 time elapsed: 11.5037 learning rate: 0.000580, scenario: 0, slope: -10.157233524179548, fluctuations: 0.0\n",
      "step: 9460 loss: 1502.945915 time elapsed: 11.5157 learning rate: 0.000580, scenario: 0, slope: -9.1417647179073, fluctuations: 0.0\n",
      "step: 9470 loss: 1451.092765 time elapsed: 11.5277 learning rate: 0.000580, scenario: 0, slope: -8.273699874553404, fluctuations: 0.0\n",
      "step: 9480 loss: 1403.567882 time elapsed: 11.5398 learning rate: 0.000580, scenario: 0, slope: -7.509296485501681, fluctuations: 0.0\n",
      "step: 9490 loss: 1359.724880 time elapsed: 11.5523 learning rate: 0.000580, scenario: 0, slope: -6.821678773501072, fluctuations: 0.0\n",
      "step: 9500 loss: 1319.048251 time elapsed: 11.5658 learning rate: 0.000580, scenario: 0, slope: -6.256638253983467, fluctuations: 0.0\n",
      "step: 9510 loss: 1281.113999 time elapsed: 11.5790 learning rate: 0.000580, scenario: 0, slope: -5.630889722809517, fluctuations: 0.0\n",
      "step: 9520 loss: 1245.559407 time elapsed: 11.5909 learning rate: 0.000580, scenario: 0, slope: -5.127506659293464, fluctuations: 0.0\n",
      "step: 9530 loss: 1212.068741 time elapsed: 11.6030 learning rate: 0.000580, scenario: 0, slope: -4.6921577319540715, fluctuations: 0.0\n",
      "step: 9540 loss: 1180.380206 time elapsed: 11.6150 learning rate: 0.000580, scenario: 0, slope: -4.323818376052425, fluctuations: 0.0\n",
      "step: 9550 loss: 1150.301075 time elapsed: 11.6267 learning rate: 0.000580, scenario: 0, slope: -4.011139968362569, fluctuations: 0.0\n",
      "step: 9560 loss: 1121.721710 time elapsed: 11.6388 learning rate: 0.000580, scenario: 0, slope: -3.741752824327506, fluctuations: 0.0\n",
      "step: 9570 loss: 1094.607896 time elapsed: 11.6507 learning rate: 0.000580, scenario: 0, slope: -3.5072710716956528, fluctuations: 0.0\n",
      "step: 9580 loss: 1068.954431 time elapsed: 11.6625 learning rate: 0.000580, scenario: 0, slope: -3.3004396147497186, fluctuations: 0.0\n",
      "step: 9590 loss: 1044.717529 time elapsed: 11.6744 learning rate: 0.000580, scenario: 0, slope: -3.1150115849358286, fluctuations: 0.0\n",
      "step: 9600 loss: 1021.782348 time elapsed: 11.6860 learning rate: 0.000580, scenario: 0, slope: -2.9622246499605263, fluctuations: 0.0\n",
      "step: 9610 loss: 999.993998 time elapsed: 11.6985 learning rate: 0.000580, scenario: 0, slope: -2.7895665837116415, fluctuations: 0.0\n",
      "step: 9620 loss: 979.208675 time elapsed: 11.7100 learning rate: 0.000580, scenario: 0, slope: -2.6435796392752744, fluctuations: 0.0\n",
      "step: 9630 loss: 959.315493 time elapsed: 11.7218 learning rate: 0.000580, scenario: 0, slope: -2.50679661846622, fluctuations: 0.0\n",
      "step: 9640 loss: 940.230157 time elapsed: 11.7335 learning rate: 0.000580, scenario: 0, slope: -2.3789336521811575, fluctuations: 0.0\n",
      "step: 9650 loss: 921.883516 time elapsed: 11.7452 learning rate: 0.000580, scenario: 0, slope: -2.2603199526919378, fluctuations: 0.0\n",
      "step: 9660 loss: 904.215340 time elapsed: 11.7565 learning rate: 0.000580, scenario: 0, slope: -2.151507162646446, fluctuations: 0.0\n",
      "step: 9670 loss: 887.172137 time elapsed: 11.7700 learning rate: 0.000580, scenario: 0, slope: -2.0527896031063677, fluctuations: 0.0\n",
      "step: 9680 loss: 870.706328 time elapsed: 11.7826 learning rate: 0.000580, scenario: 0, slope: -1.9638266787955028, fluctuations: 0.0\n",
      "step: 9690 loss: 854.775630 time elapsed: 11.7946 learning rate: 0.000580, scenario: 0, slope: -1.8836171899630518, fluctuations: 0.0\n",
      "step: 9700 loss: 839.342475 time elapsed: 11.8064 learning rate: 0.000580, scenario: 0, slope: -1.8178269489346173, fluctuations: 0.0\n",
      "step: 9710 loss: 824.373480 time elapsed: 11.8189 learning rate: 0.000580, scenario: 0, slope: -1.744312183038775, fluctuations: 0.0\n",
      "step: 9720 loss: 809.838995 time elapsed: 11.8308 learning rate: 0.000580, scenario: 0, slope: -1.6830888186288795, fluctuations: 0.0\n",
      "step: 9730 loss: 795.712703 time elapsed: 11.8427 learning rate: 0.000580, scenario: 0, slope: -1.62650159516824, fluctuations: 0.0\n",
      "step: 9740 loss: 781.971267 time elapsed: 11.8548 learning rate: 0.000580, scenario: 0, slope: -1.5740143264214574, fluctuations: 0.0\n",
      "step: 9750 loss: 768.594001 time elapsed: 11.8669 learning rate: 0.000580, scenario: 0, slope: -1.5251676024464282, fluctuations: 0.0\n",
      "step: 9760 loss: 755.562574 time elapsed: 11.8787 learning rate: 0.000580, scenario: 0, slope: -1.4795545272047375, fluctuations: 0.0\n",
      "step: 9770 loss: 742.860736 time elapsed: 11.8902 learning rate: 0.000580, scenario: 0, slope: -1.4368125195764867, fluctuations: 0.0\n",
      "step: 9780 loss: 730.474069 time elapsed: 11.9021 learning rate: 0.000580, scenario: 0, slope: -1.396618693486803, fluctuations: 0.0\n",
      "step: 9790 loss: 718.389755 time elapsed: 11.9140 learning rate: 0.000580, scenario: 0, slope: -1.358685814970953, fluctuations: 0.0\n",
      "step: 9800 loss: 706.596373 time elapsed: 11.9258 learning rate: 0.000580, scenario: 0, slope: -1.326267843011913, fluctuations: 0.0\n",
      "step: 9810 loss: 695.083707 time elapsed: 11.9383 learning rate: 0.000580, scenario: 0, slope: -1.2886113792078888, fluctuations: 0.0\n",
      "step: 9820 loss: 683.842584 time elapsed: 11.9502 learning rate: 0.000580, scenario: 0, slope: -1.2560441055308056, fluctuations: 0.0\n",
      "step: 9830 loss: 672.864725 time elapsed: 11.9634 learning rate: 0.000580, scenario: 0, slope: -1.2248813602963335, fluctuations: 0.0\n",
      "step: 9840 loss: 662.142620 time elapsed: 11.9777 learning rate: 0.000580, scenario: 0, slope: -1.1949694451426236, fluctuations: 0.0\n",
      "step: 9850 loss: 651.669411 time elapsed: 11.9903 learning rate: 0.000580, scenario: 0, slope: -1.16617440428465, fluctuations: 0.0\n",
      "step: 9860 loss: 641.438801 time elapsed: 12.0030 learning rate: 0.000580, scenario: 0, slope: -1.1383800006723979, fluctuations: 0.0\n",
      "step: 9870 loss: 631.444967 time elapsed: 12.0155 learning rate: 0.000580, scenario: 0, slope: -1.1114857887592768, fluctuations: 0.0\n",
      "step: 9880 loss: 621.682487 time elapsed: 12.0285 learning rate: 0.000580, scenario: 0, slope: -1.0854052998880444, fluctuations: 0.0\n",
      "step: 9890 loss: 612.146279 time elapsed: 12.0410 learning rate: 0.000580, scenario: 0, slope: -1.0600643604611037, fluctuations: 0.0\n",
      "step: 9900 loss: 602.831541 time elapsed: 12.0534 learning rate: 0.000580, scenario: 0, slope: -1.0378372222197982, fluctuations: 0.0\n",
      "step: 9910 loss: 593.733703 time elapsed: 12.0663 learning rate: 0.000580, scenario: 0, slope: -1.0113568910273554, fluctuations: 0.0\n",
      "step: 9920 loss: 584.848385 time elapsed: 12.0785 learning rate: 0.000580, scenario: 0, slope: -0.9878905361755645, fluctuations: 0.0\n",
      "step: 9930 loss: 576.171352 time elapsed: 12.0905 learning rate: 0.000580, scenario: 0, slope: -0.9649618448575034, fluctuations: 0.0\n",
      "step: 9940 loss: 567.698484 time elapsed: 12.1028 learning rate: 0.000580, scenario: 0, slope: -0.9425384385514824, fluctuations: 0.0\n",
      "step: 9950 loss: 559.425744 time elapsed: 12.1148 learning rate: 0.000580, scenario: 0, slope: -0.9205934536655448, fluctuations: 0.0\n",
      "step: 9960 loss: 551.349148 time elapsed: 12.1270 learning rate: 0.000580, scenario: 0, slope: -0.899104889397945, fluctuations: 0.0\n",
      "step: 9970 loss: 543.464748 time elapsed: 12.1390 learning rate: 0.000580, scenario: 0, slope: -0.8780550380727513, fluctuations: 0.0\n",
      "step: 9980 loss: 535.768608 time elapsed: 12.1509 learning rate: 0.000580, scenario: 0, slope: -0.8574299756509381, fluctuations: 0.0\n",
      "step: 9990 loss: 528.256794 time elapsed: 12.1629 learning rate: 0.000580, scenario: 0, slope: -0.8372190934993236, fluctuations: 0.0\n",
      "step: 10000 loss: 520.925362 time elapsed: 12.1762 learning rate: 0.000580, scenario: 0, slope: -0.8193769785143775, fluctuations: 0.0\n",
      "step: 10010 loss: 513.770352 time elapsed: 12.1914 learning rate: 0.000580, scenario: 0, slope: -0.7980113814299128, fluctuations: 0.0\n",
      "step: 10020 loss: 506.787782 time elapsed: 12.2040 learning rate: 0.000580, scenario: 0, slope: -0.7790060205439886, fluctuations: 0.0\n",
      "step: 10030 loss: 499.973652 time elapsed: 12.2166 learning rate: 0.000580, scenario: 0, slope: -0.7603969669441583, fluctuations: 0.0\n",
      "step: 10040 loss: 493.323945 time elapsed: 12.2289 learning rate: 0.000580, scenario: 0, slope: -0.7421838671845236, fluctuations: 0.0\n",
      "step: 10050 loss: 486.834631 time elapsed: 12.2409 learning rate: 0.000580, scenario: 0, slope: -0.7243672562728082, fluctuations: 0.0\n",
      "step: 10060 loss: 480.501674 time elapsed: 12.2530 learning rate: 0.000580, scenario: 0, slope: -0.7069482176292664, fluctuations: 0.0\n",
      "step: 10070 loss: 474.321037 time elapsed: 12.2651 learning rate: 0.000580, scenario: 0, slope: -0.6899280746748789, fluctuations: 0.0\n",
      "step: 10080 loss: 468.288697 time elapsed: 12.2769 learning rate: 0.000580, scenario: 0, slope: -0.6733081195911114, fluctuations: 0.0\n",
      "step: 10090 loss: 462.400644 time elapsed: 12.2891 learning rate: 0.000580, scenario: 0, slope: -0.6570893835286011, fluctuations: 0.0\n",
      "step: 10100 loss: 456.652897 time elapsed: 12.3009 learning rate: 0.000580, scenario: 0, slope: -0.6428360579553697, fluctuations: 0.0\n",
      "step: 10110 loss: 451.041510 time elapsed: 12.3133 learning rate: 0.000580, scenario: 0, slope: -0.6258573181998016, fluctuations: 0.0\n",
      "step: 10120 loss: 445.562575 time elapsed: 12.3253 learning rate: 0.000580, scenario: 0, slope: -0.6108432969753375, fluctuations: 0.0\n",
      "step: 10130 loss: 440.212235 time elapsed: 12.3369 learning rate: 0.000580, scenario: 0, slope: -0.5962289566818519, fluctuations: 0.0\n",
      "step: 10140 loss: 434.986685 time elapsed: 12.3489 learning rate: 0.000580, scenario: 0, slope: -0.5820121043941286, fluctuations: 0.0\n",
      "step: 10150 loss: 429.882179 time elapsed: 12.3610 learning rate: 0.000580, scenario: 0, slope: -0.5681897951181246, fluctuations: 0.0\n",
      "step: 10160 loss: 424.895034 time elapsed: 12.3726 learning rate: 0.000580, scenario: 0, slope: -0.5547583668685625, fluctuations: 0.0\n",
      "step: 10170 loss: 420.021633 time elapsed: 12.3851 learning rate: 0.000580, scenario: 0, slope: -0.5417134941446468, fluctuations: 0.0\n",
      "step: 10180 loss: 415.258427 time elapsed: 12.3978 learning rate: 0.000580, scenario: 0, slope: -0.52905025364617, fluctuations: 0.0\n",
      "step: 10190 loss: 410.601937 time elapsed: 12.4104 learning rate: 0.000580, scenario: 0, slope: -0.5167631965839271, fluctuations: 0.0\n",
      "step: 10200 loss: 406.048758 time elapsed: 12.4227 learning rate: 0.000580, scenario: 0, slope: -0.5060216136697385, fluctuations: 0.0\n",
      "step: 10210 loss: 401.595557 time elapsed: 12.4349 learning rate: 0.000580, scenario: 0, slope: -0.49329365215142196, fluctuations: 0.0\n",
      "step: 10220 loss: 397.239076 time elapsed: 12.4470 learning rate: 0.000580, scenario: 0, slope: -0.482098292118869, fluctuations: 0.0\n",
      "step: 10230 loss: 392.976130 time elapsed: 12.4588 learning rate: 0.000580, scenario: 0, slope: -0.47125349667079763, fluctuations: 0.0\n",
      "step: 10240 loss: 388.803613 time elapsed: 12.4709 learning rate: 0.000580, scenario: 0, slope: -0.4607522185652266, fluctuations: 0.0\n",
      "step: 10250 loss: 384.718493 time elapsed: 12.4830 learning rate: 0.000580, scenario: 0, slope: -0.450587253137397, fluctuations: 0.0\n",
      "step: 10260 loss: 380.717812 time elapsed: 12.4948 learning rate: 0.000580, scenario: 0, slope: -0.4407512747595863, fluctuations: 0.0\n",
      "step: 10270 loss: 376.798692 time elapsed: 12.5070 learning rate: 0.000580, scenario: 0, slope: -0.4312368669260507, fluctuations: 0.0\n",
      "step: 10280 loss: 372.958327 time elapsed: 12.5189 learning rate: 0.000580, scenario: 0, slope: -0.4220365472953399, fluctuations: 0.0\n",
      "step: 10290 loss: 369.193989 time elapsed: 12.5306 learning rate: 0.000580, scenario: 0, slope: -0.4131427891427498, fluctuations: 0.0\n",
      "step: 10300 loss: 365.466463 time elapsed: 12.5420 learning rate: 0.000702, scenario: 1, slope: -0.40539427551322643, fluctuations: 0.0\n",
      "step: 10310 loss: 359.119026 time elapsed: 12.5546 learning rate: 0.001504, scenario: 0, slope: -0.4023589577580768, fluctuations: 0.0\n",
      "step: 10320 loss: 350.145374 time elapsed: 12.5663 learning rate: 0.001504, scenario: 0, slope: -0.4247759987795962, fluctuations: 0.0\n",
      "step: 10330 loss: 341.616424 time elapsed: 12.5783 learning rate: 0.001504, scenario: 0, slope: -0.46865580515365596, fluctuations: 0.0\n",
      "step: 10340 loss: 333.515355 time elapsed: 12.5901 learning rate: 0.001504, scenario: 0, slope: -0.5257188247009971, fluctuations: 0.0\n",
      "step: 10350 loss: 325.807905 time elapsed: 12.6028 learning rate: 0.001504, scenario: 0, slope: -0.5882756323801193, fluctuations: 0.0\n",
      "step: 10360 loss: 318.456614 time elapsed: 12.6150 learning rate: 0.001504, scenario: 0, slope: -0.6492398592399194, fluctuations: 0.0\n",
      "step: 10370 loss: 311.425356 time elapsed: 12.6272 learning rate: 0.001504, scenario: 0, slope: -0.7020882999836197, fluctuations: 0.0\n",
      "step: 10380 loss: 304.680548 time elapsed: 12.6393 learning rate: 0.001504, scenario: 0, slope: -0.7408079142452925, fluctuations: 0.0\n",
      "step: 10390 loss: 298.191311 time elapsed: 12.6512 learning rate: 0.001504, scenario: 0, slope: -0.759842585614963, fluctuations: 0.0\n",
      "step: 10400 loss: 291.929457 time elapsed: 12.6628 learning rate: 0.001504, scenario: 0, slope: -0.7558795267721963, fluctuations: 0.0\n",
      "step: 10410 loss: 285.869657 time elapsed: 12.6758 learning rate: 0.001504, scenario: 0, slope: -0.7254597405403514, fluctuations: 0.0\n",
      "step: 10420 loss: 279.989949 time elapsed: 12.6880 learning rate: 0.001504, scenario: 0, slope: -0.6948238288542365, fluctuations: 0.0\n",
      "step: 10430 loss: 274.272606 time elapsed: 12.7005 learning rate: 0.001504, scenario: 0, slope: -0.6672595986362806, fluctuations: 0.0\n",
      "step: 10440 loss: 268.705176 time elapsed: 12.7130 learning rate: 0.001504, scenario: 0, slope: -0.6425665251116183, fluctuations: 0.0\n",
      "step: 10450 loss: 263.281253 time elapsed: 12.7255 learning rate: 0.001504, scenario: 0, slope: -0.6204387074794, fluctuations: 0.0\n",
      "step: 10460 loss: 258.000375 time elapsed: 12.7376 learning rate: 0.001504, scenario: 0, slope: -0.6005113324609002, fluctuations: 0.0\n",
      "step: 10470 loss: 252.866544 time elapsed: 12.7499 learning rate: 0.001504, scenario: 0, slope: -0.5823749008155704, fluctuations: 0.0\n",
      "step: 10480 loss: 247.885440 time elapsed: 12.7619 learning rate: 0.001504, scenario: 0, slope: -0.5655920578748943, fluctuations: 0.0\n",
      "step: 10490 loss: 243.061123 time elapsed: 12.7737 learning rate: 0.001504, scenario: 0, slope: -0.5497290398995857, fluctuations: 0.0\n",
      "step: 10500 loss: 238.393374 time elapsed: 12.7858 learning rate: 0.001504, scenario: 0, slope: -0.5359180332599013, fluctuations: 0.0\n",
      "step: 10510 loss: 233.876523 time elapsed: 12.7989 learning rate: 0.001504, scenario: 0, slope: -0.5193133056504904, fluctuations: 0.0\n",
      "step: 10520 loss: 229.499773 time elapsed: 12.8124 learning rate: 0.001504, scenario: 0, slope: -0.504309174277166, fluctuations: 0.0\n",
      "step: 10530 loss: 225.248485 time elapsed: 12.8253 learning rate: 0.001504, scenario: 0, slope: -0.4893755963514071, fluctuations: 0.0\n",
      "step: 10540 loss: 221.105850 time elapsed: 12.8383 learning rate: 0.001504, scenario: 0, slope: -0.47464092225074456, fluctuations: 0.0\n",
      "step: 10550 loss: 217.054946 time elapsed: 12.8518 learning rate: 0.001504, scenario: 0, slope: -0.4603403213738298, fluctuations: 0.0\n",
      "step: 10560 loss: 213.082023 time elapsed: 12.8646 learning rate: 0.001504, scenario: 0, slope: -0.4467574662936411, fluctuations: 0.0\n",
      "step: 10570 loss: 209.182281 time elapsed: 12.8765 learning rate: 0.001504, scenario: 0, slope: -0.4341411240135545, fluctuations: 0.0\n",
      "step: 10580 loss: 205.366705 time elapsed: 12.8885 learning rate: 0.001504, scenario: 0, slope: -0.4226008791377309, fluctuations: 0.0\n",
      "step: 10590 loss: 201.660782 time elapsed: 12.9006 learning rate: 0.001504, scenario: 0, slope: -0.412017831927478, fluctuations: 0.0\n",
      "step: 10600 loss: 198.087203 time elapsed: 12.9133 learning rate: 0.001504, scenario: 0, slope: -0.4030331114677335, fluctuations: 0.0\n",
      "step: 10610 loss: 194.649145 time elapsed: 12.9263 learning rate: 0.001504, scenario: 0, slope: -0.3922775894872862, fluctuations: 0.0\n",
      "step: 10620 loss: 191.333614 time elapsed: 12.9389 learning rate: 0.001504, scenario: 0, slope: -0.38234067148918877, fluctuations: 0.0\n",
      "step: 10630 loss: 188.123864 time elapsed: 12.9510 learning rate: 0.001504, scenario: 0, slope: -0.3720402609734585, fluctuations: 0.0\n",
      "step: 10640 loss: 185.006551 time elapsed: 12.9639 learning rate: 0.001504, scenario: 0, slope: -0.36133916520997433, fluctuations: 0.0\n",
      "step: 10650 loss: 181.972248 time elapsed: 12.9760 learning rate: 0.001504, scenario: 0, slope: -0.35035020425243035, fluctuations: 0.0\n",
      "step: 10660 loss: 179.012312 time elapsed: 12.9888 learning rate: 0.001504, scenario: 0, slope: -0.3393209528912925, fluctuations: 0.0\n",
      "step: 10670 loss: 176.111958 time elapsed: 13.0031 learning rate: 0.001504, scenario: 0, slope: -0.3286166095557351, fluctuations: 0.0\n",
      "step: 10680 loss: 173.247091 time elapsed: 13.0166 learning rate: 0.001504, scenario: 0, slope: -0.3186937092423595, fluctuations: 0.0\n",
      "step: 10690 loss: 170.511292 time elapsed: 13.0297 learning rate: 0.001504, scenario: 0, slope: -0.30962358361981085, fluctuations: 0.0\n",
      "step: 10700 loss: 167.850237 time elapsed: 13.0419 learning rate: 0.001504, scenario: 0, slope: -0.30201952594350684, fluctuations: 0.0\n",
      "step: 10710 loss: 165.264496 time elapsed: 13.0549 learning rate: 0.001504, scenario: 0, slope: -0.2932415782335698, fluctuations: 0.0\n",
      "step: 10720 loss: 162.750508 time elapsed: 13.0668 learning rate: 0.001504, scenario: 0, slope: -0.2855537674596501, fluctuations: 0.0\n",
      "step: 10730 loss: 160.304595 time elapsed: 13.0787 learning rate: 0.001504, scenario: 0, slope: -0.2780030945755744, fluctuations: 0.0\n",
      "step: 10740 loss: 157.923694 time elapsed: 13.0906 learning rate: 0.001504, scenario: 0, slope: -0.27051903772280134, fluctuations: 0.0\n",
      "step: 10750 loss: 155.605274 time elapsed: 13.1022 learning rate: 0.001504, scenario: 0, slope: -0.2630807515614268, fluctuations: 0.0\n",
      "step: 10760 loss: 153.346891 time elapsed: 13.1142 learning rate: 0.001504, scenario: 0, slope: -0.25570982036188944, fluctuations: 0.0\n",
      "step: 10770 loss: 151.146204 time elapsed: 13.1262 learning rate: 0.001504, scenario: 0, slope: -0.2484929720075512, fluctuations: 0.0\n",
      "step: 10780 loss: 149.000937 time elapsed: 13.1380 learning rate: 0.001504, scenario: 0, slope: -0.24166459904193716, fluctuations: 0.0\n",
      "step: 10790 loss: 146.908859 time elapsed: 13.1499 learning rate: 0.001504, scenario: 0, slope: -0.23525005775257823, fluctuations: 0.0\n",
      "step: 10800 loss: 144.867787 time elapsed: 13.1617 learning rate: 0.001504, scenario: 0, slope: -0.2297067911026026, fluctuations: 0.0\n",
      "step: 10810 loss: 142.875588 time elapsed: 13.1739 learning rate: 0.001504, scenario: 0, slope: -0.2232066082036398, fluctuations: 0.0\n",
      "step: 10820 loss: 140.930173 time elapsed: 13.1860 learning rate: 0.001504, scenario: 0, slope: -0.2175472190234499, fluctuations: 0.0\n",
      "step: 10830 loss: 139.029507 time elapsed: 13.1978 learning rate: 0.001504, scenario: 0, slope: -0.21211580768726687, fluctuations: 0.0\n",
      "step: 10840 loss: 137.171609 time elapsed: 13.2109 learning rate: 0.001504, scenario: 0, slope: -0.20690559189056323, fluctuations: 0.0\n",
      "step: 10850 loss: 135.354553 time elapsed: 13.2235 learning rate: 0.001504, scenario: 0, slope: -0.2019109897237945, fluctuations: 0.0\n",
      "step: 10860 loss: 133.576474 time elapsed: 13.2360 learning rate: 0.001504, scenario: 0, slope: -0.19712684381204257, fluctuations: 0.0\n",
      "step: 10870 loss: 131.835574 time elapsed: 13.2476 learning rate: 0.001504, scenario: 0, slope: -0.19254799229052807, fluctuations: 0.0\n",
      "step: 10880 loss: 130.130124 time elapsed: 13.2591 learning rate: 0.001504, scenario: 0, slope: -0.18816904750687857, fluctuations: 0.0\n",
      "step: 10890 loss: 128.458477 time elapsed: 13.2711 learning rate: 0.001504, scenario: 0, slope: -0.18398431509609958, fluctuations: 0.0\n",
      "step: 10900 loss: 126.819071 time elapsed: 13.2828 learning rate: 0.001504, scenario: 0, slope: -0.18037908179915857, fluctuations: 0.0\n",
      "step: 10910 loss: 125.210443 time elapsed: 13.2950 learning rate: 0.001504, scenario: 0, slope: -0.17617263352472892, fluctuations: 0.0\n",
      "step: 10920 loss: 123.631234 time elapsed: 13.3066 learning rate: 0.001504, scenario: 0, slope: -0.1725319550909241, fluctuations: 0.0\n",
      "step: 10930 loss: 122.080203 time elapsed: 13.3186 learning rate: 0.001504, scenario: 0, slope: -0.16905784936436866, fluctuations: 0.0\n",
      "step: 10940 loss: 120.556230 time elapsed: 13.3307 learning rate: 0.001504, scenario: 0, slope: -0.1657417612766669, fluctuations: 0.0\n",
      "step: 10950 loss: 119.058318 time elapsed: 13.3426 learning rate: 0.001504, scenario: 0, slope: -0.1625743603728243, fluctuations: 0.0\n",
      "step: 10960 loss: 117.585598 time elapsed: 13.3545 learning rate: 0.001504, scenario: 0, slope: -0.15954555637217338, fluctuations: 0.0\n",
      "step: 10970 loss: 116.137318 time elapsed: 13.3668 learning rate: 0.001504, scenario: 0, slope: -0.1566445833602715, fluctuations: 0.0\n",
      "step: 10980 loss: 114.712835 time elapsed: 13.3783 learning rate: 0.001504, scenario: 0, slope: -0.15386016276425687, fluctuations: 0.0\n",
      "step: 10990 loss: 113.311600 time elapsed: 13.3906 learning rate: 0.001504, scenario: 0, slope: -0.15118074548977287, fluctuations: 0.0\n",
      "step: 11000 loss: 111.933142 time elapsed: 13.4029 learning rate: 0.001504, scenario: 0, slope: -0.14884952742459712, fluctuations: 0.0\n",
      "step: 11010 loss: 110.577045 time elapsed: 13.4158 learning rate: 0.001504, scenario: 0, slope: -0.14609126452279472, fluctuations: 0.0\n",
      "step: 11020 loss: 109.242932 time elapsed: 13.4304 learning rate: 0.001504, scenario: 0, slope: -0.14365970096201605, fluctuations: 0.0\n",
      "step: 11030 loss: 107.930449 time elapsed: 13.4441 learning rate: 0.001504, scenario: 0, slope: -0.14129082380798022, fluctuations: 0.0\n",
      "step: 11040 loss: 106.639252 time elapsed: 13.4567 learning rate: 0.001504, scenario: 0, slope: -0.1389766530461004, fluctuations: 0.0\n",
      "step: 11050 loss: 105.368994 time elapsed: 13.4698 learning rate: 0.001504, scenario: 0, slope: -0.13671068872565487, fluctuations: 0.0\n",
      "step: 11060 loss: 104.119328 time elapsed: 13.4825 learning rate: 0.001504, scenario: 0, slope: -0.13448794806695413, fluctuations: 0.0\n",
      "step: 11070 loss: 102.889897 time elapsed: 13.4947 learning rate: 0.001504, scenario: 0, slope: -0.1323048851416226, fluctuations: 0.0\n",
      "step: 11080 loss: 101.680341 time elapsed: 13.5069 learning rate: 0.001504, scenario: 0, slope: -0.13015920866232536, fluctuations: 0.0\n",
      "step: 11090 loss: 100.490301 time elapsed: 13.5197 learning rate: 0.001504, scenario: 0, slope: -0.12804962591157754, fluctuations: 0.0\n",
      "step: 11100 loss: 99.319421 time elapsed: 13.5320 learning rate: 0.001504, scenario: 0, slope: -0.12618136508991246, fluctuations: 0.0\n",
      "step: 11110 loss: 98.167352 time elapsed: 13.5446 learning rate: 0.001504, scenario: 0, slope: -0.1239367917134201, fluctuations: 0.0\n",
      "step: 11120 loss: 97.033763 time elapsed: 13.5563 learning rate: 0.001504, scenario: 0, slope: -0.12193331140539895, fluctuations: 0.0\n",
      "step: 11130 loss: 95.918336 time elapsed: 13.5676 learning rate: 0.001504, scenario: 0, slope: -0.11996498021967375, fluctuations: 0.0\n",
      "step: 11140 loss: 94.820774 time elapsed: 13.5794 learning rate: 0.001504, scenario: 0, slope: -0.11803143683219222, fluctuations: 0.0\n",
      "step: 11150 loss: 93.740804 time elapsed: 13.5912 learning rate: 0.001504, scenario: 0, slope: -0.11613199835997577, fluctuations: 0.0\n",
      "step: 11160 loss: 92.678169 time elapsed: 13.6031 learning rate: 0.001504, scenario: 0, slope: -0.11426563487265574, fluctuations: 0.0\n",
      "step: 11170 loss: 91.632637 time elapsed: 13.6311 learning rate: 0.001504, scenario: 0, slope: -0.11243099465818097, fluctuations: 0.0\n",
      "step: 11180 loss: 90.603992 time elapsed: 13.6538 learning rate: 0.001504, scenario: 0, slope: -0.11062646669370033, fluctuations: 0.0\n",
      "step: 11190 loss: 89.592034 time elapsed: 13.6670 learning rate: 0.001504, scenario: 0, slope: -0.10885026626132473, fluctuations: 0.0\n",
      "step: 11200 loss: 88.596575 time elapsed: 13.6794 learning rate: 0.001504, scenario: 0, slope: -0.10727436656276804, fluctuations: 0.0\n",
      "step: 11210 loss: 87.617439 time elapsed: 13.6929 learning rate: 0.001504, scenario: 0, slope: -0.10537541531025305, fluctuations: 0.0\n",
      "step: 11220 loss: 86.654453 time elapsed: 13.7052 learning rate: 0.001504, scenario: 0, slope: -0.10367317904275519, fluctuations: 0.0\n",
      "step: 11230 loss: 85.707450 time elapsed: 13.7169 learning rate: 0.001504, scenario: 0, slope: -0.10199225822653064, fluctuations: 0.0\n",
      "step: 11240 loss: 84.776262 time elapsed: 13.7289 learning rate: 0.001504, scenario: 0, slope: -0.10033132148464184, fluctuations: 0.0\n",
      "step: 11250 loss: 83.860717 time elapsed: 13.7411 learning rate: 0.001504, scenario: 0, slope: -0.0986893069736826, fluctuations: 0.0\n",
      "step: 11260 loss: 82.960639 time elapsed: 13.7532 learning rate: 0.001504, scenario: 0, slope: -0.09706544164434427, fluctuations: 0.0\n",
      "step: 11270 loss: 82.075847 time elapsed: 13.7655 learning rate: 0.001504, scenario: 0, slope: -0.09545924420651347, fluctuations: 0.0\n",
      "step: 11280 loss: 81.206149 time elapsed: 13.7772 learning rate: 0.001504, scenario: 0, slope: -0.09387051411971745, fluctuations: 0.0\n",
      "step: 11290 loss: 80.351349 time elapsed: 13.7894 learning rate: 0.001504, scenario: 0, slope: -0.09229930934521519, fluctuations: 0.0\n",
      "step: 11300 loss: 79.511236 time elapsed: 13.8011 learning rate: 0.001504, scenario: 0, slope: -0.09090044131617467, fluctuations: 0.0\n",
      "step: 11310 loss: 78.685593 time elapsed: 13.8138 learning rate: 0.001504, scenario: 0, slope: -0.0892108109351126, fluctuations: 0.0\n",
      "step: 11320 loss: 77.874194 time elapsed: 13.8258 learning rate: 0.001504, scenario: 0, slope: -0.08769462523642103, fluctuations: 0.0\n",
      "step: 11330 loss: 77.076803 time elapsed: 13.8378 learning rate: 0.001504, scenario: 0, slope: -0.08619810193428466, fluctuations: 0.0\n",
      "step: 11340 loss: 76.293176 time elapsed: 13.8496 learning rate: 0.001504, scenario: 0, slope: -0.08472205880431993, fluctuations: 0.0\n",
      "step: 11350 loss: 75.174138 time elapsed: 13.8634 learning rate: 0.003546, scenario: 0, slope: -0.08389858379962616, fluctuations: 0.0\n",
      "step: 11360 loss: 73.421402 time elapsed: 13.8761 learning rate: 0.003546, scenario: 0, slope: -0.08730315307345105, fluctuations: 0.0\n",
      "step: 11370 loss: 74.716404 time elapsed: 13.8884 learning rate: 0.003546, scenario: 0, slope: -0.08991292598044169, fluctuations: 0.03\n",
      "step: 11380 loss: 2347.309487 time elapsed: 13.9003 learning rate: 0.004415, scenario: -1, slope: 2.8851031912134597, fluctuations: 0.04\n",
      "step: 11390 loss: 379.562974 time elapsed: 13.9124 learning rate: 0.001539, scenario: -1, slope: 1.9584113568435142, fluctuations: 0.08\n",
      "step: 11400 loss: 133.796996 time elapsed: 13.9246 learning rate: 0.000596, scenario: -1, slope: 1.7173075153304784, fluctuations: 0.1\n",
      "step: 11410 loss: 77.710051 time elapsed: 13.9372 learning rate: 0.000208, scenario: -1, slope: 1.2273156877402414, fluctuations: 0.12\n",
      "step: 11420 loss: 74.893466 time elapsed: 13.9494 learning rate: 0.000073, scenario: -1, slope: 0.6869243118581707, fluctuations: 0.13\n",
      "step: 11430 loss: 72.082592 time elapsed: 13.9610 learning rate: 0.000025, scenario: -1, slope: 0.13572183590333498, fluctuations: 0.13\n",
      "step: 11440 loss: 71.822802 time elapsed: 13.9727 learning rate: 0.000024, scenario: 0, slope: -0.3940894862111056, fluctuations: 0.13\n",
      "step: 11450 loss: 71.765693 time elapsed: 13.9848 learning rate: 0.000024, scenario: 0, slope: -0.9593984939013765, fluctuations: 0.13\n",
      "step: 11460 loss: 71.688801 time elapsed: 13.9963 learning rate: 0.000024, scenario: 0, slope: -1.643185720789897, fluctuations: 0.13\n",
      "step: 11470 loss: 71.618199 time elapsed: 14.0082 learning rate: 0.000024, scenario: 0, slope: -2.106227079950551, fluctuations: 0.1\n",
      "step: 11480 loss: 71.560833 time elapsed: 14.0203 learning rate: 0.000024, scenario: 0, slope: -0.8810293459814008, fluctuations: 0.08\n",
      "step: 11490 loss: 71.506759 time elapsed: 14.0323 learning rate: 0.000024, scenario: 0, slope: -0.42659774888281077, fluctuations: 0.04\n",
      "step: 11500 loss: 71.453283 time elapsed: 14.0445 learning rate: 0.000024, scenario: 0, slope: -0.1371424897294294, fluctuations: 0.02\n",
      "step: 11510 loss: 71.377161 time elapsed: 14.0584 learning rate: 0.000062, scenario: 1, slope: -0.024241451829148903, fluctuations: 0.01\n",
      "step: 11520 loss: 71.188833 time elapsed: 14.0714 learning rate: 0.000160, scenario: 1, slope: -0.012668740916013357, fluctuations: 0.0\n",
      "step: 11530 loss: 70.744860 time elapsed: 14.0845 learning rate: 0.000415, scenario: 1, slope: -0.008489844081470566, fluctuations: 0.0\n",
      "step: 11540 loss: 69.833783 time elapsed: 14.0976 learning rate: 0.001076, scenario: 1, slope: -0.012882010271298486, fluctuations: 0.0\n",
      "step: 11550 loss: 68.428323 time elapsed: 14.1105 learning rate: 0.002791, scenario: 1, slope: -0.022585948484425737, fluctuations: 0.0\n",
      "step: 11560 loss: 66.644905 time elapsed: 14.1225 learning rate: 0.007240, scenario: 1, slope: -0.038036524422850174, fluctuations: 0.0\n",
      "step: 11570 loss: 63.120131 time elapsed: 14.1347 learning rate: 0.018779, scenario: 1, slope: -0.06398713175659879, fluctuations: 0.0\n",
      "step: 11580 loss: 21228.359062 time elapsed: 14.1470 learning rate: 0.008488, scenario: -1, slope: 99.6047144662085, fluctuations: 0.03\n",
      "step: 11590 loss: 7657.396767 time elapsed: 14.1593 learning rate: 0.002960, scenario: -1, slope: 140.43154904477373, fluctuations: 0.05\n",
      "step: 11600 loss: 4312.428049 time elapsed: 14.1713 learning rate: 0.001147, scenario: -1, slope: 135.39874909010175, fluctuations: 0.05\n",
      "step: 11610 loss: 3727.410044 time elapsed: 14.1838 learning rate: 0.000400, scenario: -1, slope: 111.02686940557419, fluctuations: 0.05\n",
      "step: 11620 loss: 3414.980944 time elapsed: 14.1959 learning rate: 0.000139, scenario: -1, slope: 83.14812225572517, fluctuations: 0.05\n",
      "step: 11630 loss: 3351.291896 time elapsed: 14.2084 learning rate: 0.000049, scenario: -1, slope: 51.66453478895881, fluctuations: 0.05\n",
      "step: 11640 loss: 3328.633449 time elapsed: 14.2204 learning rate: 0.000017, scenario: -1, slope: 16.092188725575912, fluctuations: 0.05\n",
      "step: 11650 loss: 3320.214819 time elapsed: 14.2324 learning rate: 0.000011, scenario: 0, slope: -24.883860897639597, fluctuations: 0.05\n",
      "step: 11660 loss: 3313.305597 time elapsed: 14.2446 learning rate: 0.000011, scenario: 0, slope: -73.23475286640087, fluctuations: 0.05\n",
      "step: 11670 loss: 3306.533017 time elapsed: 14.2564 learning rate: 0.000011, scenario: 0, slope: -131.84953674226355, fluctuations: 0.05\n",
      "step: 11680 loss: 3299.883548 time elapsed: 14.2696 learning rate: 0.000011, scenario: 0, slope: -68.74169246024876, fluctuations: 0.01\n",
      "step: 11690 loss: 3293.340676 time elapsed: 14.2824 learning rate: 0.000011, scenario: 0, slope: -17.61291636374962, fluctuations: 0.0\n",
      "step: 11700 loss: 3286.889104 time elapsed: 14.2942 learning rate: 0.000011, scenario: 0, slope: -5.499989006791558, fluctuations: 0.0\n",
      "step: 11710 loss: 3278.940031 time elapsed: 14.3070 learning rate: 0.000024, scenario: 1, slope: -1.765361169553351, fluctuations: 0.0\n",
      "step: 11720 loss: 3259.563216 time elapsed: 14.3185 learning rate: 0.000062, scenario: 1, slope: -0.9489708737727905, fluctuations: 0.0\n",
      "step: 11730 loss: 3211.347453 time elapsed: 14.3309 learning rate: 0.000160, scenario: 1, slope: -0.9214940172147322, fluctuations: 0.0\n",
      "step: 11740 loss: 3095.399665 time elapsed: 14.3427 learning rate: 0.000416, scenario: 1, slope: -1.4247020668989274, fluctuations: 0.0\n",
      "step: 11750 loss: 2832.832050 time elapsed: 14.3546 learning rate: 0.001079, scenario: 1, slope: -2.830334633845223, fluctuations: 0.0\n",
      "step: 11760 loss: 2403.783728 time elapsed: 14.3665 learning rate: 0.001305, scenario: 0, slope: -5.882618487544259, fluctuations: 0.0\n",
      "step: 11770 loss: 2071.679268 time elapsed: 14.3785 learning rate: 0.001305, scenario: 0, slope: -10.173886867212524, fluctuations: 0.0\n",
      "step: 11780 loss: 1831.605167 time elapsed: 14.3903 learning rate: 0.001305, scenario: 0, slope: -14.687482607055877, fluctuations: 0.0\n",
      "step: 11790 loss: 1651.541526 time elapsed: 14.4022 learning rate: 0.001305, scenario: 0, slope: -18.703219068924522, fluctuations: 0.0\n",
      "step: 11800 loss: 1512.614429 time elapsed: 14.4137 learning rate: 0.001305, scenario: 0, slope: -21.474274492578175, fluctuations: 0.0\n",
      "step: 11810 loss: 1402.275839 time elapsed: 14.4264 learning rate: 0.001305, scenario: 0, slope: -23.378124937096253, fluctuations: 0.0\n",
      "step: 11820 loss: 1311.903097 time elapsed: 14.4386 learning rate: 0.001305, scenario: 0, slope: -23.454228632350397, fluctuations: 0.0\n",
      "step: 11830 loss: 1235.531371 time elapsed: 14.4504 learning rate: 0.001305, scenario: 0, slope: -21.886822330153752, fluctuations: 0.0\n",
      "step: 11840 loss: 1168.948292 time elapsed: 14.4634 learning rate: 0.001305, scenario: 0, slope: -18.86575078471208, fluctuations: 0.0\n",
      "step: 11850 loss: 1107.717042 time elapsed: 14.4765 learning rate: 0.001305, scenario: 0, slope: -15.06242516339694, fluctuations: 0.0\n",
      "step: 11860 loss: 1048.710119 time elapsed: 14.4898 learning rate: 0.001305, scenario: 0, slope: -11.828489115312937, fluctuations: 0.0\n",
      "step: 11870 loss: 997.505653 time elapsed: 14.5030 learning rate: 0.001305, scenario: 0, slope: -9.602274870460395, fluctuations: 0.0\n",
      "step: 11880 loss: 952.540748 time elapsed: 14.5157 learning rate: 0.001305, scenario: 0, slope: -8.048628060145994, fluctuations: 0.0\n",
      "step: 11890 loss: 912.104117 time elapsed: 14.5290 learning rate: 0.001305, scenario: 0, slope: -6.929956243036113, fluctuations: 0.0\n",
      "step: 11900 loss: 875.245456 time elapsed: 14.5410 learning rate: 0.001305, scenario: 0, slope: -6.16890133384138, fluctuations: 0.0\n",
      "step: 11910 loss: 841.469105 time elapsed: 14.5542 learning rate: 0.001305, scenario: 0, slope: -5.44275322928472, fluctuations: 0.0\n",
      "step: 11920 loss: 810.268993 time elapsed: 14.5666 learning rate: 0.001305, scenario: 0, slope: -4.906396109860752, fluctuations: 0.0\n",
      "step: 11930 loss: 781.233034 time elapsed: 14.5788 learning rate: 0.001305, scenario: 0, slope: -4.443564677751254, fluctuations: 0.0\n",
      "step: 11940 loss: 753.967868 time elapsed: 14.5913 learning rate: 0.001305, scenario: 0, slope: -4.029552038049463, fluctuations: 0.0\n",
      "step: 11950 loss: 727.829108 time elapsed: 14.6034 learning rate: 0.001305, scenario: 0, slope: -3.6576853071195186, fluctuations: 0.0\n",
      "step: 11960 loss: 700.206032 time elapsed: 14.6157 learning rate: 0.001305, scenario: 0, slope: -3.3564848427825025, fluctuations: 0.0\n",
      "step: 11970 loss: 651.512861 time elapsed: 14.6276 learning rate: 0.001305, scenario: 0, slope: -3.1818147241182997, fluctuations: 0.0\n",
      "step: 11980 loss: 613.642869 time elapsed: 14.6398 learning rate: 0.001305, scenario: 0, slope: -3.154028526905937, fluctuations: 0.0\n",
      "step: 11990 loss: 578.310977 time elapsed: 14.6520 learning rate: 0.001305, scenario: 0, slope: -3.197264487278586, fluctuations: 0.0\n",
      "step: 12000 loss: 552.494488 time elapsed: 14.6634 learning rate: 0.001305, scenario: 0, slope: -3.2486480613158637, fluctuations: 0.0\n",
      "step: 12010 loss: 533.977811 time elapsed: 14.6786 learning rate: 0.001305, scenario: 0, slope: -3.2600123666645304, fluctuations: 0.0\n",
      "step: 12020 loss: 517.710485 time elapsed: 14.6917 learning rate: 0.001305, scenario: 0, slope: -3.1961158069008313, fluctuations: 0.0\n",
      "step: 12030 loss: 503.134144 time elapsed: 14.7040 learning rate: 0.001305, scenario: 0, slope: -3.053715623626217, fluctuations: 0.0\n",
      "step: 12040 loss: 489.656342 time elapsed: 14.7166 learning rate: 0.001305, scenario: 0, slope: -2.8300508368365134, fluctuations: 0.0\n",
      "step: 12050 loss: 476.966623 time elapsed: 14.7294 learning rate: 0.001305, scenario: 0, slope: -2.5275887239522445, fluctuations: 0.0\n",
      "step: 12060 loss: 464.979007 time elapsed: 14.7419 learning rate: 0.001305, scenario: 0, slope: -2.157817199784838, fluctuations: 0.0\n",
      "step: 12070 loss: 453.612026 time elapsed: 14.7546 learning rate: 0.001305, scenario: 0, slope: -1.8000970400188796, fluctuations: 0.0\n",
      "step: 12080 loss: 442.791605 time elapsed: 14.7667 learning rate: 0.001305, scenario: 0, slope: -1.5430593587183774, fluctuations: 0.0\n",
      "step: 12090 loss: 432.455365 time elapsed: 14.7781 learning rate: 0.001305, scenario: 0, slope: -1.3695236040891658, fluctuations: 0.0\n",
      "step: 12100 loss: 422.554797 time elapsed: 14.7902 learning rate: 0.001305, scenario: 0, slope: -1.2717819398713057, fluctuations: 0.0\n",
      "step: 12110 loss: 413.049733 time elapsed: 14.8028 learning rate: 0.001305, scenario: 0, slope: -1.1856575266533558, fluctuations: 0.0\n",
      "step: 12120 loss: 403.906798 time elapsed: 14.8143 learning rate: 0.001305, scenario: 0, slope: -1.1223728848190002, fluctuations: 0.0\n",
      "step: 12130 loss: 395.097359 time elapsed: 14.8263 learning rate: 0.001305, scenario: 0, slope: -1.068373613358915, fluctuations: 0.0\n",
      "step: 12140 loss: 386.596257 time elapsed: 14.8382 learning rate: 0.001305, scenario: 0, slope: -1.0203400049256677, fluctuations: 0.0\n",
      "step: 12150 loss: 378.380958 time elapsed: 14.8499 learning rate: 0.001305, scenario: 0, slope: -0.9771381052639161, fluctuations: 0.0\n",
      "step: 12160 loss: 370.430742 time elapsed: 14.8618 learning rate: 0.001305, scenario: 0, slope: -0.9380261046927817, fluctuations: 0.0\n",
      "step: 12170 loss: 362.726016 time elapsed: 14.8737 learning rate: 0.001305, scenario: 0, slope: -0.9023690999954084, fluctuations: 0.0\n",
      "step: 12180 loss: 355.247591 time elapsed: 14.8855 learning rate: 0.001305, scenario: 0, slope: -0.8696614028991431, fluctuations: 0.0\n",
      "step: 12190 loss: 347.975806 time elapsed: 14.8977 learning rate: 0.001305, scenario: 0, slope: -0.8395414224185366, fluctuations: 0.0\n",
      "step: 12200 loss: 340.889317 time elapsed: 14.9108 learning rate: 0.001305, scenario: 0, slope: -0.814440376564208, fluctuations: 0.0\n",
      "step: 12210 loss: 333.963233 time elapsed: 14.9235 learning rate: 0.001305, scenario: 0, slope: -0.7861802001287443, fluctuations: 0.0\n",
      "step: 12220 loss: 327.166035 time elapsed: 14.9372 learning rate: 0.001305, scenario: 0, slope: -0.7627513417038735, fluctuations: 0.0\n",
      "step: 12230 loss: 320.454202 time elapsed: 14.9504 learning rate: 0.001305, scenario: 0, slope: -0.7415616023197753, fluctuations: 0.0\n",
      "step: 12240 loss: 313.762734 time elapsed: 14.9640 learning rate: 0.001305, scenario: 0, slope: -0.7228801999434347, fluctuations: 0.0\n",
      "step: 12250 loss: 306.989349 time elapsed: 14.9759 learning rate: 0.001305, scenario: 0, slope: -0.7072686318047244, fluctuations: 0.0\n",
      "step: 12260 loss: 299.975595 time elapsed: 14.9885 learning rate: 0.001305, scenario: 0, slope: -0.6957553560050882, fluctuations: 0.0\n",
      "step: 12270 loss: 292.525296 time elapsed: 15.0008 learning rate: 0.001305, scenario: 0, slope: -0.6899787347383156, fluctuations: 0.0\n",
      "step: 12280 loss: 284.609000 time elapsed: 15.0132 learning rate: 0.001305, scenario: 0, slope: -0.6916876411785006, fluctuations: 0.0\n",
      "step: 12290 loss: 276.726476 time elapsed: 15.0260 learning rate: 0.001305, scenario: 0, slope: -0.7005123971085722, fluctuations: 0.0\n",
      "step: 12300 loss: 269.491467 time elapsed: 15.0380 learning rate: 0.001305, scenario: 0, slope: -0.7110764353430196, fluctuations: 0.0\n",
      "step: 12310 loss: 262.990865 time elapsed: 15.0507 learning rate: 0.001305, scenario: 0, slope: -0.7211884808043494, fluctuations: 0.0\n",
      "step: 12320 loss: 256.952980 time elapsed: 15.0627 learning rate: 0.001305, scenario: 0, slope: -0.7234886367919406, fluctuations: 0.0\n",
      "step: 12330 loss: 251.000038 time elapsed: 15.0751 learning rate: 0.001305, scenario: 0, slope: -0.7177848790130588, fluctuations: 0.0\n",
      "step: 12340 loss: 244.386732 time elapsed: 15.0873 learning rate: 0.001305, scenario: 0, slope: -0.7061671433535522, fluctuations: 0.0\n",
      "step: 12350 loss: 234.711395 time elapsed: 15.0989 learning rate: 0.001305, scenario: 0, slope: -0.6992252941177244, fluctuations: 0.0\n",
      "step: 12360 loss: 222.931444 time elapsed: 15.1122 learning rate: 0.001305, scenario: 0, slope: -0.7155657530801499, fluctuations: 0.0\n",
      "step: 12370 loss: 214.094288 time elapsed: 15.1245 learning rate: 0.001305, scenario: 0, slope: -0.7438142163994557, fluctuations: 0.0\n",
      "step: 12380 loss: 208.266327 time elapsed: 15.1362 learning rate: 0.001305, scenario: 0, slope: -0.7670251894339594, fluctuations: 0.0\n",
      "step: 12390 loss: 202.824478 time elapsed: 15.1483 learning rate: 0.001305, scenario: 0, slope: -0.7795436429766542, fluctuations: 0.0\n",
      "step: 12400 loss: 197.850922 time elapsed: 15.1601 learning rate: 0.001305, scenario: 0, slope: -0.7807260483526509, fluctuations: 0.0\n",
      "step: 12410 loss: 193.316430 time elapsed: 15.1728 learning rate: 0.001305, scenario: 0, slope: -0.7640386696791527, fluctuations: 0.0\n",
      "step: 12420 loss: 189.117490 time elapsed: 15.1846 learning rate: 0.001305, scenario: 0, slope: -0.7278255812247895, fluctuations: 0.0\n",
      "step: 12430 loss: 185.213729 time elapsed: 15.1968 learning rate: 0.001305, scenario: 0, slope: -0.6702724449527147, fluctuations: 0.0\n",
      "step: 12440 loss: 181.567013 time elapsed: 15.2087 learning rate: 0.001305, scenario: 0, slope: -0.5938621759535058, fluctuations: 0.0\n",
      "step: 12450 loss: 178.147713 time elapsed: 15.2210 learning rate: 0.001305, scenario: 0, slope: -0.5116915423660958, fluctuations: 0.0\n",
      "step: 12460 loss: 174.935843 time elapsed: 15.2333 learning rate: 0.001305, scenario: 0, slope: -0.4515526013192889, fluctuations: 0.0\n",
      "step: 12470 loss: 171.915838 time elapsed: 15.2453 learning rate: 0.001305, scenario: 0, slope: -0.41344531194955186, fluctuations: 0.0\n",
      "step: 12480 loss: 169.070836 time elapsed: 15.2573 learning rate: 0.001305, scenario: 0, slope: -0.38448209855879606, fluctuations: 0.0\n",
      "step: 12490 loss: 166.380661 time elapsed: 15.2699 learning rate: 0.001305, scenario: 0, slope: -0.3582308774731019, fluctuations: 0.0\n",
      "step: 12500 loss: 163.823716 time elapsed: 15.2815 learning rate: 0.001305, scenario: 0, slope: -0.3374821674981674, fluctuations: 0.0\n",
      "step: 12510 loss: 161.380193 time elapsed: 15.2946 learning rate: 0.001305, scenario: 0, slope: -0.3149554336659763, fluctuations: 0.0\n",
      "step: 12520 loss: 159.033858 time elapsed: 15.3071 learning rate: 0.001305, scenario: 0, slope: -0.2967646302818921, fluctuations: 0.0\n",
      "step: 12530 loss: 156.772371 time elapsed: 15.3210 learning rate: 0.001305, scenario: 0, slope: -0.2805509915423867, fluctuations: 0.0\n",
      "step: 12540 loss: 154.586715 time elapsed: 15.3340 learning rate: 0.001305, scenario: 0, slope: -0.26615559380036696, fluctuations: 0.0\n",
      "step: 12550 loss: 152.470349 time elapsed: 15.3476 learning rate: 0.001305, scenario: 0, slope: -0.2534507437438827, fluctuations: 0.0\n",
      "step: 12560 loss: 150.418441 time elapsed: 15.3639 learning rate: 0.001305, scenario: 0, slope: -0.24229888447921466, fluctuations: 0.0\n",
      "step: 12570 loss: 148.427288 time elapsed: 15.3775 learning rate: 0.001305, scenario: 0, slope: -0.23251827209083378, fluctuations: 0.0\n",
      "step: 12580 loss: 146.493914 time elapsed: 15.3906 learning rate: 0.001305, scenario: 0, slope: -0.22388594133515813, fluctuations: 0.0\n",
      "step: 12590 loss: 144.615792 time elapsed: 15.4034 learning rate: 0.001305, scenario: 0, slope: -0.21616801448621095, fluctuations: 0.0\n",
      "step: 12600 loss: 142.790650 time elapsed: 15.4153 learning rate: 0.001305, scenario: 0, slope: -0.20982893159912308, fluctuations: 0.0\n",
      "step: 12610 loss: 141.016332 time elapsed: 15.4277 learning rate: 0.001305, scenario: 0, slope: -0.20267653818009307, fluctuations: 0.0\n",
      "step: 12620 loss: 139.290695 time elapsed: 15.4393 learning rate: 0.001305, scenario: 0, slope: -0.1966136803962915, fluctuations: 0.0\n",
      "step: 12630 loss: 137.611527 time elapsed: 15.4514 learning rate: 0.001305, scenario: 0, slope: -0.19088336165673786, fluctuations: 0.0\n",
      "step: 12640 loss: 135.976478 time elapsed: 15.4633 learning rate: 0.001305, scenario: 0, slope: -0.18543391645775292, fluctuations: 0.0\n",
      "step: 12650 loss: 134.383031 time elapsed: 15.4752 learning rate: 0.001305, scenario: 0, slope: -0.18023582963816517, fluctuations: 0.0\n",
      "step: 12660 loss: 132.828520 time elapsed: 15.4871 learning rate: 0.001305, scenario: 0, slope: -0.17527526139276828, fluctuations: 0.0\n",
      "step: 12670 loss: 131.310298 time elapsed: 15.4986 learning rate: 0.001305, scenario: 0, slope: -0.1705488180321823, fluctuations: 0.0\n",
      "step: 12680 loss: 129.826173 time elapsed: 15.5097 learning rate: 0.001305, scenario: 0, slope: -0.16605810241241742, fluctuations: 0.0\n",
      "step: 12690 loss: 128.375134 time elapsed: 15.5230 learning rate: 0.001305, scenario: 0, slope: -0.1618023382798854, fluctuations: 0.0\n",
      "step: 12700 loss: 126.957704 time elapsed: 15.5349 learning rate: 0.001305, scenario: 0, slope: -0.1581634280663851, fluctuations: 0.0\n",
      "step: 12710 loss: 125.574446 time elapsed: 15.5475 learning rate: 0.001305, scenario: 0, slope: -0.15393315522067486, fluctuations: 0.0\n",
      "step: 12720 loss: 124.223539 time elapsed: 15.5592 learning rate: 0.001305, scenario: 0, slope: -0.15026329456739368, fluctuations: 0.0\n",
      "step: 12730 loss: 122.901444 time elapsed: 15.5707 learning rate: 0.001305, scenario: 0, slope: -0.14674096592332408, fluctuations: 0.0\n",
      "step: 12740 loss: 121.604981 time elapsed: 15.5826 learning rate: 0.001305, scenario: 0, slope: -0.14336032519958655, fluctuations: 0.0\n",
      "step: 12750 loss: 120.331502 time elapsed: 15.5944 learning rate: 0.001305, scenario: 0, slope: -0.14012517224913817, fluctuations: 0.0\n",
      "step: 12760 loss: 119.078599 time elapsed: 15.6063 learning rate: 0.001305, scenario: 0, slope: -0.13704763846088364, fluctuations: 0.0\n",
      "step: 12770 loss: 117.843962 time elapsed: 15.6178 learning rate: 0.001305, scenario: 0, slope: -0.13414801608254698, fluctuations: 0.0\n",
      "step: 12780 loss: 116.625381 time elapsed: 15.6297 learning rate: 0.001305, scenario: 0, slope: -0.13145317116485944, fluctuations: 0.0\n",
      "step: 12790 loss: 115.420769 time elapsed: 15.6412 learning rate: 0.001305, scenario: 0, slope: -0.1289906847763323, fluctuations: 0.0\n",
      "step: 12800 loss: 114.228200 time elapsed: 15.6532 learning rate: 0.001305, scenario: 0, slope: -0.12698776156094455, fluctuations: 0.0\n",
      "step: 12810 loss: 112.507641 time elapsed: 15.6661 learning rate: 0.002798, scenario: 0, slope: -0.12578417446808682, fluctuations: 0.0\n",
      "step: 12820 loss: 110.015081 time elapsed: 15.6780 learning rate: 0.002798, scenario: 0, slope: -0.1308173617148894, fluctuations: 0.0\n",
      "step: 12830 loss: 107.568391 time elapsed: 15.6899 learning rate: 0.002798, scenario: 0, slope: -0.14154773320625919, fluctuations: 0.0\n",
      "step: 12840 loss: 105.169402 time elapsed: 15.7021 learning rate: 0.002798, scenario: 0, slope: -0.15618516921768602, fluctuations: 0.0\n",
      "step: 12850 loss: 102.822534 time elapsed: 15.7136 learning rate: 0.002798, scenario: 0, slope: -0.1729638682028941, fluctuations: 0.0\n",
      "step: 12860 loss: 100.750984 time elapsed: 15.7271 learning rate: 0.002798, scenario: 0, slope: -0.1889053706555192, fluctuations: 0.01\n",
      "step: 12870 loss: 98.727046 time elapsed: 15.7396 learning rate: 0.002798, scenario: 0, slope: -0.20301688567624457, fluctuations: 0.04\n",
      "step: 12880 loss: 96.269206 time elapsed: 15.7519 learning rate: 0.002798, scenario: 0, slope: -0.21617670354542984, fluctuations: 0.05\n",
      "step: 12890 loss: 94.192338 time elapsed: 15.7644 learning rate: 0.002798, scenario: 0, slope: -0.22443306065284901, fluctuations: 0.05\n",
      "step: 12900 loss: 92.198802 time elapsed: 15.7762 learning rate: 0.002798, scenario: 0, slope: -0.22649704887258423, fluctuations: 0.05\n",
      "step: 12910 loss: 90.280926 time elapsed: 15.7887 learning rate: 0.002798, scenario: 0, slope: -0.22188760544048264, fluctuations: 0.05\n",
      "step: 12920 loss: 88.415799 time elapsed: 15.8005 learning rate: 0.002798, scenario: 0, slope: -0.2158702313424141, fluctuations: 0.05\n",
      "step: 12930 loss: 86.615887 time elapsed: 15.8125 learning rate: 0.002798, scenario: 0, slope: -0.20966980333653742, fluctuations: 0.05\n",
      "step: 12940 loss: 84.873797 time elapsed: 15.8239 learning rate: 0.002798, scenario: 0, slope: -0.20330204909785982, fluctuations: 0.05\n",
      "step: 12950 loss: 83.187312 time elapsed: 15.8355 learning rate: 0.002798, scenario: 0, slope: -0.19678969536803123, fluctuations: 0.05\n",
      "step: 12960 loss: 81.551934 time elapsed: 15.8471 learning rate: 0.002798, scenario: 0, slope: -0.19062438603322934, fluctuations: 0.03\n",
      "step: 12970 loss: 79.965030 time elapsed: 15.8585 learning rate: 0.002798, scenario: 0, slope: -0.18413303960620697, fluctuations: 0.0\n",
      "step: 12980 loss: 78.424375 time elapsed: 15.8703 learning rate: 0.002798, scenario: 0, slope: -0.1777841377464128, fluctuations: 0.0\n",
      "step: 12990 loss: 77.057184 time elapsed: 15.8820 learning rate: 0.002798, scenario: 0, slope: -0.1718221400161041, fluctuations: 0.0\n",
      "step: 13000 loss: 75.538580 time elapsed: 15.8937 learning rate: 0.002798, scenario: 0, slope: -0.16622802772582154, fluctuations: 0.0\n",
      "step: 13010 loss: 74.111524 time elapsed: 15.9063 learning rate: 0.002798, scenario: 0, slope: -0.1604525565901098, fluctuations: 0.0\n",
      "step: 13020 loss: 72.745107 time elapsed: 15.9179 learning rate: 0.002798, scenario: 0, slope: -0.15564404504232351, fluctuations: 0.0\n",
      "step: 13030 loss: 71.417865 time elapsed: 15.9306 learning rate: 0.002798, scenario: 0, slope: -0.15108017082264202, fluctuations: 0.0\n",
      "step: 13040 loss: 70.132238 time elapsed: 15.9432 learning rate: 0.002798, scenario: 0, slope: -0.14672354775855778, fluctuations: 0.0\n",
      "step: 13050 loss: 68.886216 time elapsed: 15.9552 learning rate: 0.002798, scenario: 0, slope: -0.14254745626238022, fluctuations: 0.0\n",
      "step: 13060 loss: 67.674938 time elapsed: 15.9671 learning rate: 0.002798, scenario: 0, slope: -0.13854766415001218, fluctuations: 0.0\n",
      "step: 13070 loss: 66.497753 time elapsed: 15.9788 learning rate: 0.002798, scenario: 0, slope: -0.13472602946380444, fluctuations: 0.0\n",
      "step: 13080 loss: 65.353715 time elapsed: 15.9910 learning rate: 0.002798, scenario: 0, slope: -0.13108866544139983, fluctuations: 0.0\n",
      "step: 13090 loss: 64.241902 time elapsed: 16.0032 learning rate: 0.002798, scenario: 0, slope: -0.12748990583388112, fluctuations: 0.0\n",
      "step: 13100 loss: 63.161619 time elapsed: 16.0153 learning rate: 0.002798, scenario: 0, slope: -0.1236096753557506, fluctuations: 0.0\n",
      "step: 13110 loss: 62.111978 time elapsed: 16.0279 learning rate: 0.002798, scenario: 0, slope: -0.11956099491180436, fluctuations: 0.0\n",
      "step: 13120 loss: 61.092167 time elapsed: 16.0396 learning rate: 0.002798, scenario: 0, slope: -0.11613974898922186, fluctuations: 0.0\n",
      "step: 13130 loss: 60.101361 time elapsed: 16.0518 learning rate: 0.002798, scenario: 0, slope: -0.11283941121993413, fluctuations: 0.0\n",
      "step: 13140 loss: 59.138736 time elapsed: 16.0638 learning rate: 0.002798, scenario: 0, slope: -0.1096445286171176, fluctuations: 0.0\n",
      "step: 13150 loss: 58.203469 time elapsed: 16.0759 learning rate: 0.002798, scenario: 0, slope: -0.10653787336605278, fluctuations: 0.0\n",
      "step: 13160 loss: 57.295081 time elapsed: 16.0879 learning rate: 0.002798, scenario: 0, slope: -0.10351583595891282, fluctuations: 0.0\n",
      "step: 13170 loss: 56.474080 time elapsed: 16.0999 learning rate: 0.002798, scenario: 0, slope: -0.1004886460416029, fluctuations: 0.0\n",
      "step: 13180 loss: 55.617938 time elapsed: 16.1117 learning rate: 0.002798, scenario: 0, slope: -0.09727235091289946, fluctuations: 0.0\n",
      "step: 13190 loss: 54.740439 time elapsed: 16.1250 learning rate: 0.002798, scenario: 0, slope: -0.09449502577539698, fluctuations: 0.0\n",
      "step: 13200 loss: 53.918975 time elapsed: 16.1380 learning rate: 0.002798, scenario: 0, slope: -0.09211532822180497, fluctuations: 0.0\n",
      "step: 13210 loss: 53.131946 time elapsed: 16.1516 learning rate: 0.002798, scenario: 0, slope: -0.0893270542678121, fluctuations: 0.0\n",
      "step: 13220 loss: 52.368046 time elapsed: 16.1644 learning rate: 0.002798, scenario: 0, slope: -0.08689026740772904, fluctuations: 0.0\n",
      "step: 13230 loss: 51.622764 time elapsed: 16.1771 learning rate: 0.002798, scenario: 0, slope: -0.08454566943187883, fluctuations: 0.0\n",
      "step: 13240 loss: 50.897647 time elapsed: 16.1895 learning rate: 0.002798, scenario: 0, slope: -0.08229492516903504, fluctuations: 0.0\n",
      "step: 13250 loss: 50.190867 time elapsed: 16.2016 learning rate: 0.002798, scenario: 0, slope: -0.08014300348322721, fluctuations: 0.0\n",
      "step: 13260 loss: 49.501745 time elapsed: 16.2139 learning rate: 0.002798, scenario: 0, slope: -0.07809343236786828, fluctuations: 0.0\n",
      "step: 13270 loss: 48.829286 time elapsed: 16.2260 learning rate: 0.002798, scenario: 0, slope: -0.07605500074352661, fluctuations: 0.0\n",
      "step: 13280 loss: 48.172753 time elapsed: 16.2384 learning rate: 0.002798, scenario: 0, slope: -0.07371760728801137, fluctuations: 0.0\n",
      "step: 13290 loss: 47.531591 time elapsed: 16.2500 learning rate: 0.002798, scenario: 0, slope: -0.07175467815156826, fluctuations: 0.0\n",
      "step: 13300 loss: 46.905513 time elapsed: 16.2615 learning rate: 0.002798, scenario: 0, slope: -0.07009545709281455, fluctuations: 0.0\n",
      "step: 13310 loss: 46.294569 time elapsed: 16.2738 learning rate: 0.002798, scenario: 0, slope: -0.06817712751620894, fluctuations: 0.0\n",
      "step: 13320 loss: 45.700055 time elapsed: 16.2856 learning rate: 0.002798, scenario: 0, slope: -0.0665082814376197, fluctuations: 0.0\n",
      "step: 13330 loss: 45.328120 time elapsed: 16.2976 learning rate: 0.002798, scenario: 0, slope: -0.06456359464566241, fluctuations: 0.0\n",
      "step: 13340 loss: 44.574822 time elapsed: 16.3096 learning rate: 0.002798, scenario: 0, slope: -0.06264489893224254, fluctuations: 0.01\n",
      "step: 13350 loss: 44.023728 time elapsed: 16.3212 learning rate: 0.002798, scenario: 0, slope: -0.061064738096136086, fluctuations: 0.01\n",
      "step: 13360 loss: 43.485414 time elapsed: 16.3337 learning rate: 0.002798, scenario: 0, slope: -0.05962104242223022, fluctuations: 0.01\n",
      "step: 13370 loss: 42.971964 time elapsed: 16.3471 learning rate: 0.002798, scenario: 0, slope: -0.0582288813634794, fluctuations: 0.01\n",
      "step: 13380 loss: 42.468028 time elapsed: 16.3591 learning rate: 0.002798, scenario: 0, slope: -0.05685075239941909, fluctuations: 0.01\n",
      "step: 13390 loss: 41.983824 time elapsed: 16.3720 learning rate: 0.002798, scenario: 0, slope: -0.05547685240036603, fluctuations: 0.01\n",
      "step: 13400 loss: 41.512361 time elapsed: 16.3841 learning rate: 0.002798, scenario: 0, slope: -0.054247923178379544, fluctuations: 0.01\n",
      "step: 13410 loss: 41.054168 time elapsed: 16.3967 learning rate: 0.002798, scenario: 0, slope: -0.052766035928276875, fluctuations: 0.01\n",
      "step: 13420 loss: 40.608604 time elapsed: 16.4084 learning rate: 0.002798, scenario: 0, slope: -0.05144822175076945, fluctuations: 0.01\n",
      "step: 13430 loss: 40.180579 time elapsed: 16.4204 learning rate: 0.002798, scenario: 0, slope: -0.04997129062481528, fluctuations: 0.0\n",
      "step: 13440 loss: 40.035672 time elapsed: 16.4320 learning rate: 0.002798, scenario: 0, slope: -0.04755732101314871, fluctuations: 0.01\n",
      "step: 13450 loss: 39.359691 time elapsed: 16.4441 learning rate: 0.002798, scenario: 0, slope: -0.04584203416893838, fluctuations: 0.02\n",
      "step: 13460 loss: 38.960910 time elapsed: 16.4560 learning rate: 0.002798, scenario: 0, slope: -0.0444930315995409, fluctuations: 0.02\n",
      "step: 13470 loss: 38.552448 time elapsed: 16.4679 learning rate: 0.002798, scenario: 0, slope: -0.0434640873340303, fluctuations: 0.02\n",
      "step: 13480 loss: 38.159889 time elapsed: 16.4798 learning rate: 0.002798, scenario: 0, slope: -0.04256135194664731, fluctuations: 0.02\n",
      "step: 13490 loss: 38.034773 time elapsed: 16.4921 learning rate: 0.004957, scenario: 1, slope: -0.041607246684342736, fluctuations: 0.02\n",
      "step: 13500 loss: 375.616275 time elapsed: 16.5045 learning rate: 0.003043, scenario: -1, slope: 1.2322932130907776, fluctuations: 0.04\n",
      "step: 13510 loss: 76.851157 time elapsed: 16.5169 learning rate: 0.001061, scenario: -1, slope: 1.4544725732631134, fluctuations: 0.08\n",
      "step: 13520 loss: 51.807930 time elapsed: 16.5292 learning rate: 0.000370, scenario: -1, slope: 1.202660591396962, fluctuations: 0.1\n",
      "step: 13530 loss: 43.786666 time elapsed: 16.5423 learning rate: 0.000129, scenario: -1, slope: 0.7995858072259722, fluctuations: 0.11\n",
      "step: 13540 loss: 42.824217 time elapsed: 16.5559 learning rate: 0.000045, scenario: -1, slope: 0.39985780629313716, fluctuations: 0.11\n",
      "step: 13550 loss: 42.289401 time elapsed: 16.5689 learning rate: 0.000016, scenario: -1, slope: 0.002907634986570327, fluctuations: 0.1\n",
      "step: 13560 loss: 42.152954 time elapsed: 16.5819 learning rate: 0.000017, scenario: 0, slope: -0.3900685578782057, fluctuations: 0.1\n",
      "step: 13570 loss: 42.087416 time elapsed: 16.5954 learning rate: 0.000017, scenario: 0, slope: -0.8025182226540643, fluctuations: 0.1\n",
      "step: 13580 loss: 42.037105 time elapsed: 16.6089 learning rate: 0.000017, scenario: 0, slope: -1.271084735365608, fluctuations: 0.1\n",
      "step: 13590 loss: 41.990611 time elapsed: 16.6216 learning rate: 0.000017, scenario: 0, slope: -1.8477161736862506, fluctuations: 0.1\n",
      "step: 13600 loss: 41.947047 time elapsed: 16.6364 learning rate: 0.000017, scenario: 0, slope: -0.6933662421496408, fluctuations: 0.07\n",
      "step: 13610 loss: 41.904972 time elapsed: 16.6524 learning rate: 0.000017, scenario: 0, slope: -0.10526394647988925, fluctuations: 0.04\n",
      "step: 13620 loss: 41.861029 time elapsed: 16.6683 learning rate: 0.000027, scenario: 1, slope: -0.027803307420117104, fluctuations: 0.02\n",
      "step: 13630 loss: 41.767142 time elapsed: 16.6820 learning rate: 0.000069, scenario: 1, slope: -0.009978778296934734, fluctuations: 0.01\n",
      "step: 13640 loss: 41.536748 time elapsed: 16.6956 learning rate: 0.000179, scenario: 1, slope: -0.007133007709579744, fluctuations: 0.0\n",
      "step: 13650 loss: 41.011206 time elapsed: 16.7090 learning rate: 0.000465, scenario: 1, slope: -0.008027402571722827, fluctuations: 0.0\n",
      "step: 13660 loss: 39.984831 time elapsed: 16.7212 learning rate: 0.001207, scenario: 1, slope: -0.013598632874865334, fluctuations: 0.0\n",
      "step: 13670 loss: 38.491637 time elapsed: 16.7373 learning rate: 0.003129, scenario: 1, slope: -0.02483242848792225, fluctuations: 0.0\n",
      "step: 13680 loss: 36.980935 time elapsed: 16.7520 learning rate: 0.008117, scenario: 1, slope: -0.041196230513336554, fluctuations: 0.0\n",
      "step: 13690 loss: 36.477878 time elapsed: 16.7667 learning rate: 0.008117, scenario: 0, slope: -0.05961383142482935, fluctuations: 0.0\n",
      "step: 13700 loss: 418.883262 time elapsed: 16.7799 learning rate: 0.005033, scenario: -1, slope: 6.85683721888945, fluctuations: 0.01\n",
      "step: 13710 loss: 542.422122 time elapsed: 16.7938 learning rate: 0.001755, scenario: -1, slope: 5.808035742496855, fluctuations: 0.06\n",
      "step: 13720 loss: 153.512148 time elapsed: 16.8064 learning rate: 0.000612, scenario: -1, slope: 5.122298976354314, fluctuations: 0.08\n",
      "step: 13730 loss: 69.177554 time elapsed: 16.8190 learning rate: 0.000213, scenario: -1, slope: 3.6120371894130954, fluctuations: 0.09\n",
      "step: 13740 loss: 63.936535 time elapsed: 16.8310 learning rate: 0.000074, scenario: -1, slope: 2.0556244914316095, fluctuations: 0.1\n",
      "step: 13750 loss: 58.994897 time elapsed: 16.8439 learning rate: 0.000026, scenario: -1, slope: 0.4417772425412862, fluctuations: 0.1\n",
      "step: 13760 loss: 57.918886 time elapsed: 16.8562 learning rate: 0.000022, scenario: 0, slope: -1.115243632335894, fluctuations: 0.1\n",
      "step: 13770 loss: 57.594315 time elapsed: 16.8681 learning rate: 0.000022, scenario: 0, slope: -2.7367730752498804, fluctuations: 0.1\n",
      "step: 13780 loss: 57.385426 time elapsed: 16.8810 learning rate: 0.000022, scenario: 0, slope: -4.56577418989436, fluctuations: 0.1\n",
      "step: 13790 loss: 57.159161 time elapsed: 16.8929 learning rate: 0.000022, scenario: 0, slope: -6.799443760459866, fluctuations: 0.1\n",
      "step: 13800 loss: 56.935038 time elapsed: 16.9047 learning rate: 0.000022, scenario: 0, slope: -2.393976295690653, fluctuations: 0.08\n",
      "step: 13810 loss: 56.722776 time elapsed: 16.9169 learning rate: 0.000022, scenario: 0, slope: -0.6961445611604281, fluctuations: 0.04\n",
      "step: 13820 loss: 56.517783 time elapsed: 16.9287 learning rate: 0.000022, scenario: 0, slope: -0.1830691879288501, fluctuations: 0.02\n",
      "step: 13830 loss: 56.314595 time elapsed: 16.9408 learning rate: 0.000030, scenario: 1, slope: -0.05672942287832146, fluctuations: 0.01\n",
      "step: 13840 loss: 55.935101 time elapsed: 16.9537 learning rate: 0.000077, scenario: 1, slope: -0.03872577642200226, fluctuations: 0.0\n",
      "step: 13850 loss: 54.998883 time elapsed: 16.9672 learning rate: 0.000199, scenario: 1, slope: -0.02665241894185134, fluctuations: 0.0\n",
      "step: 13860 loss: 52.852931 time elapsed: 16.9792 learning rate: 0.000515, scenario: 1, slope: -0.03382753643858469, fluctuations: 0.0\n",
      "step: 13870 loss: 48.696983 time elapsed: 16.9917 learning rate: 0.001336, scenario: 1, slope: -0.05713453162470025, fluctuations: 0.0\n",
      "step: 13880 loss: 44.453640 time elapsed: 17.0037 learning rate: 0.001336, scenario: 0, slope: -0.0985958091543939, fluctuations: 0.0\n",
      "step: 13890 loss: 42.394674 time elapsed: 17.0158 learning rate: 0.001336, scenario: 0, slope: -0.1434412286295227, fluctuations: 0.0\n",
      "step: 13900 loss: 41.226636 time elapsed: 17.0274 learning rate: 0.001336, scenario: 0, slope: -0.1776211722491534, fluctuations: 0.0\n",
      "step: 13910 loss: 40.347891 time elapsed: 17.0398 learning rate: 0.001336, scenario: 0, slope: -0.20666256419602566, fluctuations: 0.0\n",
      "step: 13920 loss: 39.598605 time elapsed: 17.0521 learning rate: 0.001336, scenario: 0, slope: -0.21882630988655286, fluctuations: 0.0\n",
      "step: 13930 loss: 38.939907 time elapsed: 17.0642 learning rate: 0.001336, scenario: 0, slope: -0.21606847699462878, fluctuations: 0.0\n",
      "step: 13940 loss: 38.355820 time elapsed: 17.0760 learning rate: 0.001336, scenario: 0, slope: -0.19779724409118563, fluctuations: 0.0\n",
      "step: 13950 loss: 37.835200 time elapsed: 17.0881 learning rate: 0.001336, scenario: 0, slope: -0.16566773350821465, fluctuations: 0.0\n",
      "step: 13960 loss: 37.369266 time elapsed: 17.1002 learning rate: 0.001336, scenario: 0, slope: -0.12526382698478056, fluctuations: 0.0\n",
      "step: 13970 loss: 36.951170 time elapsed: 17.1121 learning rate: 0.001336, scenario: 0, slope: -0.08851457885545871, fluctuations: 0.0\n",
      "step: 13980 loss: 36.574956 time elapsed: 17.1240 learning rate: 0.001336, scenario: 0, slope: -0.06808717864831969, fluctuations: 0.0\n",
      "step: 13990 loss: 36.234949 time elapsed: 17.1359 learning rate: 0.001336, scenario: 0, slope: -0.05781628920047447, fluctuations: 0.0\n",
      "step: 14000 loss: 35.925730 time elapsed: 17.1478 learning rate: 0.001336, scenario: 0, slope: -0.05162770555209752, fluctuations: 0.0\n",
      "step: 14010 loss: 35.642291 time elapsed: 17.1602 learning rate: 0.001336, scenario: 0, slope: -0.04555969011466812, fluctuations: 0.0\n",
      "step: 14020 loss: 35.380183 time elapsed: 17.1734 learning rate: 0.001336, scenario: 0, slope: -0.04091165819260529, fluctuations: 0.0\n",
      "step: 14030 loss: 35.094921 time elapsed: 17.1857 learning rate: 0.002603, scenario: 1, slope: -0.036974341114840895, fluctuations: 0.0\n",
      "step: 14040 loss: 34.472352 time elapsed: 17.1977 learning rate: 0.006752, scenario: 1, slope: -0.03473615650485327, fluctuations: 0.0\n",
      "step: 14050 loss: 33.125912 time elapsed: 17.2096 learning rate: 0.017513, scenario: 1, slope: -0.03692257697889121, fluctuations: 0.0\n",
      "step: 14060 loss: 31026.261807 time elapsed: 17.2215 learning rate: 0.014746, scenario: -1, slope: 36.677654904210016, fluctuations: 0.01\n",
      "step: 14070 loss: 26841.413350 time elapsed: 17.2333 learning rate: 0.005142, scenario: -1, slope: 196.57681871198807, fluctuations: 0.03\n",
      "step: 14080 loss: 7796.965284 time elapsed: 17.2448 learning rate: 0.001793, scenario: -1, slope: 209.16144014736446, fluctuations: 0.05\n",
      "step: 14090 loss: 5184.482644 time elapsed: 17.2568 learning rate: 0.000625, scenario: -1, slope: 182.40052273403631, fluctuations: 0.06\n",
      "step: 14100 loss: 4153.493338 time elapsed: 17.2686 learning rate: 0.000242, scenario: -1, slope: 142.91783819735838, fluctuations: 0.06\n",
      "step: 14110 loss: 3958.779815 time elapsed: 17.2811 learning rate: 0.000084, scenario: -1, slope: 90.34039843426325, fluctuations: 0.06\n",
      "step: 14120 loss: 3892.220892 time elapsed: 17.2927 learning rate: 0.000029, scenario: -1, slope: 39.101686976215184, fluctuations: 0.06\n",
      "step: 14130 loss: 3871.667767 time elapsed: 17.3045 learning rate: 0.000014, scenario: 0, slope: -17.19190186324656, fluctuations: 0.06\n",
      "step: 14140 loss: 3859.288251 time elapsed: 17.3164 learning rate: 0.000014, scenario: 0, slope: -81.20432964766492, fluctuations: 0.06\n",
      "step: 14150 loss: 3847.583292 time elapsed: 17.3287 learning rate: 0.000014, scenario: 0, slope: -156.71214763564174, fluctuations: 0.06\n",
      "step: 14160 loss: 3836.381139 time elapsed: 17.3405 learning rate: 0.000014, scenario: 0, slope: -275.61751113761403, fluctuations: 0.04\n",
      "step: 14170 loss: 3825.569722 time elapsed: 17.3525 learning rate: 0.000014, scenario: 0, slope: -53.26258586398659, fluctuations: 0.02\n",
      "step: 14180 loss: 3815.072627 time elapsed: 17.3641 learning rate: 0.000014, scenario: 0, slope: -13.224795659078751, fluctuations: 0.01\n",
      "step: 14190 loss: 3804.835749 time elapsed: 17.3780 learning rate: 0.000015, scenario: 1, slope: -5.021944462376381, fluctuations: 0.0\n",
      "step: 14200 loss: 3788.935529 time elapsed: 17.3908 learning rate: 0.000037, scenario: 1, slope: -2.1150170075206507, fluctuations: 0.0\n",
      "step: 14210 loss: 3752.599751 time elapsed: 17.4052 learning rate: 0.000095, scenario: 1, slope: -1.3748666663242646, fluctuations: 0.0\n",
      "step: 14220 loss: 3665.342752 time elapsed: 17.4177 learning rate: 0.000246, scenario: 1, slope: -1.5541697845800766, fluctuations: 0.0\n",
      "step: 14230 loss: 3466.056860 time elapsed: 17.4298 learning rate: 0.000637, scenario: 1, slope: -2.5129035183894923, fluctuations: 0.0\n",
      "step: 14240 loss: 3070.107087 time elapsed: 17.4418 learning rate: 0.001129, scenario: 0, slope: -4.859313605373322, fluctuations: 0.0\n",
      "step: 14250 loss: 2704.471319 time elapsed: 17.4539 learning rate: 0.001129, scenario: 0, slope: -8.743932373296914, fluctuations: 0.0\n",
      "step: 14260 loss: 2427.024069 time elapsed: 17.4661 learning rate: 0.001129, scenario: 0, slope: -13.195681003358905, fluctuations: 0.0\n",
      "step: 14270 loss: 2213.497426 time elapsed: 17.4784 learning rate: 0.001129, scenario: 0, slope: -17.461447180327408, fluctuations: 0.0\n",
      "step: 14280 loss: 2046.885999 time elapsed: 17.4906 learning rate: 0.001129, scenario: 0, slope: -20.966706712304422, fluctuations: 0.0\n",
      "step: 14290 loss: 1907.969774 time elapsed: 17.5028 learning rate: 0.001129, scenario: 0, slope: -23.307041469348544, fluctuations: 0.0\n",
      "step: 14300 loss: 1787.055273 time elapsed: 17.5145 learning rate: 0.001129, scenario: 0, slope: -24.190016350513172, fluctuations: 0.0\n",
      "step: 14310 loss: 1682.288470 time elapsed: 17.5271 learning rate: 0.001129, scenario: 0, slope: -23.53731650734087, fluctuations: 0.0\n",
      "step: 14320 loss: 1593.524822 time elapsed: 17.5389 learning rate: 0.001129, scenario: 0, slope: -21.323711653852992, fluctuations: 0.0\n",
      "step: 14330 loss: 1518.563520 time elapsed: 17.5509 learning rate: 0.001129, scenario: 0, slope: -17.977698306813345, fluctuations: 0.0\n",
      "step: 14340 loss: 1453.728754 time elapsed: 17.5626 learning rate: 0.001129, scenario: 0, slope: -14.572801811950873, fluctuations: 0.0\n",
      "step: 14350 loss: 1396.005295 time elapsed: 17.5766 learning rate: 0.001129, scenario: 0, slope: -12.032944151630062, fluctuations: 0.0\n",
      "step: 14360 loss: 1343.424056 time elapsed: 17.5907 learning rate: 0.001129, scenario: 0, slope: -10.139470661700688, fluctuations: 0.0\n",
      "step: 14370 loss: 1294.824500 time elapsed: 17.6038 learning rate: 0.001129, scenario: 0, slope: -8.700038376719512, fluctuations: 0.0\n",
      "step: 14380 loss: 1249.499481 time elapsed: 17.6166 learning rate: 0.001129, scenario: 0, slope: -7.5588799681347165, fluctuations: 0.0\n",
      "step: 14390 loss: 1206.972450 time elapsed: 17.6300 learning rate: 0.001129, scenario: 0, slope: -6.628026948845995, fluctuations: 0.0\n",
      "step: 14400 loss: 1166.905642 time elapsed: 17.6427 learning rate: 0.001129, scenario: 0, slope: -5.945669549385276, fluctuations: 0.0\n",
      "step: 14410 loss: 1129.047174 time elapsed: 17.6560 learning rate: 0.001129, scenario: 0, slope: -5.29296472867199, fluctuations: 0.0\n",
      "step: 14420 loss: 1093.206468 time elapsed: 17.6682 learning rate: 0.001129, scenario: 0, slope: -4.841515774923001, fluctuations: 0.0\n",
      "step: 14430 loss: 1059.239906 time elapsed: 17.6808 learning rate: 0.001129, scenario: 0, slope: -4.485424431438252, fluctuations: 0.0\n",
      "step: 14440 loss: 1027.032663 time elapsed: 17.6931 learning rate: 0.001129, scenario: 0, slope: -4.192006518338036, fluctuations: 0.0\n",
      "step: 14450 loss: 996.477486 time elapsed: 17.7058 learning rate: 0.001129, scenario: 0, slope: -3.939857512242743, fluctuations: 0.0\n",
      "step: 14460 loss: 967.456816 time elapsed: 17.7179 learning rate: 0.001129, scenario: 0, slope: -3.7162553653649897, fluctuations: 0.0\n",
      "step: 14470 loss: 939.835040 time elapsed: 17.7305 learning rate: 0.001129, scenario: 0, slope: -3.5139211177739114, fluctuations: 0.0\n",
      "step: 14480 loss: 913.468162 time elapsed: 17.7430 learning rate: 0.001129, scenario: 0, slope: -3.328620811897531, fluctuations: 0.0\n",
      "step: 14490 loss: 888.235344 time elapsed: 17.7560 learning rate: 0.001129, scenario: 0, slope: -3.157934705066716, fluctuations: 0.0\n",
      "step: 14500 loss: 864.084369 time elapsed: 17.7691 learning rate: 0.001129, scenario: 0, slope: -3.0155528245864063, fluctuations: 0.0\n",
      "step: 14510 loss: 841.058585 time elapsed: 17.7840 learning rate: 0.001129, scenario: 0, slope: -2.854418649860924, fluctuations: 0.0\n",
      "step: 14520 loss: 819.203168 time elapsed: 17.7970 learning rate: 0.001129, scenario: 0, slope: -2.7183864831053306, fluctuations: 0.0\n",
      "step: 14530 loss: 798.424006 time elapsed: 17.8093 learning rate: 0.001129, scenario: 0, slope: -2.5905508274522258, fluctuations: 0.0\n",
      "step: 14540 loss: 778.365401 time elapsed: 17.8216 learning rate: 0.001129, scenario: 0, slope: -2.4704696914511732, fluctuations: 0.0\n",
      "step: 14550 loss: 760.052067 time elapsed: 17.8336 learning rate: 0.001129, scenario: 0, slope: -2.3560382607299757, fluctuations: 0.0\n",
      "step: 14560 loss: 742.896502 time elapsed: 17.8457 learning rate: 0.001129, scenario: 0, slope: -2.242119141791527, fluctuations: 0.0\n",
      "step: 14570 loss: 726.703050 time elapsed: 17.8578 learning rate: 0.001129, scenario: 0, slope: -2.1283103319647476, fluctuations: 0.0\n",
      "step: 14580 loss: 711.251823 time elapsed: 17.8694 learning rate: 0.001129, scenario: 0, slope: -2.0155736197885927, fluctuations: 0.0\n",
      "step: 14590 loss: 696.432672 time elapsed: 17.8813 learning rate: 0.001129, scenario: 0, slope: -1.9057583745336315, fluctuations: 0.0\n",
      "step: 14600 loss: 682.173543 time elapsed: 17.8930 learning rate: 0.001129, scenario: 0, slope: -1.811346921433111, fluctuations: 0.0\n",
      "step: 14610 loss: 668.413659 time elapsed: 17.9054 learning rate: 0.001129, scenario: 0, slope: -1.7039551830478907, fluctuations: 0.0\n",
      "step: 14620 loss: 655.099447 time elapsed: 17.9175 learning rate: 0.001129, scenario: 0, slope: -1.615364091997484, fluctuations: 0.0\n",
      "step: 14630 loss: 642.239247 time elapsed: 17.9296 learning rate: 0.001129, scenario: 0, slope: -1.5361752256271466, fluctuations: 0.0\n",
      "step: 14640 loss: 629.958261 time elapsed: 17.9418 learning rate: 0.001129, scenario: 0, slope: -1.4674201277961372, fluctuations: 0.0\n",
      "step: 14650 loss: 618.185317 time elapsed: 17.9541 learning rate: 0.001129, scenario: 0, slope: -1.4084579511944058, fluctuations: 0.0\n",
      "step: 14660 loss: 606.879417 time elapsed: 17.9660 learning rate: 0.001129, scenario: 0, slope: -1.354084957618903, fluctuations: 0.0\n",
      "step: 14670 loss: 595.988311 time elapsed: 17.9785 learning rate: 0.001129, scenario: 0, slope: -1.3027133346071667, fluctuations: 0.0\n",
      "step: 14680 loss: 585.493281 time elapsed: 17.9914 learning rate: 0.001129, scenario: 0, slope: -1.2534753239854268, fluctuations: 0.0\n",
      "step: 14690 loss: 575.379330 time elapsed: 18.0047 learning rate: 0.001129, scenario: 0, slope: -1.2058759196702615, fluctuations: 0.0\n",
      "step: 14700 loss: 565.628050 time elapsed: 18.0180 learning rate: 0.001129, scenario: 0, slope: -1.1643165859956428, fluctuations: 0.0\n",
      "step: 14710 loss: 556.221131 time elapsed: 18.0313 learning rate: 0.001129, scenario: 0, slope: -1.1153129505787638, fluctuations: 0.0\n",
      "step: 14720 loss: 547.140107 time elapsed: 18.0433 learning rate: 0.001129, scenario: 0, slope: -1.0728887654726889, fluctuations: 0.0\n",
      "step: 14730 loss: 538.366667 time elapsed: 18.0555 learning rate: 0.001129, scenario: 0, slope: -1.033048207506494, fluctuations: 0.0\n",
      "step: 14740 loss: 529.882780 time elapsed: 18.0678 learning rate: 0.001129, scenario: 0, slope: -0.995690482172934, fluctuations: 0.0\n",
      "step: 14750 loss: 521.670797 time elapsed: 18.0797 learning rate: 0.001129, scenario: 0, slope: -0.9604297146322941, fluctuations: 0.0\n",
      "step: 14760 loss: 513.713409 time elapsed: 18.0914 learning rate: 0.001129, scenario: 0, slope: -0.9270266572824319, fluctuations: 0.0\n",
      "step: 14770 loss: 505.993505 time elapsed: 18.1033 learning rate: 0.001129, scenario: 0, slope: -0.8954056435518374, fluctuations: 0.0\n",
      "step: 14780 loss: 498.495803 time elapsed: 18.1151 learning rate: 0.001129, scenario: 0, slope: -0.8655717637525532, fluctuations: 0.0\n",
      "step: 14790 loss: 491.220109 time elapsed: 18.1268 learning rate: 0.001129, scenario: 0, slope: -0.8374647031513198, fluctuations: 0.0\n",
      "step: 14800 loss: 484.185821 time elapsed: 18.1382 learning rate: 0.001129, scenario: 0, slope: -0.8134614411621999, fluctuations: 0.0\n",
      "step: 14810 loss: 477.375052 time elapsed: 18.1506 learning rate: 0.001129, scenario: 0, slope: -0.7854863239282853, fluctuations: 0.0\n",
      "step: 14820 loss: 470.763221 time elapsed: 18.1623 learning rate: 0.001129, scenario: 0, slope: -0.7611937286610944, fluctuations: 0.0\n",
      "step: 14830 loss: 464.337007 time elapsed: 18.1740 learning rate: 0.001129, scenario: 0, slope: -0.7378950965947263, fluctuations: 0.0\n",
      "step: 14840 loss: 458.084201 time elapsed: 18.1859 learning rate: 0.001129, scenario: 0, slope: -0.7155424955511003, fluctuations: 0.0\n",
      "step: 14850 loss: 451.995456 time elapsed: 18.1979 learning rate: 0.001129, scenario: 0, slope: -0.6941199114472086, fluctuations: 0.0\n",
      "step: 14860 loss: 446.062828 time elapsed: 18.2117 learning rate: 0.001129, scenario: 0, slope: -0.6736382205447892, fluctuations: 0.0\n",
      "step: 14870 loss: 440.279578 time elapsed: 18.2253 learning rate: 0.001129, scenario: 0, slope: -0.6541377412072327, fluctuations: 0.0\n",
      "step: 14880 loss: 434.640024 time elapsed: 18.2380 learning rate: 0.001129, scenario: 0, slope: -0.6356893497472644, fluctuations: 0.0\n",
      "step: 14890 loss: 429.139298 time elapsed: 18.2506 learning rate: 0.001129, scenario: 0, slope: -0.6183534383223959, fluctuations: 0.0\n",
      "step: 14900 loss: 423.773013 time elapsed: 18.2623 learning rate: 0.001129, scenario: 0, slope: -0.6036247166667288, fluctuations: 0.0\n",
      "step: 14910 loss: 418.536811 time elapsed: 18.2743 learning rate: 0.001129, scenario: 0, slope: -0.5865215584031247, fluctuations: 0.0\n",
      "step: 14920 loss: 413.425945 time elapsed: 18.2861 learning rate: 0.001129, scenario: 0, slope: -0.5716914956307513, fluctuations: 0.0\n",
      "step: 14930 loss: 408.435121 time elapsed: 18.2981 learning rate: 0.001129, scenario: 0, slope: -0.5574564064753873, fluctuations: 0.0\n",
      "step: 14940 loss: 403.558617 time elapsed: 18.3097 learning rate: 0.001129, scenario: 0, slope: -0.5437620230129909, fluctuations: 0.0\n",
      "step: 14950 loss: 398.790520 time elapsed: 18.3212 learning rate: 0.001129, scenario: 0, slope: -0.5305817867808433, fluctuations: 0.0\n",
      "step: 14960 loss: 394.124918 time elapsed: 18.3331 learning rate: 0.001129, scenario: 0, slope: -0.5179089281951976, fluctuations: 0.0\n",
      "step: 14970 loss: 389.556008 time elapsed: 18.3452 learning rate: 0.001129, scenario: 0, slope: -0.5057511499102729, fluctuations: 0.0\n",
      "step: 14980 loss: 385.078103 time elapsed: 18.3574 learning rate: 0.001129, scenario: 0, slope: -0.4941241242435761, fluctuations: 0.0\n",
      "step: 14990 loss: 380.685588 time elapsed: 18.3695 learning rate: 0.001129, scenario: 0, slope: -0.48304530930226225, fluctuations: 0.0\n",
      "step: 15000 loss: 376.372823 time elapsed: 18.3812 learning rate: 0.001129, scenario: 0, slope: -0.47355507049523443, fluctuations: 0.0\n",
      "step: 15010 loss: 372.134002 time elapsed: 18.3941 learning rate: 0.001129, scenario: 0, slope: -0.4625842812611178, fluctuations: 0.0\n",
      "step: 15020 loss: 367.962938 time elapsed: 18.4072 learning rate: 0.001129, scenario: 0, slope: -0.45321568681178037, fluctuations: 0.0\n",
      "step: 15030 loss: 363.852744 time elapsed: 18.4211 learning rate: 0.001129, scenario: 0, slope: -0.44442830626403584, fluctuations: 0.0\n",
      "step: 15040 loss: 359.795317 time elapsed: 18.4343 learning rate: 0.001129, scenario: 0, slope: -0.4362342647888482, fluctuations: 0.0\n",
      "step: 15050 loss: 355.780478 time elapsed: 18.4469 learning rate: 0.001129, scenario: 0, slope: -0.42866144278109214, fluctuations: 0.0\n",
      "step: 15060 loss: 351.794447 time elapsed: 18.4602 learning rate: 0.001129, scenario: 0, slope: -0.42176552997991296, fluctuations: 0.0\n",
      "step: 15070 loss: 347.817004 time elapsed: 18.4726 learning rate: 0.001129, scenario: 0, slope: -0.41564965751991756, fluctuations: 0.0\n",
      "step: 15080 loss: 343.815844 time elapsed: 18.4851 learning rate: 0.001129, scenario: 0, slope: -0.410500580776399, fluctuations: 0.0\n",
      "step: 15090 loss: 339.734662 time elapsed: 18.4974 learning rate: 0.001129, scenario: 0, slope: -0.40666123313419156, fluctuations: 0.0\n",
      "step: 15100 loss: 335.466618 time elapsed: 18.5094 learning rate: 0.001129, scenario: 0, slope: -0.4048554222673874, fluctuations: 0.0\n",
      "step: 15110 loss: 330.795774 time elapsed: 18.5218 learning rate: 0.001129, scenario: 0, slope: -0.4061847038625354, fluctuations: 0.0\n",
      "step: 15120 loss: 325.313694 time elapsed: 18.5336 learning rate: 0.001129, scenario: 0, slope: -0.41349793163307447, fluctuations: 0.0\n",
      "step: 15130 loss: 318.797101 time elapsed: 18.5456 learning rate: 0.001129, scenario: 0, slope: -0.4307276295204669, fluctuations: 0.0\n",
      "step: 15140 loss: 313.026793 time elapsed: 18.5572 learning rate: 0.001129, scenario: 0, slope: -0.45620490313191564, fluctuations: 0.0\n",
      "step: 15150 loss: 307.988251 time elapsed: 18.5693 learning rate: 0.001129, scenario: 0, slope: -0.4809547400040189, fluctuations: 0.0\n",
      "step: 15160 loss: 303.254091 time elapsed: 18.5810 learning rate: 0.001129, scenario: 0, slope: -0.50182027142923, fluctuations: 0.0\n",
      "step: 15170 loss: 298.763028 time elapsed: 18.5924 learning rate: 0.001129, scenario: 0, slope: -0.5158448246827494, fluctuations: 0.0\n",
      "step: 15180 loss: 294.506575 time elapsed: 18.6045 learning rate: 0.001129, scenario: 0, slope: -0.5210373269414469, fluctuations: 0.0\n",
      "step: 15190 loss: 290.411710 time elapsed: 18.6165 learning rate: 0.001129, scenario: 0, slope: -0.5163099988378059, fluctuations: 0.0\n",
      "step: 15200 loss: 286.453702 time elapsed: 18.6296 learning rate: 0.001129, scenario: 0, slope: -0.5034748205726146, fluctuations: 0.0\n",
      "step: 15210 loss: 282.620786 time elapsed: 18.6426 learning rate: 0.001129, scenario: 0, slope: -0.4782289112367181, fluctuations: 0.0\n",
      "step: 15220 loss: 278.910293 time elapsed: 18.6547 learning rate: 0.001129, scenario: 0, slope: -0.4502891791524286, fluctuations: 0.0\n",
      "step: 15230 loss: 275.325069 time elapsed: 18.6667 learning rate: 0.001129, scenario: 0, slope: -0.42530232612818053, fluctuations: 0.0\n",
      "step: 15240 loss: 271.869126 time elapsed: 18.6783 learning rate: 0.001129, scenario: 0, slope: -0.40721921240256914, fluctuations: 0.0\n",
      "step: 15250 loss: 268.544584 time elapsed: 18.6908 learning rate: 0.001129, scenario: 0, slope: -0.39131857474686127, fluctuations: 0.0\n",
      "step: 15260 loss: 265.349632 time elapsed: 18.7029 learning rate: 0.001129, scenario: 0, slope: -0.3771634781126667, fluctuations: 0.0\n",
      "step: 15270 loss: 262.278244 time elapsed: 18.7146 learning rate: 0.001129, scenario: 0, slope: -0.36391874654125983, fluctuations: 0.0\n",
      "step: 15280 loss: 259.321049 time elapsed: 18.7266 learning rate: 0.001129, scenario: 0, slope: -0.3511132014853968, fluctuations: 0.0\n",
      "step: 15290 loss: 256.466886 time elapsed: 18.7382 learning rate: 0.001129, scenario: 0, slope: -0.3385534132623418, fluctuations: 0.0\n",
      "step: 15300 loss: 253.704339 time elapsed: 18.7498 learning rate: 0.001129, scenario: 0, slope: -0.32746986884177787, fluctuations: 0.0\n",
      "step: 15310 loss: 251.022767 time elapsed: 18.7626 learning rate: 0.001129, scenario: 0, slope: -0.31435677183889005, fluctuations: 0.0\n",
      "step: 15320 loss: 248.412682 time elapsed: 18.7749 learning rate: 0.001129, scenario: 0, slope: -0.3030464275591703, fluctuations: 0.0\n",
      "step: 15330 loss: 245.865639 time elapsed: 18.7870 learning rate: 0.001129, scenario: 0, slope: -0.29249709735427665, fluctuations: 0.0\n",
      "step: 15340 loss: 243.373952 time elapsed: 18.7986 learning rate: 0.001129, scenario: 0, slope: -0.28283158235873723, fluctuations: 0.0\n",
      "step: 15350 loss: 240.930462 time elapsed: 18.8106 learning rate: 0.001129, scenario: 0, slope: -0.27410866630920355, fluctuations: 0.0\n",
      "step: 15360 loss: 238.528455 time elapsed: 18.8222 learning rate: 0.001129, scenario: 0, slope: -0.2663285819071757, fluctuations: 0.0\n",
      "step: 15370 loss: 235.758487 time elapsed: 18.8361 learning rate: 0.002200, scenario: 1, slope: -0.2599806850982242, fluctuations: 0.0\n",
      "step: 15380 loss: 229.692444 time elapsed: 18.8486 learning rate: 0.003544, scenario: 0, slope: -0.2657827873354588, fluctuations: 0.0\n",
      "step: 15390 loss: 222.636951 time elapsed: 18.8608 learning rate: 0.003544, scenario: 0, slope: -0.29475963649597814, fluctuations: 0.0\n",
      "step: 15400 loss: 215.648544 time elapsed: 18.8723 learning rate: 0.003544, scenario: 0, slope: -0.3363921703119434, fluctuations: 0.0\n",
      "step: 15410 loss: 208.237703 time elapsed: 18.8844 learning rate: 0.003544, scenario: 0, slope: -0.40186864858334576, fluctuations: 0.0\n",
      "step: 15420 loss: 199.245415 time elapsed: 18.8963 learning rate: 0.003544, scenario: 0, slope: -0.476130505244702, fluctuations: 0.0\n",
      "step: 15430 loss: 192.344635 time elapsed: 18.9081 learning rate: 0.003544, scenario: 0, slope: -0.5538796252235111, fluctuations: 0.0\n",
      "step: 15440 loss: 185.977463 time elapsed: 18.9199 learning rate: 0.003544, scenario: 0, slope: -0.6225375173417207, fluctuations: 0.0\n",
      "step: 15450 loss: 179.947664 time elapsed: 18.9317 learning rate: 0.003544, scenario: 0, slope: -0.6735824399285233, fluctuations: 0.0\n",
      "step: 15460 loss: 174.300590 time elapsed: 18.9436 learning rate: 0.003544, scenario: 0, slope: -0.7007448369203838, fluctuations: 0.0\n",
      "step: 15470 loss: 168.975028 time elapsed: 18.9555 learning rate: 0.003544, scenario: 0, slope: -0.6983553738333793, fluctuations: 0.0\n",
      "step: 15480 loss: 163.973185 time elapsed: 18.9672 learning rate: 0.003544, scenario: 0, slope: -0.6734565574790651, fluctuations: 0.0\n",
      "step: 15490 loss: 159.306265 time elapsed: 18.9792 learning rate: 0.003544, scenario: 0, slope: -0.6403913874097219, fluctuations: 0.0\n",
      "step: 15500 loss: 154.969066 time elapsed: 18.9909 learning rate: 0.003544, scenario: 0, slope: -0.6047276180026663, fluctuations: 0.0\n",
      "step: 15510 loss: 150.948939 time elapsed: 19.0034 learning rate: 0.003544, scenario: 0, slope: -0.5557346699507879, fluctuations: 0.0\n",
      "step: 15520 loss: 147.222800 time elapsed: 19.0152 learning rate: 0.003544, scenario: 0, slope: -0.516144313800417, fluctuations: 0.0\n",
      "step: 15530 loss: 143.762348 time elapsed: 19.0271 learning rate: 0.003544, scenario: 0, slope: -0.48245230110138043, fluctuations: 0.0\n",
      "step: 15540 loss: 140.536770 time elapsed: 19.0408 learning rate: 0.003544, scenario: 0, slope: -0.4508461162448408, fluctuations: 0.0\n",
      "step: 15550 loss: 137.486442 time elapsed: 19.0537 learning rate: 0.003544, scenario: 0, slope: -0.4203078472110868, fluctuations: 0.0\n",
      "step: 15560 loss: 134.543869 time elapsed: 19.0665 learning rate: 0.003544, scenario: 0, slope: -0.39182343711752354, fluctuations: 0.0\n",
      "step: 15570 loss: 131.810303 time elapsed: 19.0795 learning rate: 0.003544, scenario: 0, slope: -0.3657086977514158, fluctuations: 0.0\n",
      "step: 15580 loss: 129.253904 time elapsed: 19.0929 learning rate: 0.003544, scenario: 0, slope: -0.341786188474927, fluctuations: 0.0\n",
      "step: 15590 loss: 126.876879 time elapsed: 19.1050 learning rate: 0.003544, scenario: 0, slope: -0.31992293190213766, fluctuations: 0.0\n",
      "step: 15600 loss: 124.642632 time elapsed: 19.1185 learning rate: 0.003544, scenario: 0, slope: -0.30187071605905746, fluctuations: 0.0\n",
      "step: 15610 loss: 122.527525 time elapsed: 19.1316 learning rate: 0.003544, scenario: 0, slope: -0.28166381462042456, fluctuations: 0.0\n",
      "step: 15620 loss: 120.519043 time elapsed: 19.1435 learning rate: 0.003544, scenario: 0, slope: -0.26482629309589345, fluctuations: 0.0\n",
      "step: 15630 loss: 118.603783 time elapsed: 19.1556 learning rate: 0.003544, scenario: 0, slope: -0.24922755184755946, fluctuations: 0.0\n",
      "step: 15640 loss: 116.771304 time elapsed: 19.1677 learning rate: 0.003544, scenario: 0, slope: -0.23469580478153862, fluctuations: 0.0\n",
      "step: 15650 loss: 115.012102 time elapsed: 19.1801 learning rate: 0.003544, scenario: 0, slope: -0.22117654249936233, fluctuations: 0.0\n",
      "step: 15660 loss: 113.317589 time elapsed: 19.1921 learning rate: 0.003544, scenario: 0, slope: -0.20902537490720238, fluctuations: 0.0\n",
      "step: 15670 loss: 111.679322 time elapsed: 19.2046 learning rate: 0.003544, scenario: 0, slope: -0.1985347554801955, fluctuations: 0.0\n",
      "step: 15680 loss: 110.087352 time elapsed: 19.2166 learning rate: 0.003544, scenario: 0, slope: -0.18939629477669623, fluctuations: 0.0\n",
      "step: 15690 loss: 108.525648 time elapsed: 19.2311 learning rate: 0.003544, scenario: 0, slope: -0.18143028872912667, fluctuations: 0.0\n",
      "step: 15700 loss: 106.996700 time elapsed: 19.2456 learning rate: 0.003544, scenario: 0, slope: -0.17516503982302237, fluctuations: 0.0\n",
      "step: 15710 loss: 105.371984 time elapsed: 19.2594 learning rate: 0.003544, scenario: 0, slope: -0.1689563589733698, fluctuations: 0.0\n",
      "step: 15720 loss: 103.924658 time elapsed: 19.2715 learning rate: 0.003544, scenario: 0, slope: -0.16441525889390468, fluctuations: 0.0\n",
      "step: 15730 loss: 102.506885 time elapsed: 19.2843 learning rate: 0.003544, scenario: 0, slope: -0.1602325093570502, fluctuations: 0.0\n",
      "step: 15740 loss: 101.207320 time elapsed: 19.2964 learning rate: 0.003544, scenario: 0, slope: -0.15611353513435267, fluctuations: 0.0\n",
      "step: 15750 loss: 99.835421 time elapsed: 19.3087 learning rate: 0.003544, scenario: 0, slope: -0.15196127091914752, fluctuations: 0.0\n",
      "step: 15760 loss: 98.623971 time elapsed: 19.3207 learning rate: 0.003544, scenario: 0, slope: -0.14797064985712996, fluctuations: 0.0\n",
      "step: 15770 loss: 97.301877 time elapsed: 19.3328 learning rate: 0.003544, scenario: 0, slope: -0.14382970944366, fluctuations: 0.0\n",
      "step: 15780 loss: 95.999261 time elapsed: 19.3448 learning rate: 0.003544, scenario: 0, slope: -0.13962514020756636, fluctuations: 0.0\n",
      "step: 15790 loss: 94.799496 time elapsed: 19.3569 learning rate: 0.003544, scenario: 0, slope: -0.13557637182080404, fluctuations: 0.0\n",
      "step: 15800 loss: 93.698442 time elapsed: 19.3684 learning rate: 0.003544, scenario: 0, slope: -0.13169487976634892, fluctuations: 0.0\n",
      "step: 15810 loss: 92.606858 time elapsed: 19.3803 learning rate: 0.003544, scenario: 0, slope: -0.1277304023220213, fluctuations: 0.0\n",
      "step: 15820 loss: 91.502257 time elapsed: 19.3919 learning rate: 0.003544, scenario: 0, slope: -0.12454759911136466, fluctuations: 0.0\n",
      "step: 15830 loss: 90.455755 time elapsed: 19.4040 learning rate: 0.003544, scenario: 0, slope: -0.12193243880143273, fluctuations: 0.0\n",
      "step: 15840 loss: 89.323965 time elapsed: 19.4161 learning rate: 0.003544, scenario: 0, slope: -0.11902906295326032, fluctuations: 0.0\n",
      "step: 15850 loss: 88.217188 time elapsed: 19.4283 learning rate: 0.003544, scenario: 0, slope: -0.11650879032225807, fluctuations: 0.0\n",
      "step: 15860 loss: 87.147367 time elapsed: 19.4401 learning rate: 0.003544, scenario: 0, slope: -0.11365389037855793, fluctuations: 0.01\n",
      "step: 15870 loss: 86.160244 time elapsed: 19.4520 learning rate: 0.003544, scenario: 0, slope: -0.11134164324460936, fluctuations: 0.02\n",
      "step: 15880 loss: 85.033188 time elapsed: 19.4655 learning rate: 0.003544, scenario: 0, slope: -0.10893059582708621, fluctuations: 0.02\n",
      "step: 15890 loss: 85.070427 time elapsed: 19.4786 learning rate: 0.003544, scenario: 0, slope: -0.10570779529414383, fluctuations: 0.02\n",
      "step: 15900 loss: 83.297340 time elapsed: 19.4913 learning rate: 0.003544, scenario: 0, slope: -0.10346120793579855, fluctuations: 0.04\n",
      "step: 15910 loss: 82.208494 time elapsed: 19.5037 learning rate: 0.003544, scenario: 0, slope: -0.101532711562925, fluctuations: 0.06\n",
      "step: 15920 loss: 81.376991 time elapsed: 19.5159 learning rate: 0.003544, scenario: 0, slope: -0.09967442494573166, fluctuations: 0.06\n",
      "step: 15930 loss: 80.451035 time elapsed: 19.5278 learning rate: 0.003544, scenario: 0, slope: -0.09853826015144829, fluctuations: 0.06\n",
      "step: 15940 loss: 79.719713 time elapsed: 19.5395 learning rate: 0.003544, scenario: 0, slope: -0.09630660984091012, fluctuations: 0.06\n",
      "step: 15950 loss: 78.570162 time elapsed: 19.5514 learning rate: 0.003544, scenario: 0, slope: -0.09525100434309249, fluctuations: 0.06\n",
      "step: 15960 loss: 78.401557 time elapsed: 19.5629 learning rate: 0.003544, scenario: 0, slope: -0.09306596125880623, fluctuations: 0.04\n",
      "step: 15970 loss: 77.109075 time elapsed: 19.5746 learning rate: 0.003544, scenario: 0, slope: -0.09147961052504523, fluctuations: 0.06\n",
      "step: 15980 loss: 75.880246 time elapsed: 19.5869 learning rate: 0.003544, scenario: 0, slope: -0.09137754223373765, fluctuations: 0.06\n",
      "step: 15990 loss: 74.989709 time elapsed: 19.5987 learning rate: 0.003544, scenario: 0, slope: -0.09133353561955111, fluctuations: 0.05\n",
      "step: 16000 loss: 74.073294 time elapsed: 19.6104 learning rate: 0.003544, scenario: 0, slope: -0.09078281559816477, fluctuations: 0.04\n",
      "step: 16010 loss: 74.339488 time elapsed: 19.6224 learning rate: 0.003544, scenario: 0, slope: -0.08869477162089989, fluctuations: 0.02\n",
      "step: 16020 loss: 72.588282 time elapsed: 19.6356 learning rate: 0.003544, scenario: 0, slope: -0.08789634253539007, fluctuations: 0.04\n",
      "step: 16030 loss: 71.640670 time elapsed: 19.6489 learning rate: 0.003544, scenario: 0, slope: -0.08826229349670055, fluctuations: 0.05\n",
      "step: 16040 loss: 70.888842 time elapsed: 19.6631 learning rate: 0.003544, scenario: 0, slope: -0.08746182193612927, fluctuations: 0.05\n",
      "step: 16050 loss: 70.092891 time elapsed: 19.6753 learning rate: 0.003544, scenario: 0, slope: -0.08671545170678294, fluctuations: 0.05\n",
      "step: 16060 loss: 69.453397 time elapsed: 19.6875 learning rate: 0.003544, scenario: 0, slope: -0.08507275673826453, fluctuations: 0.04\n",
      "step: 16070 loss: 69.435963 time elapsed: 19.6995 learning rate: 0.003544, scenario: 0, slope: -0.08003718686985196, fluctuations: 0.04\n",
      "step: 16080 loss: 68.246021 time elapsed: 19.7115 learning rate: 0.003544, scenario: 0, slope: -0.07744030456615798, fluctuations: 0.05\n",
      "step: 16090 loss: 67.361768 time elapsed: 19.7233 learning rate: 0.003544, scenario: 0, slope: -0.0763040268766121, fluctuations: 0.05\n",
      "step: 16100 loss: 66.540460 time elapsed: 19.7351 learning rate: 0.003544, scenario: 0, slope: -0.0761011217687483, fluctuations: 0.05\n",
      "step: 16110 loss: 65.998356 time elapsed: 19.7473 learning rate: 0.003544, scenario: 0, slope: -0.07420591266078941, fluctuations: 0.04\n",
      "step: 16120 loss: 254.979809 time elapsed: 19.7596 learning rate: 0.004369, scenario: -1, slope: 1.3203304718620144, fluctuations: 0.04\n",
      "step: 16130 loss: 145.158159 time elapsed: 19.7715 learning rate: 0.001523, scenario: -1, slope: 1.803523285688367, fluctuations: 0.07\n",
      "step: 16140 loss: 90.199455 time elapsed: 19.7832 learning rate: 0.000531, scenario: -1, slope: 1.6845692275149908, fluctuations: 0.09\n",
      "step: 16150 loss: 86.701896 time elapsed: 19.7955 learning rate: 0.000185, scenario: -1, slope: 1.2757816792299426, fluctuations: 0.1\n",
      "step: 16160 loss: 82.872520 time elapsed: 19.8077 learning rate: 0.000065, scenario: -1, slope: 0.821500671527544, fluctuations: 0.11\n",
      "step: 16170 loss: 82.506993 time elapsed: 19.8195 learning rate: 0.000023, scenario: -1, slope: 0.3345536035122802, fluctuations: 0.1\n",
      "step: 16180 loss: 82.313596 time elapsed: 19.8316 learning rate: 0.000014, scenario: 0, slope: -0.12585940433513776, fluctuations: 0.09\n",
      "step: 16190 loss: 82.135937 time elapsed: 19.8435 learning rate: 0.000014, scenario: 0, slope: -0.6310542290581495, fluctuations: 0.09\n",
      "step: 16200 loss: 82.021258 time elapsed: 19.8551 learning rate: 0.000014, scenario: 0, slope: -1.1421636508949027, fluctuations: 0.09\n",
      "step: 16210 loss: 81.943220 time elapsed: 19.8674 learning rate: 0.000014, scenario: 0, slope: -1.8987794066625654, fluctuations: 0.09\n",
      "step: 16220 loss: 81.874886 time elapsed: 19.8816 learning rate: 0.000014, scenario: 0, slope: -1.5033606645949833, fluctuations: 0.07\n",
      "step: 16230 loss: 81.807808 time elapsed: 19.8955 learning rate: 0.000014, scenario: 0, slope: -0.2599521491850234, fluctuations: 0.04\n",
      "step: 16240 loss: 81.739326 time elapsed: 19.9083 learning rate: 0.000020, scenario: 1, slope: -0.0716029105313067, fluctuations: 0.02\n",
      "step: 16250 loss: 81.601250 time elapsed: 19.9207 learning rate: 0.000053, scenario: 1, slope: -0.023774444315411362, fluctuations: 0.01\n",
      "step: 16260 loss: 81.255689 time elapsed: 19.9330 learning rate: 0.000137, scenario: 1, slope: -0.01109173417537846, fluctuations: 0.0\n",
      "step: 16270 loss: 80.423568 time elapsed: 19.9462 learning rate: 0.000355, scenario: 1, slope: -0.01339110795770437, fluctuations: 0.0\n",
      "step: 16280 loss: 78.581861 time elapsed: 19.9591 learning rate: 0.000921, scenario: 1, slope: -0.02206930878288202, fluctuations: 0.0\n",
      "step: 16290 loss: 74.915016 time elapsed: 19.9716 learning rate: 0.002388, scenario: 1, slope: -0.04364011051596173, fluctuations: 0.0\n",
      "step: 16300 loss: 69.108756 time elapsed: 19.9838 learning rate: 0.005630, scenario: 1, slope: -0.08159619333268658, fluctuations: 0.0\n",
      "step: 16310 loss: 66.399968 time elapsed: 19.9966 learning rate: 0.005630, scenario: 0, slope: -0.14125801233569818, fluctuations: 0.0\n",
      "step: 16320 loss: 64.041432 time elapsed: 20.0089 learning rate: 0.005630, scenario: 0, slope: -0.19160081501869639, fluctuations: 0.0\n",
      "step: 16330 loss: 62.494978 time elapsed: 20.0206 learning rate: 0.005630, scenario: 0, slope: -0.2322816366412496, fluctuations: 0.0\n",
      "step: 16340 loss: 61.094922 time elapsed: 20.0326 learning rate: 0.005630, scenario: 0, slope: -0.2587957224139318, fluctuations: 0.0\n",
      "step: 16350 loss: 59.894047 time elapsed: 20.0443 learning rate: 0.005630, scenario: 0, slope: -0.2688142274547898, fluctuations: 0.0\n",
      "step: 16360 loss: 58.811614 time elapsed: 20.0563 learning rate: 0.005630, scenario: 0, slope: -0.2609680812519647, fluctuations: 0.0\n",
      "step: 16370 loss: 57.814792 time elapsed: 20.0682 learning rate: 0.005630, scenario: 0, slope: -0.23590251776058851, fluctuations: 0.0\n",
      "step: 16380 loss: 56.876260 time elapsed: 20.0823 learning rate: 0.005630, scenario: 0, slope: -0.19760499605752418, fluctuations: 0.0\n",
      "step: 16390 loss: 55.978291 time elapsed: 20.0955 learning rate: 0.005630, scenario: 0, slope: -0.1556203038334983, fluctuations: 0.0\n",
      "step: 16400 loss: 55.110831 time elapsed: 20.1079 learning rate: 0.005630, scenario: 0, slope: -0.12872682499707652, fluctuations: 0.0\n",
      "step: 16410 loss: 54.269230 time elapsed: 20.1208 learning rate: 0.005630, scenario: 0, slope: -0.11180246046325609, fluctuations: 0.0\n",
      "step: 16420 loss: 53.451158 time elapsed: 20.1326 learning rate: 0.005630, scenario: 0, slope: -0.10175739194558793, fluctuations: 0.0\n",
      "step: 16430 loss: 52.655627 time elapsed: 20.1447 learning rate: 0.005630, scenario: 0, slope: -0.09514480381220482, fluctuations: 0.0\n",
      "step: 16440 loss: 51.882191 time elapsed: 20.1569 learning rate: 0.005630, scenario: 0, slope: -0.09014905029329763, fluctuations: 0.0\n",
      "step: 16450 loss: 51.130532 time elapsed: 20.1686 learning rate: 0.005630, scenario: 0, slope: -0.08641594251660911, fluctuations: 0.0\n",
      "step: 16460 loss: 50.400274 time elapsed: 20.1801 learning rate: 0.005630, scenario: 0, slope: -0.08340623190764548, fluctuations: 0.0\n",
      "step: 16470 loss: 49.690887 time elapsed: 20.1921 learning rate: 0.005630, scenario: 0, slope: -0.08082791789778548, fluctuations: 0.0\n",
      "step: 16480 loss: 49.001680 time elapsed: 20.2036 learning rate: 0.005630, scenario: 0, slope: -0.07847784271593222, fluctuations: 0.0\n",
      "step: 16490 loss: 48.331823 time elapsed: 20.2153 learning rate: 0.005630, scenario: 0, slope: -0.07625281443962953, fluctuations: 0.0\n",
      "step: 16500 loss: 47.680389 time elapsed: 20.2270 learning rate: 0.005630, scenario: 0, slope: -0.07431735907650616, fluctuations: 0.0\n",
      "step: 16510 loss: 47.046389 time elapsed: 20.2387 learning rate: 0.005630, scenario: 0, slope: -0.07202215685998298, fluctuations: 0.0\n",
      "step: 16520 loss: 46.428798 time elapsed: 20.2503 learning rate: 0.005630, scenario: 0, slope: -0.07000338553822985, fluctuations: 0.0\n",
      "step: 16530 loss: 45.826576 time elapsed: 20.2622 learning rate: 0.005630, scenario: 0, slope: -0.06805693333711524, fluctuations: 0.0\n",
      "step: 16540 loss: 45.238681 time elapsed: 20.2746 learning rate: 0.005630, scenario: 0, slope: -0.06619185013435583, fluctuations: 0.0\n",
      "step: 16550 loss: 44.664087 time elapsed: 20.2864 learning rate: 0.005630, scenario: 0, slope: -0.06441614962822893, fluctuations: 0.0\n",
      "step: 16560 loss: 44.101796 time elapsed: 20.3001 learning rate: 0.005630, scenario: 0, slope: -0.06273574171237296, fluctuations: 0.0\n",
      "step: 16570 loss: 43.550861 time elapsed: 20.3127 learning rate: 0.005630, scenario: 0, slope: -0.06115417889950894, fluctuations: 0.0\n",
      "step: 16580 loss: 43.010397 time elapsed: 20.3248 learning rate: 0.005630, scenario: 0, slope: -0.05967280299734071, fluctuations: 0.0\n",
      "step: 16590 loss: 42.479595 time elapsed: 20.3383 learning rate: 0.005630, scenario: 0, slope: -0.05829101016791094, fluctuations: 0.0\n",
      "step: 16600 loss: 41.957732 time elapsed: 20.3513 learning rate: 0.005630, scenario: 0, slope: -0.05713066296193129, fluctuations: 0.0\n",
      "step: 16610 loss: 41.444164 time elapsed: 20.3652 learning rate: 0.005630, scenario: 0, slope: -0.0558154346568427, fluctuations: 0.0\n",
      "step: 16620 loss: 40.938312 time elapsed: 20.3777 learning rate: 0.005630, scenario: 0, slope: -0.05471279342552393, fluctuations: 0.0\n",
      "step: 16630 loss: 40.439613 time elapsed: 20.3902 learning rate: 0.005630, scenario: 0, slope: -0.05369274117330424, fluctuations: 0.0\n",
      "step: 16640 loss: 39.948008 time elapsed: 20.4020 learning rate: 0.005630, scenario: 0, slope: -0.05274828436508537, fluctuations: 0.0\n",
      "step: 16650 loss: 39.463448 time elapsed: 20.4146 learning rate: 0.005630, scenario: 0, slope: -0.05186856351709576, fluctuations: 0.0\n",
      "step: 16660 loss: 38.985422 time elapsed: 20.4269 learning rate: 0.005630, scenario: 0, slope: -0.051045143151954304, fluctuations: 0.0\n",
      "step: 16670 loss: 38.513629 time elapsed: 20.4390 learning rate: 0.005630, scenario: 0, slope: -0.05026977261820014, fluctuations: 0.0\n",
      "step: 16680 loss: 38.047769 time elapsed: 20.4521 learning rate: 0.005630, scenario: 0, slope: -0.049535538585344104, fluctuations: 0.0\n",
      "step: 16690 loss: 37.588071 time elapsed: 20.4659 learning rate: 0.005630, scenario: 0, slope: -0.04883597073423875, fluctuations: 0.0\n",
      "step: 16700 loss: 37.136223 time elapsed: 20.4799 learning rate: 0.005630, scenario: 0, slope: -0.04822600220810079, fluctuations: 0.0\n",
      "step: 16710 loss: 36.692024 time elapsed: 20.4951 learning rate: 0.005630, scenario: 0, slope: -0.04748861841128967, fluctuations: 0.0\n",
      "step: 16720 loss: 36.255389 time elapsed: 20.5080 learning rate: 0.005630, scenario: 0, slope: -0.04681617054957747, fluctuations: 0.0\n",
      "step: 16730 loss: 35.826327 time elapsed: 20.5201 learning rate: 0.005630, scenario: 0, slope: -0.04613440366878967, fluctuations: 0.0\n",
      "step: 16740 loss: 35.404739 time elapsed: 20.5323 learning rate: 0.005630, scenario: 0, slope: -0.04543976795746213, fluctuations: 0.0\n",
      "step: 16750 loss: 34.990552 time elapsed: 20.5446 learning rate: 0.005630, scenario: 0, slope: -0.044728094046199006, fluctuations: 0.0\n",
      "step: 16760 loss: 34.583683 time elapsed: 20.5567 learning rate: 0.005630, scenario: 0, slope: -0.04400008812071411, fluctuations: 0.0\n",
      "step: 16770 loss: 34.250356 time elapsed: 20.5683 learning rate: 0.005630, scenario: 0, slope: -0.04319844131035133, fluctuations: 0.0\n",
      "step: 16780 loss: 2880.762086 time elapsed: 20.5802 learning rate: 0.004741, scenario: -1, slope: 3.8066805101609127, fluctuations: 0.0\n",
      "step: 16790 loss: 466.333041 time elapsed: 20.5921 learning rate: 0.001653, scenario: -1, slope: 3.1389920334972627, fluctuations: 0.04\n",
      "step: 16800 loss: 92.008438 time elapsed: 20.6042 learning rate: 0.000640, scenario: -1, slope: 2.92027409318782, fluctuations: 0.07\n",
      "step: 16810 loss: 53.002577 time elapsed: 20.6167 learning rate: 0.000223, scenario: -1, slope: 2.114164946143568, fluctuations: 0.08\n",
      "step: 16820 loss: 50.426777 time elapsed: 20.6285 learning rate: 0.000078, scenario: -1, slope: 1.3303263017551834, fluctuations: 0.09\n",
      "step: 16830 loss: 49.443923 time elapsed: 20.6403 learning rate: 0.000027, scenario: -1, slope: 0.4877797767022368, fluctuations: 0.09\n",
      "step: 16840 loss: 49.186117 time elapsed: 20.6522 learning rate: 0.000014, scenario: 0, slope: -0.29487872431529566, fluctuations: 0.1\n",
      "step: 16850 loss: 48.893969 time elapsed: 20.6643 learning rate: 0.000014, scenario: 0, slope: -1.139255066953275, fluctuations: 0.1\n",
      "step: 16860 loss: 48.721253 time elapsed: 20.6762 learning rate: 0.000014, scenario: 0, slope: -2.062389641161136, fluctuations: 0.1\n",
      "step: 16870 loss: 48.626546 time elapsed: 20.6883 learning rate: 0.000014, scenario: 0, slope: -3.1531482867149556, fluctuations: 0.1\n",
      "step: 16880 loss: 48.552409 time elapsed: 20.7000 learning rate: 0.000014, scenario: 0, slope: -1.3992022706659404, fluctuations: 0.09\n",
      "step: 16890 loss: 48.480010 time elapsed: 20.7138 learning rate: 0.000014, scenario: 0, slope: -0.5266780642524854, fluctuations: 0.05\n",
      "step: 16900 loss: 48.408232 time elapsed: 20.7260 learning rate: 0.000014, scenario: 0, slope: -0.14903132441749184, fluctuations: 0.03\n",
      "step: 16910 loss: 48.330275 time elapsed: 20.7389 learning rate: 0.000026, scenario: 1, slope: -0.04284792055946653, fluctuations: 0.02\n",
      "step: 16920 loss: 48.155734 time elapsed: 20.7511 learning rate: 0.000066, scenario: 1, slope: -0.012806410201420513, fluctuations: 0.01\n",
      "step: 16930 loss: 47.728071 time elapsed: 20.7628 learning rate: 0.000172, scenario: 1, slope: -0.012515332988838944, fluctuations: 0.0\n",
      "step: 16940 loss: 46.751713 time elapsed: 20.7747 learning rate: 0.000446, scenario: 1, slope: -0.014987804164818399, fluctuations: 0.0\n",
      "step: 16950 loss: 44.786018 time elapsed: 20.7865 learning rate: 0.001157, scenario: 1, slope: -0.025071348138472678, fluctuations: 0.0\n",
      "step: 16960 loss: 41.429530 time elapsed: 20.8007 learning rate: 0.003000, scenario: 1, slope: -0.0475544602677685, fluctuations: 0.0\n",
      "step: 16970 loss: 38.347754 time elapsed: 20.8168 learning rate: 0.003000, scenario: 0, slope: -0.0830802741912281, fluctuations: 0.0\n",
      "step: 16980 loss: 36.966804 time elapsed: 20.8313 learning rate: 0.003000, scenario: 0, slope: -0.1187480459891584, fluctuations: 0.0\n",
      "step: 16990 loss: 36.012717 time elapsed: 20.8445 learning rate: 0.003000, scenario: 0, slope: -0.14757605467591636, fluctuations: 0.0\n",
      "step: 17000 loss: 35.339832 time elapsed: 20.8572 learning rate: 0.003000, scenario: 0, slope: -0.1652352269214438, fluctuations: 0.0\n",
      "step: 17010 loss: 34.836108 time elapsed: 20.8709 learning rate: 0.003000, scenario: 0, slope: -0.17384460517127606, fluctuations: 0.0\n",
      "step: 17020 loss: 34.413816 time elapsed: 20.8855 learning rate: 0.003000, scenario: 0, slope: -0.16831066459244629, fluctuations: 0.0\n",
      "step: 17030 loss: 34.039686 time elapsed: 20.9014 learning rate: 0.003000, scenario: 0, slope: -0.1504492880254726, fluctuations: 0.0\n",
      "step: 17040 loss: 33.696242 time elapsed: 20.9166 learning rate: 0.003000, scenario: 0, slope: -0.12256349159658528, fluctuations: 0.0\n",
      "step: 17050 loss: 33.376639 time elapsed: 20.9298 learning rate: 0.003000, scenario: 0, slope: -0.09004836279693869, fluctuations: 0.0\n",
      "step: 17060 loss: 33.075840 time elapsed: 20.9421 learning rate: 0.003000, scenario: 0, slope: -0.06224117323878114, fluctuations: 0.0\n",
      "step: 17070 loss: 32.790740 time elapsed: 20.9548 learning rate: 0.003000, scenario: 0, slope: -0.0476280168073926, fluctuations: 0.0\n",
      "step: 17080 loss: 32.519460 time elapsed: 20.9673 learning rate: 0.003000, scenario: 0, slope: -0.039889792044546575, fluctuations: 0.0\n",
      "step: 17090 loss: 32.257882 time elapsed: 20.9796 learning rate: 0.003993, scenario: 1, slope: -0.03507314915802953, fluctuations: 0.0\n",
      "step: 17100 loss: 31.786397 time elapsed: 20.9914 learning rate: 0.009416, scenario: 1, slope: -0.03268622050603848, fluctuations: 0.0\n",
      "step: 17110 loss: 94.110253 time elapsed: 21.0039 learning rate: 0.021194, scenario: -1, slope: 0.004677453890260862, fluctuations: 0.0\n",
      "step: 17120 loss: 54518.798039 time elapsed: 21.0160 learning rate: 0.007390, scenario: -1, slope: 338.6288322224389, fluctuations: 0.02\n",
      "step: 17130 loss: 19715.859565 time elapsed: 21.0282 learning rate: 0.002577, scenario: -1, slope: 411.46758977981114, fluctuations: 0.04\n",
      "step: 17140 loss: 11888.911784 time elapsed: 21.0404 learning rate: 0.000898, scenario: -1, slope: 376.88079167333376, fluctuations: 0.04\n",
      "step: 17150 loss: 9964.775723 time elapsed: 21.0522 learning rate: 0.000313, scenario: -1, slope: 305.66212870885346, fluctuations: 0.04\n",
      "step: 17160 loss: 9355.399763 time elapsed: 21.0639 learning rate: 0.000109, scenario: -1, slope: 220.2760710805287, fluctuations: 0.04\n",
      "step: 17170 loss: 9176.036078 time elapsed: 21.0758 learning rate: 0.000038, scenario: -1, slope: 124.37327520109169, fluctuations: 0.04\n",
      "step: 17180 loss: 9116.709870 time elapsed: 21.0876 learning rate: 0.000013, scenario: -1, slope: 16.77504819826309, fluctuations: 0.04\n",
      "step: 17190 loss: 9090.745985 time elapsed: 21.0996 learning rate: 0.000012, scenario: 0, slope: -105.58160616885755, fluctuations: 0.04\n",
      "step: 17200 loss: 9066.028717 time elapsed: 21.1114 learning rate: 0.000012, scenario: 0, slope: -232.04273360896823, fluctuations: 0.04\n",
      "step: 17210 loss: 9041.623424 time elapsed: 21.1237 learning rate: 0.000012, scenario: 0, slope: -414.4838648831053, fluctuations: 0.04\n",
      "step: 17220 loss: 9017.498137 time elapsed: 21.1365 learning rate: 0.000012, scenario: 0, slope: -146.11334942327855, fluctuations: 0.01\n",
      "step: 17230 loss: 8993.631741 time elapsed: 21.1487 learning rate: 0.000012, scenario: 0, slope: -41.473914669563975, fluctuations: 0.0\n",
      "step: 17240 loss: 8970.008052 time elapsed: 21.1607 learning rate: 0.000012, scenario: 0, slope: -12.589563131823414, fluctuations: 0.0\n",
      "step: 17250 loss: 8938.610482 time elapsed: 21.1727 learning rate: 0.000028, scenario: 1, slope: -5.034650571124717, fluctuations: 0.0\n",
      "step: 17260 loss: 8860.063883 time elapsed: 21.1847 learning rate: 0.000073, scenario: 1, slope: -3.158152553740651, fluctuations: 0.0\n",
      "step: 17270 loss: 8663.435561 time elapsed: 21.1964 learning rate: 0.000190, scenario: 1, slope: -3.4130021397649712, fluctuations: 0.0\n",
      "step: 17280 loss: 8194.432207 time elapsed: 21.2080 learning rate: 0.000492, scenario: 1, slope: -5.588934924190658, fluctuations: 0.0\n",
      "step: 17290 loss: 7214.777399 time elapsed: 21.2196 learning rate: 0.000958, scenario: 0, slope: -11.177293389211542, fluctuations: 0.0\n",
      "step: 17300 loss: 6204.750585 time elapsed: 21.2313 learning rate: 0.000958, scenario: 0, slope: -19.90854990435779, fluctuations: 0.0\n",
      "step: 17310 loss: 5440.981958 time elapsed: 21.2432 learning rate: 0.000958, scenario: 0, slope: -32.761531412905285, fluctuations: 0.0\n",
      "step: 17320 loss: 4836.830749 time elapsed: 21.2552 learning rate: 0.000958, scenario: 0, slope: -44.31043403835016, fluctuations: 0.0\n",
      "step: 17330 loss: 4278.798251 time elapsed: 21.2671 learning rate: 0.000958, scenario: 0, slope: -54.36254881480163, fluctuations: 0.0\n",
      "step: 17340 loss: 3676.868703 time elapsed: 21.2790 learning rate: 0.000958, scenario: 0, slope: -62.406166797276086, fluctuations: 0.0\n",
      "step: 17350 loss: 3146.920932 time elapsed: 21.2913 learning rate: 0.000958, scenario: 0, slope: -67.54222955305666, fluctuations: 0.0\n",
      "step: 17360 loss: 2704.742191 time elapsed: 21.3037 learning rate: 0.000958, scenario: 0, slope: -69.10805605939731, fluctuations: 0.0\n",
      "step: 17370 loss: 2430.175757 time elapsed: 21.3157 learning rate: 0.000958, scenario: 0, slope: -66.06304953593548, fluctuations: 0.0\n",
      "step: 17380 loss: 2235.725701 time elapsed: 21.3288 learning rate: 0.000958, scenario: 0, slope: -58.94358885546098, fluctuations: 0.0\n",
      "step: 17390 loss: 2081.853001 time elapsed: 21.3440 learning rate: 0.000958, scenario: 0, slope: -50.18685461775746, fluctuations: 0.0\n",
      "step: 17400 loss: 1956.229302 time elapsed: 21.3566 learning rate: 0.000958, scenario: 0, slope: -43.26818649969035, fluctuations: 0.0\n",
      "step: 17410 loss: 1849.504605 time elapsed: 21.3703 learning rate: 0.000958, scenario: 0, slope: -35.746033396760666, fluctuations: 0.0\n",
      "step: 17420 loss: 1757.199852 time elapsed: 21.3827 learning rate: 0.000958, scenario: 0, slope: -29.2442332783091, fluctuations: 0.0\n",
      "step: 17430 loss: 1676.142564 time elapsed: 21.3944 learning rate: 0.000958, scenario: 0, slope: -22.990836284967504, fluctuations: 0.0\n",
      "step: 17440 loss: 1603.992859 time elapsed: 21.4069 learning rate: 0.000958, scenario: 0, slope: -17.66348344855484, fluctuations: 0.0\n",
      "step: 17450 loss: 1539.056351 time elapsed: 21.4192 learning rate: 0.000958, scenario: 0, slope: -13.586287186146183, fluctuations: 0.0\n",
      "step: 17460 loss: 1480.073211 time elapsed: 21.4315 learning rate: 0.000958, scenario: 0, slope: -11.051592390198868, fluctuations: 0.0\n",
      "step: 17470 loss: 1426.050857 time elapsed: 21.4433 learning rate: 0.000958, scenario: 0, slope: -9.338695027809548, fluctuations: 0.0\n",
      "step: 17480 loss: 1376.211464 time elapsed: 21.4551 learning rate: 0.000958, scenario: 0, slope: -8.120371403381212, fluctuations: 0.0\n",
      "step: 17490 loss: 1329.946454 time elapsed: 21.4670 learning rate: 0.000958, scenario: 0, slope: -7.186333724891422, fluctuations: 0.0\n",
      "step: 17500 loss: 1286.778417 time elapsed: 21.4786 learning rate: 0.000958, scenario: 0, slope: -6.510812482071771, fluctuations: 0.0\n",
      "step: 17510 loss: 1246.330271 time elapsed: 21.4910 learning rate: 0.000958, scenario: 0, slope: -5.839409534262449, fluctuations: 0.0\n",
      "step: 17520 loss: 1208.300434 time elapsed: 21.5028 learning rate: 0.000958, scenario: 0, slope: -5.338984521410896, fluctuations: 0.0\n",
      "step: 17530 loss: 1172.443213 time elapsed: 21.5145 learning rate: 0.000958, scenario: 0, slope: -4.9181086064005255, fluctuations: 0.0\n",
      "step: 17540 loss: 1138.553598 time elapsed: 21.5265 learning rate: 0.000958, scenario: 0, slope: -4.5592246746871465, fluctuations: 0.0\n",
      "step: 17550 loss: 1106.455496 time elapsed: 21.5388 learning rate: 0.000958, scenario: 0, slope: -4.249132858562914, fluctuations: 0.0\n",
      "step: 17560 loss: 1075.992403 time elapsed: 21.5518 learning rate: 0.000958, scenario: 0, slope: -3.977856271756284, fluctuations: 0.0\n",
      "step: 17570 loss: 1047.017718 time elapsed: 21.5637 learning rate: 0.000958, scenario: 0, slope: -3.737823825358929, fluctuations: 0.0\n",
      "step: 17580 loss: 1019.374381 time elapsed: 21.5759 learning rate: 0.000958, scenario: 0, slope: -3.52343773126805, fluctuations: 0.0\n",
      "step: 17590 loss: 992.794222 time elapsed: 21.5880 learning rate: 0.000958, scenario: 0, slope: -3.3309681817262153, fluctuations: 0.0\n",
      "step: 17600 loss: 965.382089 time elapsed: 21.5994 learning rate: 0.000958, scenario: 0, slope: -3.176872983923425, fluctuations: 0.0\n",
      "step: 17610 loss: 885.662349 time elapsed: 21.6116 learning rate: 0.000958, scenario: 0, slope: -3.150263197563829, fluctuations: 0.0\n",
      "step: 17620 loss: 807.874769 time elapsed: 21.6234 learning rate: 0.000958, scenario: 0, slope: -3.470580973359091, fluctuations: 0.0\n",
      "step: 17630 loss: 758.681721 time elapsed: 21.6352 learning rate: 0.000958, scenario: 0, slope: -3.9088889747755684, fluctuations: 0.0\n",
      "step: 17640 loss: 732.199397 time elapsed: 21.6467 learning rate: 0.000958, scenario: 0, slope: -4.2576660402319115, fluctuations: 0.0\n",
      "step: 17650 loss: 710.475922 time elapsed: 21.6584 learning rate: 0.000958, scenario: 0, slope: -4.4590888343897, fluctuations: 0.0\n",
      "step: 17660 loss: 692.749621 time elapsed: 21.6700 learning rate: 0.000958, scenario: 0, slope: -4.489249005511403, fluctuations: 0.0\n",
      "step: 17670 loss: 677.168504 time elapsed: 21.6821 learning rate: 0.000958, scenario: 0, slope: -4.337162700193835, fluctuations: 0.0\n",
      "step: 17680 loss: 662.932272 time elapsed: 21.6942 learning rate: 0.000958, scenario: 0, slope: -3.9999361216806686, fluctuations: 0.0\n",
      "step: 17690 loss: 649.650322 time elapsed: 21.7062 learning rate: 0.000958, scenario: 0, slope: -3.4801141769292143, fluctuations: 0.0\n",
      "step: 17700 loss: 637.154114 time elapsed: 21.7184 learning rate: 0.000958, scenario: 0, slope: -2.862492016886092, fluctuations: 0.0\n",
      "step: 17710 loss: 625.290446 time elapsed: 21.7311 learning rate: 0.000958, scenario: 0, slope: -2.0786529031004752, fluctuations: 0.0\n",
      "step: 17720 loss: 613.967206 time elapsed: 21.7459 learning rate: 0.000958, scenario: 0, slope: -1.6636177059072033, fluctuations: 0.0\n",
      "step: 17730 loss: 603.117286 time elapsed: 21.7603 learning rate: 0.000958, scenario: 0, slope: -1.4629645535156084, fluctuations: 0.0\n",
      "step: 17740 loss: 592.692779 time elapsed: 21.7728 learning rate: 0.000958, scenario: 0, slope: -1.3362696827910678, fluctuations: 0.0\n",
      "step: 17750 loss: 582.657726 time elapsed: 21.7858 learning rate: 0.000958, scenario: 0, slope: -1.2458786240908033, fluctuations: 0.0\n",
      "step: 17760 loss: 572.983060 time elapsed: 21.7986 learning rate: 0.000958, scenario: 0, slope: -1.177129326517109, fluctuations: 0.0\n",
      "step: 17770 loss: 563.645103 time elapsed: 21.8109 learning rate: 0.000958, scenario: 0, slope: -1.120893919477434, fluctuations: 0.0\n",
      "step: 17780 loss: 554.623592 time elapsed: 21.8231 learning rate: 0.000958, scenario: 0, slope: -1.0721894856660774, fluctuations: 0.0\n",
      "step: 17790 loss: 545.900748 time elapsed: 21.8350 learning rate: 0.000958, scenario: 0, slope: -1.0290981187945125, fluctuations: 0.0\n",
      "step: 17800 loss: 537.460630 time elapsed: 21.8467 learning rate: 0.000958, scenario: 0, slope: -0.9938461665524905, fluctuations: 0.0\n",
      "step: 17810 loss: 529.288695 time elapsed: 21.8588 learning rate: 0.000958, scenario: 0, slope: -0.9542601370544259, fluctuations: 0.0\n",
      "step: 17820 loss: 521.371506 time elapsed: 21.8704 learning rate: 0.000958, scenario: 0, slope: -0.9209400843299055, fluctuations: 0.0\n",
      "step: 17830 loss: 513.696513 time elapsed: 21.8823 learning rate: 0.000958, scenario: 0, slope: -0.8897287044927825, fluctuations: 0.0\n",
      "step: 17840 loss: 506.251897 time elapsed: 21.8940 learning rate: 0.000958, scenario: 0, slope: -0.8603365449055357, fluctuations: 0.0\n",
      "step: 17850 loss: 499.026462 time elapsed: 21.9058 learning rate: 0.000958, scenario: 0, slope: -0.8325565867944615, fluctuations: 0.0\n",
      "step: 17860 loss: 492.009549 time elapsed: 21.9180 learning rate: 0.000958, scenario: 0, slope: -0.8062353121663417, fluctuations: 0.0\n",
      "step: 17870 loss: 485.190970 time elapsed: 21.9300 learning rate: 0.000958, scenario: 0, slope: -0.7812552315198866, fluctuations: 0.0\n",
      "step: 17880 loss: 478.560962 time elapsed: 21.9420 learning rate: 0.000958, scenario: 0, slope: -0.7575231425525077, fluctuations: 0.0\n",
      "step: 17890 loss: 472.110140 time elapsed: 21.9549 learning rate: 0.000958, scenario: 0, slope: -0.7349630471141079, fluctuations: 0.0\n",
      "step: 17900 loss: 465.829454 time elapsed: 21.9679 learning rate: 0.000958, scenario: 0, slope: -0.7156082957156029, fluctuations: 0.0\n",
      "step: 17910 loss: 459.710151 time elapsed: 21.9804 learning rate: 0.000958, scenario: 0, slope: -0.693113988501213, fluctuations: 0.0\n",
      "step: 17920 loss: 453.743722 time elapsed: 21.9924 learning rate: 0.000958, scenario: 0, slope: -0.6737238145721711, fluctuations: 0.0\n",
      "step: 17930 loss: 447.921831 time elapsed: 22.0041 learning rate: 0.000958, scenario: 0, slope: -0.6552999058702111, fluctuations: 0.0\n",
      "step: 17940 loss: 442.236226 time elapsed: 22.0158 learning rate: 0.000958, scenario: 0, slope: -0.6378067690145508, fluctuations: 0.0\n",
      "step: 17950 loss: 436.678598 time elapsed: 22.0273 learning rate: 0.000958, scenario: 0, slope: -0.621214556889703, fluctuations: 0.0\n",
      "step: 17960 loss: 431.240370 time elapsed: 22.0391 learning rate: 0.000958, scenario: 0, slope: -0.6055000240366247, fluctuations: 0.0\n",
      "step: 17970 loss: 425.912367 time elapsed: 22.0512 learning rate: 0.000958, scenario: 0, slope: -0.5906485634352128, fluctuations: 0.0\n",
      "step: 17980 loss: 420.684262 time elapsed: 22.0629 learning rate: 0.000958, scenario: 0, slope: -0.5766579029176512, fluctuations: 0.0\n",
      "step: 17990 loss: 415.543626 time elapsed: 22.0749 learning rate: 0.000958, scenario: 0, slope: -0.563544608011674, fluctuations: 0.0\n",
      "step: 18000 loss: 410.474201 time elapsed: 22.0864 learning rate: 0.000958, scenario: 0, slope: -0.552530735681214, fluctuations: 0.0\n",
      "step: 18010 loss: 405.452528 time elapsed: 22.0985 learning rate: 0.000958, scenario: 0, slope: -0.5401904843793707, fluctuations: 0.0\n",
      "step: 18020 loss: 400.440889 time elapsed: 22.1103 learning rate: 0.000958, scenario: 0, slope: -0.5302435908090559, fluctuations: 0.0\n",
      "step: 18030 loss: 395.370947 time elapsed: 22.1216 learning rate: 0.000958, scenario: 0, slope: -0.5218984192049726, fluctuations: 0.0\n",
      "step: 18040 loss: 390.100951 time elapsed: 22.1328 learning rate: 0.000958, scenario: 0, slope: -0.5159508542647362, fluctuations: 0.0\n",
      "step: 18050 loss: 384.284511 time elapsed: 22.1446 learning rate: 0.000958, scenario: 0, slope: -0.5142265043344663, fluctuations: 0.0\n",
      "step: 18060 loss: 376.884529 time elapsed: 22.1573 learning rate: 0.000958, scenario: 0, slope: -0.521615337987257, fluctuations: 0.0\n",
      "step: 18070 loss: 364.300631 time elapsed: 22.1717 learning rate: 0.000958, scenario: 0, slope: -0.5538275222307921, fluctuations: 0.0\n",
      "step: 18080 loss: 345.268159 time elapsed: 22.1848 learning rate: 0.000958, scenario: 0, slope: -0.6448976770993116, fluctuations: 0.0\n",
      "step: 18090 loss: 332.843438 time elapsed: 22.1981 learning rate: 0.000958, scenario: 0, slope: -0.769662194287269, fluctuations: 0.0\n",
      "step: 18100 loss: 323.076321 time elapsed: 22.2106 learning rate: 0.000958, scenario: 0, slope: -0.8790403453217835, fluctuations: 0.0\n",
      "step: 18110 loss: 315.848875 time elapsed: 22.2228 learning rate: 0.000958, scenario: 0, slope: -0.9862899234063326, fluctuations: 0.0\n",
      "step: 18120 loss: 309.725435 time elapsed: 22.2377 learning rate: 0.000958, scenario: 0, slope: -1.0440731313633753, fluctuations: 0.0\n",
      "step: 18130 loss: 304.321241 time elapsed: 22.2512 learning rate: 0.000958, scenario: 0, slope: -1.0574455950035062, fluctuations: 0.0\n",
      "step: 18140 loss: 299.352232 time elapsed: 22.2641 learning rate: 0.000958, scenario: 0, slope: -1.0234950411427488, fluctuations: 0.0\n",
      "step: 18150 loss: 294.694592 time elapsed: 22.2767 learning rate: 0.000958, scenario: 0, slope: -0.9426631871389385, fluctuations: 0.0\n",
      "step: 18160 loss: 290.284865 time elapsed: 22.2888 learning rate: 0.000958, scenario: 0, slope: -0.8209746408764939, fluctuations: 0.0\n",
      "step: 18170 loss: 286.085241 time elapsed: 22.3008 learning rate: 0.000958, scenario: 0, slope: -0.6804868271181579, fluctuations: 0.0\n",
      "step: 18180 loss: 282.068688 time elapsed: 22.3126 learning rate: 0.000958, scenario: 0, slope: -0.5753014926260974, fluctuations: 0.0\n",
      "step: 18190 loss: 278.214690 time elapsed: 22.3241 learning rate: 0.000958, scenario: 0, slope: -0.5106222760708967, fluctuations: 0.0\n",
      "step: 18200 loss: 274.507186 time elapsed: 22.3356 learning rate: 0.000958, scenario: 0, slope: -0.4714737613389749, fluctuations: 0.0\n",
      "step: 18210 loss: 270.932941 time elapsed: 22.3481 learning rate: 0.000958, scenario: 0, slope: -0.4387671519922863, fluctuations: 0.0\n",
      "step: 18220 loss: 267.480934 time elapsed: 22.3598 learning rate: 0.000958, scenario: 0, slope: -0.4160531687228976, fluctuations: 0.0\n",
      "step: 18230 loss: 264.141841 time elapsed: 22.3729 learning rate: 0.000958, scenario: 0, slope: -0.3972568703867477, fluctuations: 0.0\n",
      "step: 18240 loss: 260.907688 time elapsed: 22.3861 learning rate: 0.000958, scenario: 0, slope: -0.38091355314828995, fluctuations: 0.0\n",
      "step: 18250 loss: 257.771568 time elapsed: 22.3984 learning rate: 0.000958, scenario: 0, slope: -0.36633129259737235, fluctuations: 0.0\n",
      "step: 18260 loss: 254.727444 time elapsed: 22.4104 learning rate: 0.000958, scenario: 0, slope: -0.3531269688167816, fluctuations: 0.0\n",
      "step: 18270 loss: 251.769987 time elapsed: 22.4228 learning rate: 0.000958, scenario: 0, slope: -0.341045330547426, fluctuations: 0.0\n",
      "step: 18280 loss: 248.894451 time elapsed: 22.4345 learning rate: 0.000958, scenario: 0, slope: -0.3299008234019449, fluctuations: 0.0\n",
      "step: 18290 loss: 246.096570 time elapsed: 22.4467 learning rate: 0.000958, scenario: 0, slope: -0.31955355999635765, fluctuations: 0.0\n",
      "step: 18300 loss: 243.372480 time elapsed: 22.4582 learning rate: 0.000958, scenario: 0, slope: -0.3108313694906349, fluctuations: 0.0\n",
      "step: 18310 loss: 240.718651 time elapsed: 22.4704 learning rate: 0.000958, scenario: 0, slope: -0.30083316719266767, fluctuations: 0.0\n",
      "step: 18320 loss: 238.131839 time elapsed: 22.4820 learning rate: 0.000958, scenario: 0, slope: -0.29230084572485215, fluctuations: 0.0\n",
      "step: 18330 loss: 235.609037 time elapsed: 22.4936 learning rate: 0.000958, scenario: 0, slope: -0.2842384721139722, fluctuations: 0.0\n",
      "step: 18340 loss: 233.147443 time elapsed: 22.5056 learning rate: 0.000958, scenario: 0, slope: -0.27659804038078095, fluctuations: 0.0\n",
      "step: 18350 loss: 230.744431 time elapsed: 22.5176 learning rate: 0.000958, scenario: 0, slope: -0.2693396869541574, fluctuations: 0.0\n",
      "step: 18360 loss: 228.397528 time elapsed: 22.5294 learning rate: 0.000958, scenario: 0, slope: -0.2624300951491806, fluctuations: 0.0\n",
      "step: 18370 loss: 226.104392 time elapsed: 22.5413 learning rate: 0.000958, scenario: 0, slope: -0.2558412321044782, fluctuations: 0.0\n",
      "step: 18380 loss: 223.862802 time elapsed: 22.5532 learning rate: 0.000958, scenario: 0, slope: -0.24954934743607074, fluctuations: 0.0\n",
      "step: 18390 loss: 221.131692 time elapsed: 22.5651 learning rate: 0.002054, scenario: 1, slope: -0.24433536955062518, fluctuations: 0.0\n",
      "step: 18400 loss: 215.330810 time elapsed: 22.5789 learning rate: 0.003008, scenario: 0, slope: -0.2496719874928009, fluctuations: 0.0\n",
      "step: 18410 loss: 209.126639 time elapsed: 22.5919 learning rate: 0.003008, scenario: 0, slope: -0.2783109291125041, fluctuations: 0.0\n",
      "step: 18420 loss: 203.336540 time elapsed: 22.6047 learning rate: 0.003008, scenario: 0, slope: -0.31873601424925035, fluctuations: 0.0\n",
      "step: 18430 loss: 197.922325 time elapsed: 22.6177 learning rate: 0.003008, scenario: 0, slope: -0.3660430593125064, fluctuations: 0.0\n",
      "step: 18440 loss: 192.839116 time elapsed: 22.6310 learning rate: 0.003008, scenario: 0, slope: -0.4142929354625897, fluctuations: 0.0\n",
      "step: 18450 loss: 188.044226 time elapsed: 22.6439 learning rate: 0.003008, scenario: 0, slope: -0.45814814158075956, fluctuations: 0.0\n",
      "step: 18460 loss: 183.499938 time elapsed: 22.6559 learning rate: 0.003008, scenario: 0, slope: -0.4928022603389936, fluctuations: 0.0\n",
      "step: 18470 loss: 179.189093 time elapsed: 22.6683 learning rate: 0.003008, scenario: 0, slope: -0.5138666146844398, fluctuations: 0.0\n",
      "step: 18480 loss: 175.085994 time elapsed: 22.6808 learning rate: 0.003008, scenario: 0, slope: -0.5172775185015538, fluctuations: 0.0\n",
      "step: 18490 loss: 171.171592 time elapsed: 22.6932 learning rate: 0.003008, scenario: 0, slope: -0.5001927498219693, fluctuations: 0.0\n",
      "step: 18500 loss: 167.429025 time elapsed: 22.7056 learning rate: 0.003008, scenario: 0, slope: -0.47527809295510137, fluctuations: 0.0\n",
      "step: 18510 loss: 163.845149 time elapsed: 22.7184 learning rate: 0.003008, scenario: 0, slope: -0.44725805472541985, fluctuations: 0.0\n",
      "step: 18520 loss: 160.408625 time elapsed: 22.7308 learning rate: 0.003008, scenario: 0, slope: -0.4245704482753837, fluctuations: 0.0\n",
      "step: 18530 loss: 157.109590 time elapsed: 22.7670 learning rate: 0.003008, scenario: 0, slope: -0.40411823711790823, fluctuations: 0.0\n",
      "step: 18540 loss: 153.939466 time elapsed: 22.7828 learning rate: 0.003008, scenario: 0, slope: -0.3855268017456721, fluctuations: 0.0\n",
      "step: 18550 loss: 150.890677 time elapsed: 22.7961 learning rate: 0.003008, scenario: 0, slope: -0.3684929174016055, fluctuations: 0.0\n",
      "step: 18560 loss: 147.956461 time elapsed: 22.8086 learning rate: 0.003008, scenario: 0, slope: -0.352799268883807, fluctuations: 0.0\n",
      "step: 18570 loss: 145.130714 time elapsed: 22.8211 learning rate: 0.003008, scenario: 0, slope: -0.3382722541671424, fluctuations: 0.0\n",
      "step: 18580 loss: 142.407873 time elapsed: 22.8333 learning rate: 0.003008, scenario: 0, slope: -0.3247227552395624, fluctuations: 0.0\n",
      "step: 18590 loss: 139.782819 time elapsed: 22.8451 learning rate: 0.003008, scenario: 0, slope: -0.3120307156585631, fluctuations: 0.0\n",
      "step: 18600 loss: 137.250807 time elapsed: 22.8568 learning rate: 0.003008, scenario: 0, slope: -0.30125555478425176, fluctuations: 0.0\n",
      "step: 18610 loss: 134.807408 time elapsed: 22.8690 learning rate: 0.003008, scenario: 0, slope: -0.2888283943727805, fluctuations: 0.0\n",
      "step: 18620 loss: 132.448463 time elapsed: 22.8804 learning rate: 0.003008, scenario: 0, slope: -0.27816978288305877, fluctuations: 0.0\n",
      "step: 18630 loss: 130.170051 time elapsed: 22.8921 learning rate: 0.003008, scenario: 0, slope: -0.26806307460150824, fluctuations: 0.0\n",
      "step: 18640 loss: 127.968465 time elapsed: 22.9041 learning rate: 0.003008, scenario: 0, slope: -0.25846329814865227, fluctuations: 0.0\n",
      "step: 18650 loss: 125.840193 time elapsed: 22.9162 learning rate: 0.003008, scenario: 0, slope: -0.24933281846469604, fluctuations: 0.0\n",
      "step: 18660 loss: 123.781902 time elapsed: 22.9284 learning rate: 0.003008, scenario: 0, slope: -0.240639689200775, fluctuations: 0.0\n",
      "step: 18670 loss: 121.790432 time elapsed: 22.9404 learning rate: 0.003008, scenario: 0, slope: -0.2323563582704917, fluctuations: 0.0\n",
      "step: 18680 loss: 119.862780 time elapsed: 22.9521 learning rate: 0.003008, scenario: 0, slope: -0.22445864803980306, fluctuations: 0.0\n",
      "step: 18690 loss: 117.996099 time elapsed: 22.9639 learning rate: 0.003008, scenario: 0, slope: -0.2169249551464538, fluctuations: 0.0\n",
      "step: 18700 loss: 116.187687 time elapsed: 22.9756 learning rate: 0.003008, scenario: 0, slope: -0.21043960870258807, fluctuations: 0.0\n",
      "step: 18710 loss: 114.434981 time elapsed: 22.9885 learning rate: 0.003008, scenario: 0, slope: -0.2028726277965083, fluctuations: 0.0\n",
      "step: 18720 loss: 112.735549 time elapsed: 23.0021 learning rate: 0.003008, scenario: 0, slope: -0.19631899991626545, fluctuations: 0.0\n",
      "step: 18730 loss: 111.087086 time elapsed: 23.0149 learning rate: 0.003008, scenario: 0, slope: -0.19005887349472989, fluctuations: 0.0\n",
      "step: 18740 loss: 109.487401 time elapsed: 23.0281 learning rate: 0.003008, scenario: 0, slope: -0.18407721853479714, fluctuations: 0.0\n",
      "step: 18750 loss: 107.934418 time elapsed: 23.0408 learning rate: 0.003008, scenario: 0, slope: -0.1783597855914433, fluctuations: 0.0\n",
      "step: 18760 loss: 106.426162 time elapsed: 23.0530 learning rate: 0.003008, scenario: 0, slope: -0.1728930598693497, fluctuations: 0.0\n",
      "step: 18770 loss: 104.960758 time elapsed: 23.0662 learning rate: 0.003008, scenario: 0, slope: -0.1676642361909627, fluctuations: 0.0\n",
      "step: 18780 loss: 103.536420 time elapsed: 23.0780 learning rate: 0.003008, scenario: 0, slope: -0.16266120352722357, fluctuations: 0.0\n",
      "step: 18790 loss: 102.151450 time elapsed: 23.0900 learning rate: 0.003008, scenario: 0, slope: -0.1578725313182399, fluctuations: 0.0\n",
      "step: 18800 loss: 100.804228 time elapsed: 23.1022 learning rate: 0.003008, scenario: 0, slope: -0.15373709254145457, fluctuations: 0.0\n",
      "step: 18810 loss: 99.493213 time elapsed: 23.1149 learning rate: 0.003008, scenario: 0, slope: -0.14889584409044382, fluctuations: 0.0\n",
      "step: 18820 loss: 98.216933 time elapsed: 23.1272 learning rate: 0.003008, scenario: 0, slope: -0.14468819489649506, fluctuations: 0.0\n",
      "step: 18830 loss: 96.973983 time elapsed: 23.1394 learning rate: 0.003008, scenario: 0, slope: -0.14065557724357755, fluctuations: 0.0\n",
      "step: 18840 loss: 95.763021 time elapsed: 23.1520 learning rate: 0.003008, scenario: 0, slope: -0.13678960753145455, fluctuations: 0.0\n",
      "step: 18850 loss: 94.582769 time elapsed: 23.1642 learning rate: 0.003008, scenario: 0, slope: -0.13308240694747178, fluctuations: 0.0\n",
      "step: 18860 loss: 93.432001 time elapsed: 23.1767 learning rate: 0.003008, scenario: 0, slope: -0.1295265606009591, fluctuations: 0.0\n",
      "step: 18870 loss: 92.309550 time elapsed: 23.1915 learning rate: 0.003008, scenario: 0, slope: -0.12611507697111943, fluctuations: 0.0\n",
      "step: 18880 loss: 91.214301 time elapsed: 23.2053 learning rate: 0.003008, scenario: 0, slope: -0.12284134887734724, fluctuations: 0.0\n",
      "step: 18890 loss: 90.145186 time elapsed: 23.2183 learning rate: 0.003008, scenario: 0, slope: -0.11969911693232832, fluctuations: 0.0\n",
      "step: 18900 loss: 89.101190 time elapsed: 23.2307 learning rate: 0.003008, scenario: 0, slope: -0.11697861788335871, fluctuations: 0.0\n",
      "step: 18910 loss: 88.081340 time elapsed: 23.2443 learning rate: 0.003008, scenario: 0, slope: -0.11378564647201855, fluctuations: 0.0\n",
      "step: 18920 loss: 87.084710 time elapsed: 23.2560 learning rate: 0.003008, scenario: 0, slope: -0.11100334667790503, fluctuations: 0.0\n",
      "step: 18930 loss: 86.110416 time elapsed: 23.2691 learning rate: 0.003008, scenario: 0, slope: -0.10833037320036756, fluctuations: 0.0\n",
      "step: 18940 loss: 85.157614 time elapsed: 23.2828 learning rate: 0.003008, scenario: 0, slope: -0.105761782463581, fluctuations: 0.0\n",
      "step: 18950 loss: 84.225501 time elapsed: 23.2958 learning rate: 0.003008, scenario: 0, slope: -0.10329283745190208, fluctuations: 0.0\n",
      "step: 18960 loss: 83.313309 time elapsed: 23.3078 learning rate: 0.003008, scenario: 0, slope: -0.1009189979791093, fluctuations: 0.0\n",
      "step: 18970 loss: 82.420306 time elapsed: 23.3202 learning rate: 0.003008, scenario: 0, slope: -0.09863591430068824, fluctuations: 0.0\n",
      "step: 18980 loss: 81.545793 time elapsed: 23.3326 learning rate: 0.003008, scenario: 0, slope: -0.0964394235146665, fluctuations: 0.0\n",
      "step: 18990 loss: 80.689104 time elapsed: 23.3446 learning rate: 0.003008, scenario: 0, slope: -0.09432554803910662, fluctuations: 0.0\n",
      "step: 19000 loss: 79.849602 time elapsed: 23.3566 learning rate: 0.003008, scenario: 0, slope: -0.09249055783363244, fluctuations: 0.0\n",
      "step: 19010 loss: 79.026675 time elapsed: 23.3692 learning rate: 0.003008, scenario: 0, slope: -0.09033065787605273, fluctuations: 0.0\n",
      "step: 19020 loss: 78.219742 time elapsed: 23.3812 learning rate: 0.003008, scenario: 0, slope: -0.08844261271365075, fluctuations: 0.0\n",
      "step: 19030 loss: 77.428244 time elapsed: 23.3936 learning rate: 0.003008, scenario: 0, slope: -0.08662311944057058, fluctuations: 0.0\n",
      "step: 19040 loss: 76.643950 time elapsed: 23.4050 learning rate: 0.004003, scenario: 1, slope: -0.0848736879239515, fluctuations: 0.0\n",
      "step: 19050 loss: 75.203887 time elapsed: 23.4178 learning rate: 0.007801, scenario: 0, slope: -0.08475973049230884, fluctuations: 0.0\n",
      "step: 19060 loss: 440.642368 time elapsed: 23.4318 learning rate: 0.005374, scenario: -1, slope: 2.214738337049874, fluctuations: 0.01\n",
      "step: 19070 loss: 468.879761 time elapsed: 23.4450 learning rate: 0.001874, scenario: -1, slope: 3.96446654148402, fluctuations: 0.04\n",
      "step: 19080 loss: 148.247232 time elapsed: 23.4576 learning rate: 0.000653, scenario: -1, slope: 3.6709533541496744, fluctuations: 0.06\n",
      "step: 19090 loss: 109.336043 time elapsed: 23.4708 learning rate: 0.000228, scenario: -1, slope: 2.764896328668855, fluctuations: 0.07\n",
      "step: 19100 loss: 96.403419 time elapsed: 23.4841 learning rate: 0.000088, scenario: -1, slope: 1.8613648522949762, fluctuations: 0.08\n",
      "step: 19110 loss: 94.697069 time elapsed: 23.4981 learning rate: 0.000031, scenario: -1, slope: 0.6398988810332248, fluctuations: 0.08\n",
      "step: 19120 loss: 94.363341 time elapsed: 23.5105 learning rate: 0.000019, scenario: 0, slope: -0.4339938837542428, fluctuations: 0.08\n",
      "step: 19130 loss: 93.969420 time elapsed: 23.5230 learning rate: 0.000019, scenario: 0, slope: -1.5446550687763596, fluctuations: 0.08\n",
      "step: 19140 loss: 93.644808 time elapsed: 23.5354 learning rate: 0.000019, scenario: 0, slope: -2.766777251846986, fluctuations: 0.08\n",
      "step: 19150 loss: 93.394065 time elapsed: 23.5475 learning rate: 0.000019, scenario: 0, slope: -4.198747032859341, fluctuations: 0.08\n",
      "step: 19160 loss: 93.177682 time elapsed: 23.5603 learning rate: 0.000019, scenario: 0, slope: -4.160854336385453, fluctuations: 0.06\n",
      "step: 19170 loss: 92.972556 time elapsed: 23.5724 learning rate: 0.000019, scenario: 0, slope: -0.8396432430252372, fluctuations: 0.03\n",
      "step: 19180 loss: 92.771884 time elapsed: 23.5845 learning rate: 0.000019, scenario: 0, slope: -0.20133116811005408, fluctuations: 0.02\n",
      "step: 19190 loss: 92.526179 time elapsed: 23.5965 learning rate: 0.000041, scenario: 1, slope: -0.07334070107928806, fluctuations: 0.0\n",
      "step: 19200 loss: 91.935912 time elapsed: 23.6083 learning rate: 0.000097, scenario: 1, slope: -0.029160723092574992, fluctuations: 0.0\n",
      "step: 19210 loss: 90.639401 time elapsed: 23.6228 learning rate: 0.000251, scenario: 1, slope: -0.030412462478959634, fluctuations: 0.0\n",
      "step: 19220 loss: 87.839667 time elapsed: 23.6353 learning rate: 0.000652, scenario: 1, slope: -0.04245113279020745, fluctuations: 0.0\n",
      "step: 19230 loss: 82.886610 time elapsed: 23.6473 learning rate: 0.001692, scenario: 1, slope: -0.07227092937699524, fluctuations: 0.0\n",
      "step: 19240 loss: 77.946312 time elapsed: 23.6592 learning rate: 0.002477, scenario: 0, slope: -0.1235763125218549, fluctuations: 0.0\n",
      "step: 19250 loss: 75.422828 time elapsed: 23.6709 learning rate: 0.002477, scenario: 0, slope: -0.17799223666861982, fluctuations: 0.0\n",
      "step: 19260 loss: 73.810425 time elapsed: 23.6828 learning rate: 0.002477, scenario: 0, slope: -0.2238029775840525, fluctuations: 0.0\n",
      "step: 19270 loss: 72.709606 time elapsed: 23.6945 learning rate: 0.002477, scenario: 0, slope: -0.25535234833697773, fluctuations: 0.0\n",
      "step: 19280 loss: 71.866818 time elapsed: 23.7063 learning rate: 0.002477, scenario: 0, slope: -0.2694290590884718, fluctuations: 0.0\n",
      "step: 19290 loss: 71.146967 time elapsed: 23.7178 learning rate: 0.002477, scenario: 0, slope: -0.2642659321262008, fluctuations: 0.0\n",
      "step: 19300 loss: 70.496208 time elapsed: 23.7294 learning rate: 0.002477, scenario: 0, slope: -0.2432050613132655, fluctuations: 0.0\n",
      "step: 19310 loss: 69.889123 time elapsed: 23.7415 learning rate: 0.002477, scenario: 0, slope: -0.19941604746886088, fluctuations: 0.0\n",
      "step: 19320 loss: 69.313172 time elapsed: 23.7532 learning rate: 0.002477, scenario: 0, slope: -0.1500158196543963, fluctuations: 0.0\n",
      "step: 19330 loss: 68.760880 time elapsed: 23.7647 learning rate: 0.002477, scenario: 0, slope: -0.10596586360188943, fluctuations: 0.0\n",
      "step: 19340 loss: 68.227105 time elapsed: 23.7769 learning rate: 0.002477, scenario: 0, slope: -0.08165529763883483, fluctuations: 0.0\n",
      "step: 19350 loss: 67.620697 time elapsed: 23.7889 learning rate: 0.004827, scenario: 1, slope: -0.06931458456802397, fluctuations: 0.0\n",
      "step: 19360 loss: 66.220984 time elapsed: 23.8006 learning rate: 0.012519, scenario: 1, slope: -0.06498187343895138, fluctuations: 0.0\n",
      "step: 19370 loss: 192432.578256 time elapsed: 23.8121 learning rate: 0.018863, scenario: -1, slope: 176.5215056353127, fluctuations: 0.0\n",
      "step: 19380 loss: 29594.551940 time elapsed: 23.8253 learning rate: 0.006577, scenario: -1, slope: 247.05816982465691, fluctuations: 0.03\n",
      "step: 19390 loss: 8176.460795 time elapsed: 23.8378 learning rate: 0.002293, scenario: -1, slope: 262.210283988279, fluctuations: 0.05\n",
      "step: 19400 loss: 5494.009417 time elapsed: 23.8496 learning rate: 0.000888, scenario: -1, slope: 223.66091800161618, fluctuations: 0.05\n",
      "step: 19410 loss: 4822.740594 time elapsed: 23.8629 learning rate: 0.000310, scenario: -1, slope: 163.15765472707562, fluctuations: 0.05\n",
      "step: 19420 loss: 4586.797291 time elapsed: 23.8750 learning rate: 0.000108, scenario: -1, slope: 103.44551868706142, fluctuations: 0.05\n",
      "step: 19430 loss: 4520.392590 time elapsed: 23.8869 learning rate: 0.000038, scenario: -1, slope: 39.25162671172204, fluctuations: 0.05\n",
      "step: 19440 loss: 4499.030139 time elapsed: 23.8990 learning rate: 0.000022, scenario: 0, slope: -31.230765835672184, fluctuations: 0.05\n",
      "step: 19450 loss: 4483.691169 time elapsed: 23.9111 learning rate: 0.000022, scenario: 0, slope: -110.82780223458359, fluctuations: 0.05\n",
      "step: 19460 loss: 4469.300860 time elapsed: 23.9232 learning rate: 0.000022, scenario: 0, slope: -203.45273077153516, fluctuations: 0.05\n",
      "step: 19470 loss: 4455.648477 time elapsed: 23.9354 learning rate: 0.000022, scenario: 0, slope: -221.43829477325508, fluctuations: 0.04\n",
      "step: 19480 loss: 4442.572366 time elapsed: 23.9475 learning rate: 0.000022, scenario: 0, slope: -65.25446682667248, fluctuations: 0.01\n",
      "step: 19490 loss: 4429.957439 time elapsed: 23.9594 learning rate: 0.000022, scenario: 0, slope: -15.417085567515997, fluctuations: 0.0\n",
      "step: 19500 loss: 4417.723421 time elapsed: 23.9708 learning rate: 0.000024, scenario: 1, slope: -5.391053084928824, fluctuations: 0.0\n",
      "step: 19510 loss: 4398.729781 time elapsed: 23.9828 learning rate: 0.000063, scenario: 1, slope: -2.245155300707046, fluctuations: 0.0\n",
      "step: 19520 loss: 4352.386133 time elapsed: 23.9947 learning rate: 0.000165, scenario: 1, slope: -1.6217731730464964, fluctuations: 0.0\n",
      "step: 19530 loss: 4245.296964 time elapsed: 24.0070 learning rate: 0.000427, scenario: 1, slope: -1.900465206414903, fluctuations: 0.0\n",
      "step: 19540 loss: 4023.162106 time elapsed: 24.0186 learning rate: 0.001107, scenario: 1, slope: -3.0379697916041786, fluctuations: 0.0\n",
      "step: 19550 loss: 3638.143680 time elapsed: 24.0329 learning rate: 0.001961, scenario: 0, slope: -5.53848381932422, fluctuations: 0.0\n",
      "step: 19560 loss: 3258.069696 time elapsed: 24.0462 learning rate: 0.001961, scenario: 0, slope: -9.484649382861804, fluctuations: 0.0\n",
      "step: 19570 loss: 2896.052076 time elapsed: 24.0591 learning rate: 0.001961, scenario: 0, slope: -14.261597532989393, fluctuations: 0.0\n",
      "step: 19580 loss: 2614.358783 time elapsed: 24.0716 learning rate: 0.001961, scenario: 0, slope: -19.14946887510833, fluctuations: 0.0\n",
      "step: 19590 loss: 2389.672831 time elapsed: 24.0851 learning rate: 0.001961, scenario: 0, slope: -23.411617723315878, fluctuations: 0.0\n",
      "step: 19600 loss: 2203.062007 time elapsed: 24.0979 learning rate: 0.001961, scenario: 0, slope: -26.2609054737266, fluctuations: 0.0\n",
      "step: 19610 loss: 2041.160712 time elapsed: 24.1113 learning rate: 0.001961, scenario: 0, slope: -28.055594527774904, fluctuations: 0.0\n",
      "step: 19620 loss: 1896.285480 time elapsed: 24.1235 learning rate: 0.001961, scenario: 0, slope: -27.878140246129732, fluctuations: 0.0\n",
      "step: 19630 loss: 1766.647501 time elapsed: 24.1363 learning rate: 0.001961, scenario: 0, slope: -26.037573794829985, fluctuations: 0.0\n",
      "step: 19640 loss: 1649.392662 time elapsed: 24.1491 learning rate: 0.001961, scenario: 0, slope: -22.958932866433983, fluctuations: 0.0\n",
      "step: 19650 loss: 1540.303696 time elapsed: 24.1617 learning rate: 0.001961, scenario: 0, slope: -19.606038768367977, fluctuations: 0.0\n",
      "step: 19660 loss: 1439.410281 time elapsed: 24.1740 learning rate: 0.001961, scenario: 0, slope: -16.76277359610039, fluctuations: 0.0\n",
      "step: 19670 loss: 1346.044864 time elapsed: 24.1864 learning rate: 0.001961, scenario: 0, slope: -14.559268295122095, fluctuations: 0.0\n",
      "step: 19680 loss: 1260.352920 time elapsed: 24.1985 learning rate: 0.001961, scenario: 0, slope: -12.944934003700693, fluctuations: 0.0\n",
      "step: 19690 loss: 1181.952973 time elapsed: 24.2109 learning rate: 0.001961, scenario: 0, slope: -11.700470613486594, fluctuations: 0.0\n",
      "step: 19700 loss: 1111.479285 time elapsed: 24.2223 learning rate: 0.001961, scenario: 0, slope: -10.766345323057273, fluctuations: 0.0\n",
      "step: 19710 loss: 1048.508259 time elapsed: 24.2367 learning rate: 0.001961, scenario: 0, slope: -9.764537985737773, fluctuations: 0.0\n",
      "step: 19720 loss: 991.910317 time elapsed: 24.2506 learning rate: 0.001961, scenario: 0, slope: -8.94100820050803, fluctuations: 0.0\n",
      "step: 19730 loss: 940.544910 time elapsed: 24.2631 learning rate: 0.001961, scenario: 0, slope: -8.182937658771499, fluctuations: 0.0\n",
      "step: 19740 loss: 894.590479 time elapsed: 24.2753 learning rate: 0.001961, scenario: 0, slope: -7.459741278304812, fluctuations: 0.0\n",
      "step: 19750 loss: 851.277088 time elapsed: 24.2875 learning rate: 0.001961, scenario: 0, slope: -6.775832752755411, fluctuations: 0.0\n",
      "step: 19760 loss: 812.407236 time elapsed: 24.2995 learning rate: 0.001961, scenario: 0, slope: -6.137372613303602, fluctuations: 0.0\n",
      "step: 19770 loss: 777.208452 time elapsed: 24.3117 learning rate: 0.001961, scenario: 0, slope: -5.552496041489686, fluctuations: 0.0\n",
      "step: 19780 loss: 745.419886 time elapsed: 24.3238 learning rate: 0.001961, scenario: 0, slope: -5.022190011739873, fluctuations: 0.0\n",
      "step: 19790 loss: 716.581088 time elapsed: 24.3357 learning rate: 0.001961, scenario: 0, slope: -4.548008973526188, fluctuations: 0.0\n",
      "step: 19800 loss: 690.286392 time elapsed: 24.3474 learning rate: 0.001961, scenario: 0, slope: -4.166395318566012, fluctuations: 0.0\n",
      "step: 19810 loss: 666.193841 time elapsed: 24.3596 learning rate: 0.001961, scenario: 0, slope: -3.749249331860459, fluctuations: 0.0\n",
      "step: 19820 loss: 643.983058 time elapsed: 24.3715 learning rate: 0.001961, scenario: 0, slope: -3.409423575248315, fluctuations: 0.0\n",
      "step: 19830 loss: 623.386020 time elapsed: 24.3833 learning rate: 0.001961, scenario: 0, slope: -3.1033575503301494, fluctuations: 0.0\n",
      "step: 19840 loss: 604.186902 time elapsed: 24.3951 learning rate: 0.001961, scenario: 0, slope: -2.828696058238967, fluctuations: 0.0\n",
      "step: 19850 loss: 586.392784 time elapsed: 24.4065 learning rate: 0.001961, scenario: 0, slope: -2.584592328520644, fluctuations: 0.0\n",
      "step: 19860 loss: 570.069238 time elapsed: 24.4185 learning rate: 0.001961, scenario: 0, slope: -2.3688264858248056, fluctuations: 0.0\n",
      "step: 19870 loss: 553.684903 time elapsed: 24.4306 learning rate: 0.001961, scenario: 0, slope: -2.18373849175697, fluctuations: 0.0\n",
      "step: 19880 loss: 538.731479 time elapsed: 24.4447 learning rate: 0.001961, scenario: 0, slope: -2.02351731175438, fluctuations: 0.0\n",
      "step: 19890 loss: 524.562140 time elapsed: 24.4570 learning rate: 0.001961, scenario: 0, slope: -1.8840411812621682, fluctuations: 0.0\n",
      "step: 19900 loss: 511.093097 time elapsed: 24.4685 learning rate: 0.001961, scenario: 0, slope: -1.7734224328629034, fluctuations: 0.0\n",
      "step: 19910 loss: 498.293984 time elapsed: 24.4816 learning rate: 0.001961, scenario: 0, slope: -1.6541537844070546, fluctuations: 0.0\n",
      "step: 19920 loss: 486.093224 time elapsed: 24.4956 learning rate: 0.001961, scenario: 0, slope: -1.5584419005537529, fluctuations: 0.0\n",
      "step: 19930 loss: 474.441667 time elapsed: 24.5081 learning rate: 0.001961, scenario: 0, slope: -1.4729124599842278, fluctuations: 0.0\n",
      "step: 19940 loss: 463.298152 time elapsed: 24.5201 learning rate: 0.001961, scenario: 0, slope: -1.3961673230845155, fluctuations: 0.0\n",
      "step: 19950 loss: 452.628624 time elapsed: 24.5330 learning rate: 0.001961, scenario: 0, slope: -1.326789377451407, fluctuations: 0.0\n",
      "step: 19960 loss: 442.401003 time elapsed: 24.5458 learning rate: 0.001961, scenario: 0, slope: -1.2599989191048404, fluctuations: 0.0\n",
      "step: 19970 loss: 432.587496 time elapsed: 24.5583 learning rate: 0.001961, scenario: 0, slope: -1.2005977810068258, fluctuations: 0.0\n",
      "step: 19980 loss: 423.163132 time elapsed: 24.5708 learning rate: 0.001961, scenario: 0, slope: -1.1463199423850434, fluctuations: 0.0\n",
      "step: 19990 loss: 414.111891 time elapsed: 24.5833 learning rate: 0.001961, scenario: 0, slope: -1.0961766871954015, fluctuations: 0.0\n",
      "step: 20000 loss: 406.446132 time elapsed: 24.5958 learning rate: 0.001961, scenario: 0, slope: -1.052631089413756, fluctuations: 0.0\n",
      "step: 20010 loss: 397.293185 time elapsed: 24.6087 learning rate: 0.001961, scenario: 0, slope: -1.003042816120506, fluctuations: 0.0\n",
      "step: 20020 loss: 389.057332 time elapsed: 24.6210 learning rate: 0.001961, scenario: 0, slope: -0.9625660425384956, fluctuations: 0.0\n",
      "step: 20030 loss: 381.189350 time elapsed: 24.6329 learning rate: 0.001961, scenario: 0, slope: -0.92505119578582, fluctuations: 0.0\n",
      "step: 20040 loss: 373.638404 time elapsed: 24.6470 learning rate: 0.001961, scenario: 0, slope: -0.889989419373842, fluctuations: 0.0\n",
      "step: 20050 loss: 366.371164 time elapsed: 24.6599 learning rate: 0.001961, scenario: 0, slope: -0.8572158591496379, fluctuations: 0.0\n",
      "step: 20060 loss: 359.371691 time elapsed: 24.6724 learning rate: 0.001961, scenario: 0, slope: -0.8263399048584296, fluctuations: 0.0\n",
      "step: 20070 loss: 352.615390 time elapsed: 24.6841 learning rate: 0.001961, scenario: 0, slope: -0.7971683344558809, fluctuations: 0.0\n",
      "step: 20080 loss: 346.083908 time elapsed: 24.6960 learning rate: 0.001961, scenario: 0, slope: -0.7695684959033986, fluctuations: 0.0\n",
      "step: 20090 loss: 339.766086 time elapsed: 24.7081 learning rate: 0.001961, scenario: 0, slope: -0.7434217247186036, fluctuations: 0.0\n",
      "step: 20100 loss: 333.649862 time elapsed: 24.7200 learning rate: 0.001961, scenario: 0, slope: -0.7194403274548284, fluctuations: 0.0\n",
      "step: 20110 loss: 327.724807 time elapsed: 24.7326 learning rate: 0.001961, scenario: 0, slope: -0.6905231670493355, fluctuations: 0.0\n",
      "step: 20120 loss: 321.980741 time elapsed: 24.7444 learning rate: 0.001961, scenario: 0, slope: -0.666735456563611, fluctuations: 0.0\n",
      "step: 20130 loss: 316.407630 time elapsed: 24.7559 learning rate: 0.001961, scenario: 0, slope: -0.6444188156049498, fluctuations: 0.0\n",
      "step: 20140 loss: 310.995238 time elapsed: 24.7675 learning rate: 0.001961, scenario: 0, slope: -0.6233412825846755, fluctuations: 0.0\n",
      "step: 20150 loss: 305.732413 time elapsed: 24.7793 learning rate: 0.001961, scenario: 0, slope: -0.6035858915218328, fluctuations: 0.0\n",
      "step: 20160 loss: 300.612540 time elapsed: 24.7908 learning rate: 0.001961, scenario: 0, slope: -0.5849623975184326, fluctuations: 0.0\n",
      "step: 20170 loss: 296.483029 time elapsed: 24.8024 learning rate: 0.001961, scenario: 0, slope: -0.5653381711542667, fluctuations: 0.0\n",
      "step: 20180 loss: 290.969994 time elapsed: 24.8142 learning rate: 0.001961, scenario: 0, slope: -0.5481055668624236, fluctuations: 0.0\n",
      "step: 20190 loss: 286.100030 time elapsed: 24.8262 learning rate: 0.001961, scenario: 0, slope: -0.532705013035702, fluctuations: 0.0\n",
      "step: 20200 loss: 281.417831 time elapsed: 24.8375 learning rate: 0.001961, scenario: 0, slope: -0.5199640691343023, fluctuations: 0.0\n",
      "step: 20210 loss: 276.856224 time elapsed: 24.8517 learning rate: 0.001961, scenario: 0, slope: -0.5055380053569296, fluctuations: 0.0\n",
      "step: 20220 loss: 272.397964 time elapsed: 24.8640 learning rate: 0.001961, scenario: 0, slope: -0.4934367686605898, fluctuations: 0.0\n",
      "step: 20230 loss: 268.026778 time elapsed: 24.8758 learning rate: 0.001961, scenario: 0, slope: -0.48224498510984903, fluctuations: 0.0\n",
      "step: 20240 loss: 263.407838 time elapsed: 24.8879 learning rate: 0.001961, scenario: 0, slope: -0.47243858627181984, fluctuations: 0.0\n",
      "step: 20250 loss: 257.051252 time elapsed: 24.9001 learning rate: 0.001961, scenario: 0, slope: -0.4747914309195259, fluctuations: 0.02\n",
      "step: 20260 loss: 250.491941 time elapsed: 24.9114 learning rate: 0.001961, scenario: 0, slope: -0.4845697888378627, fluctuations: 0.06\n",
      "step: 20270 loss: 246.065503 time elapsed: 24.9233 learning rate: 0.001961, scenario: 0, slope: -0.4953421845547964, fluctuations: 0.06\n",
      "step: 20280 loss: 241.708136 time elapsed: 24.9356 learning rate: 0.001961, scenario: 0, slope: -0.5018951661459891, fluctuations: 0.06\n",
      "step: 20290 loss: 237.623272 time elapsed: 24.9517 learning rate: 0.001961, scenario: 0, slope: -0.5036832849422468, fluctuations: 0.06\n",
      "step: 20300 loss: 233.556869 time elapsed: 24.9674 learning rate: 0.001961, scenario: 0, slope: -0.5009953108550325, fluctuations: 0.06\n",
      "step: 20310 loss: 229.570885 time elapsed: 24.9824 learning rate: 0.001961, scenario: 0, slope: -0.4917781119717816, fluctuations: 0.06\n",
      "step: 20320 loss: 225.630227 time elapsed: 24.9963 learning rate: 0.001961, scenario: 0, slope: -0.4770738815040856, fluctuations: 0.06\n",
      "step: 20330 loss: 221.742578 time elapsed: 25.0101 learning rate: 0.001961, scenario: 0, slope: -0.45504220767437525, fluctuations: 0.06\n",
      "step: 20340 loss: 217.924664 time elapsed: 25.0231 learning rate: 0.001961, scenario: 0, slope: -0.42408463811915426, fluctuations: 0.06\n",
      "step: 20350 loss: 214.194963 time elapsed: 25.0392 learning rate: 0.001961, scenario: 0, slope: -0.40649947540330467, fluctuations: 0.03\n",
      "step: 20360 loss: 210.577210 time elapsed: 25.0541 learning rate: 0.001961, scenario: 0, slope: -0.3977439685294553, fluctuations: 0.0\n",
      "step: 20370 loss: 207.093780 time elapsed: 25.0674 learning rate: 0.001961, scenario: 0, slope: -0.38937094695054675, fluctuations: 0.0\n",
      "step: 20380 loss: 203.762954 time elapsed: 25.0797 learning rate: 0.001961, scenario: 0, slope: -0.3813579121953879, fluctuations: 0.0\n",
      "step: 20390 loss: 200.596134 time elapsed: 25.0918 learning rate: 0.001961, scenario: 0, slope: -0.3724007374651482, fluctuations: 0.0\n",
      "step: 20400 loss: 197.596914 time elapsed: 25.1036 learning rate: 0.001961, scenario: 0, slope: -0.3631513092785115, fluctuations: 0.0\n",
      "step: 20410 loss: 194.761664 time elapsed: 25.1159 learning rate: 0.001961, scenario: 0, slope: -0.35015016131435805, fluctuations: 0.0\n",
      "step: 20420 loss: 192.081236 time elapsed: 25.1274 learning rate: 0.001961, scenario: 0, slope: -0.3368381386268784, fluctuations: 0.0\n",
      "step: 20430 loss: 189.543094 time elapsed: 25.1387 learning rate: 0.001961, scenario: 0, slope: -0.32243417175263067, fluctuations: 0.0\n",
      "step: 20440 loss: 187.133255 time elapsed: 25.1503 learning rate: 0.001961, scenario: 0, slope: -0.3073810164678265, fluctuations: 0.0\n",
      "step: 20450 loss: 184.837747 time elapsed: 25.1619 learning rate: 0.001961, scenario: 0, slope: -0.2921695893537702, fluctuations: 0.0\n",
      "step: 20460 loss: 182.643513 time elapsed: 25.1735 learning rate: 0.001961, scenario: 0, slope: -0.27726721581926256, fluctuations: 0.0\n",
      "step: 20470 loss: 180.538842 time elapsed: 25.1853 learning rate: 0.001961, scenario: 0, slope: -0.2630621640104792, fluctuations: 0.0\n",
      "step: 20480 loss: 178.513471 time elapsed: 25.1971 learning rate: 0.001961, scenario: 0, slope: -0.24983050770471835, fluctuations: 0.0\n",
      "step: 20490 loss: 176.558489 time elapsed: 25.2092 learning rate: 0.001961, scenario: 0, slope: -0.23772890353414416, fluctuations: 0.0\n",
      "step: 20500 loss: 174.666156 time elapsed: 25.2206 learning rate: 0.001961, scenario: 0, slope: -0.22784760660790124, fluctuations: 0.0\n",
      "step: 20510 loss: 172.829726 time elapsed: 25.2329 learning rate: 0.001961, scenario: 0, slope: -0.2170404451776583, fluctuations: 0.0\n",
      "step: 20520 loss: 171.043648 time elapsed: 25.2445 learning rate: 0.001961, scenario: 0, slope: -0.20834581599689816, fluctuations: 0.0\n",
      "step: 20530 loss: 170.012245 time elapsed: 25.2563 learning rate: 0.001961, scenario: 0, slope: -0.19977986317009713, fluctuations: 0.0\n",
      "step: 20540 loss: 167.613770 time elapsed: 25.2695 learning rate: 0.001961, scenario: 0, slope: -0.19146474253409634, fluctuations: 0.02\n",
      "step: 20550 loss: 166.056633 time elapsed: 25.2825 learning rate: 0.001961, scenario: 0, slope: -0.18501417174940904, fluctuations: 0.03\n",
      "step: 20560 loss: 320.752963 time elapsed: 25.2945 learning rate: 0.004013, scenario: -1, slope: 0.014386977071215347, fluctuations: 0.03\n",
      "step: 20570 loss: 197.697071 time elapsed: 25.3069 learning rate: 0.003083, scenario: -1, slope: 0.042919766036561606, fluctuations: 0.07\n",
      "step: 20580 loss: 160.071535 time elapsed: 25.3190 learning rate: 0.001075, scenario: -1, slope: 0.01072933669412418, fluctuations: 0.11\n",
      "step: 20590 loss: 160.676175 time elapsed: 25.3310 learning rate: 0.001785, scenario: 1, slope: -0.040877667178615455, fluctuations: 0.13\n",
      "step: 20600 loss: 157.636429 time elapsed: 25.3426 learning rate: 0.004209, scenario: 1, slope: -0.07842700369359924, fluctuations: 0.17\n",
      "step: 20610 loss: 152.680120 time elapsed: 25.3549 learning rate: 0.003474, scenario: -1, slope: -0.14862038208089198, fluctuations: 0.21\n",
      "step: 20620 loss: 150.296548 time elapsed: 25.3668 learning rate: 0.002814, scenario: 0, slope: -0.23225239058410455, fluctuations: 0.22\n",
      "step: 20630 loss: 148.152644 time elapsed: 25.3787 learning rate: 0.002814, scenario: 0, slope: -0.30550563155363614, fluctuations: 0.21\n",
      "step: 20640 loss: 146.138843 time elapsed: 25.3908 learning rate: 0.002814, scenario: 0, slope: -0.36207113443959726, fluctuations: 0.19\n",
      "step: 20650 loss: 144.166107 time elapsed: 25.4025 learning rate: 0.002814, scenario: 0, slope: -0.43911744713278034, fluctuations: 0.19\n",
      "step: 20660 loss: 142.200608 time elapsed: 25.4143 learning rate: 0.002814, scenario: 0, slope: -0.3875902731769778, fluctuations: 0.18\n",
      "step: 20670 loss: 140.231616 time elapsed: 25.4264 learning rate: 0.002814, scenario: 0, slope: -0.2565954220727972, fluctuations: 0.14\n",
      "step: 20680 loss: 138.256428 time elapsed: 25.4388 learning rate: 0.002814, scenario: 0, slope: -0.22883735055550034, fluctuations: 0.11\n",
      "step: 20690 loss: 136.271692 time elapsed: 25.4505 learning rate: 0.002814, scenario: 0, slope: -0.2158891376803396, fluctuations: 0.08\n",
      "step: 20700 loss: 134.275501 time elapsed: 25.4643 learning rate: 0.002814, scenario: 0, slope: -0.2068630323415828, fluctuations: 0.04\n",
      "step: 20710 loss: 132.269132 time elapsed: 25.4790 learning rate: 0.002814, scenario: 0, slope: -0.19916238016940563, fluctuations: 0.01\n",
      "step: 20720 loss: 130.256723 time elapsed: 25.4926 learning rate: 0.002814, scenario: 0, slope: -0.1984666479171624, fluctuations: 0.0\n",
      "step: 20730 loss: 128.243854 time elapsed: 25.5056 learning rate: 0.002814, scenario: 0, slope: -0.19860900978094667, fluctuations: 0.0\n",
      "step: 20740 loss: 126.234169 time elapsed: 25.5185 learning rate: 0.002814, scenario: 0, slope: -0.1990962608993186, fluctuations: 0.0\n",
      "step: 20750 loss: 124.221462 time elapsed: 25.5310 learning rate: 0.002814, scenario: 0, slope: -0.19967277221415028, fluctuations: 0.0\n",
      "step: 20760 loss: 122.187462 time elapsed: 25.5435 learning rate: 0.002814, scenario: 0, slope: -0.20027943242994828, fluctuations: 0.0\n",
      "step: 20770 loss: 120.189903 time elapsed: 25.5555 learning rate: 0.002814, scenario: 0, slope: -0.2008141859966743, fluctuations: 0.0\n",
      "step: 20780 loss: 118.237473 time elapsed: 25.5678 learning rate: 0.002814, scenario: 0, slope: -0.20095023505008036, fluctuations: 0.0\n",
      "step: 20790 loss: 116.588158 time elapsed: 25.5798 learning rate: 0.002814, scenario: 0, slope: -0.20035331223889666, fluctuations: 0.0\n",
      "step: 20800 loss: 114.626570 time elapsed: 25.5916 learning rate: 0.002814, scenario: 0, slope: -0.19903565379145285, fluctuations: 0.01\n",
      "step: 20810 loss: 112.826634 time elapsed: 25.6040 learning rate: 0.002814, scenario: 0, slope: -0.1962296946411918, fluctuations: 0.01\n",
      "step: 20820 loss: 111.171464 time elapsed: 25.6160 learning rate: 0.002814, scenario: 0, slope: -0.19263911394188024, fluctuations: 0.01\n",
      "step: 20830 loss: 109.578493 time elapsed: 25.6276 learning rate: 0.002814, scenario: 0, slope: -0.18807700152403672, fluctuations: 0.01\n",
      "step: 20840 loss: 108.026535 time elapsed: 25.6396 learning rate: 0.002814, scenario: 0, slope: -0.18266936333919787, fluctuations: 0.01\n",
      "step: 20850 loss: 106.531854 time elapsed: 25.6516 learning rate: 0.002814, scenario: 0, slope: -0.17662752649863053, fluctuations: 0.01\n",
      "step: 20860 loss: 105.085633 time elapsed: 25.6636 learning rate: 0.002814, scenario: 0, slope: -0.17015969347083276, fluctuations: 0.01\n",
      "step: 20870 loss: 103.680525 time elapsed: 25.6758 learning rate: 0.002814, scenario: 0, slope: -0.1639257749864859, fluctuations: 0.01\n",
      "step: 20880 loss: 102.338292 time elapsed: 25.6884 learning rate: 0.002814, scenario: 0, slope: -0.158000251984535, fluctuations: 0.01\n",
      "step: 20890 loss: 101.092687 time elapsed: 25.7010 learning rate: 0.002814, scenario: 0, slope: -0.15274871455480335, fluctuations: 0.0\n",
      "step: 20900 loss: 99.726965 time elapsed: 25.7128 learning rate: 0.002814, scenario: 0, slope: -0.14793632830701306, fluctuations: 0.0\n",
      "step: 20910 loss: 98.472263 time elapsed: 25.7252 learning rate: 0.002814, scenario: 0, slope: -0.14271826163818171, fluctuations: 0.0\n",
      "step: 20920 loss: 97.276615 time elapsed: 25.7367 learning rate: 0.002814, scenario: 0, slope: -0.13836859444026356, fluctuations: 0.0\n",
      "step: 20930 loss: 96.146015 time elapsed: 25.7482 learning rate: 0.002814, scenario: 0, slope: -0.13435961370383417, fluctuations: 0.0\n",
      "step: 20940 loss: 94.892872 time elapsed: 25.7601 learning rate: 0.002814, scenario: 0, slope: -0.13033881544483691, fluctuations: 0.0\n",
      "step: 20950 loss: 93.769432 time elapsed: 25.7716 learning rate: 0.002814, scenario: 0, slope: -0.1269048543090539, fluctuations: 0.0\n",
      "step: 20960 loss: 92.668810 time elapsed: 25.7843 learning rate: 0.002814, scenario: 0, slope: -0.12361956373872272, fluctuations: 0.0\n",
      "step: 20970 loss: 91.580448 time elapsed: 25.7966 learning rate: 0.002814, scenario: 0, slope: -0.12052504058057707, fluctuations: 0.0\n",
      "step: 20980 loss: 90.777081 time elapsed: 25.8084 learning rate: 0.002814, scenario: 0, slope: -0.11741186540804184, fluctuations: 0.0\n",
      "step: 20990 loss: 89.589130 time elapsed: 25.8206 learning rate: 0.002814, scenario: 0, slope: -0.11433254024580906, fluctuations: 0.01\n",
      "step: 21000 loss: 88.499292 time elapsed: 25.8325 learning rate: 0.002814, scenario: 0, slope: -0.1118974719862774, fluctuations: 0.01\n",
      "step: 21010 loss: 87.487957 time elapsed: 25.8451 learning rate: 0.002814, scenario: 0, slope: -0.10936662909217465, fluctuations: 0.01\n",
      "step: 21020 loss: 86.490652 time elapsed: 25.8567 learning rate: 0.002814, scenario: 0, slope: -0.10728027216559174, fluctuations: 0.01\n",
      "step: 21030 loss: 85.563250 time elapsed: 25.8693 learning rate: 0.002814, scenario: 0, slope: -0.10528099094963611, fluctuations: 0.01\n",
      "step: 21040 loss: 84.614316 time elapsed: 25.8833 learning rate: 0.002814, scenario: 0, slope: -0.10267165623216104, fluctuations: 0.01\n",
      "step: 21050 loss: 83.686587 time elapsed: 25.8982 learning rate: 0.002814, scenario: 0, slope: -0.10070741420905767, fluctuations: 0.01\n",
      "step: 21060 loss: 82.827963 time elapsed: 25.9114 learning rate: 0.002814, scenario: 0, slope: -0.09898707335927913, fluctuations: 0.01\n",
      "step: 21070 loss: 81.866579 time elapsed: 25.9236 learning rate: 0.002814, scenario: 0, slope: -0.09727799763375218, fluctuations: 0.01\n",
      "step: 21080 loss: 81.086577 time elapsed: 25.9371 learning rate: 0.002814, scenario: 0, slope: -0.09515357755988983, fluctuations: 0.01\n",
      "step: 21090 loss: 80.177411 time elapsed: 25.9504 learning rate: 0.002814, scenario: 0, slope: -0.0930854568353127, fluctuations: 0.0\n",
      "step: 21100 loss: 79.309062 time elapsed: 25.9636 learning rate: 0.002814, scenario: 0, slope: -0.09167244265235529, fluctuations: 0.0\n",
      "step: 21110 loss: 78.449066 time elapsed: 25.9771 learning rate: 0.002814, scenario: 0, slope: -0.08995248142580921, fluctuations: 0.0\n",
      "step: 21120 loss: 77.646239 time elapsed: 25.9892 learning rate: 0.002814, scenario: 0, slope: -0.08877255219586627, fluctuations: 0.0\n",
      "step: 21130 loss: 76.797012 time elapsed: 26.0015 learning rate: 0.002814, scenario: 0, slope: -0.0869952293635965, fluctuations: 0.0\n",
      "step: 21140 loss: 76.055150 time elapsed: 26.0135 learning rate: 0.002814, scenario: 0, slope: -0.08540142621553776, fluctuations: 0.0\n",
      "step: 21150 loss: 75.228560 time elapsed: 26.0254 learning rate: 0.002814, scenario: 0, slope: -0.08423204858099206, fluctuations: 0.0\n",
      "step: 21160 loss: 74.446064 time elapsed: 26.0373 learning rate: 0.002814, scenario: 0, slope: -0.0832075871042166, fluctuations: 0.0\n",
      "step: 21170 loss: 73.682491 time elapsed: 26.0491 learning rate: 0.002814, scenario: 0, slope: -0.08217690219859386, fluctuations: 0.0\n",
      "step: 21180 loss: 73.612060 time elapsed: 26.0608 learning rate: 0.004120, scenario: 1, slope: -0.08018388646651903, fluctuations: 0.0\n",
      "step: 21190 loss: 1744.379358 time elapsed: 26.0728 learning rate: 0.006207, scenario: -1, slope: 1.377949312938316, fluctuations: 0.01\n",
      "step: 21200 loss: 228.108007 time elapsed: 26.0844 learning rate: 0.002405, scenario: -1, slope: 1.8906172116616458, fluctuations: 0.05\n",
      "step: 21210 loss: 108.055698 time elapsed: 26.0982 learning rate: 0.000838, scenario: -1, slope: 1.841178054704396, fluctuations: 0.08\n",
      "step: 21220 loss: 82.561154 time elapsed: 26.1115 learning rate: 0.000292, scenario: -1, slope: 1.3217926398977922, fluctuations: 0.09\n",
      "step: 21230 loss: 77.463950 time elapsed: 26.1239 learning rate: 0.000102, scenario: -1, slope: 0.7854313648721194, fluctuations: 0.1\n",
      "step: 21240 loss: 77.018153 time elapsed: 26.1358 learning rate: 0.000036, scenario: -1, slope: 0.2322907673271629, fluctuations: 0.1\n",
      "step: 21250 loss: 76.586817 time elapsed: 26.1477 learning rate: 0.000025, scenario: 0, slope: -0.2927263510040914, fluctuations: 0.1\n",
      "step: 21260 loss: 76.396306 time elapsed: 26.1598 learning rate: 0.000025, scenario: 0, slope: -0.83307084666843, fluctuations: 0.1\n",
      "step: 21270 loss: 76.310613 time elapsed: 26.1716 learning rate: 0.000025, scenario: 0, slope: -1.4354105992932293, fluctuations: 0.1\n",
      "step: 21280 loss: 76.234415 time elapsed: 26.1837 learning rate: 0.000025, scenario: 0, slope: -2.1632777481561662, fluctuations: 0.1\n",
      "step: 21290 loss: 76.158232 time elapsed: 26.1953 learning rate: 0.000025, scenario: 0, slope: -2.2194188205248846, fluctuations: 0.08\n",
      "step: 21300 loss: 76.086580 time elapsed: 26.2070 learning rate: 0.000025, scenario: 0, slope: -0.3920960756894119, fluctuations: 0.05\n",
      "step: 21310 loss: 76.017465 time elapsed: 26.2195 learning rate: 0.000033, scenario: 1, slope: -0.08354202489896054, fluctuations: 0.02\n",
      "step: 21320 loss: 75.889960 time elapsed: 26.2315 learning rate: 0.000085, scenario: 1, slope: -0.02854327412626899, fluctuations: 0.01\n",
      "step: 21330 loss: 75.580565 time elapsed: 26.2429 learning rate: 0.000220, scenario: 1, slope: -0.013471357785467515, fluctuations: 0.0\n",
      "step: 21340 loss: 74.892614 time elapsed: 26.2545 learning rate: 0.000572, scenario: 1, slope: -0.013006587134763126, fluctuations: 0.0\n",
      "step: 21350 loss: 73.603972 time elapsed: 26.2666 learning rate: 0.001483, scenario: 1, slope: -0.01901464672754428, fluctuations: 0.0\n",
      "step: 21360 loss: 71.737489 time elapsed: 26.2785 learning rate: 0.003846, scenario: 1, slope: -0.03273382715577027, fluctuations: 0.0\n",
      "step: 21370 loss: 69.343810 time elapsed: 26.2907 learning rate: 0.009976, scenario: 1, slope: -0.05419330123796916, fluctuations: 0.0\n",
      "step: 21380 loss: 216355.538014 time elapsed: 26.3051 learning rate: 0.015031, scenario: -1, slope: 152.03976927690132, fluctuations: 0.0\n",
      "step: 21390 loss: 20672.635512 time elapsed: 26.3188 learning rate: 0.005241, scenario: -1, slope: 189.612161174521, fluctuations: 0.03\n",
      "step: 21400 loss: 10497.742978 time elapsed: 26.3316 learning rate: 0.002030, scenario: -1, slope: 218.2422140599386, fluctuations: 0.05\n",
      "step: 21410 loss: 6438.751184 time elapsed: 26.3455 learning rate: 0.000708, scenario: -1, slope: 193.68783164225871, fluctuations: 0.05\n",
      "step: 21420 loss: 5653.181060 time elapsed: 26.3579 learning rate: 0.000247, scenario: -1, slope: 156.58898535928466, fluctuations: 0.05\n",
      "step: 21430 loss: 5236.942817 time elapsed: 26.3708 learning rate: 0.000086, scenario: -1, slope: 111.47044749258825, fluctuations: 0.05\n",
      "step: 21440 loss: 5129.293966 time elapsed: 26.3842 learning rate: 0.000030, scenario: -1, slope: 60.3560198971804, fluctuations: 0.05\n",
      "step: 21450 loss: 5097.404602 time elapsed: 26.3975 learning rate: 0.000010, scenario: -1, slope: 2.42469925280766, fluctuations: 0.05\n",
      "step: 21460 loss: 5083.148985 time elapsed: 26.4092 learning rate: 0.000010, scenario: 0, slope: -64.54231368757972, fluctuations: 0.05\n",
      "step: 21470 loss: 5069.737468 time elapsed: 26.4216 learning rate: 0.000010, scenario: 0, slope: -143.90347885270563, fluctuations: 0.05\n",
      "step: 21480 loss: 5056.818253 time elapsed: 26.4334 learning rate: 0.000010, scenario: 0, slope: -197.09108875679948, fluctuations: 0.04\n",
      "step: 21490 loss: 5044.288503 time elapsed: 26.4459 learning rate: 0.000010, scenario: 0, slope: -62.781219368272936, fluctuations: 0.01\n",
      "step: 21500 loss: 5032.084759 time elapsed: 26.4580 learning rate: 0.000010, scenario: 0, slope: -20.858723998215375, fluctuations: 0.0\n",
      "step: 21510 loss: 5020.163046 time elapsed: 26.4705 learning rate: 0.000010, scenario: 0, slope: -7.545777246967777, fluctuations: 0.0\n",
      "step: 21520 loss: 5005.609072 time elapsed: 26.4822 learning rate: 0.000022, scenario: 1, slope: -2.919865436533333, fluctuations: 0.0\n",
      "step: 21530 loss: 4970.343355 time elapsed: 26.4938 learning rate: 0.000058, scenario: 1, slope: -1.6562121847644395, fluctuations: 0.0\n",
      "step: 21540 loss: 4882.588906 time elapsed: 26.5071 learning rate: 0.000151, scenario: 1, slope: -1.6828677063894735, fluctuations: 0.0\n",
      "step: 21550 loss: 4669.480844 time elapsed: 26.5207 learning rate: 0.000391, scenario: 1, slope: -2.6247344558822903, fluctuations: 0.0\n",
      "step: 21560 loss: 4199.929141 time elapsed: 26.5336 learning rate: 0.000923, scenario: 0, slope: -5.185324893724809, fluctuations: 0.0\n",
      "step: 21570 loss: 3662.938080 time elapsed: 26.5457 learning rate: 0.000923, scenario: 0, slope: -10.01348055904538, fluctuations: 0.0\n",
      "step: 21580 loss: 3288.869044 time elapsed: 26.5577 learning rate: 0.000923, scenario: 0, slope: -15.88467534693039, fluctuations: 0.0\n",
      "step: 21590 loss: 3009.858375 time elapsed: 26.5697 learning rate: 0.000923, scenario: 0, slope: -21.59419990538628, fluctuations: 0.0\n",
      "step: 21600 loss: 2786.198763 time elapsed: 26.5819 learning rate: 0.000923, scenario: 0, slope: -25.983463564062017, fluctuations: 0.0\n",
      "step: 21610 loss: 2597.954516 time elapsed: 26.5945 learning rate: 0.000923, scenario: 0, slope: -29.809365522966996, fluctuations: 0.0\n",
      "step: 21620 loss: 2431.784800 time elapsed: 26.6065 learning rate: 0.000923, scenario: 0, slope: -31.422114546550635, fluctuations: 0.0\n",
      "step: 21630 loss: 2281.743087 time elapsed: 26.6184 learning rate: 0.000923, scenario: 0, slope: -31.02986657376436, fluctuations: 0.0\n",
      "step: 21640 loss: 2150.279558 time elapsed: 26.6303 learning rate: 0.000923, scenario: 0, slope: -28.61595766983115, fluctuations: 0.0\n",
      "step: 21650 loss: 2035.059423 time elapsed: 26.6419 learning rate: 0.000923, scenario: 0, slope: -24.556478511619318, fluctuations: 0.0\n",
      "step: 21660 loss: 1920.162217 time elapsed: 26.6536 learning rate: 0.000923, scenario: 0, slope: -20.119002452137774, fluctuations: 0.0\n",
      "step: 21670 loss: 1785.927530 time elapsed: 26.6654 learning rate: 0.000923, scenario: 0, slope: -17.00805085906516, fluctuations: 0.0\n",
      "step: 21680 loss: 1652.921564 time elapsed: 26.6771 learning rate: 0.000923, scenario: 0, slope: -15.138334364148296, fluctuations: 0.0\n",
      "step: 21690 loss: 1550.123538 time elapsed: 26.6889 learning rate: 0.000923, scenario: 0, slope: -13.897399604017622, fluctuations: 0.0\n",
      "step: 21700 loss: 1469.653861 time elapsed: 26.7012 learning rate: 0.000923, scenario: 0, slope: -13.025888164872628, fluctuations: 0.0\n",
      "step: 21710 loss: 1405.597848 time elapsed: 26.7159 learning rate: 0.000923, scenario: 0, slope: -12.04084839696268, fluctuations: 0.0\n",
      "step: 21720 loss: 1351.628440 time elapsed: 26.7290 learning rate: 0.000923, scenario: 0, slope: -11.121673113711314, fluctuations: 0.0\n",
      "step: 21730 loss: 1304.636141 time elapsed: 26.7406 learning rate: 0.000923, scenario: 0, slope: -10.154997594205597, fluctuations: 0.0\n",
      "step: 21740 loss: 1263.163457 time elapsed: 26.7540 learning rate: 0.000923, scenario: 0, slope: -9.124173018876272, fluctuations: 0.0\n",
      "step: 21750 loss: 1226.079677 time elapsed: 26.7681 learning rate: 0.000923, scenario: 0, slope: -7.996682596352467, fluctuations: 0.0\n",
      "step: 21760 loss: 1192.480667 time elapsed: 26.7813 learning rate: 0.000923, scenario: 0, slope: -6.794033620092172, fluctuations: 0.0\n",
      "step: 21770 loss: 1161.651887 time elapsed: 26.7951 learning rate: 0.000923, scenario: 0, slope: -5.664772617337037, fluctuations: 0.0\n",
      "step: 21780 loss: 1133.062341 time elapsed: 26.8075 learning rate: 0.000923, scenario: 0, slope: -4.8000958148925115, fluctuations: 0.0\n",
      "step: 21790 loss: 1106.322175 time elapsed: 26.8198 learning rate: 0.000923, scenario: 0, slope: -4.171269904559862, fluctuations: 0.0\n",
      "step: 21800 loss: 1081.146666 time elapsed: 26.8324 learning rate: 0.000923, scenario: 0, slope: -3.750751718138317, fluctuations: 0.0\n",
      "step: 21810 loss: 1057.324907 time elapsed: 26.8461 learning rate: 0.000923, scenario: 0, slope: -3.3551742613927247, fluctuations: 0.0\n",
      "step: 21820 loss: 1034.696945 time elapsed: 26.8585 learning rate: 0.000923, scenario: 0, slope: -3.0712922848879356, fluctuations: 0.0\n",
      "step: 21830 loss: 1013.138384 time elapsed: 26.8704 learning rate: 0.000923, scenario: 0, slope: -2.8400565026340083, fluctuations: 0.0\n",
      "step: 21840 loss: 992.549829 time elapsed: 26.8825 learning rate: 0.000923, scenario: 0, slope: -2.649240680742694, fluctuations: 0.0\n",
      "step: 21850 loss: 972.849971 time elapsed: 26.8944 learning rate: 0.000923, scenario: 0, slope: -2.488902122849796, fluctuations: 0.0\n",
      "step: 21860 loss: 953.970947 time elapsed: 26.9069 learning rate: 0.000923, scenario: 0, slope: -2.3514345704739084, fluctuations: 0.0\n",
      "step: 21870 loss: 935.855161 time elapsed: 26.9210 learning rate: 0.000923, scenario: 0, slope: -2.231251375907637, fluctuations: 0.0\n",
      "step: 21880 loss: 918.453041 time elapsed: 26.9337 learning rate: 0.000923, scenario: 0, slope: -2.124348084114213, fluctuations: 0.0\n",
      "step: 21890 loss: 901.721390 time elapsed: 26.9458 learning rate: 0.000923, scenario: 0, slope: -2.027880254276005, fluctuations: 0.0\n",
      "step: 21900 loss: 885.622149 time elapsed: 26.9580 learning rate: 0.000923, scenario: 0, slope: -1.9482864237679227, fluctuations: 0.0\n",
      "step: 21910 loss: 870.121464 time elapsed: 26.9704 learning rate: 0.000923, scenario: 0, slope: -1.858658975142414, fluctuations: 0.0\n",
      "step: 21920 loss: 855.188810 time elapsed: 26.9825 learning rate: 0.000923, scenario: 0, slope: -1.7833158393493185, fluctuations: 0.0\n",
      "step: 21930 loss: 840.796231 time elapsed: 26.9940 learning rate: 0.000923, scenario: 0, slope: -1.7129395315221936, fluctuations: 0.0\n",
      "step: 21940 loss: 826.918048 time elapsed: 27.0060 learning rate: 0.000923, scenario: 0, slope: -1.6468745110647793, fluctuations: 0.0\n",
      "step: 21950 loss: 813.530235 time elapsed: 27.0179 learning rate: 0.000923, scenario: 0, slope: -1.5846029375240602, fluctuations: 0.0\n",
      "step: 21960 loss: 800.609834 time elapsed: 27.0298 learning rate: 0.000923, scenario: 0, slope: -1.5257127398260157, fluctuations: 0.0\n",
      "step: 21970 loss: 788.134040 time elapsed: 27.0417 learning rate: 0.000923, scenario: 0, slope: -1.4698776712047603, fluctuations: 0.0\n",
      "step: 21980 loss: 776.077682 time elapsed: 27.0536 learning rate: 0.000923, scenario: 0, slope: -1.4168506209581062, fluctuations: 0.0\n",
      "step: 21990 loss: 764.405614 time elapsed: 27.0655 learning rate: 0.000923, scenario: 0, slope: -1.366484689230259, fluctuations: 0.0\n",
      "step: 22000 loss: 753.068208 time elapsed: 27.0772 learning rate: 0.000923, scenario: 0, slope: -1.3234459886442111, fluctuations: 0.0\n",
      "step: 22010 loss: 742.128820 time elapsed: 27.0895 learning rate: 0.000923, scenario: 0, slope: -1.273635161840767, fluctuations: 0.0\n",
      "step: 22020 loss: 731.525366 time elapsed: 27.1013 learning rate: 0.000923, scenario: 0, slope: -1.230751092713087, fluctuations: 0.0\n",
      "step: 22030 loss: 721.224593 time elapsed: 27.1130 learning rate: 0.000923, scenario: 0, slope: -1.1901159464880575, fluctuations: 0.0\n",
      "step: 22040 loss: 711.195974 time elapsed: 27.1267 learning rate: 0.000923, scenario: 0, slope: -1.151746763097038, fluctuations: 0.0\n",
      "step: 22050 loss: 701.418103 time elapsed: 27.1395 learning rate: 0.000923, scenario: 0, slope: -1.1156870404824457, fluctuations: 0.0\n",
      "step: 22060 loss: 691.886048 time elapsed: 27.1516 learning rate: 0.000923, scenario: 0, slope: -1.0819145723368735, fluctuations: 0.0\n",
      "step: 22070 loss: 682.629021 time elapsed: 27.1638 learning rate: 0.000923, scenario: 0, slope: -1.0502053259640458, fluctuations: 0.0\n",
      "step: 22080 loss: 673.693081 time elapsed: 27.1760 learning rate: 0.000923, scenario: 0, slope: -1.0200266320671174, fluctuations: 0.0\n",
      "step: 22090 loss: 665.077437 time elapsed: 27.1880 learning rate: 0.000923, scenario: 0, slope: -0.9907544004824163, fluctuations: 0.0\n",
      "step: 22100 loss: 656.735479 time elapsed: 27.2003 learning rate: 0.000923, scenario: 0, slope: -0.9649727234885942, fluctuations: 0.0\n",
      "step: 22110 loss: 648.632543 time elapsed: 27.2129 learning rate: 0.000923, scenario: 0, slope: -0.9340505369197195, fluctuations: 0.0\n",
      "step: 22120 loss: 640.750822 time elapsed: 27.2247 learning rate: 0.000923, scenario: 0, slope: -0.9063071896616868, fluctuations: 0.0\n",
      "step: 22130 loss: 633.075805 time elapsed: 27.2367 learning rate: 0.000923, scenario: 0, slope: -0.8790002210542281, fluctuations: 0.0\n",
      "step: 22140 loss: 625.593589 time elapsed: 27.2495 learning rate: 0.000923, scenario: 0, slope: -0.8523613009625962, fluctuations: 0.0\n",
      "step: 22150 loss: 618.290919 time elapsed: 27.2641 learning rate: 0.000923, scenario: 0, slope: -0.8267744321892322, fluctuations: 0.0\n",
      "step: 22160 loss: 611.154778 time elapsed: 27.2768 learning rate: 0.000923, scenario: 0, slope: -0.8026949496998437, fluctuations: 0.0\n",
      "step: 22170 loss: 604.172278 time elapsed: 27.2891 learning rate: 0.000923, scenario: 0, slope: -0.780477221664141, fluctuations: 0.0\n",
      "step: 22180 loss: 597.330467 time elapsed: 27.3018 learning rate: 0.000923, scenario: 0, slope: -0.760148048531183, fluctuations: 0.0\n",
      "step: 22190 loss: 590.616167 time elapsed: 27.3168 learning rate: 0.000923, scenario: 0, slope: -0.7414482052536385, fluctuations: 0.0\n",
      "step: 22200 loss: 584.015830 time elapsed: 27.3311 learning rate: 0.000923, scenario: 0, slope: -0.7258266036509567, fluctuations: 0.0\n",
      "step: 22210 loss: 577.515382 time elapsed: 27.3450 learning rate: 0.000923, scenario: 0, slope: -0.7082079053671074, fluctuations: 0.0\n",
      "step: 22220 loss: 571.100084 time elapsed: 27.3576 learning rate: 0.000923, scenario: 0, slope: -0.6935874144032175, fluctuations: 0.0\n",
      "step: 22230 loss: 564.754305 time elapsed: 27.3696 learning rate: 0.000923, scenario: 0, slope: -0.6803135915548504, fluctuations: 0.0\n",
      "step: 22240 loss: 558.461281 time elapsed: 27.3819 learning rate: 0.000923, scenario: 0, slope: -0.6684155278051902, fluctuations: 0.0\n",
      "step: 22250 loss: 552.203232 time elapsed: 27.3941 learning rate: 0.000923, scenario: 0, slope: -0.6579392382344317, fluctuations: 0.0\n",
      "step: 22260 loss: 545.961018 time elapsed: 27.4074 learning rate: 0.000923, scenario: 0, slope: -0.6489470402882803, fluctuations: 0.0\n",
      "step: 22270 loss: 539.714461 time elapsed: 27.4206 learning rate: 0.000923, scenario: 0, slope: -0.6415160746158652, fluctuations: 0.0\n",
      "step: 22280 loss: 533.442570 time elapsed: 27.4331 learning rate: 0.000923, scenario: 0, slope: -0.6357349750860777, fluctuations: 0.0\n",
      "step: 22290 loss: 527.124579 time elapsed: 27.4452 learning rate: 0.000923, scenario: 0, slope: -0.6316968426722864, fluctuations: 0.0\n",
      "step: 22300 loss: 520.741727 time elapsed: 27.4568 learning rate: 0.000923, scenario: 0, slope: -0.6296226895051773, fluctuations: 0.0\n",
      "step: 22310 loss: 514.280444 time elapsed: 27.4692 learning rate: 0.000923, scenario: 0, slope: -0.629151870592196, fluctuations: 0.0\n",
      "step: 22320 loss: 507.736527 time elapsed: 27.4810 learning rate: 0.000923, scenario: 0, slope: -0.6306672577308327, fluctuations: 0.0\n",
      "step: 22330 loss: 501.119577 time elapsed: 27.4929 learning rate: 0.000923, scenario: 0, slope: -0.6338928203409435, fluctuations: 0.0\n",
      "step: 22340 loss: 494.463413 time elapsed: 27.5048 learning rate: 0.000923, scenario: 0, slope: -0.6384727030082905, fluctuations: 0.0\n",
      "step: 22350 loss: 487.824377 time elapsed: 27.5166 learning rate: 0.000923, scenario: 0, slope: -0.6437785607402494, fluctuations: 0.0\n",
      "step: 22360 loss: 481.279225 time elapsed: 27.5280 learning rate: 0.000923, scenario: 0, slope: -0.6488685750982809, fluctuations: 0.0\n",
      "step: 22370 loss: 474.912320 time elapsed: 27.5413 learning rate: 0.000923, scenario: 0, slope: -0.6525246466536756, fluctuations: 0.0\n",
      "step: 22380 loss: 468.786010 time elapsed: 27.5542 learning rate: 0.000923, scenario: 0, slope: -0.6534329064874299, fluctuations: 0.0\n",
      "step: 22390 loss: 462.938641 time elapsed: 27.5667 learning rate: 0.000923, scenario: 0, slope: -0.6504222945794164, fluctuations: 0.0\n",
      "step: 22400 loss: 457.374123 time elapsed: 27.5789 learning rate: 0.000923, scenario: 0, slope: -0.6437199269566698, fluctuations: 0.0\n",
      "step: 22410 loss: 452.081668 time elapsed: 27.5916 learning rate: 0.000923, scenario: 0, slope: -0.630117380147932, fluctuations: 0.0\n",
      "step: 22420 loss: 447.057021 time elapsed: 27.6038 learning rate: 0.000923, scenario: 0, slope: -0.6128936736942086, fluctuations: 0.0\n",
      "step: 22430 loss: 442.290617 time elapsed: 27.6164 learning rate: 0.000923, scenario: 0, slope: -0.5917965673818176, fluctuations: 0.0\n",
      "step: 22440 loss: 437.770977 time elapsed: 27.6286 learning rate: 0.000923, scenario: 0, slope: -0.5678810570994145, fluctuations: 0.0\n",
      "step: 22450 loss: 433.463573 time elapsed: 27.6412 learning rate: 0.000923, scenario: 0, slope: -0.5424040694775545, fluctuations: 0.0\n",
      "step: 22460 loss: 429.339990 time elapsed: 27.6535 learning rate: 0.000923, scenario: 0, slope: -0.5166275793280448, fluctuations: 0.0\n",
      "step: 22470 loss: 425.360180 time elapsed: 27.6660 learning rate: 0.000923, scenario: 0, slope: -0.4916343221479582, fluctuations: 0.0\n",
      "step: 22480 loss: 421.492058 time elapsed: 27.6781 learning rate: 0.000923, scenario: 0, slope: -0.46824227257073686, fluctuations: 0.0\n",
      "step: 22490 loss: 416.125778 time elapsed: 27.6899 learning rate: 0.002394, scenario: 1, slope: -0.4492413819873329, fluctuations: 0.0\n",
      "step: 22500 loss: 402.960793 time elapsed: 27.7018 learning rate: 0.005645, scenario: 1, slope: -0.45617917562019855, fluctuations: 0.01\n",
      "step: 22510 loss: 383.761820 time elapsed: 27.7146 learning rate: 0.005645, scenario: 0, slope: -0.5180439395191088, fluctuations: 0.05\n",
      "step: 22520 loss: 364.147846 time elapsed: 27.7265 learning rate: 0.005645, scenario: 0, slope: -0.6796804126761856, fluctuations: 0.06\n",
      "step: 22530 loss: 347.166108 time elapsed: 27.7396 learning rate: 0.005645, scenario: 0, slope: -0.8814559125257526, fluctuations: 0.06\n",
      "step: 22540 loss: 330.973587 time elapsed: 27.7534 learning rate: 0.005645, scenario: 0, slope: -1.0831649228772564, fluctuations: 0.06\n",
      "step: 22550 loss: 316.133428 time elapsed: 27.7670 learning rate: 0.005645, scenario: 0, slope: -1.2695607833022429, fluctuations: 0.06\n",
      "step: 22560 loss: 301.434117 time elapsed: 27.7792 learning rate: 0.005645, scenario: 0, slope: -1.424063774175115, fluctuations: 0.06\n",
      "step: 22570 loss: 289.001094 time elapsed: 27.7917 learning rate: 0.005645, scenario: 0, slope: -1.5382606869097306, fluctuations: 0.06\n",
      "step: 22580 loss: 275.824661 time elapsed: 27.8039 learning rate: 0.005645, scenario: 0, slope: -1.5942471696288856, fluctuations: 0.06\n",
      "step: 22590 loss: 264.324053 time elapsed: 27.8158 learning rate: 0.005645, scenario: 0, slope: -1.5720093637419261, fluctuations: 0.05\n",
      "step: 22600 loss: 253.034689 time elapsed: 27.8276 learning rate: 0.005645, scenario: 0, slope: -1.4863195899414328, fluctuations: 0.05\n",
      "step: 22610 loss: 242.836901 time elapsed: 27.8402 learning rate: 0.005645, scenario: 0, slope: -1.3924285398789267, fluctuations: 0.01\n",
      "step: 22620 loss: 232.676998 time elapsed: 27.8521 learning rate: 0.005645, scenario: 0, slope: -1.3076636649977957, fluctuations: 0.0\n",
      "step: 22630 loss: 224.794278 time elapsed: 27.8641 learning rate: 0.005645, scenario: 0, slope: -1.224032213382197, fluctuations: 0.0\n",
      "step: 22640 loss: 215.425372 time elapsed: 27.8757 learning rate: 0.005645, scenario: 0, slope: -1.1459861105625528, fluctuations: 0.0\n",
      "step: 22650 loss: 206.423268 time elapsed: 27.8874 learning rate: 0.005645, scenario: 0, slope: -1.0768947628963388, fluctuations: 0.0\n",
      "step: 22660 loss: 195.995018 time elapsed: 27.8992 learning rate: 0.005645, scenario: 0, slope: -1.0171632057030713, fluctuations: 0.0\n",
      "step: 22670 loss: 184.578393 time elapsed: 27.9110 learning rate: 0.005645, scenario: 0, slope: -0.9889810613227124, fluctuations: 0.0\n",
      "step: 22680 loss: 176.228945 time elapsed: 27.9225 learning rate: 0.005645, scenario: 0, slope: -0.9691278339017387, fluctuations: 0.02\n",
      "step: 22690 loss: 166.968459 time elapsed: 27.9342 learning rate: 0.005645, scenario: 0, slope: -0.9641101172693394, fluctuations: 0.04\n",
      "step: 22700 loss: 159.685170 time elapsed: 27.9462 learning rate: 0.005645, scenario: 0, slope: -0.9561531902966479, fluctuations: 0.04\n",
      "step: 22710 loss: 153.840785 time elapsed: 27.9585 learning rate: 0.005645, scenario: 0, slope: -0.9328496974029857, fluctuations: 0.04\n",
      "step: 22720 loss: 148.694210 time elapsed: 27.9721 learning rate: 0.005645, scenario: 0, slope: -0.8957682980273614, fluctuations: 0.04\n",
      "step: 22730 loss: 144.023886 time elapsed: 27.9852 learning rate: 0.005645, scenario: 0, slope: -0.84271387333417, fluctuations: 0.04\n",
      "step: 22740 loss: 139.675296 time elapsed: 27.9975 learning rate: 0.005645, scenario: 0, slope: -0.7731955472891798, fluctuations: 0.04\n",
      "step: 22750 loss: 135.551528 time elapsed: 28.0099 learning rate: 0.005645, scenario: 0, slope: -0.690714938523382, fluctuations: 0.04\n",
      "step: 22760 loss: 131.605066 time elapsed: 28.0216 learning rate: 0.005645, scenario: 0, slope: -0.6009207296660609, fluctuations: 0.04\n",
      "step: 22770 loss: 127.867772 time elapsed: 28.0337 learning rate: 0.005645, scenario: 0, slope: -0.528726381266333, fluctuations: 0.04\n",
      "step: 22780 loss: 124.302742 time elapsed: 28.0458 learning rate: 0.005645, scenario: 0, slope: -0.4710875663212857, fluctuations: 0.02\n",
      "step: 22790 loss: 120.849305 time elapsed: 28.0576 learning rate: 0.005645, scenario: 0, slope: -0.43729483300180455, fluctuations: 0.0\n",
      "step: 22800 loss: 117.426196 time elapsed: 28.0691 learning rate: 0.005645, scenario: 0, slope: -0.4127133441382426, fluctuations: 0.0\n",
      "step: 22810 loss: 113.737011 time elapsed: 28.0816 learning rate: 0.005645, scenario: 0, slope: -0.3902722049309792, fluctuations: 0.0\n",
      "step: 22820 loss: 109.131020 time elapsed: 28.0935 learning rate: 0.005645, scenario: 0, slope: -0.379184268507062, fluctuations: 0.0\n",
      "step: 22830 loss: 105.504418 time elapsed: 28.1055 learning rate: 0.005645, scenario: 0, slope: -0.3751879827254162, fluctuations: 0.0\n",
      "step: 22840 loss: 102.468116 time elapsed: 28.1175 learning rate: 0.005645, scenario: 0, slope: -0.371400753621283, fluctuations: 0.0\n",
      "step: 22850 loss: 99.819535 time elapsed: 28.1294 learning rate: 0.005645, scenario: 0, slope: -0.3652469981681288, fluctuations: 0.0\n",
      "step: 22860 loss: 97.367449 time elapsed: 28.1413 learning rate: 0.005645, scenario: 0, slope: -0.3556425698010665, fluctuations: 0.0\n",
      "step: 22870 loss: 95.244397 time elapsed: 28.1533 learning rate: 0.005645, scenario: 0, slope: -0.3420894276278121, fluctuations: 0.0\n",
      "step: 22880 loss: 93.734845 time elapsed: 28.1657 learning rate: 0.005645, scenario: 0, slope: -0.32271363772588024, fluctuations: 0.02\n",
      "step: 22890 loss: 91.632553 time elapsed: 28.1803 learning rate: 0.005645, scenario: 0, slope: -0.30200750724050446, fluctuations: 0.05\n",
      "step: 22900 loss: 88.638104 time elapsed: 28.1936 learning rate: 0.005645, scenario: 0, slope: -0.28079059042914306, fluctuations: 0.08\n",
      "step: 22910 loss: 86.987396 time elapsed: 28.2071 learning rate: 0.005645, scenario: 0, slope: -0.2529277376914895, fluctuations: 0.12\n",
      "step: 22920 loss: 84.794095 time elapsed: 28.2197 learning rate: 0.005645, scenario: 0, slope: -0.2329983654892729, fluctuations: 0.16\n",
      "step: 22930 loss: 82.722055 time elapsed: 28.2320 learning rate: 0.005645, scenario: 0, slope: -0.22074427482972245, fluctuations: 0.17\n",
      "step: 22940 loss: 80.875810 time elapsed: 28.2441 learning rate: 0.005645, scenario: 0, slope: -0.21329251058043258, fluctuations: 0.17\n",
      "step: 22950 loss: 78.990471 time elapsed: 28.2563 learning rate: 0.005645, scenario: 0, slope: -0.20853612456915804, fluctuations: 0.17\n",
      "step: 22960 loss: 77.124010 time elapsed: 28.2683 learning rate: 0.005645, scenario: 0, slope: -0.20523674819494336, fluctuations: 0.17\n",
      "step: 22970 loss: 75.260961 time elapsed: 28.2807 learning rate: 0.005645, scenario: 0, slope: -0.20289389766655846, fluctuations: 0.17\n",
      "step: 22980 loss: 73.365036 time elapsed: 28.2928 learning rate: 0.005645, scenario: 0, slope: -0.19690736439859455, fluctuations: 0.14\n",
      "step: 22990 loss: 71.411283 time elapsed: 28.3049 learning rate: 0.005645, scenario: 0, slope: -0.19266158443298517, fluctuations: 0.11\n",
      "step: 23000 loss: 69.398670 time elapsed: 28.3163 learning rate: 0.005645, scenario: 0, slope: -0.19127331705523812, fluctuations: 0.08\n",
      "step: 23010 loss: 67.370897 time elapsed: 28.3292 learning rate: 0.005645, scenario: 0, slope: -0.1912058095453122, fluctuations: 0.04\n",
      "step: 23020 loss: 65.426081 time elapsed: 28.3412 learning rate: 0.005645, scenario: 0, slope: -0.1920072724746304, fluctuations: 0.01\n",
      "step: 23030 loss: 63.669709 time elapsed: 28.3528 learning rate: 0.005645, scenario: 0, slope: -0.19267017791593058, fluctuations: 0.0\n",
      "step: 23040 loss: 62.142227 time elapsed: 28.3643 learning rate: 0.005645, scenario: 0, slope: -0.19181568210441172, fluctuations: 0.0\n",
      "step: 23050 loss: 60.807901 time elapsed: 28.3765 learning rate: 0.005645, scenario: 0, slope: -0.18843574393938833, fluctuations: 0.0\n",
      "step: 23060 loss: 59.612462 time elapsed: 28.3900 learning rate: 0.005645, scenario: 0, slope: -0.18200948208865209, fluctuations: 0.0\n",
      "step: 23070 loss: 58.516108 time elapsed: 28.4025 learning rate: 0.005645, scenario: 0, slope: -0.17259871679161312, fluctuations: 0.0\n",
      "step: 23080 loss: 57.491765 time elapsed: 28.4144 learning rate: 0.005645, scenario: 0, slope: -0.16075559432091524, fluctuations: 0.0\n",
      "step: 23090 loss: 56.521397 time elapsed: 28.4265 learning rate: 0.005645, scenario: 0, slope: -0.1474493822116544, fluctuations: 0.0\n",
      "step: 23100 loss: 55.593825 time elapsed: 28.4380 learning rate: 0.005645, scenario: 0, slope: -0.1352684973499415, fluctuations: 0.0\n",
      "step: 23110 loss: 54.702088 time elapsed: 28.4503 learning rate: 0.005645, scenario: 0, slope: -0.12156099504880238, fluctuations: 0.0\n",
      "step: 23120 loss: 53.841559 time elapsed: 28.4624 learning rate: 0.005645, scenario: 0, slope: -0.1112286631546721, fluctuations: 0.0\n",
      "step: 23130 loss: 53.009062 time elapsed: 28.4745 learning rate: 0.005645, scenario: 0, slope: -0.10319718392113103, fluctuations: 0.0\n",
      "step: 23140 loss: 52.202398 time elapsed: 28.4860 learning rate: 0.005645, scenario: 0, slope: -0.09709046135285142, fluctuations: 0.0\n",
      "step: 23150 loss: 51.420093 time elapsed: 28.4978 learning rate: 0.005645, scenario: 0, slope: -0.09233536609307677, fluctuations: 0.0\n",
      "step: 23160 loss: 50.663423 time elapsed: 28.5094 learning rate: 0.005645, scenario: 0, slope: -0.08846974013513297, fluctuations: 0.0\n",
      "step: 23170 loss: 50.535687 time elapsed: 28.5212 learning rate: 0.005645, scenario: 0, slope: -0.08434906384439142, fluctuations: 0.0\n",
      "step: 23180 loss: 49.842595 time elapsed: 28.5333 learning rate: 0.005645, scenario: 0, slope: -0.07874940025816787, fluctuations: 0.02\n",
      "step: 23190 loss: 48.720188 time elapsed: 28.5453 learning rate: 0.005645, scenario: 0, slope: -0.07608685143889476, fluctuations: 0.03\n",
      "step: 23200 loss: 47.901913 time elapsed: 28.5571 learning rate: 0.005645, scenario: 0, slope: -0.07444401868154488, fluctuations: 0.05\n",
      "step: 23210 loss: 47.212783 time elapsed: 28.5699 learning rate: 0.005645, scenario: 0, slope: -0.07281013319748704, fluctuations: 0.05\n",
      "step: 23220 loss: 46.582701 time elapsed: 28.5831 learning rate: 0.005645, scenario: 0, slope: -0.07156937532415972, fluctuations: 0.05\n",
      "step: 23230 loss: 45.979727 time elapsed: 28.5983 learning rate: 0.005645, scenario: 0, slope: -0.07044586214572227, fluctuations: 0.05\n",
      "step: 23240 loss: 45.392003 time elapsed: 28.6112 learning rate: 0.005645, scenario: 0, slope: -0.06939804633784251, fluctuations: 0.05\n",
      "step: 23250 loss: 44.824240 time elapsed: 28.6236 learning rate: 0.005645, scenario: 0, slope: -0.06841445292370772, fluctuations: 0.05\n",
      "step: 23260 loss: 44.275240 time elapsed: 28.6364 learning rate: 0.005645, scenario: 0, slope: -0.06750245906039949, fluctuations: 0.05\n",
      "step: 23270 loss: 43.742758 time elapsed: 28.6490 learning rate: 0.005645, scenario: 0, slope: -0.06566315655400223, fluctuations: 0.05\n",
      "step: 23280 loss: 43.226045 time elapsed: 28.6624 learning rate: 0.005645, scenario: 0, slope: -0.0605629913722465, fluctuations: 0.03\n",
      "step: 23290 loss: 42.723940 time elapsed: 28.6756 learning rate: 0.005645, scenario: 0, slope: -0.05815400491968447, fluctuations: 0.01\n",
      "step: 23300 loss: 42.235350 time elapsed: 28.6877 learning rate: 0.005645, scenario: 0, slope: -0.05633916739356573, fluctuations: 0.0\n",
      "step: 23310 loss: 41.759269 time elapsed: 28.7004 learning rate: 0.005645, scenario: 0, slope: -0.05430552909536505, fluctuations: 0.0\n",
      "step: 23320 loss: 41.294731 time elapsed: 28.7122 learning rate: 0.005645, scenario: 0, slope: -0.052645668346410254, fluctuations: 0.0\n",
      "step: 23330 loss: 40.840820 time elapsed: 28.7244 learning rate: 0.005645, scenario: 0, slope: -0.051120648354239816, fluctuations: 0.0\n",
      "step: 23340 loss: 40.396653 time elapsed: 28.7364 learning rate: 0.005645, scenario: 0, slope: -0.049709985788755885, fluctuations: 0.0\n",
      "step: 23350 loss: 39.961353 time elapsed: 28.7484 learning rate: 0.005645, scenario: 0, slope: -0.048401949702376386, fluctuations: 0.0\n",
      "step: 23360 loss: 39.534013 time elapsed: 28.7600 learning rate: 0.005645, scenario: 0, slope: -0.047194009566144855, fluctuations: 0.0\n",
      "step: 23370 loss: 39.113669 time elapsed: 28.7719 learning rate: 0.005645, scenario: 0, slope: -0.04608241012260419, fluctuations: 0.0\n",
      "step: 23380 loss: 38.702339 time elapsed: 28.7838 learning rate: 0.005645, scenario: 0, slope: -0.04506100395248542, fluctuations: 0.0\n",
      "step: 23390 loss: 38.848984 time elapsed: 28.7968 learning rate: 0.005645, scenario: 0, slope: -0.04334165309773724, fluctuations: 0.0\n",
      "step: 23400 loss: 4623.275927 time elapsed: 28.8101 learning rate: 0.007029, scenario: -1, slope: 4.657596715884255, fluctuations: 0.0\n",
      "step: 23410 loss: 797.156789 time elapsed: 28.8229 learning rate: 0.002451, scenario: -1, slope: 7.359544815628759, fluctuations: 0.04\n",
      "step: 23420 loss: 289.767637 time elapsed: 28.8351 learning rate: 0.000855, scenario: -1, slope: 7.590245510018494, fluctuations: 0.07\n",
      "step: 23430 loss: 167.266028 time elapsed: 28.8471 learning rate: 0.000298, scenario: -1, slope: 6.005394863890534, fluctuations: 0.07\n",
      "step: 23440 loss: 141.149481 time elapsed: 28.8594 learning rate: 0.000104, scenario: -1, slope: 4.1704389097578, fluctuations: 0.07\n",
      "step: 23450 loss: 134.001491 time elapsed: 28.8712 learning rate: 0.000036, scenario: -1, slope: 2.3100934035518, fluctuations: 0.07\n",
      "step: 23460 loss: 131.728318 time elapsed: 28.8830 learning rate: 0.000013, scenario: -1, slope: 0.38258506979309886, fluctuations: 0.07\n",
      "step: 23470 loss: 130.867333 time elapsed: 28.8955 learning rate: 0.000012, scenario: 0, slope: -1.6967636076026356, fluctuations: 0.07\n",
      "step: 23480 loss: 130.135621 time elapsed: 28.9077 learning rate: 0.000012, scenario: 0, slope: -4.049181725924074, fluctuations: 0.07\n",
      "step: 23490 loss: 129.477631 time elapsed: 28.9198 learning rate: 0.000012, scenario: 0, slope: -6.843168572702179, fluctuations: 0.07\n",
      "step: 23500 loss: 128.859904 time elapsed: 28.9316 learning rate: 0.000012, scenario: 0, slope: -8.196852196563157, fluctuations: 0.06\n",
      "step: 23510 loss: 128.262217 time elapsed: 28.9441 learning rate: 0.000012, scenario: 0, slope: -1.7666164550287577, fluctuations: 0.02\n",
      "step: 23520 loss: 127.674400 time elapsed: 28.9561 learning rate: 0.000012, scenario: 0, slope: -0.5195496193177692, fluctuations: 0.0\n",
      "step: 23530 loss: 127.091875 time elapsed: 28.9675 learning rate: 0.000012, scenario: 0, slope: -0.17096773198673182, fluctuations: 0.0\n",
      "step: 23540 loss: 126.248365 time elapsed: 28.9794 learning rate: 0.000031, scenario: 1, slope: -0.09082882631878843, fluctuations: 0.0\n",
      "step: 23550 loss: 124.100386 time elapsed: 28.9931 learning rate: 0.000081, scenario: 1, slope: -0.07238619710804325, fluctuations: 0.0\n",
      "step: 23560 loss: 118.696196 time elapsed: 29.0080 learning rate: 0.000209, scenario: 1, slope: -0.08850152147360055, fluctuations: 0.0\n",
      "step: 23570 loss: 105.945031 time elapsed: 29.0218 learning rate: 0.000408, scenario: 0, slope: -0.1496250030673798, fluctuations: 0.0\n",
      "step: 23580 loss: 91.355897 time elapsed: 29.0351 learning rate: 0.000408, scenario: 0, slope: -0.27367187518867114, fluctuations: 0.0\n",
      "step: 23590 loss: 80.226812 time elapsed: 29.0490 learning rate: 0.000408, scenario: 0, slope: -0.4310193924957564, fluctuations: 0.0\n",
      "step: 23600 loss: 72.174053 time elapsed: 29.0620 learning rate: 0.000408, scenario: 0, slope: -0.5740664612310452, fluctuations: 0.0\n",
      "step: 23610 loss: 66.421223 time elapsed: 29.0751 learning rate: 0.000408, scenario: 0, slope: -0.7230424720166694, fluctuations: 0.0\n",
      "step: 23620 loss: 62.226649 time elapsed: 29.0875 learning rate: 0.000408, scenario: 0, slope: -0.8152398688263164, fluctuations: 0.0\n",
      "step: 23630 loss: 59.069104 time elapsed: 29.1008 learning rate: 0.000408, scenario: 0, slope: -0.8538653085792947, fluctuations: 0.0\n",
      "step: 23640 loss: 56.620826 time elapsed: 29.1129 learning rate: 0.000408, scenario: 0, slope: -0.8312784248532186, fluctuations: 0.0\n",
      "step: 23650 loss: 54.675279 time elapsed: 29.1253 learning rate: 0.000408, scenario: 0, slope: -0.7469713546322884, fluctuations: 0.0\n",
      "step: 23660 loss: 53.096195 time elapsed: 29.1374 learning rate: 0.000408, scenario: 0, slope: -0.6124380255095733, fluctuations: 0.0\n",
      "step: 23670 loss: 51.789704 time elapsed: 29.1497 learning rate: 0.000408, scenario: 0, slope: -0.46321939835733666, fluctuations: 0.0\n",
      "step: 23680 loss: 50.689444 time elapsed: 29.1615 learning rate: 0.000408, scenario: 0, slope: -0.3461038748986742, fluctuations: 0.0\n",
      "step: 23690 loss: 49.747719 time elapsed: 29.1732 learning rate: 0.000408, scenario: 0, slope: -0.26270667695306693, fluctuations: 0.0\n",
      "step: 23700 loss: 48.929781 time elapsed: 29.1846 learning rate: 0.000408, scenario: 0, slope: -0.2094081127199279, fluctuations: 0.0\n",
      "step: 23710 loss: 48.209996 time elapsed: 29.1965 learning rate: 0.000408, scenario: 0, slope: -0.1633799969454818, fluctuations: 0.0\n",
      "step: 23720 loss: 47.569224 time elapsed: 29.2105 learning rate: 0.000408, scenario: 0, slope: -0.13364844557205113, fluctuations: 0.0\n",
      "step: 23730 loss: 46.992987 time elapsed: 29.2238 learning rate: 0.000408, scenario: 0, slope: -0.11157990211714006, fluctuations: 0.0\n",
      "step: 23740 loss: 46.470189 time elapsed: 29.2364 learning rate: 0.000408, scenario: 0, slope: -0.0948428613046481, fluctuations: 0.0\n",
      "step: 23750 loss: 45.992211 time elapsed: 29.2482 learning rate: 0.000408, scenario: 0, slope: -0.08191438914362817, fluctuations: 0.0\n",
      "step: 23760 loss: 45.552267 time elapsed: 29.2602 learning rate: 0.000408, scenario: 0, slope: -0.07175902119895451, fluctuations: 0.0\n",
      "step: 23770 loss: 45.144945 time elapsed: 29.2720 learning rate: 0.000408, scenario: 0, slope: -0.06365347953834308, fluctuations: 0.0\n",
      "step: 23780 loss: 44.765874 time elapsed: 29.2839 learning rate: 0.000408, scenario: 0, slope: -0.057084212404862396, fluctuations: 0.0\n",
      "step: 23790 loss: 44.411480 time elapsed: 29.2958 learning rate: 0.000408, scenario: 0, slope: -0.05168205727628393, fluctuations: 0.0\n",
      "step: 23800 loss: 44.058094 time elapsed: 29.3072 learning rate: 0.000597, scenario: 1, slope: -0.047602559234102094, fluctuations: 0.0\n",
      "step: 23810 loss: 43.406444 time elapsed: 29.3199 learning rate: 0.001549, scenario: 1, slope: -0.04430535631506184, fluctuations: 0.0\n",
      "step: 23820 loss: 41.991958 time elapsed: 29.3323 learning rate: 0.004019, scenario: 1, slope: -0.045784827296806656, fluctuations: 0.0\n",
      "step: 23830 loss: 39.928968 time elapsed: 29.3441 learning rate: 0.004863, scenario: 0, slope: -0.055337154878703755, fluctuations: 0.0\n",
      "step: 23840 loss: 38.516872 time elapsed: 29.3559 learning rate: 0.004863, scenario: 0, slope: -0.06999964429075495, fluctuations: 0.0\n",
      "step: 23850 loss: 37.527444 time elapsed: 29.3680 learning rate: 0.004863, scenario: 0, slope: -0.08505774185060949, fluctuations: 0.0\n",
      "step: 23860 loss: 36.791317 time elapsed: 29.3798 learning rate: 0.004863, scenario: 0, slope: -0.09773811857696225, fluctuations: 0.0\n",
      "step: 23870 loss: 36.214389 time elapsed: 29.3914 learning rate: 0.004863, scenario: 0, slope: -0.1062877356896642, fluctuations: 0.0\n",
      "step: 23880 loss: 35.734472 time elapsed: 29.4033 learning rate: 0.004863, scenario: 0, slope: -0.109584695364565, fluctuations: 0.0\n",
      "step: 23890 loss: 35.311826 time elapsed: 29.4190 learning rate: 0.004863, scenario: 0, slope: -0.10690851619442344, fluctuations: 0.0\n",
      "step: 23900 loss: 34.923051 time elapsed: 29.4346 learning rate: 0.004863, scenario: 0, slope: -0.09900073325001656, fluctuations: 0.0\n",
      "step: 23910 loss: 34.556187 time elapsed: 29.4487 learning rate: 0.004863, scenario: 0, slope: -0.08290815903398219, fluctuations: 0.0\n",
      "step: 23920 loss: 34.209013 time elapsed: 29.4615 learning rate: 0.004863, scenario: 0, slope: -0.06584307364998403, fluctuations: 0.0\n",
      "step: 23930 loss: 33.878033 time elapsed: 29.4743 learning rate: 0.004863, scenario: 0, slope: -0.05296825274433153, fluctuations: 0.0\n",
      "step: 23940 loss: 33.558461 time elapsed: 29.4873 learning rate: 0.004863, scenario: 0, slope: -0.045098409467411865, fluctuations: 0.0\n",
      "step: 23950 loss: 33.248399 time elapsed: 29.5004 learning rate: 0.004863, scenario: 0, slope: -0.04013283243813762, fluctuations: 0.0\n",
      "step: 23960 loss: 32.946463 time elapsed: 29.5133 learning rate: 0.004863, scenario: 0, slope: -0.03691649491811329, fluctuations: 0.0\n",
      "step: 23970 loss: 32.518206 time elapsed: 29.5266 learning rate: 0.012613, scenario: 1, slope: -0.03497557305096876, fluctuations: 0.0\n",
      "step: 23980 loss: 34.338722 time elapsed: 29.5396 learning rate: 0.032715, scenario: 1, slope: -0.03426614667571438, fluctuations: 0.0\n",
      "step: 23990 loss: 114729.574508 time elapsed: 29.5529 learning rate: 0.012098, scenario: -1, slope: 393.51167171072535, fluctuations: 0.03\n",
      "step: 24000 loss: 45079.506903 time elapsed: 29.5654 learning rate: 0.004687, scenario: -1, slope: 666.905451932772, fluctuations: 0.03\n",
      "step: 24010 loss: 26328.827984 time elapsed: 29.5779 learning rate: 0.001634, scenario: -1, slope: 683.9593605218379, fluctuations: 0.03\n",
      "step: 24020 loss: 19507.952573 time elapsed: 29.5901 learning rate: 0.000570, scenario: -1, slope: 589.6365171133275, fluctuations: 0.03\n",
      "step: 24030 loss: 18290.582891 time elapsed: 29.6022 learning rate: 0.000199, scenario: -1, slope: 457.00025280481907, fluctuations: 0.03\n",
      "step: 24040 loss: 17876.212356 time elapsed: 29.6150 learning rate: 0.000069, scenario: -1, slope: 301.2078077207767, fluctuations: 0.03\n",
      "step: 24050 loss: 17739.557676 time elapsed: 29.6283 learning rate: 0.000024, scenario: -1, slope: 122.16799992697874, fluctuations: 0.03\n",
      "step: 24060 loss: 17692.942478 time elapsed: 29.6405 learning rate: 0.000013, scenario: 0, slope: -83.56224413982741, fluctuations: 0.03\n",
      "step: 24070 loss: 17660.435764 time elapsed: 29.6529 learning rate: 0.000013, scenario: 0, slope: -321.67818279686526, fluctuations: 0.03\n",
      "step: 24080 loss: 17628.684953 time elapsed: 29.6652 learning rate: 0.000013, scenario: 0, slope: -600.1714190445101, fluctuations: 0.03\n",
      "step: 24090 loss: 17597.595087 time elapsed: 29.6772 learning rate: 0.000013, scenario: 0, slope: -352.1875986350264, fluctuations: 0.0\n",
      "step: 24100 loss: 17567.098018 time elapsed: 29.6891 learning rate: 0.000013, scenario: 0, slope: -124.70892177261634, fluctuations: 0.0\n",
      "step: 24110 loss: 17537.139941 time elapsed: 29.7015 learning rate: 0.000013, scenario: 0, slope: -30.934580476724175, fluctuations: 0.0\n",
      "step: 24120 loss: 17497.616254 time elapsed: 29.7134 learning rate: 0.000030, scenario: 1, slope: -9.542466949067496, fluctuations: 0.0\n",
      "step: 24130 loss: 17399.413329 time elapsed: 29.7253 learning rate: 0.000079, scenario: 1, slope: -4.992835271449833, fluctuations: 0.0\n",
      "step: 24140 loss: 17155.048128 time elapsed: 29.7370 learning rate: 0.000204, scenario: 1, slope: -4.5485338833925635, fluctuations: 0.0\n",
      "step: 24150 loss: 16568.568345 time elapsed: 29.7483 learning rate: 0.000528, scenario: 1, slope: -7.048063750083445, fluctuations: 0.0\n",
      "step: 24160 loss: 15227.958731 time elapsed: 29.7597 learning rate: 0.001370, scenario: 1, slope: -14.205662896955355, fluctuations: 0.0\n",
      "step: 24170 loss: 12803.800230 time elapsed: 29.7714 learning rate: 0.001658, scenario: 0, slope: -30.28245180979362, fluctuations: 0.0\n",
      "step: 24180 loss: 10747.166015 time elapsed: 29.7834 learning rate: 0.001658, scenario: 0, slope: -54.554379000126495, fluctuations: 0.0\n",
      "step: 24190 loss: 9041.123376 time elapsed: 29.7952 learning rate: 0.001658, scenario: 0, slope: -81.66239738699186, fluctuations: 0.0\n",
      "step: 24200 loss: 7678.549145 time elapsed: 29.8068 learning rate: 0.001658, scenario: 0, slope: -105.41569617468541, fluctuations: 0.0\n",
      "step: 24210 loss: 6641.388673 time elapsed: 29.8191 learning rate: 0.001658, scenario: 0, slope: -129.25185804930138, fluctuations: 0.0\n",
      "step: 24220 loss: 5814.246159 time elapsed: 29.8320 learning rate: 0.001658, scenario: 0, slope: -143.21353505851008, fluctuations: 0.0\n",
      "step: 24230 loss: 5166.930089 time elapsed: 29.8456 learning rate: 0.001658, scenario: 0, slope: -147.94472324251194, fluctuations: 0.0\n",
      "step: 24240 loss: 4747.706884 time elapsed: 29.8578 learning rate: 0.001658, scenario: 0, slope: -142.14560936255066, fluctuations: 0.0\n",
      "step: 24250 loss: 4442.025561 time elapsed: 29.8698 learning rate: 0.001658, scenario: 0, slope: -126.03423643195016, fluctuations: 0.0\n",
      "step: 24260 loss: 4196.109722 time elapsed: 29.8826 learning rate: 0.001658, scenario: 0, slope: -102.85066281785693, fluctuations: 0.0\n",
      "step: 24270 loss: 3992.092738 time elapsed: 29.8947 learning rate: 0.001658, scenario: 0, slope: -79.9092696602836, fluctuations: 0.0\n",
      "step: 24280 loss: 3814.319562 time elapsed: 29.9069 learning rate: 0.001658, scenario: 0, slope: -61.505652407516045, fluctuations: 0.0\n",
      "step: 24290 loss: 3658.558104 time elapsed: 29.9186 learning rate: 0.001658, scenario: 0, slope: -46.86568938385665, fluctuations: 0.0\n",
      "step: 24300 loss: 3521.125940 time elapsed: 29.9305 learning rate: 0.001658, scenario: 0, slope: -36.91100490985736, fluctuations: 0.0\n",
      "step: 24310 loss: 3394.974539 time elapsed: 29.9424 learning rate: 0.001658, scenario: 0, slope: -27.778201749747737, fluctuations: 0.0\n",
      "step: 24320 loss: 3277.816760 time elapsed: 29.9541 learning rate: 0.001658, scenario: 0, slope: -21.948635696047642, fluctuations: 0.0\n",
      "step: 24330 loss: 3167.801073 time elapsed: 29.9659 learning rate: 0.001658, scenario: 0, slope: -18.177691795409906, fluctuations: 0.0\n",
      "step: 24340 loss: 3064.086234 time elapsed: 29.9778 learning rate: 0.001658, scenario: 0, slope: -15.770827876492811, fluctuations: 0.0\n",
      "step: 24350 loss: 2964.057029 time elapsed: 29.9902 learning rate: 0.001658, scenario: 0, slope: -14.031049922406467, fluctuations: 0.0\n",
      "step: 24360 loss: 2865.803982 time elapsed: 30.0022 learning rate: 0.001658, scenario: 0, slope: -12.75181749608362, fluctuations: 0.0\n",
      "step: 24370 loss: 2766.275968 time elapsed: 30.0144 learning rate: 0.001658, scenario: 0, slope: -11.793086228830674, fluctuations: 0.0\n",
      "step: 24380 loss: 2665.324214 time elapsed: 30.0263 learning rate: 0.001658, scenario: 0, slope: -11.10088817081929, fluctuations: 0.0\n",
      "step: 24390 loss: 2565.402157 time elapsed: 30.0404 learning rate: 0.001658, scenario: 0, slope: -10.635225565394856, fluctuations: 0.0\n",
      "step: 24400 loss: 2476.478092 time elapsed: 30.0527 learning rate: 0.001658, scenario: 0, slope: -10.327681694103141, fluctuations: 0.0\n",
      "step: 24410 loss: 2402.495258 time elapsed: 30.0661 learning rate: 0.001658, scenario: 0, slope: -9.975789851832408, fluctuations: 0.0\n",
      "step: 24420 loss: 2335.883482 time elapsed: 30.0786 learning rate: 0.001658, scenario: 0, slope: -9.61197838216669, fluctuations: 0.0\n",
      "step: 24430 loss: 2275.451714 time elapsed: 30.0906 learning rate: 0.001658, scenario: 0, slope: -9.176125849089761, fluctuations: 0.0\n",
      "step: 24440 loss: 2219.084182 time elapsed: 30.1034 learning rate: 0.001658, scenario: 0, slope: -8.658226629296564, fluctuations: 0.0\n",
      "step: 24450 loss: 2166.073993 time elapsed: 30.1173 learning rate: 0.001658, scenario: 0, slope: -8.065053731202415, fluctuations: 0.0\n",
      "step: 24460 loss: 2115.869636 time elapsed: 30.1303 learning rate: 0.001658, scenario: 0, slope: -7.419961765211627, fluctuations: 0.0\n",
      "step: 24470 loss: 2068.077832 time elapsed: 30.1427 learning rate: 0.001658, scenario: 0, slope: -6.763784650616573, fluctuations: 0.0\n",
      "step: 24480 loss: 2022.381115 time elapsed: 30.1552 learning rate: 0.001658, scenario: 0, slope: -6.154529781711503, fluctuations: 0.0\n",
      "step: 24490 loss: 1978.524474 time elapsed: 30.1678 learning rate: 0.001658, scenario: 0, slope: -5.651341548841154, fluctuations: 0.0\n",
      "step: 24500 loss: 1936.301632 time elapsed: 30.1798 learning rate: 0.001658, scenario: 0, slope: -5.305322523971476, fluctuations: 0.0\n",
      "step: 24510 loss: 1895.549386 time elapsed: 30.1925 learning rate: 0.001658, scenario: 0, slope: -4.976777332191789, fluctuations: 0.0\n",
      "step: 24520 loss: 1856.486132 time elapsed: 30.2047 learning rate: 0.001658, scenario: 0, slope: -4.732219832601102, fluctuations: 0.0\n",
      "step: 24530 loss: 1818.152591 time elapsed: 30.2168 learning rate: 0.001658, scenario: 0, slope: -4.521390577880691, fluctuations: 0.0\n",
      "step: 24540 loss: 1781.103142 time elapsed: 30.2298 learning rate: 0.001658, scenario: 0, slope: -4.337489064272324, fluctuations: 0.0\n",
      "step: 24550 loss: 1745.198441 time elapsed: 30.2433 learning rate: 0.001658, scenario: 0, slope: -4.174044291702026, fluctuations: 0.0\n",
      "step: 24560 loss: 1710.393135 time elapsed: 30.2570 learning rate: 0.001658, scenario: 0, slope: -4.026618719669184, fluctuations: 0.0\n",
      "step: 24570 loss: 1676.671775 time elapsed: 30.2702 learning rate: 0.001658, scenario: 0, slope: -3.891654418187298, fluctuations: 0.0\n",
      "step: 24580 loss: 1644.038335 time elapsed: 30.2844 learning rate: 0.001658, scenario: 0, slope: -3.766140696333999, fluctuations: 0.0\n",
      "step: 24590 loss: 1612.477576 time elapsed: 30.2973 learning rate: 0.001658, scenario: 0, slope: -3.647560243664593, fluctuations: 0.0\n",
      "step: 24600 loss: 1581.988592 time elapsed: 30.3094 learning rate: 0.001658, scenario: 0, slope: -3.545059615967117, fluctuations: 0.0\n",
      "step: 24610 loss: 1552.550374 time elapsed: 30.3218 learning rate: 0.001658, scenario: 0, slope: -3.4235031224455703, fluctuations: 0.0\n",
      "step: 24620 loss: 1524.129790 time elapsed: 30.3333 learning rate: 0.001658, scenario: 0, slope: -3.3149427631182125, fluctuations: 0.0\n",
      "step: 24630 loss: 1496.684366 time elapsed: 30.3451 learning rate: 0.001658, scenario: 0, slope: -3.2064351703744247, fluctuations: 0.0\n",
      "step: 24640 loss: 1470.186786 time elapsed: 30.3570 learning rate: 0.001658, scenario: 0, slope: -3.100746648591889, fluctuations: 0.0\n",
      "step: 24650 loss: 1444.602710 time elapsed: 30.3690 learning rate: 0.001658, scenario: 0, slope: -2.995385027665147, fluctuations: 0.0\n",
      "step: 24660 loss: 1419.780426 time elapsed: 30.3806 learning rate: 0.001658, scenario: 0, slope: -2.894064526107365, fluctuations: 0.0\n",
      "step: 24670 loss: 1395.664703 time elapsed: 30.3921 learning rate: 0.001658, scenario: 0, slope: -2.796714266893788, fluctuations: 0.0\n",
      "step: 24680 loss: 1372.276131 time elapsed: 30.4040 learning rate: 0.001658, scenario: 0, slope: -2.7036966555753006, fluctuations: 0.0\n",
      "step: 24690 loss: 1349.520529 time elapsed: 30.4158 learning rate: 0.001658, scenario: 0, slope: -2.6155583319039044, fluctuations: 0.0\n",
      "step: 24700 loss: 1327.353501 time elapsed: 30.4274 learning rate: 0.001658, scenario: 0, slope: -2.540761780232172, fluctuations: 0.0\n",
      "step: 24710 loss: 1305.732687 time elapsed: 30.4404 learning rate: 0.001658, scenario: 0, slope: -2.455393508451235, fluctuations: 0.0\n",
      "step: 24720 loss: 1284.634050 time elapsed: 30.4542 learning rate: 0.001658, scenario: 0, slope: -2.3835258931842445, fluctuations: 0.0\n",
      "step: 24730 loss: 1264.051998 time elapsed: 30.4676 learning rate: 0.001658, scenario: 0, slope: -2.316827557494553, fluctuations: 0.0\n",
      "step: 24740 loss: 1244.348435 time elapsed: 30.4808 learning rate: 0.001658, scenario: 0, slope: -2.2530976276329437, fluctuations: 0.0\n",
      "step: 24750 loss: 1224.470308 time elapsed: 30.4944 learning rate: 0.001658, scenario: 0, slope: -2.1926378410098533, fluctuations: 0.0\n",
      "step: 24760 loss: 1205.218861 time elapsed: 30.5073 learning rate: 0.001658, scenario: 0, slope: -2.1367241095733305, fluctuations: 0.0\n",
      "step: 24770 loss: 1186.537489 time elapsed: 30.5201 learning rate: 0.001658, scenario: 0, slope: -2.0841765403838646, fluctuations: 0.0\n",
      "step: 24780 loss: 1168.185132 time elapsed: 30.5333 learning rate: 0.001658, scenario: 0, slope: -2.034262931236144, fluctuations: 0.0\n",
      "step: 24790 loss: 1150.161819 time elapsed: 30.5465 learning rate: 0.001658, scenario: 0, slope: -1.9868836351989652, fluctuations: 0.0\n",
      "step: 24800 loss: 1132.349755 time elapsed: 30.5598 learning rate: 0.001658, scenario: 0, slope: -1.9467623386163566, fluctuations: 0.0\n",
      "step: 24810 loss: 1114.635321 time elapsed: 30.5732 learning rate: 0.001658, scenario: 0, slope: -1.9019535164398302, fluctuations: 0.0\n",
      "step: 24820 loss: 1097.090576 time elapsed: 30.5860 learning rate: 0.001658, scenario: 0, slope: -1.866318524788724, fluctuations: 0.0\n",
      "step: 24830 loss: 1079.168505 time elapsed: 30.5985 learning rate: 0.001658, scenario: 0, slope: -1.8360376571455637, fluctuations: 0.0\n",
      "step: 24840 loss: 1060.511357 time elapsed: 30.6109 learning rate: 0.001658, scenario: 0, slope: -1.8144488085064374, fluctuations: 0.0\n",
      "step: 24850 loss: 1041.628674 time elapsed: 30.6232 learning rate: 0.001658, scenario: 0, slope: -1.8049288199591997, fluctuations: 0.0\n",
      "step: 24860 loss: 1022.033381 time elapsed: 30.6354 learning rate: 0.001658, scenario: 0, slope: -1.8081527773787645, fluctuations: 0.0\n",
      "step: 24870 loss: 1002.053721 time elapsed: 30.6490 learning rate: 0.001658, scenario: 0, slope: -1.823863739491992, fluctuations: 0.0\n",
      "step: 24880 loss: 982.069865 time elapsed: 30.6629 learning rate: 0.001658, scenario: 0, slope: -1.849320849382645, fluctuations: 0.0\n",
      "step: 24890 loss: 962.716431 time elapsed: 30.6764 learning rate: 0.001658, scenario: 0, slope: -1.8785057670244023, fluctuations: 0.0\n",
      "step: 24900 loss: 945.147533 time elapsed: 30.6888 learning rate: 0.001658, scenario: 0, slope: -1.9001872873892174, fluctuations: 0.0\n",
      "step: 24910 loss: 928.340238 time elapsed: 30.7015 learning rate: 0.001658, scenario: 0, slope: -1.9083037757761645, fluctuations: 0.0\n",
      "step: 24920 loss: 912.928833 time elapsed: 30.7139 learning rate: 0.001658, scenario: 0, slope: -1.8949844122176362, fluctuations: 0.0\n",
      "step: 24930 loss: 898.798410 time elapsed: 30.7258 learning rate: 0.001658, scenario: 0, slope: -1.8536048986673992, fluctuations: 0.0\n",
      "step: 24940 loss: 885.728152 time elapsed: 30.7377 learning rate: 0.001658, scenario: 0, slope: -1.7877002134602247, fluctuations: 0.0\n",
      "step: 24950 loss: 873.357205 time elapsed: 30.7501 learning rate: 0.001658, scenario: 0, slope: -1.7010237772451144, fluctuations: 0.0\n",
      "step: 24960 loss: 861.541612 time elapsed: 30.7621 learning rate: 0.001658, scenario: 0, slope: -1.601160349695535, fluctuations: 0.0\n",
      "step: 24970 loss: 850.181680 time elapsed: 30.7743 learning rate: 0.001658, scenario: 0, slope: -1.4975390697883826, fluctuations: 0.0\n",
      "step: 24980 loss: 839.229512 time elapsed: 30.7862 learning rate: 0.001658, scenario: 0, slope: -1.3992634610218393, fluctuations: 0.0\n",
      "step: 24990 loss: 829.935163 time elapsed: 30.7980 learning rate: 0.001658, scenario: 0, slope: -1.3099150723338444, fluctuations: 0.0\n",
      "step: 25000 loss: 818.343992 time elapsed: 30.8092 learning rate: 0.001658, scenario: 0, slope: -1.2427513492233875, fluctuations: 0.0\n",
      "step: 25010 loss: 808.302039 time elapsed: 30.8216 learning rate: 0.001658, scenario: 0, slope: -1.1738874522319789, fluctuations: 0.0\n",
      "step: 25020 loss: 798.594459 time elapsed: 30.8331 learning rate: 0.001658, scenario: 0, slope: -1.1255979020588118, fluctuations: 0.0\n",
      "step: 25030 loss: 789.045556 time elapsed: 30.8452 learning rate: 0.001658, scenario: 0, slope: -1.0857007440306061, fluctuations: 0.0\n",
      "step: 25040 loss: 779.759226 time elapsed: 30.8590 learning rate: 0.001658, scenario: 0, slope: -1.051439157092471, fluctuations: 0.0\n",
      "step: 25050 loss: 770.674589 time elapsed: 30.8729 learning rate: 0.001658, scenario: 0, slope: -1.02129878744536, fluctuations: 0.0\n",
      "step: 25060 loss: 761.791576 time elapsed: 30.8882 learning rate: 0.001658, scenario: 0, slope: -0.9942348284668099, fluctuations: 0.0\n",
      "step: 25070 loss: 753.092483 time elapsed: 30.9017 learning rate: 0.001658, scenario: 0, slope: -0.9696608092670708, fluctuations: 0.0\n",
      "step: 25080 loss: 744.572538 time elapsed: 30.9163 learning rate: 0.001658, scenario: 0, slope: -0.9471485345420221, fluctuations: 0.0\n",
      "step: 25090 loss: 736.470104 time elapsed: 30.9317 learning rate: 0.001658, scenario: 0, slope: -0.9227219145709883, fluctuations: 0.0\n",
      "step: 25100 loss: 728.092001 time elapsed: 30.9474 learning rate: 0.001658, scenario: 0, slope: -0.8994899931089615, fluctuations: 0.0\n",
      "step: 25110 loss: 720.173102 time elapsed: 30.9613 learning rate: 0.001658, scenario: 0, slope: -0.8770345661386394, fluctuations: 0.0\n",
      "step: 25120 loss: 712.142927 time elapsed: 30.9737 learning rate: 0.001658, scenario: 0, slope: -0.8591863375345288, fluctuations: 0.0\n",
      "step: 25130 loss: 704.398560 time elapsed: 30.9873 learning rate: 0.001658, scenario: 0, slope: -0.8425585735696256, fluctuations: 0.0\n",
      "step: 25140 loss: 696.776153 time elapsed: 31.0016 learning rate: 0.001658, scenario: 0, slope: -0.8268834144310491, fluctuations: 0.0\n",
      "step: 25150 loss: 689.309374 time elapsed: 31.0160 learning rate: 0.001658, scenario: 0, slope: -0.811977209956963, fluctuations: 0.0\n",
      "step: 25160 loss: 681.966838 time elapsed: 31.0291 learning rate: 0.001658, scenario: 0, slope: -0.797748883464495, fluctuations: 0.0\n",
      "step: 25170 loss: 674.748102 time elapsed: 31.0416 learning rate: 0.001658, scenario: 0, slope: -0.7841506586991802, fluctuations: 0.0\n",
      "step: 25180 loss: 667.650734 time elapsed: 31.0557 learning rate: 0.001658, scenario: 0, slope: -0.771130746889335, fluctuations: 0.0\n",
      "step: 25190 loss: 660.669594 time elapsed: 31.0704 learning rate: 0.001658, scenario: 0, slope: -0.7582349697105604, fluctuations: 0.0\n",
      "step: 25200 loss: 653.802352 time elapsed: 31.0844 learning rate: 0.001658, scenario: 0, slope: -0.7436699646334985, fluctuations: 0.0\n",
      "step: 25210 loss: 647.053158 time elapsed: 31.0984 learning rate: 0.001658, scenario: 0, slope: -0.7286459128815707, fluctuations: 0.0\n",
      "step: 25220 loss: 641.322078 time elapsed: 31.1106 learning rate: 0.001658, scenario: 0, slope: -0.7147163262925613, fluctuations: 0.0\n",
      "step: 25230 loss: 954.274248 time elapsed: 31.1225 learning rate: 0.003553, scenario: 1, slope: -0.4513607436172458, fluctuations: 0.0\n",
      "step: 25240 loss: 1096.530397 time elapsed: 31.1347 learning rate: 0.006544, scenario: -1, slope: 0.3147687203115016, fluctuations: 0.03\n",
      "step: 25250 loss: 642.565973 time elapsed: 31.1468 learning rate: 0.002282, scenario: -1, slope: 0.13248510908766092, fluctuations: 0.07\n",
      "step: 25260 loss: 599.851384 time elapsed: 31.1589 learning rate: 0.001704, scenario: 1, slope: -0.07570572520793899, fluctuations: 0.1\n",
      "step: 25270 loss: 594.070368 time elapsed: 31.1706 learning rate: 0.004419, scenario: 1, slope: -0.32258912143914653, fluctuations: 0.13\n",
      "step: 25280 loss: 592.019949 time elapsed: 31.1826 learning rate: 0.011461, scenario: 1, slope: -0.6062762700375262, fluctuations: 0.16\n",
      "step: 25290 loss: 6872.332787 time elapsed: 31.1945 learning rate: 0.006332, scenario: -1, slope: 17.087286455097306, fluctuations: 0.18\n",
      "step: 25300 loss: 1084.316651 time elapsed: 31.2067 learning rate: 0.002453, scenario: -1, slope: 17.809814935596023, fluctuations: 0.21\n",
      "step: 25310 loss: 1006.265248 time elapsed: 31.2191 learning rate: 0.000855, scenario: -1, slope: 14.366330288261208, fluctuations: 0.22\n",
      "step: 25320 loss: 763.576828 time elapsed: 31.2313 learning rate: 0.000298, scenario: -1, slope: 9.535292283200604, fluctuations: 0.23\n",
      "step: 25330 loss: 730.334427 time elapsed: 31.2431 learning rate: 0.000104, scenario: -1, slope: 4.332640040971756, fluctuations: 0.23\n",
      "step: 25340 loss: 720.198709 time elapsed: 31.2553 learning rate: 0.000036, scenario: -1, slope: -0.2684585554141424, fluctuations: 0.2\n",
      "step: 25350 loss: 717.878571 time elapsed: 31.2675 learning rate: 0.000038, scenario: 0, slope: -4.644987794218583, fluctuations: 0.16\n",
      "step: 25360 loss: 716.095163 time elapsed: 31.2800 learning rate: 0.000038, scenario: 0, slope: -10.205973884394638, fluctuations: 0.14\n",
      "step: 25370 loss: 714.505645 time elapsed: 31.2931 learning rate: 0.000038, scenario: 0, slope: -14.129971186005386, fluctuations: 0.1\n",
      "step: 25380 loss: 713.025840 time elapsed: 31.3069 learning rate: 0.000038, scenario: 0, slope: -19.35706881617308, fluctuations: 0.08\n",
      "step: 25390 loss: 711.620366 time elapsed: 31.3191 learning rate: 0.000038, scenario: 0, slope: -8.728751990663353, fluctuations: 0.05\n",
      "step: 25400 loss: 710.266239 time elapsed: 31.3309 learning rate: 0.000038, scenario: 0, slope: -2.5592301254900183, fluctuations: 0.03\n",
      "step: 25410 loss: 708.935929 time elapsed: 31.3434 learning rate: 0.000051, scenario: 1, slope: -0.7359252378306392, fluctuations: 0.01\n",
      "step: 25420 loss: 706.469380 time elapsed: 31.3555 learning rate: 0.000132, scenario: 1, slope: -0.3100014955043779, fluctuations: 0.0\n",
      "step: 25430 loss: 700.416531 time elapsed: 31.3677 learning rate: 0.000343, scenario: 1, slope: -0.18561198428708123, fluctuations: 0.0\n",
      "step: 25440 loss: 686.557055 time elapsed: 31.3800 learning rate: 0.000889, scenario: 1, slope: -0.2258278522132961, fluctuations: 0.0\n",
      "step: 25450 loss: 659.308450 time elapsed: 31.3920 learning rate: 0.002305, scenario: 1, slope: -0.37347914697097306, fluctuations: 0.0\n",
      "step: 25460 loss: 620.173466 time elapsed: 31.4045 learning rate: 0.005980, scenario: 1, slope: -0.666552995074328, fluctuations: 0.0\n",
      "step: 25470 loss: 584.127713 time elapsed: 31.4167 learning rate: 0.006578, scenario: 0, slope: -1.0870478110650297, fluctuations: 0.0\n",
      "step: 25480 loss: 555.587634 time elapsed: 31.4296 learning rate: 0.006578, scenario: 0, slope: -1.5460866156362547, fluctuations: 0.0\n",
      "step: 25490 loss: 531.546683 time elapsed: 31.4415 learning rate: 0.006578, scenario: 0, slope: -1.9794951343249632, fluctuations: 0.0\n",
      "step: 25500 loss: 509.939774 time elapsed: 31.4540 learning rate: 0.006578, scenario: 0, slope: -2.3078445890056427, fluctuations: 0.0\n",
      "step: 25510 loss: 489.905917 time elapsed: 31.4673 learning rate: 0.006578, scenario: 0, slope: -2.590344427009778, fluctuations: 0.0\n",
      "step: 25520 loss: 471.851872 time elapsed: 31.4813 learning rate: 0.006578, scenario: 0, slope: -2.7023662927337204, fluctuations: 0.0\n",
      "step: 25530 loss: 456.080290 time elapsed: 31.4958 learning rate: 0.006578, scenario: 0, slope: -2.6584341967386895, fluctuations: 0.0\n",
      "step: 25540 loss: 442.076999 time elapsed: 31.5111 learning rate: 0.006578, scenario: 0, slope: -2.4698452097176595, fluctuations: 0.0\n",
      "step: 25550 loss: 429.406677 time elapsed: 31.5237 learning rate: 0.006578, scenario: 0, slope: -2.194324056006992, fluctuations: 0.0\n",
      "step: 25560 loss: 417.719428 time elapsed: 31.5369 learning rate: 0.006578, scenario: 0, slope: -1.9254009268360577, fluctuations: 0.0\n",
      "step: 25570 loss: 406.839012 time elapsed: 31.5502 learning rate: 0.006578, scenario: 0, slope: -1.7170654259056173, fluctuations: 0.0\n",
      "step: 25580 loss: 396.658070 time elapsed: 31.5628 learning rate: 0.006578, scenario: 0, slope: -1.5466930518107795, fluctuations: 0.0\n",
      "step: 25590 loss: 387.080494 time elapsed: 31.5748 learning rate: 0.006578, scenario: 0, slope: -1.4008818328336936, fluctuations: 0.0\n",
      "step: 25600 loss: 378.021458 time elapsed: 31.5869 learning rate: 0.006578, scenario: 0, slope: -1.2854077859907043, fluctuations: 0.0\n",
      "step: 25610 loss: 369.412526 time elapsed: 31.5995 learning rate: 0.006578, scenario: 0, slope: -1.165593576817609, fluctuations: 0.0\n",
      "step: 25620 loss: 361.201801 time elapsed: 31.6112 learning rate: 0.006578, scenario: 0, slope: -1.0779644641209425, fluctuations: 0.0\n",
      "step: 25630 loss: 353.350331 time elapsed: 31.6229 learning rate: 0.006578, scenario: 0, slope: -1.0065799480279882, fluctuations: 0.0\n",
      "step: 25640 loss: 345.827693 time elapsed: 31.6347 learning rate: 0.006578, scenario: 0, slope: -0.946760160379536, fluctuations: 0.0\n",
      "step: 25650 loss: 338.608758 time elapsed: 31.6464 learning rate: 0.006578, scenario: 0, slope: -0.8952314692326243, fluctuations: 0.0\n",
      "step: 25660 loss: 331.671685 time elapsed: 31.6582 learning rate: 0.006578, scenario: 0, slope: -0.850004943870639, fluctuations: 0.0\n",
      "step: 25670 loss: 324.996660 time elapsed: 31.6701 learning rate: 0.006578, scenario: 0, slope: -0.8098838065760152, fluctuations: 0.0\n",
      "step: 25680 loss: 318.564991 time elapsed: 31.6822 learning rate: 0.006578, scenario: 0, slope: -0.7739068973812143, fluctuations: 0.0\n",
      "step: 25690 loss: 312.358326 time elapsed: 31.6942 learning rate: 0.006578, scenario: 0, slope: -0.7412807800271579, fluctuations: 0.0\n",
      "step: 25700 loss: 306.357474 time elapsed: 31.7066 learning rate: 0.006578, scenario: 0, slope: -0.7142906670867958, fluctuations: 0.0\n",
      "step: 25710 loss: 300.537474 time elapsed: 31.7205 learning rate: 0.006578, scenario: 0, slope: -0.6839339073758279, fluctuations: 0.0\n",
      "step: 25720 loss: 294.759519 time elapsed: 31.7344 learning rate: 0.006578, scenario: 0, slope: -0.6588100449920541, fluctuations: 0.0\n",
      "step: 25730 loss: 289.339079 time elapsed: 31.7470 learning rate: 0.006578, scenario: 0, slope: -0.6356745547710317, fluctuations: 0.0\n",
      "step: 25740 loss: 283.946543 time elapsed: 31.7589 learning rate: 0.006578, scenario: 0, slope: -0.6145751396559083, fluctuations: 0.0\n",
      "step: 25750 loss: 278.678813 time elapsed: 31.7711 learning rate: 0.006578, scenario: 0, slope: -0.5953213097392412, fluctuations: 0.0\n",
      "step: 25760 loss: 273.442486 time elapsed: 31.7833 learning rate: 0.006578, scenario: 0, slope: -0.5780602372932625, fluctuations: 0.0\n",
      "step: 25770 loss: 268.082635 time elapsed: 31.7957 learning rate: 0.006578, scenario: 0, slope: -0.563424780898109, fluctuations: 0.0\n",
      "step: 25780 loss: 262.691684 time elapsed: 31.8087 learning rate: 0.006578, scenario: 0, slope: -0.5527632254679437, fluctuations: 0.01\n",
      "step: 25790 loss: 257.274493 time elapsed: 31.8218 learning rate: 0.006578, scenario: 0, slope: -0.5446144463810331, fluctuations: 0.01\n",
      "step: 25800 loss: 251.954140 time elapsed: 31.8345 learning rate: 0.006578, scenario: 0, slope: -0.539638779760074, fluctuations: 0.01\n",
      "step: 25810 loss: 246.794866 time elapsed: 31.8477 learning rate: 0.006578, scenario: 0, slope: -0.5354194327936478, fluctuations: 0.01\n",
      "step: 25820 loss: 241.922104 time elapsed: 31.8605 learning rate: 0.006578, scenario: 0, slope: -0.5318367246965897, fluctuations: 0.01\n",
      "step: 25830 loss: 237.352344 time elapsed: 31.8732 learning rate: 0.006578, scenario: 0, slope: -0.5264738974699601, fluctuations: 0.01\n",
      "step: 25840 loss: 233.013146 time elapsed: 31.8859 learning rate: 0.006578, scenario: 0, slope: -0.5181533570620263, fluctuations: 0.01\n",
      "step: 25850 loss: 228.842832 time elapsed: 31.8995 learning rate: 0.006578, scenario: 0, slope: -0.5060662530294463, fluctuations: 0.01\n",
      "step: 25860 loss: 224.806543 time elapsed: 31.9138 learning rate: 0.006578, scenario: 0, slope: -0.4901195675264553, fluctuations: 0.01\n",
      "step: 25870 loss: 220.882597 time elapsed: 31.9289 learning rate: 0.006578, scenario: 0, slope: -0.4711640234589821, fluctuations: 0.01\n",
      "step: 25880 loss: 217.060206 time elapsed: 31.9428 learning rate: 0.006578, scenario: 0, slope: -0.4528841282760192, fluctuations: 0.0\n",
      "step: 25890 loss: 213.340720 time elapsed: 31.9556 learning rate: 0.006578, scenario: 0, slope: -0.43403615961803227, fluctuations: 0.0\n",
      "step: 25900 loss: 209.727894 time elapsed: 31.9684 learning rate: 0.006578, scenario: 0, slope: -0.41831555824656963, fluctuations: 0.0\n",
      "step: 25910 loss: 206.204221 time elapsed: 31.9819 learning rate: 0.006578, scenario: 0, slope: -0.4015713049256105, fluctuations: 0.0\n",
      "step: 25920 loss: 202.818087 time elapsed: 31.9947 learning rate: 0.006578, scenario: 0, slope: -0.38864970959316986, fluctuations: 0.0\n",
      "step: 25930 loss: 199.559718 time elapsed: 32.0070 learning rate: 0.006578, scenario: 0, slope: -0.3768781663170188, fluctuations: 0.0\n",
      "step: 25940 loss: 196.388827 time elapsed: 32.0190 learning rate: 0.006578, scenario: 0, slope: -0.3657114967068345, fluctuations: 0.0\n",
      "step: 25950 loss: 193.290486 time elapsed: 32.0308 learning rate: 0.006578, scenario: 0, slope: -0.35489637213881, fluctuations: 0.0\n",
      "step: 25960 loss: 190.258220 time elapsed: 32.0428 learning rate: 0.006578, scenario: 0, slope: -0.3444079669942422, fluctuations: 0.0\n",
      "step: 25970 loss: 187.286778 time elapsed: 32.0548 learning rate: 0.006578, scenario: 0, slope: -0.3343435207643239, fluctuations: 0.0\n",
      "step: 25980 loss: 184.372808 time elapsed: 32.0668 learning rate: 0.006578, scenario: 0, slope: -0.3248596536697132, fluctuations: 0.0\n",
      "step: 25990 loss: 181.515591 time elapsed: 32.0787 learning rate: 0.006578, scenario: 0, slope: -0.31609412267695736, fluctuations: 0.0\n",
      "step: 26000 loss: 178.716612 time elapsed: 32.0899 learning rate: 0.006578, scenario: 0, slope: -0.3088642772564171, fluctuations: 0.0\n",
      "step: 26010 loss: 175.975869 time elapsed: 32.1027 learning rate: 0.006578, scenario: 0, slope: -0.30090411669474754, fluctuations: 0.0\n",
      "step: 26020 loss: 173.279840 time elapsed: 32.1143 learning rate: 0.006578, scenario: 0, slope: -0.2945303914456739, fluctuations: 0.0\n",
      "step: 26030 loss: 170.787923 time elapsed: 32.1268 learning rate: 0.006578, scenario: 0, slope: -0.28829958370838354, fluctuations: 0.0\n",
      "step: 26040 loss: 168.019475 time elapsed: 32.1399 learning rate: 0.006578, scenario: 0, slope: -0.28256906015306144, fluctuations: 0.0\n",
      "step: 26050 loss: 165.316254 time elapsed: 32.1526 learning rate: 0.006578, scenario: 0, slope: -0.27787554611356097, fluctuations: 0.0\n",
      "step: 26060 loss: 162.600320 time elapsed: 32.1655 learning rate: 0.006578, scenario: 0, slope: -0.2741326922552368, fluctuations: 0.0\n",
      "step: 26070 loss: 159.945002 time elapsed: 32.1782 learning rate: 0.006578, scenario: 0, slope: -0.27130407280086477, fluctuations: 0.0\n",
      "step: 26080 loss: 157.361572 time elapsed: 32.1899 learning rate: 0.006578, scenario: 0, slope: -0.2690236443075588, fluctuations: 0.0\n",
      "step: 26090 loss: 154.891559 time elapsed: 32.2018 learning rate: 0.006578, scenario: 0, slope: -0.2667891880898902, fluctuations: 0.0\n",
      "step: 26100 loss: 152.530279 time elapsed: 32.2141 learning rate: 0.006578, scenario: 0, slope: -0.2643776836489353, fluctuations: 0.0\n",
      "step: 26110 loss: 150.268415 time elapsed: 32.2262 learning rate: 0.006578, scenario: 0, slope: -0.2603903127424293, fluctuations: 0.0\n",
      "step: 26120 loss: 148.102175 time elapsed: 32.2381 learning rate: 0.006578, scenario: 0, slope: -0.2554247233992652, fluctuations: 0.0\n",
      "step: 26130 loss: 146.017764 time elapsed: 32.2498 learning rate: 0.006578, scenario: 0, slope: -0.24873589565885706, fluctuations: 0.0\n",
      "step: 26140 loss: 144.003747 time elapsed: 32.2614 learning rate: 0.006578, scenario: 0, slope: -0.24058199263069796, fluctuations: 0.0\n",
      "step: 26150 loss: 142.050675 time elapsed: 32.2733 learning rate: 0.006578, scenario: 0, slope: -0.23186057173233832, fluctuations: 0.0\n",
      "step: 26160 loss: 140.150450 time elapsed: 32.2867 learning rate: 0.006578, scenario: 0, slope: -0.22298878944212677, fluctuations: 0.0\n",
      "step: 26170 loss: 138.073429 time elapsed: 32.3002 learning rate: 0.006578, scenario: 0, slope: -0.21476157628005346, fluctuations: 0.0\n",
      "step: 26180 loss: 138.569063 time elapsed: 32.3145 learning rate: 0.006578, scenario: 0, slope: -0.20475810788859253, fluctuations: 0.04\n",
      "step: 26190 loss: 135.383992 time elapsed: 32.3290 learning rate: 0.006578, scenario: 0, slope: -0.19759729173425744, fluctuations: 0.09\n",
      "step: 26200 loss: 133.217577 time elapsed: 32.3433 learning rate: 0.006578, scenario: 0, slope: -0.19112806133985372, fluctuations: 0.12\n",
      "step: 26210 loss: 131.445800 time elapsed: 32.3565 learning rate: 0.006578, scenario: 0, slope: -0.18450743407798878, fluctuations: 0.12\n",
      "step: 26220 loss: 129.775837 time elapsed: 32.3696 learning rate: 0.006578, scenario: 0, slope: -0.18024798134685466, fluctuations: 0.12\n",
      "step: 26230 loss: 128.165935 time elapsed: 32.3830 learning rate: 0.006578, scenario: 0, slope: -0.17675513301080809, fluctuations: 0.12\n",
      "step: 26240 loss: 126.594907 time elapsed: 32.3970 learning rate: 0.006578, scenario: 0, slope: -0.1736827455670921, fluctuations: 0.12\n",
      "step: 26250 loss: 125.054758 time elapsed: 32.4102 learning rate: 0.006578, scenario: 0, slope: -0.17085644655191645, fluctuations: 0.12\n",
      "step: 26260 loss: 123.546512 time elapsed: 32.4228 learning rate: 0.006578, scenario: 0, slope: -0.1681721887977677, fluctuations: 0.12\n",
      "step: 26270 loss: 122.068064 time elapsed: 32.4360 learning rate: 0.006578, scenario: 0, slope: -0.16586448445683258, fluctuations: 0.12\n",
      "step: 26280 loss: 120.617531 time elapsed: 32.4495 learning rate: 0.006578, scenario: 0, slope: -0.16098215651682388, fluctuations: 0.07\n",
      "step: 26290 loss: 119.198588 time elapsed: 32.4624 learning rate: 0.006578, scenario: 0, slope: -0.15680385229999583, fluctuations: 0.02\n",
      "step: 26300 loss: 117.843784 time elapsed: 32.4749 learning rate: 0.006578, scenario: 0, slope: -0.15332453642764998, fluctuations: 0.0\n",
      "step: 26310 loss: 116.427209 time elapsed: 32.4883 learning rate: 0.006578, scenario: 0, slope: -0.14942629120022105, fluctuations: 0.0\n",
      "step: 26320 loss: 115.075572 time elapsed: 32.5006 learning rate: 0.006578, scenario: 0, slope: -0.14643687792690377, fluctuations: 0.0\n",
      "step: 26330 loss: 113.379222 time elapsed: 32.5134 learning rate: 0.006578, scenario: 0, slope: -0.14418169392733868, fluctuations: 0.0\n",
      "step: 26340 loss: 139.638573 time elapsed: 32.5277 learning rate: 0.010594, scenario: 1, slope: -0.10304502697680808, fluctuations: 0.03\n",
      "step: 26350 loss: 246.797017 time elapsed: 32.5421 learning rate: 0.007153, scenario: -1, slope: 0.4085580383631256, fluctuations: 0.04\n",
      "step: 26360 loss: 122.106220 time elapsed: 32.5568 learning rate: 0.002494, scenario: -1, slope: 0.3755963171602651, fluctuations: 0.08\n",
      "step: 26370 loss: 112.041150 time elapsed: 32.5712 learning rate: 0.000870, scenario: -1, slope: 0.2680518147414991, fluctuations: 0.1\n",
      "step: 26380 loss: 110.015633 time elapsed: 32.5844 learning rate: 0.000303, scenario: -1, slope: 0.14688691256442127, fluctuations: 0.12\n",
      "step: 26390 loss: 109.561560 time elapsed: 32.5967 learning rate: 0.000106, scenario: -1, slope: 0.016526432036826307, fluctuations: 0.13\n",
      "step: 26400 loss: 109.354004 time elapsed: 32.6086 learning rate: 0.000196, scenario: 1, slope: -0.0978153153946862, fluctuations: 0.13\n",
      "step: 26410 loss: 109.274439 time elapsed: 32.6215 learning rate: 0.000196, scenario: 0, slope: -0.22834899159865954, fluctuations: 0.14\n",
      "step: 26420 loss: 109.218848 time elapsed: 32.6334 learning rate: 0.000196, scenario: 0, slope: -0.3601517933799623, fluctuations: 0.14\n",
      "step: 26430 loss: 109.157471 time elapsed: 32.6453 learning rate: 0.000196, scenario: 0, slope: -0.5212490887515199, fluctuations: 0.14\n",
      "step: 26440 loss: 109.099361 time elapsed: 32.6574 learning rate: 0.000196, scenario: 0, slope: -0.5782757906776939, fluctuations: 0.11\n",
      "step: 26450 loss: 109.044233 time elapsed: 32.6693 learning rate: 0.000215, scenario: 0, slope: -0.14610135853966924, fluctuations: 0.09\n",
      "step: 26460 loss: 108.958376 time elapsed: 32.6813 learning rate: 0.000558, scenario: 1, slope: -0.03961976722234933, fluctuations: 0.05\n",
      "step: 26470 loss: 108.748480 time elapsed: 32.6930 learning rate: 0.001448, scenario: 1, slope: -0.013357319194280204, fluctuations: 0.03\n",
      "step: 26480 loss: 108.263812 time elapsed: 32.7050 learning rate: 0.003757, scenario: 1, slope: -0.009348468778657045, fluctuations: 0.02\n",
      "step: 26490 loss: 107.164308 time elapsed: 32.7170 learning rate: 0.009744, scenario: 1, slope: -0.014320776120296069, fluctuations: 0.01\n",
      "step: 26500 loss: 26805.474321 time elapsed: 32.7286 learning rate: 0.019938, scenario: -1, slope: 0.4992268300245673, fluctuations: 0.01\n",
      "step: 26510 loss: 25473.641179 time elapsed: 32.7413 learning rate: 0.006952, scenario: -1, slope: 153.45952789864066, fluctuations: 0.03\n",
      "step: 26520 loss: 7707.447665 time elapsed: 32.7554 learning rate: 0.002424, scenario: -1, slope: 183.80059864134958, fluctuations: 0.05\n",
      "step: 26530 loss: 4820.663470 time elapsed: 32.7704 learning rate: 0.000845, scenario: -1, slope: 166.07774782995673, fluctuations: 0.06\n",
      "step: 26540 loss: 3888.291021 time elapsed: 32.7846 learning rate: 0.000295, scenario: -1, slope: 128.7097671899986, fluctuations: 0.06\n",
      "step: 26550 loss: 3731.319975 time elapsed: 32.7976 learning rate: 0.000103, scenario: -1, slope: 88.53414034803197, fluctuations: 0.06\n",
      "step: 26560 loss: 3662.104342 time elapsed: 32.8110 learning rate: 0.000036, scenario: -1, slope: 45.346963825235306, fluctuations: 0.06\n",
      "step: 26570 loss: 3639.389591 time elapsed: 32.8240 learning rate: 0.000014, scenario: 0, slope: -2.273783672574119, fluctuations: 0.06\n",
      "step: 26580 loss: 3628.043788 time elapsed: 32.8366 learning rate: 0.000014, scenario: 0, slope: -56.516350857674006, fluctuations: 0.06\n",
      "step: 26590 loss: 3617.299888 time elapsed: 32.8491 learning rate: 0.000014, scenario: 0, slope: -120.53613876877398, fluctuations: 0.06\n",
      "step: 26600 loss: 3607.091595 time elapsed: 32.8614 learning rate: 0.000014, scenario: 0, slope: -189.68676272545923, fluctuations: 0.06\n",
      "step: 26610 loss: 3597.356820 time elapsed: 32.8742 learning rate: 0.000014, scenario: 0, slope: -67.36082636134951, fluctuations: 0.02\n",
      "step: 26620 loss: 3588.040141 time elapsed: 32.8859 learning rate: 0.000014, scenario: 0, slope: -14.743249798324644, fluctuations: 0.01\n",
      "step: 26630 loss: 3579.093053 time elapsed: 32.8978 learning rate: 0.000017, scenario: 1, slope: -4.070078423258818, fluctuations: 0.0\n",
      "step: 26640 loss: 3564.124633 time elapsed: 32.9097 learning rate: 0.000044, scenario: 1, slope: -1.9040478118437882, fluctuations: 0.0\n",
      "step: 26650 loss: 3528.402496 time elapsed: 32.9217 learning rate: 0.000113, scenario: 1, slope: -1.3045699770518073, fluctuations: 0.0\n",
      "step: 26660 loss: 3449.906186 time elapsed: 32.9335 learning rate: 0.000293, scenario: 1, slope: -1.4415556101897686, fluctuations: 0.0\n",
      "step: 26670 loss: 3290.010517 time elapsed: 32.9455 learning rate: 0.000760, scenario: 1, slope: -2.25857305511799, fluctuations: 0.0\n",
      "step: 26680 loss: 2987.834460 time elapsed: 32.9570 learning rate: 0.001481, scenario: 0, slope: -4.1181416048408535, fluctuations: 0.0\n",
      "step: 26690 loss: 2673.111522 time elapsed: 32.9704 learning rate: 0.001481, scenario: 0, slope: -7.227419661993028, fluctuations: 0.0\n",
      "step: 26700 loss: 2426.532983 time elapsed: 32.9838 learning rate: 0.001481, scenario: 0, slope: -10.543315217176007, fluctuations: 0.0\n",
      "step: 26710 loss: 2234.819668 time elapsed: 32.9975 learning rate: 0.001481, scenario: 0, slope: -14.533142288393393, fluctuations: 0.0\n",
      "step: 26720 loss: 2079.580604 time elapsed: 33.0097 learning rate: 0.001481, scenario: 0, slope: -17.58636160546888, fluctuations: 0.0\n",
      "step: 26730 loss: 1948.682972 time elapsed: 33.0218 learning rate: 0.001481, scenario: 0, slope: -19.72456700560965, fluctuations: 0.0\n",
      "step: 26740 loss: 1832.626955 time elapsed: 33.0342 learning rate: 0.001481, scenario: 0, slope: -20.700225327413047, fluctuations: 0.0\n",
      "step: 26750 loss: 1728.744478 time elapsed: 33.0460 learning rate: 0.001481, scenario: 0, slope: -20.404408274777953, fluctuations: 0.0\n",
      "step: 26760 loss: 1637.445043 time elapsed: 33.0579 learning rate: 0.001481, scenario: 0, slope: -18.874351685079162, fluctuations: 0.0\n",
      "step: 26770 loss: 1556.237155 time elapsed: 33.0696 learning rate: 0.001481, scenario: 0, slope: -16.405864427897754, fluctuations: 0.0\n",
      "step: 26780 loss: 1482.068950 time elapsed: 33.0812 learning rate: 0.001481, scenario: 0, slope: -13.747910854957011, fluctuations: 0.0\n",
      "step: 26790 loss: 1410.593158 time elapsed: 33.0928 learning rate: 0.001481, scenario: 0, slope: -11.698714742161332, fluctuations: 0.0\n",
      "step: 26800 loss: 1335.359607 time elapsed: 33.1046 learning rate: 0.001481, scenario: 0, slope: -10.367024583773777, fluctuations: 0.0\n",
      "step: 26810 loss: 1277.852038 time elapsed: 33.1169 learning rate: 0.001481, scenario: 0, slope: -9.170068977613305, fluctuations: 0.0\n",
      "step: 26820 loss: 1228.984917 time elapsed: 33.1285 learning rate: 0.001481, scenario: 0, slope: -8.290597333481172, fluctuations: 0.0\n",
      "step: 26830 loss: 1184.780701 time elapsed: 33.1406 learning rate: 0.001481, scenario: 0, slope: -7.51634644218534, fluctuations: 0.0\n",
      "step: 26840 loss: 1144.987191 time elapsed: 33.1528 learning rate: 0.001481, scenario: 0, slope: -6.810913090430216, fluctuations: 0.0\n",
      "step: 26850 loss: 1108.559947 time elapsed: 33.1648 learning rate: 0.001481, scenario: 0, slope: -6.169423485495815, fluctuations: 0.0\n",
      "step: 26860 loss: 1075.147228 time elapsed: 33.1794 learning rate: 0.001481, scenario: 0, slope: -5.57591443976191, fluctuations: 0.0\n",
      "step: 26870 loss: 1044.270766 time elapsed: 33.1941 learning rate: 0.001481, scenario: 0, slope: -5.013699886729636, fluctuations: 0.0\n",
      "step: 26880 loss: 1015.680555 time elapsed: 33.2071 learning rate: 0.001481, scenario: 0, slope: -4.480301944547829, fluctuations: 0.0\n",
      "step: 26890 loss: 988.799120 time elapsed: 33.2203 learning rate: 0.001481, scenario: 0, slope: -3.9885278466649914, fluctuations: 0.0\n",
      "step: 26900 loss: 963.376184 time elapsed: 33.2331 learning rate: 0.001481, scenario: 0, slope: -3.6266235080240765, fluctuations: 0.0\n",
      "step: 26910 loss: 939.213069 time elapsed: 33.2475 learning rate: 0.001481, scenario: 0, slope: -3.2974964494601826, fluctuations: 0.0\n",
      "step: 26920 loss: 916.351468 time elapsed: 33.2602 learning rate: 0.001481, scenario: 0, slope: -3.051976914284258, fluctuations: 0.0\n",
      "step: 26930 loss: 894.519064 time elapsed: 33.2731 learning rate: 0.001481, scenario: 0, slope: -2.8426558502690193, fluctuations: 0.0\n",
      "step: 26940 loss: 873.556058 time elapsed: 33.2864 learning rate: 0.001481, scenario: 0, slope: -2.663819571758486, fluctuations: 0.0\n",
      "step: 26950 loss: 853.365900 time elapsed: 33.2993 learning rate: 0.001481, scenario: 0, slope: -2.510705826703964, fluctuations: 0.0\n",
      "step: 26960 loss: 833.854500 time elapsed: 33.3120 learning rate: 0.001481, scenario: 0, slope: -2.3792584608543175, fluctuations: 0.0\n",
      "step: 26970 loss: 814.954496 time elapsed: 33.3240 learning rate: 0.001481, scenario: 0, slope: -2.2651427779330824, fluctuations: 0.0\n",
      "step: 26980 loss: 796.690673 time elapsed: 33.3364 learning rate: 0.001481, scenario: 0, slope: -2.1654089488443033, fluctuations: 0.0\n",
      "step: 26990 loss: 778.930629 time elapsed: 33.3485 learning rate: 0.001481, scenario: 0, slope: -2.077621635208954, fluctuations: 0.0\n",
      "step: 27000 loss: 761.866470 time elapsed: 33.3607 learning rate: 0.001481, scenario: 0, slope: -2.0060371848154954, fluctuations: 0.0\n",
      "step: 27010 loss: 745.305317 time elapsed: 33.3741 learning rate: 0.001481, scenario: 0, slope: -1.9273824867674705, fluctuations: 0.0\n",
      "step: 27020 loss: 729.208109 time elapsed: 33.3878 learning rate: 0.001481, scenario: 0, slope: -1.862043492509431, fluctuations: 0.0\n",
      "step: 27030 loss: 713.549801 time elapsed: 33.4018 learning rate: 0.001481, scenario: 0, slope: -1.801353065357966, fluctuations: 0.0\n",
      "step: 27040 loss: 698.307982 time elapsed: 33.4145 learning rate: 0.001481, scenario: 0, slope: -1.744464639741032, fluctuations: 0.0\n",
      "step: 27050 loss: 683.502318 time elapsed: 33.4267 learning rate: 0.001481, scenario: 0, slope: -1.6908040118263978, fluctuations: 0.0\n",
      "step: 27060 loss: 669.116074 time elapsed: 33.4388 learning rate: 0.001481, scenario: 0, slope: -1.6396534504410527, fluctuations: 0.0\n",
      "step: 27070 loss: 655.084560 time elapsed: 33.4508 learning rate: 0.001481, scenario: 0, slope: -1.5914947960137766, fluctuations: 0.0\n",
      "step: 27080 loss: 641.495202 time elapsed: 33.4626 learning rate: 0.001481, scenario: 0, slope: -1.5455703292174676, fluctuations: 0.0\n",
      "step: 27090 loss: 628.314503 time elapsed: 33.4745 learning rate: 0.001481, scenario: 0, slope: -1.502282397704329, fluctuations: 0.0\n",
      "step: 27100 loss: 615.514071 time elapsed: 33.4861 learning rate: 0.001481, scenario: 0, slope: -1.4644169057501175, fluctuations: 0.0\n",
      "step: 27110 loss: 603.106615 time elapsed: 33.4986 learning rate: 0.001481, scenario: 0, slope: -1.4191038314037572, fluctuations: 0.0\n",
      "step: 27120 loss: 591.089966 time elapsed: 33.5106 learning rate: 0.001481, scenario: 0, slope: -1.378520369757455, fluctuations: 0.0\n",
      "step: 27130 loss: 579.598355 time elapsed: 33.5225 learning rate: 0.001481, scenario: 0, slope: -1.338180262246025, fluctuations: 0.0\n",
      "step: 27140 loss: 568.288556 time elapsed: 33.5349 learning rate: 0.001481, scenario: 0, slope: -1.2977860954601743, fluctuations: 0.0\n",
      "step: 27150 loss: 557.504345 time elapsed: 33.5465 learning rate: 0.001481, scenario: 0, slope: -1.2579082505366848, fluctuations: 0.0\n",
      "step: 27160 loss: 547.136169 time elapsed: 33.5582 learning rate: 0.001481, scenario: 0, slope: -1.2176265764875251, fluctuations: 0.0\n",
      "step: 27170 loss: 537.154137 time elapsed: 33.5702 learning rate: 0.001481, scenario: 0, slope: -1.177509145756838, fluctuations: 0.0\n",
      "step: 27180 loss: 527.540242 time elapsed: 33.5827 learning rate: 0.001481, scenario: 0, slope: -1.1374908074059742, fluctuations: 0.0\n",
      "step: 27190 loss: 518.273591 time elapsed: 33.5969 learning rate: 0.001481, scenario: 0, slope: -1.0977137798894905, fluctuations: 0.0\n",
      "step: 27200 loss: 509.332265 time elapsed: 33.6119 learning rate: 0.001481, scenario: 0, slope: -1.0623654080523193, fluctuations: 0.0\n",
      "step: 27210 loss: 500.699040 time elapsed: 33.6255 learning rate: 0.001481, scenario: 0, slope: -1.020159744418276, fluctuations: 0.0\n",
      "step: 27220 loss: 492.355611 time elapsed: 33.6387 learning rate: 0.001481, scenario: 0, slope: -0.9831681537945218, fluctuations: 0.0\n",
      "step: 27230 loss: 484.286313 time elapsed: 33.6522 learning rate: 0.001481, scenario: 0, slope: -0.9476431453117907, fluctuations: 0.0\n",
      "step: 27240 loss: 476.477005 time elapsed: 33.6660 learning rate: 0.001481, scenario: 0, slope: -0.9134439426186755, fluctuations: 0.0\n",
      "step: 27250 loss: 468.914873 time elapsed: 33.6794 learning rate: 0.001481, scenario: 0, slope: -0.8814430573633198, fluctuations: 0.0\n",
      "step: 27260 loss: 461.590416 time elapsed: 33.6921 learning rate: 0.001481, scenario: 0, slope: -0.8512692230611854, fluctuations: 0.0\n",
      "step: 27270 loss: 454.628815 time elapsed: 33.7044 learning rate: 0.001481, scenario: 0, slope: -0.8224637221273154, fluctuations: 0.0\n",
      "step: 27280 loss: 447.604253 time elapsed: 33.7172 learning rate: 0.001481, scenario: 0, slope: -0.7953271537494638, fluctuations: 0.0\n",
      "step: 27290 loss: 440.931748 time elapsed: 33.7304 learning rate: 0.001481, scenario: 0, slope: -0.7697397811653577, fluctuations: 0.0\n",
      "step: 27300 loss: 434.453453 time elapsed: 33.7422 learning rate: 0.001481, scenario: 0, slope: -0.7478420760469922, fluctuations: 0.0\n",
      "step: 27310 loss: 428.161573 time elapsed: 33.7551 learning rate: 0.001481, scenario: 0, slope: -0.722355021383634, fluctuations: 0.0\n",
      "step: 27320 loss: 422.052882 time elapsed: 33.7670 learning rate: 0.001481, scenario: 0, slope: -0.7002922747343167, fluctuations: 0.0\n",
      "step: 27330 loss: 416.121849 time elapsed: 33.7787 learning rate: 0.001481, scenario: 0, slope: -0.6791967730211406, fluctuations: 0.0\n",
      "step: 27340 loss: 410.359266 time elapsed: 33.7911 learning rate: 0.001481, scenario: 0, slope: -0.6589994034156542, fluctuations: 0.0\n",
      "step: 27350 loss: 404.759881 time elapsed: 33.8046 learning rate: 0.001481, scenario: 0, slope: -0.6396403083735487, fluctuations: 0.0\n",
      "step: 27360 loss: 399.317668 time elapsed: 33.8185 learning rate: 0.001481, scenario: 0, slope: -0.6210641986934381, fluctuations: 0.0\n",
      "step: 27370 loss: 394.027018 time elapsed: 33.8317 learning rate: 0.001481, scenario: 0, slope: -0.6028950121797892, fluctuations: 0.0\n",
      "step: 27380 loss: 388.882874 time elapsed: 33.8438 learning rate: 0.001481, scenario: 0, slope: -0.5854112280518888, fluctuations: 0.0\n",
      "step: 27390 loss: 383.956645 time elapsed: 33.8560 learning rate: 0.001481, scenario: 0, slope: -0.5685264409633515, fluctuations: 0.0\n",
      "step: 27400 loss: 379.014593 time elapsed: 33.8681 learning rate: 0.001481, scenario: 0, slope: -0.5539682797892621, fluctuations: 0.0\n",
      "step: 27410 loss: 374.280261 time elapsed: 33.8809 learning rate: 0.001481, scenario: 0, slope: -0.5369379596884712, fluctuations: 0.0\n",
      "step: 27420 loss: 369.665227 time elapsed: 33.8931 learning rate: 0.001481, scenario: 0, slope: -0.5220938222205903, fluctuations: 0.0\n",
      "step: 27430 loss: 365.169430 time elapsed: 33.9053 learning rate: 0.001481, scenario: 0, slope: -0.5078146769606302, fluctuations: 0.0\n",
      "step: 27440 loss: 360.791261 time elapsed: 33.9174 learning rate: 0.001481, scenario: 0, slope: -0.4940815636886922, fluctuations: 0.0\n",
      "step: 27450 loss: 356.522677 time elapsed: 33.9293 learning rate: 0.001481, scenario: 0, slope: -0.48088293491543516, fluctuations: 0.0\n",
      "step: 27460 loss: 352.360010 time elapsed: 33.9414 learning rate: 0.001481, scenario: 0, slope: -0.4682072620848071, fluctuations: 0.0\n",
      "step: 27470 loss: 348.298880 time elapsed: 33.9529 learning rate: 0.001481, scenario: 0, slope: -0.4560421814107448, fluctuations: 0.0\n",
      "step: 27480 loss: 344.335246 time elapsed: 33.9640 learning rate: 0.001481, scenario: 0, slope: -0.44437224930280667, fluctuations: 0.0\n",
      "step: 27490 loss: 340.480657 time elapsed: 33.9754 learning rate: 0.001481, scenario: 0, slope: -0.4329749791385184, fluctuations: 0.0\n",
      "step: 27500 loss: 336.700437 time elapsed: 33.9874 learning rate: 0.001481, scenario: 0, slope: -0.42300347665830973, fluctuations: 0.0\n",
      "step: 27510 loss: 332.993848 time elapsed: 34.0003 learning rate: 0.001481, scenario: 0, slope: -0.4114995414142606, fluctuations: 0.0\n",
      "step: 27520 loss: 329.380449 time elapsed: 34.0150 learning rate: 0.001481, scenario: 0, slope: -0.401556740248313, fluctuations: 0.0\n",
      "step: 27530 loss: 325.847805 time elapsed: 34.0299 learning rate: 0.001481, scenario: 0, slope: -0.3920311435025581, fluctuations: 0.0\n",
      "step: 27540 loss: 322.390066 time elapsed: 34.0433 learning rate: 0.001481, scenario: 0, slope: -0.38290060907529017, fluctuations: 0.0\n",
      "step: 27550 loss: 319.005624 time elapsed: 34.0563 learning rate: 0.001481, scenario: 0, slope: -0.3741474923294466, fluctuations: 0.0\n",
      "step: 27560 loss: 315.690559 time elapsed: 34.0700 learning rate: 0.001481, scenario: 0, slope: -0.3657567754342675, fluctuations: 0.0\n",
      "step: 27570 loss: 312.442384 time elapsed: 34.0832 learning rate: 0.001481, scenario: 0, slope: -0.35771610070385673, fluctuations: 0.0\n",
      "step: 27580 loss: 309.261018 time elapsed: 34.0961 learning rate: 0.001481, scenario: 0, slope: -0.3500085336635443, fluctuations: 0.0\n",
      "step: 27590 loss: 306.144546 time elapsed: 34.1089 learning rate: 0.001481, scenario: 0, slope: -0.3423803068467067, fluctuations: 0.0\n",
      "step: 27600 loss: 303.034150 time elapsed: 34.1211 learning rate: 0.001972, scenario: 1, slope: -0.33581130838010664, fluctuations: 0.0\n",
      "step: 27610 loss: 312.594423 time elapsed: 34.1344 learning rate: 0.005114, scenario: 1, slope: -0.30840683094409566, fluctuations: 0.02\n",
      "step: 27620 loss: 1652.543138 time elapsed: 34.1474 learning rate: 0.009418, scenario: -1, slope: 1.0286823931051938, fluctuations: 0.04\n",
      "step: 27630 loss: 658.709896 time elapsed: 34.1598 learning rate: 0.003284, scenario: -1, slope: 1.7343769165542517, fluctuations: 0.08\n",
      "step: 27640 loss: 317.457494 time elapsed: 34.1724 learning rate: 0.001145, scenario: -1, slope: 1.573020467932227, fluctuations: 0.11\n",
      "step: 27650 loss: 309.241272 time elapsed: 34.1844 learning rate: 0.000399, scenario: -1, slope: 1.0983326302433154, fluctuations: 0.12\n",
      "step: 27660 loss: 297.781846 time elapsed: 34.1960 learning rate: 0.000139, scenario: -1, slope: 0.5958674880251859, fluctuations: 0.13\n",
      "step: 27670 loss: 297.394193 time elapsed: 34.2087 learning rate: 0.000049, scenario: -1, slope: 0.11266286548269541, fluctuations: 0.14\n",
      "step: 27680 loss: 296.352289 time elapsed: 34.2223 learning rate: 0.000067, scenario: 0, slope: -0.39528304503162376, fluctuations: 0.14\n",
      "step: 27690 loss: 295.876246 time elapsed: 34.2360 learning rate: 0.000067, scenario: 0, slope: -0.9339445725717218, fluctuations: 0.14\n",
      "step: 27700 loss: 295.419711 time elapsed: 34.2490 learning rate: 0.000067, scenario: 0, slope: -1.5101758485575068, fluctuations: 0.14\n",
      "step: 27710 loss: 294.960639 time elapsed: 34.2615 learning rate: 0.000067, scenario: 0, slope: -2.032147388986237, fluctuations: 0.11\n",
      "step: 27720 loss: 294.558708 time elapsed: 34.2736 learning rate: 0.000067, scenario: 0, slope: -2.5873157948902143, fluctuations: 0.09\n",
      "step: 27730 loss: 294.159990 time elapsed: 34.2858 learning rate: 0.000067, scenario: 0, slope: -0.5329348906157149, fluctuations: 0.05\n",
      "step: 27740 loss: 293.679290 time elapsed: 34.2992 learning rate: 0.000143, scenario: 1, slope: -0.1547464990459518, fluctuations: 0.03\n",
      "step: 27750 loss: 292.534339 time elapsed: 34.3116 learning rate: 0.000372, scenario: 1, slope: -0.07806204453576424, fluctuations: 0.01\n",
      "step: 27760 loss: 289.837430 time elapsed: 34.3238 learning rate: 0.000964, scenario: 1, slope: -0.06066871281472684, fluctuations: 0.01\n",
      "step: 27770 loss: 284.117668 time elapsed: 34.3365 learning rate: 0.002500, scenario: 1, slope: -0.08440995147463319, fluctuations: 0.0\n",
      "step: 27780 loss: 273.480585 time elapsed: 34.3485 learning rate: 0.006484, scenario: 1, slope: -0.14680627666823845, fluctuations: 0.0\n",
      "step: 27790 loss: 254.991671 time elapsed: 34.3602 learning rate: 0.016818, scenario: 1, slope: -0.2724068792497691, fluctuations: 0.0\n",
      "step: 27800 loss: 18502.497956 time elapsed: 34.3722 learning rate: 0.015734, scenario: -1, slope: 20.141678365394554, fluctuations: 0.0\n",
      "step: 27810 loss: 3417.794149 time elapsed: 34.3841 learning rate: 0.005486, scenario: -1, slope: 46.77152419285625, fluctuations: 0.03\n",
      "step: 27820 loss: 1192.788127 time elapsed: 34.3962 learning rate: 0.001913, scenario: -1, slope: 44.4405068872241, fluctuations: 0.06\n",
      "step: 27830 loss: 742.235406 time elapsed: 34.4080 learning rate: 0.000667, scenario: -1, slope: 34.0057118702945, fluctuations: 0.07\n",
      "step: 27840 loss: 673.470512 time elapsed: 34.4222 learning rate: 0.000233, scenario: -1, slope: 21.539312891265816, fluctuations: 0.07\n",
      "step: 27850 loss: 645.420195 time elapsed: 34.4369 learning rate: 0.000081, scenario: -1, slope: 9.551116491781745, fluctuations: 0.07\n",
      "step: 27860 loss: 637.774270 time elapsed: 34.4515 learning rate: 0.000041, scenario: 0, slope: -2.428508728867291, fluctuations: 0.07\n",
      "step: 27870 loss: 634.302348 time elapsed: 34.4644 learning rate: 0.000041, scenario: 0, slope: -14.997578983847305, fluctuations: 0.07\n",
      "step: 27880 loss: 631.568257 time elapsed: 34.4772 learning rate: 0.000041, scenario: 0, slope: -28.90355509105789, fluctuations: 0.07\n",
      "step: 27890 loss: 629.059361 time elapsed: 34.4896 learning rate: 0.000041, scenario: 0, slope: -45.145581897705796, fluctuations: 0.07\n",
      "step: 27900 loss: 626.626051 time elapsed: 34.5024 learning rate: 0.000041, scenario: 0, slope: -47.475765192133395, fluctuations: 0.06\n",
      "step: 27910 loss: 624.251695 time elapsed: 34.5164 learning rate: 0.000041, scenario: 0, slope: -6.767788928959086, fluctuations: 0.03\n",
      "step: 27920 loss: 621.931228 time elapsed: 34.5295 learning rate: 0.000041, scenario: 0, slope: -1.7366634768035925, fluctuations: 0.01\n",
      "step: 27930 loss: 619.632564 time elapsed: 34.5424 learning rate: 0.000054, scenario: 1, slope: -0.6476730539907185, fluctuations: 0.0\n",
      "step: 27940 loss: 615.339575 time elapsed: 34.5548 learning rate: 0.000141, scenario: 1, slope: -0.34727554738415106, fluctuations: 0.0\n",
      "step: 27950 loss: 604.695329 time elapsed: 34.5676 learning rate: 0.000367, scenario: 1, slope: -0.2934958646855217, fluctuations: 0.0\n",
      "step: 27960 loss: 579.905879 time elapsed: 34.5804 learning rate: 0.000951, scenario: 1, slope: -0.38494584615975563, fluctuations: 0.0\n",
      "step: 27970 loss: 528.781333 time elapsed: 34.5929 learning rate: 0.002242, scenario: 0, slope: -0.6584394652063814, fluctuations: 0.0\n",
      "step: 27980 loss: 468.111606 time elapsed: 34.6051 learning rate: 0.002242, scenario: 0, slope: -1.1835652909813588, fluctuations: 0.0\n",
      "step: 27990 loss: 424.139354 time elapsed: 34.6171 learning rate: 0.002242, scenario: 0, slope: -1.831925571393661, fluctuations: 0.0\n",
      "step: 28000 loss: 392.117942 time elapsed: 34.6295 learning rate: 0.002242, scenario: 0, slope: -2.4108708977779076, fluctuations: 0.0\n",
      "step: 28010 loss: 369.228272 time elapsed: 34.6436 learning rate: 0.002242, scenario: 0, slope: -3.00356532950241, fluctuations: 0.0\n",
      "step: 28020 loss: 352.156772 time elapsed: 34.6566 learning rate: 0.002242, scenario: 0, slope: -3.3599788695605515, fluctuations: 0.0\n",
      "step: 28030 loss: 338.513345 time elapsed: 34.6709 learning rate: 0.002242, scenario: 0, slope: -3.496235987964184, fluctuations: 0.0\n",
      "step: 28040 loss: 327.170728 time elapsed: 34.6835 learning rate: 0.002242, scenario: 0, slope: -3.387441339289086, fluctuations: 0.0\n",
      "step: 28050 loss: 317.635778 time elapsed: 34.6963 learning rate: 0.002242, scenario: 0, slope: -3.0393197396015155, fluctuations: 0.0\n",
      "step: 28060 loss: 309.398284 time elapsed: 34.7083 learning rate: 0.002242, scenario: 0, slope: -2.50686229510812, fluctuations: 0.0\n",
      "step: 28070 loss: 302.092804 time elapsed: 34.7206 learning rate: 0.002242, scenario: 0, slope: -1.929501392244739, fluctuations: 0.0\n",
      "step: 28080 loss: 295.540462 time elapsed: 34.7335 learning rate: 0.002242, scenario: 0, slope: -1.491014670948395, fluctuations: 0.0\n",
      "step: 28090 loss: 289.631435 time elapsed: 34.7457 learning rate: 0.002242, scenario: 0, slope: -1.18895200511416, fluctuations: 0.0\n",
      "step: 28100 loss: 284.528052 time elapsed: 34.7581 learning rate: 0.002242, scenario: 0, slope: -0.999528222160007, fluctuations: 0.0\n",
      "step: 28110 loss: 279.964794 time elapsed: 34.7708 learning rate: 0.002242, scenario: 0, slope: -0.8357292680039148, fluctuations: 0.0\n",
      "step: 28120 loss: 275.819274 time elapsed: 34.7831 learning rate: 0.002242, scenario: 0, slope: -0.7252715032624959, fluctuations: 0.0\n",
      "step: 28130 loss: 271.990270 time elapsed: 34.7954 learning rate: 0.002242, scenario: 0, slope: -0.6376124402265999, fluctuations: 0.0\n",
      "step: 28140 loss: 268.409129 time elapsed: 34.8070 learning rate: 0.002242, scenario: 0, slope: -0.5665486324186971, fluctuations: 0.0\n",
      "step: 28150 loss: 265.031682 time elapsed: 34.8190 learning rate: 0.002242, scenario: 0, slope: -0.5078736048559083, fluctuations: 0.0\n",
      "step: 28160 loss: 261.827956 time elapsed: 34.8355 learning rate: 0.002242, scenario: 0, slope: -0.45865646345000033, fluctuations: 0.0\n",
      "step: 28170 loss: 258.777699 time elapsed: 34.8505 learning rate: 0.002242, scenario: 0, slope: -0.41749807072561274, fluctuations: 0.0\n",
      "step: 28180 loss: 255.859945 time elapsed: 34.8659 learning rate: 0.002242, scenario: 0, slope: -0.3836591765435537, fluctuations: 0.0\n",
      "step: 28190 loss: 253.061836 time elapsed: 34.8790 learning rate: 0.002242, scenario: 0, slope: -0.3566898544006726, fluctuations: 0.0\n",
      "step: 28200 loss: 250.370810 time elapsed: 34.8916 learning rate: 0.002242, scenario: 0, slope: -0.3369532193324625, fluctuations: 0.0\n",
      "step: 28210 loss: 247.775573 time elapsed: 34.9048 learning rate: 0.002242, scenario: 0, slope: -0.31700785656368285, fluctuations: 0.0\n",
      "step: 28220 loss: 245.267145 time elapsed: 34.9189 learning rate: 0.002242, scenario: 0, slope: -0.30172926607492756, fluctuations: 0.0\n",
      "step: 28230 loss: 242.837737 time elapsed: 34.9323 learning rate: 0.002242, scenario: 0, slope: -0.28843175626829703, fluctuations: 0.0\n",
      "step: 28240 loss: 240.480508 time elapsed: 34.9464 learning rate: 0.002242, scenario: 0, slope: -0.2766781777245669, fluctuations: 0.0\n",
      "step: 28250 loss: 238.189412 time elapsed: 34.9591 learning rate: 0.002242, scenario: 0, slope: -0.26618533470854583, fluctuations: 0.0\n",
      "step: 28260 loss: 235.411847 time elapsed: 34.9715 learning rate: 0.004806, scenario: 1, slope: -0.25756837262877696, fluctuations: 0.0\n",
      "step: 28270 loss: 229.340479 time elapsed: 34.9837 learning rate: 0.010302, scenario: 0, slope: -0.26119772490224197, fluctuations: 0.0\n",
      "step: 28280 loss: 219.946458 time elapsed: 34.9956 learning rate: 0.010302, scenario: 0, slope: -0.2976846936186757, fluctuations: 0.0\n",
      "step: 28290 loss: 211.664565 time elapsed: 35.0079 learning rate: 0.010302, scenario: 0, slope: -0.35998074190291046, fluctuations: 0.0\n",
      "step: 28300 loss: 203.916259 time elapsed: 35.0201 learning rate: 0.010302, scenario: 0, slope: -0.42837944476597767, fluctuations: 0.0\n",
      "step: 28310 loss: 197.573768 time elapsed: 35.0356 learning rate: 0.010302, scenario: 0, slope: -0.510629369820137, fluctuations: 0.01\n",
      "step: 28320 loss: 189.226396 time elapsed: 35.0497 learning rate: 0.010302, scenario: 0, slope: -0.5900294903386094, fluctuations: 0.01\n",
      "step: 28330 loss: 182.304802 time elapsed: 35.0636 learning rate: 0.010302, scenario: 0, slope: -0.6537386828649769, fluctuations: 0.05\n",
      "step: 28340 loss: 175.132236 time elapsed: 35.0762 learning rate: 0.010302, scenario: 0, slope: -0.7123186736715388, fluctuations: 0.07\n",
      "step: 28350 loss: 167.964806 time elapsed: 35.0887 learning rate: 0.010302, scenario: 0, slope: -0.7490374873485551, fluctuations: 0.07\n",
      "step: 28360 loss: 161.345090 time elapsed: 35.1013 learning rate: 0.010302, scenario: 0, slope: -0.753598202165241, fluctuations: 0.07\n",
      "step: 28370 loss: 155.492267 time elapsed: 35.1126 learning rate: 0.010302, scenario: 0, slope: -0.731947206080875, fluctuations: 0.07\n",
      "step: 28380 loss: 150.019205 time elapsed: 35.1245 learning rate: 0.010302, scenario: 0, slope: -0.7076561646717963, fluctuations: 0.07\n",
      "step: 28390 loss: 144.875027 time elapsed: 35.1368 learning rate: 0.010302, scenario: 0, slope: -0.6831508320662613, fluctuations: 0.07\n",
      "step: 28400 loss: 140.060838 time elapsed: 35.1490 learning rate: 0.010302, scenario: 0, slope: -0.6580964201240024, fluctuations: 0.07\n",
      "step: 28410 loss: 135.565036 time elapsed: 35.1618 learning rate: 0.010302, scenario: 0, slope: -0.6170740579442564, fluctuations: 0.06\n",
      "step: 28420 loss: 131.369049 time elapsed: 35.1741 learning rate: 0.010302, scenario: 0, slope: -0.5730589939908609, fluctuations: 0.06\n",
      "step: 28430 loss: 127.454543 time elapsed: 35.1858 learning rate: 0.010302, scenario: 0, slope: -0.5402257527690106, fluctuations: 0.01\n",
      "step: 28440 loss: 123.802182 time elapsed: 35.1975 learning rate: 0.010302, scenario: 0, slope: -0.502565027541823, fluctuations: 0.0\n",
      "step: 28450 loss: 120.399149 time elapsed: 35.2097 learning rate: 0.010302, scenario: 0, slope: -0.4682675937375166, fluctuations: 0.0\n",
      "step: 28460 loss: 117.222764 time elapsed: 35.2216 learning rate: 0.010302, scenario: 0, slope: -0.4371595618010729, fluctuations: 0.0\n",
      "step: 28470 loss: 114.185287 time elapsed: 35.2340 learning rate: 0.010302, scenario: 0, slope: -0.40848165973346284, fluctuations: 0.0\n",
      "step: 28480 loss: 111.004734 time elapsed: 35.2463 learning rate: 0.010302, scenario: 0, slope: -0.3824871614193118, fluctuations: 0.0\n",
      "step: 28490 loss: 108.940185 time elapsed: 35.2601 learning rate: 0.010302, scenario: 0, slope: -0.36060965593697275, fluctuations: 0.03\n",
      "step: 28500 loss: 106.329051 time elapsed: 35.2744 learning rate: 0.010302, scenario: 0, slope: -0.33728085573642613, fluctuations: 0.03\n",
      "step: 28510 loss: 103.829366 time elapsed: 35.2898 learning rate: 0.010302, scenario: 0, slope: -0.31294255815996247, fluctuations: 0.03\n",
      "step: 28520 loss: 101.529928 time elapsed: 35.3035 learning rate: 0.010302, scenario: 0, slope: -0.2937916222080769, fluctuations: 0.03\n",
      "step: 28530 loss: 99.345008 time elapsed: 35.3166 learning rate: 0.010302, scenario: 0, slope: -0.276878610757253, fluctuations: 0.03\n",
      "step: 28540 loss: 97.269131 time elapsed: 35.3299 learning rate: 0.010302, scenario: 0, slope: -0.26175105856174274, fluctuations: 0.03\n",
      "step: 28550 loss: 95.296703 time elapsed: 35.3419 learning rate: 0.010302, scenario: 0, slope: -0.24796835451843938, fluctuations: 0.03\n",
      "step: 28560 loss: 93.421269 time elapsed: 35.3543 learning rate: 0.010302, scenario: 0, slope: -0.23506932878977804, fluctuations: 0.03\n",
      "step: 28570 loss: 91.639802 time elapsed: 35.3663 learning rate: 0.010302, scenario: 0, slope: -0.22296213937110337, fluctuations: 0.03\n",
      "step: 28580 loss: 90.677767 time elapsed: 35.3782 learning rate: 0.010302, scenario: 0, slope: -0.21050225263916517, fluctuations: 0.04\n",
      "step: 28590 loss: 88.410618 time elapsed: 35.3936 learning rate: 0.010302, scenario: 0, slope: -0.20255013782731246, fluctuations: 0.04\n",
      "step: 28600 loss: 86.735286 time elapsed: 35.4061 learning rate: 0.010302, scenario: 0, slope: -0.19341711494240763, fluctuations: 0.06\n",
      "step: 28610 loss: 85.116656 time elapsed: 35.4188 learning rate: 0.010302, scenario: 0, slope: -0.18349855782116534, fluctuations: 0.06\n",
      "step: 28620 loss: 83.554694 time elapsed: 35.4308 learning rate: 0.010302, scenario: 0, slope: -0.17634685737325473, fluctuations: 0.06\n",
      "step: 28630 loss: 82.041598 time elapsed: 35.4430 learning rate: 0.010302, scenario: 0, slope: -0.17052917734737616, fluctuations: 0.06\n",
      "step: 28640 loss: 80.554699 time elapsed: 35.4550 learning rate: 0.010302, scenario: 0, slope: -0.16576132299128074, fluctuations: 0.06\n",
      "step: 28650 loss: 79.115235 time elapsed: 35.4671 learning rate: 0.010302, scenario: 0, slope: -0.1617768285626281, fluctuations: 0.06\n",
      "step: 28660 loss: 77.721767 time elapsed: 35.4804 learning rate: 0.010302, scenario: 0, slope: -0.15829989910368017, fluctuations: 0.06\n",
      "step: 28670 loss: 76.422259 time elapsed: 35.4944 learning rate: 0.010302, scenario: 0, slope: -0.15477985983401077, fluctuations: 0.06\n",
      "step: 28680 loss: 75.112732 time elapsed: 35.5073 learning rate: 0.010302, scenario: 0, slope: -0.14940912848701607, fluctuations: 0.04\n",
      "step: 28690 loss: 73.889621 time elapsed: 35.5204 learning rate: 0.010302, scenario: 0, slope: -0.14483967174428047, fluctuations: 0.01\n",
      "step: 28700 loss: 72.727350 time elapsed: 35.5326 learning rate: 0.010302, scenario: 0, slope: -0.14064409574115988, fluctuations: 0.0\n",
      "step: 28710 loss: 71.660239 time elapsed: 35.5451 learning rate: 0.010302, scenario: 0, slope: -0.13509708615460922, fluctuations: 0.0\n",
      "step: 28720 loss: 70.438119 time elapsed: 35.5568 learning rate: 0.010302, scenario: 0, slope: -0.13019413694353135, fluctuations: 0.0\n",
      "step: 28730 loss: 69.354082 time elapsed: 35.5684 learning rate: 0.010302, scenario: 0, slope: -0.12567761733126276, fluctuations: 0.0\n",
      "step: 28740 loss: 68.273573 time elapsed: 35.5816 learning rate: 0.010302, scenario: 0, slope: -0.12158199648824271, fluctuations: 0.0\n",
      "step: 28750 loss: 67.317642 time elapsed: 35.5942 learning rate: 0.010302, scenario: 0, slope: -0.11762009347091613, fluctuations: 0.0\n",
      "step: 28760 loss: 66.484155 time elapsed: 35.6061 learning rate: 0.010302, scenario: 0, slope: -0.11276406396035414, fluctuations: 0.0\n",
      "step: 28770 loss: 65.406889 time elapsed: 35.6191 learning rate: 0.010302, scenario: 0, slope: -0.10881658002010972, fluctuations: 0.0\n",
      "step: 28780 loss: 64.510588 time elapsed: 35.6320 learning rate: 0.010302, scenario: 0, slope: -0.10529760798964309, fluctuations: 0.0\n",
      "step: 28790 loss: 63.656215 time elapsed: 35.6443 learning rate: 0.010302, scenario: 0, slope: -0.1019333295890092, fluctuations: 0.0\n",
      "step: 28800 loss: 62.842107 time elapsed: 35.6572 learning rate: 0.010302, scenario: 0, slope: -0.09900619498055714, fluctuations: 0.0\n",
      "step: 28810 loss: 62.256670 time elapsed: 35.6729 learning rate: 0.010302, scenario: 0, slope: -0.09451090466110924, fluctuations: 0.0\n",
      "step: 28820 loss: 61.267774 time elapsed: 35.6876 learning rate: 0.010302, scenario: 0, slope: -0.09074710559276065, fluctuations: 0.0\n",
      "step: 28830 loss: 60.438444 time elapsed: 35.7027 learning rate: 0.010302, scenario: 0, slope: -0.08780221500623772, fluctuations: 0.0\n",
      "step: 28840 loss: 59.741245 time elapsed: 35.7163 learning rate: 0.010302, scenario: 0, slope: -0.08551745163809116, fluctuations: 0.0\n",
      "step: 28850 loss: 58.979790 time elapsed: 35.7294 learning rate: 0.010302, scenario: 0, slope: -0.08351802558356757, fluctuations: 0.0\n",
      "step: 28860 loss: 58.398805 time elapsed: 35.7415 learning rate: 0.010302, scenario: 0, slope: -0.08030675921877585, fluctuations: 0.0\n",
      "step: 28870 loss: 57.505645 time elapsed: 35.7543 learning rate: 0.010302, scenario: 0, slope: -0.07763733755479052, fluctuations: 0.0\n",
      "step: 28880 loss: 56.879057 time elapsed: 35.7674 learning rate: 0.010302, scenario: 0, slope: -0.07600246775743748, fluctuations: 0.0\n",
      "step: 28890 loss: 56.139361 time elapsed: 35.7806 learning rate: 0.010302, scenario: 0, slope: -0.07496820124031331, fluctuations: 0.0\n",
      "step: 28900 loss: 55.666283 time elapsed: 35.7929 learning rate: 0.010302, scenario: 0, slope: -0.07381499434413862, fluctuations: 0.0\n",
      "step: 28910 loss: 54.780092 time elapsed: 35.8057 learning rate: 0.010302, scenario: 0, slope: -0.07152652059617459, fluctuations: 0.0\n",
      "step: 28920 loss: 54.256249 time elapsed: 35.8178 learning rate: 0.010302, scenario: 0, slope: -0.0700697013078992, fluctuations: 0.0\n",
      "step: 28930 loss: 53.486271 time elapsed: 35.8298 learning rate: 0.010302, scenario: 0, slope: -0.06907246809480641, fluctuations: 0.0\n",
      "step: 28940 loss: 52.852416 time elapsed: 35.8417 learning rate: 0.010302, scenario: 0, slope: -0.06862271087951553, fluctuations: 0.0\n",
      "step: 28950 loss: 53.132321 time elapsed: 35.8535 learning rate: 0.010302, scenario: 0, slope: -0.06670796916614609, fluctuations: 0.0\n",
      "step: 28960 loss: 52.111608 time elapsed: 35.8653 learning rate: 0.010302, scenario: 0, slope: -0.06325861307312629, fluctuations: 0.02\n",
      "step: 28970 loss: 51.082330 time elapsed: 35.8785 learning rate: 0.010302, scenario: 0, slope: -0.061663638262583614, fluctuations: 0.03\n",
      "step: 28980 loss: 50.417262 time elapsed: 35.8916 learning rate: 0.010302, scenario: 0, slope: -0.06176595314924889, fluctuations: 0.03\n",
      "step: 28990 loss: 49.867491 time elapsed: 35.9059 learning rate: 0.010302, scenario: 0, slope: -0.06223294869914627, fluctuations: 0.03\n",
      "step: 29000 loss: 49.571784 time elapsed: 35.9181 learning rate: 0.010302, scenario: 0, slope: -0.06183685270941628, fluctuations: 0.03\n",
      "step: 29010 loss: 86.477329 time elapsed: 35.9309 learning rate: 0.015083, scenario: 1, slope: -0.02428885025211822, fluctuations: 0.03\n",
      "step: 29020 loss: 4714.249472 time elapsed: 35.9429 learning rate: 0.005578, scenario: -1, slope: 29.31811316932805, fluctuations: 0.06\n",
      "step: 29030 loss: 2417.093304 time elapsed: 35.9546 learning rate: 0.001945, scenario: -1, slope: 41.40460711856323, fluctuations: 0.08\n",
      "step: 29040 loss: 1279.857100 time elapsed: 35.9665 learning rate: 0.000678, scenario: -1, slope: 39.90165216317939, fluctuations: 0.1\n",
      "step: 29050 loss: 1038.628655 time elapsed: 35.9788 learning rate: 0.000236, scenario: -1, slope: 32.689226116600906, fluctuations: 0.11\n",
      "step: 29060 loss: 930.417453 time elapsed: 35.9910 learning rate: 0.000082, scenario: -1, slope: 22.700190740221302, fluctuations: 0.09\n",
      "step: 29070 loss: 917.350061 time elapsed: 36.0031 learning rate: 0.000029, scenario: -1, slope: 12.877912110207063, fluctuations: 0.08\n",
      "step: 29080 loss: 914.255209 time elapsed: 36.0153 learning rate: 0.000010, scenario: -1, slope: 2.2002625965541673, fluctuations: 0.08\n",
      "step: 29090 loss: 912.815994 time elapsed: 36.0266 learning rate: 0.000009, scenario: 0, slope: -9.885924869596, fluctuations: 0.08\n",
      "step: 29100 loss: 911.409232 time elapsed: 36.0387 learning rate: 0.000009, scenario: 0, slope: -22.69895272587734, fluctuations: 0.08\n",
      "step: 29110 loss: 910.009261 time elapsed: 36.0511 learning rate: 0.000009, scenario: 0, slope: -42.270747164676166, fluctuations: 0.08\n",
      "step: 29120 loss: 908.621851 time elapsed: 36.0626 learning rate: 0.000009, scenario: 0, slope: -17.02181274412887, fluctuations: 0.05\n",
      "step: 29130 loss: 907.246645 time elapsed: 36.0745 learning rate: 0.000009, scenario: 0, slope: -5.0974326711521405, fluctuations: 0.02\n",
      "step: 29140 loss: 905.881606 time elapsed: 36.0865 learning rate: 0.000010, scenario: 1, slope: -1.089344689410317, fluctuations: 0.01\n",
      "step: 29150 loss: 903.722099 time elapsed: 36.1005 learning rate: 0.000026, scenario: 1, slope: -0.43367819789290074, fluctuations: 0.0\n",
      "step: 29160 loss: 898.193934 time elapsed: 36.1140 learning rate: 0.000067, scenario: 1, slope: -0.18513336499064295, fluctuations: 0.0\n",
      "step: 29170 loss: 884.224557 time elapsed: 36.1269 learning rate: 0.000173, scenario: 1, slope: -0.21372775902688623, fluctuations: 0.0\n",
      "step: 29180 loss: 849.773037 time elapsed: 36.1396 learning rate: 0.000449, scenario: 1, slope: -0.376697675620605, fluctuations: 0.0\n",
      "step: 29190 loss: 769.392058 time elapsed: 36.1523 learning rate: 0.001165, scenario: 1, slope: -0.807149556513709, fluctuations: 0.0\n",
      "step: 29200 loss: 650.018921 time elapsed: 36.1651 learning rate: 0.001281, scenario: 0, slope: -1.6180760922112474, fluctuations: 0.0\n",
      "step: 29210 loss: 565.430892 time elapsed: 36.1787 learning rate: 0.001281, scenario: 0, slope: -2.937903425758123, fluctuations: 0.0\n",
      "step: 29220 loss: 507.129204 time elapsed: 36.1910 learning rate: 0.001281, scenario: 0, slope: -4.1499619429004815, fluctuations: 0.0\n",
      "step: 29230 loss: 464.329178 time elapsed: 36.2042 learning rate: 0.001281, scenario: 0, slope: -5.182816254748635, fluctuations: 0.0\n",
      "step: 29240 loss: 431.071153 time elapsed: 36.2176 learning rate: 0.001281, scenario: 0, slope: -5.914109613535639, fluctuations: 0.0\n",
      "step: 29250 loss: 403.966961 time elapsed: 36.2307 learning rate: 0.001281, scenario: 0, slope: -6.262336359042791, fluctuations: 0.0\n",
      "step: 29260 loss: 380.813207 time elapsed: 36.2434 learning rate: 0.001281, scenario: 0, slope: -6.181533346558616, fluctuations: 0.0\n",
      "step: 29270 loss: 360.214532 time elapsed: 36.2555 learning rate: 0.001281, scenario: 0, slope: -5.667363215116037, fluctuations: 0.0\n",
      "step: 29280 loss: 341.183914 time elapsed: 36.2680 learning rate: 0.001281, scenario: 0, slope: -4.787496938404185, fluctuations: 0.0\n",
      "step: 29290 loss: 322.909277 time elapsed: 36.2805 learning rate: 0.001281, scenario: 0, slope: -3.756505608726057, fluctuations: 0.0\n",
      "step: 29300 loss: 304.913887 time elapsed: 36.2920 learning rate: 0.001281, scenario: 0, slope: -3.0410910587354345, fluctuations: 0.0\n",
      "step: 29310 loss: 287.032865 time elapsed: 36.3063 learning rate: 0.001281, scenario: 0, slope: -2.4931165283418086, fluctuations: 0.0\n",
      "step: 29320 loss: 268.017882 time elapsed: 36.3200 learning rate: 0.001281, scenario: 0, slope: -2.1946204160606477, fluctuations: 0.0\n",
      "step: 29330 loss: 247.098251 time elapsed: 36.3354 learning rate: 0.001281, scenario: 0, slope: -2.02484940005612, fluctuations: 0.0\n",
      "step: 29340 loss: 224.071382 time elapsed: 36.3487 learning rate: 0.001281, scenario: 0, slope: -1.9498681343791264, fluctuations: 0.0\n",
      "step: 29350 loss: 200.381640 time elapsed: 36.3613 learning rate: 0.001281, scenario: 0, slope: -1.9456938707882852, fluctuations: 0.0\n",
      "step: 29360 loss: 179.056920 time elapsed: 36.3738 learning rate: 0.001281, scenario: 0, slope: -1.9818322925966128, fluctuations: 0.0\n",
      "step: 29370 loss: 162.086094 time elapsed: 36.3854 learning rate: 0.001281, scenario: 0, slope: -2.019978673181146, fluctuations: 0.0\n",
      "step: 29380 loss: 148.700390 time elapsed: 36.3969 learning rate: 0.001281, scenario: 0, slope: -2.0280145670230354, fluctuations: 0.0\n",
      "step: 29390 loss: 137.214719 time elapsed: 36.4291 learning rate: 0.001281, scenario: 0, slope: -1.9901564124499387, fluctuations: 0.0\n",
      "step: 29400 loss: 126.305767 time elapsed: 36.4418 learning rate: 0.001281, scenario: 0, slope: -1.9155522269942404, fluctuations: 0.0\n",
      "step: 29410 loss: 115.063267 time elapsed: 36.4550 learning rate: 0.001281, scenario: 0, slope: -1.7793735071828207, fluctuations: 0.0\n",
      "step: 29420 loss: 103.498648 time elapsed: 36.4675 learning rate: 0.001281, scenario: 0, slope: -1.6273024089062065, fluctuations: 0.0\n",
      "step: 29430 loss: 93.348505 time elapsed: 36.4794 learning rate: 0.001281, scenario: 0, slope: -1.4646470729679775, fluctuations: 0.0\n",
      "step: 29440 loss: 86.744890 time elapsed: 36.4912 learning rate: 0.001281, scenario: 0, slope: -1.3013799831082307, fluctuations: 0.0\n",
      "step: 29450 loss: 83.160360 time elapsed: 36.5040 learning rate: 0.001281, scenario: 0, slope: -1.1451041267259932, fluctuations: 0.0\n",
      "step: 29460 loss: 80.661138 time elapsed: 36.5181 learning rate: 0.001281, scenario: 0, slope: -1.0023014572079074, fluctuations: 0.0\n",
      "step: 29470 loss: 78.495930 time elapsed: 36.5328 learning rate: 0.001281, scenario: 0, slope: -0.8696417875773857, fluctuations: 0.0\n",
      "step: 29480 loss: 76.617693 time elapsed: 36.5462 learning rate: 0.001281, scenario: 0, slope: -0.7383219541561659, fluctuations: 0.0\n",
      "step: 29490 loss: 74.973957 time elapsed: 36.5587 learning rate: 0.001281, scenario: 0, slope: -0.6047054480560367, fluctuations: 0.0\n",
      "step: 29500 loss: 73.496613 time elapsed: 36.5713 learning rate: 0.001281, scenario: 0, slope: -0.48555256632326627, fluctuations: 0.0\n",
      "step: 29510 loss: 72.149173 time elapsed: 36.5865 learning rate: 0.001281, scenario: 0, slope: -0.3525661761217727, fluctuations: 0.0\n",
      "step: 29520 loss: 70.912502 time elapsed: 36.5997 learning rate: 0.001281, scenario: 0, slope: -0.2582547319691873, fluctuations: 0.0\n",
      "step: 29530 loss: 69.771788 time elapsed: 36.6135 learning rate: 0.001281, scenario: 0, slope: -0.19837518192585737, fluctuations: 0.0\n",
      "step: 29540 loss: 68.715098 time elapsed: 36.6264 learning rate: 0.001281, scenario: 0, slope: -0.16660488363062934, fluctuations: 0.0\n",
      "step: 29550 loss: 67.732862 time elapsed: 36.6389 learning rate: 0.001281, scenario: 0, slope: -0.14791547125968943, fluctuations: 0.0\n",
      "step: 29560 loss: 66.816939 time elapsed: 36.6516 learning rate: 0.001281, scenario: 0, slope: -0.13359898076772905, fluctuations: 0.0\n",
      "step: 29570 loss: 65.960284 time elapsed: 36.6646 learning rate: 0.001281, scenario: 0, slope: -0.12183403262730391, fluctuations: 0.0\n",
      "step: 29580 loss: 65.156771 time elapsed: 36.6779 learning rate: 0.001281, scenario: 0, slope: -0.11207239429104511, fluctuations: 0.0\n",
      "step: 29590 loss: 64.401037 time elapsed: 36.6905 learning rate: 0.001281, scenario: 0, slope: -0.10371524623790469, fluctuations: 0.0\n",
      "step: 29600 loss: 63.688362 time elapsed: 36.7044 learning rate: 0.001281, scenario: 0, slope: -0.09707823186656855, fluctuations: 0.0\n",
      "step: 29610 loss: 63.014585 time elapsed: 36.7199 learning rate: 0.001281, scenario: 0, slope: -0.08988845144194975, fluctuations: 0.0\n",
      "step: 29620 loss: 62.376026 time elapsed: 36.7338 learning rate: 0.001281, scenario: 0, slope: -0.08409653166993344, fluctuations: 0.0\n",
      "step: 29630 loss: 61.769430 time elapsed: 36.7476 learning rate: 0.001281, scenario: 0, slope: -0.0789093872091901, fluctuations: 0.0\n",
      "step: 29640 loss: 61.191912 time elapsed: 36.7600 learning rate: 0.001281, scenario: 0, slope: -0.07424829710334252, fluctuations: 0.0\n",
      "step: 29650 loss: 60.640910 time elapsed: 36.7722 learning rate: 0.001281, scenario: 0, slope: -0.07004834794335373, fluctuations: 0.0\n",
      "step: 29660 loss: 60.114152 time elapsed: 36.7845 learning rate: 0.001550, scenario: 1, slope: -0.0662541802854028, fluctuations: 0.0\n",
      "step: 29670 loss: 59.240014 time elapsed: 36.7968 learning rate: 0.004022, scenario: 1, slope: -0.06360459203425697, fluctuations: 0.0\n",
      "step: 29680 loss: 57.205103 time elapsed: 36.8087 learning rate: 0.008621, scenario: 0, slope: -0.0663572279135577, fluctuations: 0.0\n",
      "step: 29690 loss: 54.714313 time elapsed: 36.8209 learning rate: 0.008621, scenario: 0, slope: -0.07887735933771274, fluctuations: 0.0\n",
      "step: 29700 loss: 52.711169 time elapsed: 36.8330 learning rate: 0.008621, scenario: 0, slope: -0.09538487708883116, fluctuations: 0.0\n",
      "step: 29710 loss: 51.048170 time elapsed: 36.8460 learning rate: 0.008621, scenario: 0, slope: -0.11764117475123016, fluctuations: 0.0\n",
      "step: 29720 loss: 49.627982 time elapsed: 36.8578 learning rate: 0.008621, scenario: 0, slope: -0.13658128384378215, fluctuations: 0.0\n",
      "step: 29730 loss: 48.388295 time elapsed: 36.8698 learning rate: 0.008621, scenario: 0, slope: -0.1518210083205858, fluctuations: 0.0\n",
      "step: 29740 loss: 47.287096 time elapsed: 36.8817 learning rate: 0.008621, scenario: 0, slope: -0.16148582726747182, fluctuations: 0.0\n",
      "step: 29750 loss: 46.296163 time elapsed: 36.8933 learning rate: 0.008621, scenario: 0, slope: -0.16405223901151605, fluctuations: 0.0\n",
      "step: 29760 loss: 45.396313 time elapsed: 36.9050 learning rate: 0.008621, scenario: 0, slope: -0.15824866201663942, fluctuations: 0.0\n",
      "step: 29770 loss: 44.573680 time elapsed: 36.9164 learning rate: 0.008621, scenario: 0, slope: -0.14386564870605417, fluctuations: 0.0\n",
      "step: 29780 loss: 43.815827 time elapsed: 36.9291 learning rate: 0.008621, scenario: 0, slope: -0.12541601271525388, fluctuations: 0.0\n",
      "step: 29790 loss: 43.102730 time elapsed: 36.9422 learning rate: 0.008621, scenario: 0, slope: -0.1102268035520492, fluctuations: 0.0\n",
      "step: 29800 loss: 42.444074 time elapsed: 36.9560 learning rate: 0.008621, scenario: 0, slope: -0.09956447318939383, fluctuations: 0.0\n",
      "step: 29810 loss: 41.822600 time elapsed: 36.9690 learning rate: 0.008621, scenario: 0, slope: -0.0891754479104765, fluctuations: 0.0\n",
      "step: 29820 loss: 41.233222 time elapsed: 36.9817 learning rate: 0.008621, scenario: 0, slope: -0.08156031865159388, fluctuations: 0.0\n",
      "step: 29830 loss: 40.670843 time elapsed: 36.9942 learning rate: 0.008621, scenario: 0, slope: -0.07524835425778902, fluctuations: 0.0\n",
      "step: 29840 loss: 40.134935 time elapsed: 37.0065 learning rate: 0.008621, scenario: 0, slope: -0.06995238510077101, fluctuations: 0.0\n",
      "step: 29850 loss: 39.625891 time elapsed: 37.0199 learning rate: 0.008621, scenario: 0, slope: -0.06545546773361031, fluctuations: 0.0\n",
      "step: 29860 loss: 39.142483 time elapsed: 37.0333 learning rate: 0.008621, scenario: 0, slope: -0.061580275967509916, fluctuations: 0.0\n",
      "step: 29870 loss: 39.180344 time elapsed: 37.0468 learning rate: 0.008621, scenario: 0, slope: -0.05753833018762702, fluctuations: 0.01\n",
      "step: 29880 loss: 38.662283 time elapsed: 37.0598 learning rate: 0.008621, scenario: 0, slope: -0.052408954000422434, fluctuations: 0.02\n",
      "step: 29890 loss: 38.185080 time elapsed: 37.0730 learning rate: 0.008621, scenario: 0, slope: -0.04814234930714272, fluctuations: 0.02\n",
      "step: 29900 loss: 37.679455 time elapsed: 37.0865 learning rate: 0.008621, scenario: 0, slope: -0.04520515890844551, fluctuations: 0.02\n",
      "step: 29910 loss: 37.153959 time elapsed: 37.1000 learning rate: 0.008621, scenario: 0, slope: -0.043302877139801924, fluctuations: 0.02\n",
      "step: 29920 loss: 36.730866 time elapsed: 37.1130 learning rate: 0.008621, scenario: 0, slope: -0.04231413096477453, fluctuations: 0.02\n",
      "step: 29930 loss: 36.343187 time elapsed: 37.1263 learning rate: 0.008621, scenario: 0, slope: -0.04179181011670675, fluctuations: 0.02\n",
      "step: 29940 loss: 35.970551 time elapsed: 37.1406 learning rate: 0.008621, scenario: 0, slope: -0.041647190713957574, fluctuations: 0.02\n",
      "step: 29950 loss: 35.609205 time elapsed: 37.1546 learning rate: 0.008621, scenario: 0, slope: -0.04185335065106938, fluctuations: 0.02\n",
      "step: 29960 loss: 35.257212 time elapsed: 37.1680 learning rate: 0.008621, scenario: 0, slope: -0.04239308851766056, fluctuations: 0.02\n",
      "step: 29970 loss: 34.913227 time elapsed: 37.1808 learning rate: 0.008621, scenario: 0, slope: -0.042356803791401554, fluctuations: 0.0\n",
      "step: 29980 loss: 34.576571 time elapsed: 37.1929 learning rate: 0.008621, scenario: 0, slope: -0.04032123474775939, fluctuations: 0.0\n",
      "step: 29990 loss: 34.246447 time elapsed: 37.2049 learning rate: 0.008621, scenario: 0, slope: -0.03829173573354466, fluctuations: 0.0\n",
      "step: 30000 loss: 33.777806 time elapsed: 37.2169 learning rate: 0.020327, scenario: 1, slope: -0.03662819115381972, fluctuations: 0.0\n",
      "step: 30010 loss: 74479.384681 time elapsed: 37.2297 learning rate: 0.013725, scenario: -1, slope: 169.91259162050923, fluctuations: 0.02\n",
      "step: 30020 loss: 25195.978123 time elapsed: 37.2413 learning rate: 0.004785, scenario: -1, slope: 335.7547254402378, fluctuations: 0.04\n",
      "step: 30030 loss: 14902.934945 time elapsed: 37.2533 learning rate: 0.001669, scenario: -1, slope: 357.09363710186733, fluctuations: 0.04\n",
      "step: 30040 loss: 12022.764886 time elapsed: 37.2651 learning rate: 0.000582, scenario: -1, slope: 320.19586075939344, fluctuations: 0.04\n",
      "step: 30050 loss: 10969.114555 time elapsed: 37.2770 learning rate: 0.000203, scenario: -1, slope: 261.2969069537337, fluctuations: 0.04\n",
      "step: 30060 loss: 10665.462484 time elapsed: 37.2886 learning rate: 0.000071, scenario: -1, slope: 188.4576958729306, fluctuations: 0.04\n",
      "step: 30070 loss: 10566.486749 time elapsed: 37.3004 learning rate: 0.000025, scenario: -1, slope: 102.52476677664004, fluctuations: 0.04\n",
      "step: 30080 loss: 10533.358041 time elapsed: 37.3123 learning rate: 0.000009, scenario: -1, slope: 1.4996366957532508, fluctuations: 0.04\n",
      "step: 30090 loss: 10517.611169 time elapsed: 37.3242 learning rate: 0.000009, scenario: 0, slope: -118.24676116948459, fluctuations: 0.04\n",
      "step: 30100 loss: 10502.273595 time elapsed: 37.3362 learning rate: 0.000009, scenario: 0, slope: -246.44161311981864, fluctuations: 0.04\n",
      "step: 30110 loss: 10487.125609 time elapsed: 37.3496 learning rate: 0.000009, scenario: 0, slope: -240.68155586309157, fluctuations: 0.01\n",
      "step: 30120 loss: 10472.141988 time elapsed: 37.3629 learning rate: 0.000009, scenario: 0, slope: -64.36099638259475, fluctuations: 0.0\n",
      "step: 30130 loss: 10457.305948 time elapsed: 37.3767 learning rate: 0.000009, scenario: 0, slope: -19.437190040808165, fluctuations: 0.0\n",
      "step: 30140 loss: 10440.091872 time elapsed: 37.3897 learning rate: 0.000017, scenario: 1, slope: -6.739700725292658, fluctuations: 0.0\n",
      "step: 30150 loss: 10399.104546 time elapsed: 37.4019 learning rate: 0.000043, scenario: 1, slope: -2.942003000818794, fluctuations: 0.0\n",
      "step: 30160 loss: 10295.158639 time elapsed: 37.4144 learning rate: 0.000113, scenario: 1, slope: -2.241534989807869, fluctuations: 0.0\n",
      "step: 30170 loss: 10038.164731 time elapsed: 37.4287 learning rate: 0.000292, scenario: 1, slope: -3.1580543504903624, fluctuations: 0.0\n",
      "step: 30180 loss: 9435.643443 time elapsed: 37.4410 learning rate: 0.000758, scenario: 1, slope: -6.272963566312232, fluctuations: 0.0\n",
      "step: 30190 loss: 8222.988618 time elapsed: 37.4529 learning rate: 0.001344, scenario: 0, slope: -13.657658578679458, fluctuations: 0.0\n",
      "step: 30200 loss: 7172.400579 time elapsed: 37.4654 learning rate: 0.001344, scenario: 0, slope: -24.279266105472356, fluctuations: 0.0\n",
      "step: 30210 loss: 6354.438095 time elapsed: 37.4784 learning rate: 0.001344, scenario: 0, slope: -38.97206430530736, fluctuations: 0.0\n",
      "step: 30220 loss: 5693.239828 time elapsed: 37.4902 learning rate: 0.001344, scenario: 0, slope: -51.84722511619263, fluctuations: 0.0\n",
      "step: 30230 loss: 5125.769787 time elapsed: 37.5034 learning rate: 0.001344, scenario: 0, slope: -62.673266136835, fluctuations: 0.0\n",
      "step: 30240 loss: 4634.332491 time elapsed: 37.5160 learning rate: 0.001344, scenario: 0, slope: -70.3368956487581, fluctuations: 0.0\n",
      "step: 30250 loss: 4236.427411 time elapsed: 37.5299 learning rate: 0.001344, scenario: 0, slope: -73.8290127085409, fluctuations: 0.0\n",
      "step: 30260 loss: 3922.901303 time elapsed: 37.5443 learning rate: 0.001344, scenario: 0, slope: -72.4495792148653, fluctuations: 0.0\n",
      "step: 30270 loss: 3683.303964 time elapsed: 37.5586 learning rate: 0.001344, scenario: 0, slope: -66.13395054334474, fluctuations: 0.0\n",
      "step: 30280 loss: 3494.063016 time elapsed: 37.5731 learning rate: 0.001344, scenario: 0, slope: -56.05926940640306, fluctuations: 0.0\n",
      "step: 30290 loss: 3335.272153 time elapsed: 37.5872 learning rate: 0.001344, scenario: 0, slope: -45.57330543371875, fluctuations: 0.0\n",
      "step: 30300 loss: 3154.833634 time elapsed: 37.6001 learning rate: 0.001344, scenario: 0, slope: -38.16913796907405, fluctuations: 0.0\n",
      "step: 30310 loss: 2989.336974 time elapsed: 37.6143 learning rate: 0.001344, scenario: 0, slope: -31.06700413662921, fluctuations: 0.0\n",
      "step: 30320 loss: 2845.431369 time elapsed: 37.6274 learning rate: 0.001344, scenario: 0, slope: -25.939012701909373, fluctuations: 0.0\n",
      "step: 30330 loss: 2716.756219 time elapsed: 37.6394 learning rate: 0.001344, scenario: 0, slope: -21.836566197564803, fluctuations: 0.0\n",
      "step: 30340 loss: 2598.686991 time elapsed: 37.6516 learning rate: 0.001344, scenario: 0, slope: -18.757758319487774, fluctuations: 0.0\n",
      "step: 30350 loss: 2490.193916 time elapsed: 37.6637 learning rate: 0.001344, scenario: 0, slope: -16.56361295480801, fluctuations: 0.0\n",
      "step: 30360 loss: 2390.111101 time elapsed: 37.6753 learning rate: 0.001344, scenario: 0, slope: -15.00585659804566, fluctuations: 0.0\n",
      "step: 30370 loss: 2299.174246 time elapsed: 37.6873 learning rate: 0.001344, scenario: 0, slope: -13.813343558670029, fluctuations: 0.0\n",
      "step: 30380 loss: 2217.017456 time elapsed: 37.6990 learning rate: 0.001344, scenario: 0, slope: -12.74286729740341, fluctuations: 0.0\n",
      "step: 30390 loss: 2142.309034 time elapsed: 37.7106 learning rate: 0.001344, scenario: 0, slope: -11.6319077046184, fluctuations: 0.0\n",
      "step: 30400 loss: 2074.025524 time elapsed: 37.7223 learning rate: 0.001344, scenario: 0, slope: -10.5960711490314, fluctuations: 0.0\n",
      "step: 30410 loss: 2011.263177 time elapsed: 37.7350 learning rate: 0.001344, scenario: 0, slope: -9.58214458629098, fluctuations: 0.0\n",
      "step: 30420 loss: 1953.160366 time elapsed: 37.7466 learning rate: 0.001344, scenario: 0, slope: -8.763942206770285, fluctuations: 0.0\n",
      "step: 30430 loss: 1899.120775 time elapsed: 37.7584 learning rate: 0.001344, scenario: 0, slope: -8.016803396954417, fluctuations: 0.0\n",
      "step: 30440 loss: 1848.576717 time elapsed: 37.7716 learning rate: 0.001344, scenario: 0, slope: -7.3351433929399255, fluctuations: 0.0\n",
      "step: 30450 loss: 1800.827361 time elapsed: 37.7850 learning rate: 0.001344, scenario: 0, slope: -6.725158302861998, fluctuations: 0.0\n",
      "step: 30460 loss: 1755.163739 time elapsed: 37.7990 learning rate: 0.001344, scenario: 0, slope: -6.193723942074392, fluctuations: 0.0\n",
      "step: 30470 loss: 1711.183743 time elapsed: 37.8119 learning rate: 0.001344, scenario: 0, slope: -5.741487460858394, fluctuations: 0.0\n",
      "step: 30480 loss: 1668.911430 time elapsed: 37.8242 learning rate: 0.001344, scenario: 0, slope: -5.359993480987796, fluctuations: 0.0\n",
      "step: 30490 loss: 1628.381882 time elapsed: 37.8362 learning rate: 0.001344, scenario: 0, slope: -5.037967209443891, fluctuations: 0.0\n",
      "step: 30500 loss: 1589.286762 time elapsed: 37.8482 learning rate: 0.001344, scenario: 0, slope: -4.790759509086461, fluctuations: 0.0\n",
      "step: 30510 loss: 1551.109662 time elapsed: 37.8619 learning rate: 0.001344, scenario: 0, slope: -4.534716204807688, fluctuations: 0.0\n",
      "step: 30520 loss: 1512.793207 time elapsed: 37.8740 learning rate: 0.001344, scenario: 0, slope: -4.342330444020233, fluctuations: 0.0\n",
      "step: 30530 loss: 1476.260134 time elapsed: 37.8863 learning rate: 0.001344, scenario: 0, slope: -4.182478381033221, fluctuations: 0.0\n",
      "step: 30540 loss: 1440.651511 time elapsed: 37.8982 learning rate: 0.001344, scenario: 0, slope: -4.044347842797348, fluctuations: 0.0\n",
      "step: 30550 loss: 1406.201139 time elapsed: 37.9104 learning rate: 0.001344, scenario: 0, slope: -3.9208948664956367, fluctuations: 0.0\n",
      "step: 30560 loss: 1372.812930 time elapsed: 37.9224 learning rate: 0.001344, scenario: 0, slope: -3.807004878208758, fluctuations: 0.0\n",
      "step: 30570 loss: 1340.493210 time elapsed: 37.9354 learning rate: 0.001344, scenario: 0, slope: -3.700050361504827, fluctuations: 0.0\n",
      "step: 30580 loss: 1309.168875 time elapsed: 37.9489 learning rate: 0.001344, scenario: 0, slope: -3.597793083346952, fluctuations: 0.0\n",
      "step: 30590 loss: 1278.773960 time elapsed: 37.9636 learning rate: 0.001344, scenario: 0, slope: -3.4970607605836896, fluctuations: 0.0\n",
      "step: 30600 loss: 1249.269357 time elapsed: 37.9781 learning rate: 0.001344, scenario: 0, slope: -3.4052891788112296, fluctuations: 0.0\n",
      "step: 30610 loss: 1220.655928 time elapsed: 37.9931 learning rate: 0.001344, scenario: 0, slope: -3.2907680983086878, fluctuations: 0.0\n",
      "step: 30620 loss: 1192.956241 time elapsed: 38.0055 learning rate: 0.001344, scenario: 0, slope: -3.1882852896013145, fluctuations: 0.0\n",
      "step: 30630 loss: 1166.186138 time elapsed: 38.0183 learning rate: 0.001344, scenario: 0, slope: -3.0905324305291493, fluctuations: 0.0\n",
      "step: 30640 loss: 1140.388078 time elapsed: 38.0322 learning rate: 0.001344, scenario: 0, slope: -2.9948586357942206, fluctuations: 0.0\n",
      "step: 30650 loss: 1115.680476 time elapsed: 38.0455 learning rate: 0.001344, scenario: 0, slope: -2.900538232524548, fluctuations: 0.0\n",
      "step: 30660 loss: 1091.969965 time elapsed: 38.0579 learning rate: 0.001344, scenario: 0, slope: -2.8061631422806674, fluctuations: 0.0\n",
      "step: 30670 loss: 1069.154764 time elapsed: 38.0710 learning rate: 0.001344, scenario: 0, slope: -2.711418079236103, fluctuations: 0.0\n",
      "step: 30680 loss: 1047.231103 time elapsed: 38.0854 learning rate: 0.001344, scenario: 0, slope: -2.6161511522707843, fluctuations: 0.0\n",
      "step: 30690 loss: 1026.165407 time elapsed: 38.1001 learning rate: 0.001344, scenario: 0, slope: -2.52065821821348, fluctuations: 0.0\n",
      "step: 30700 loss: 1005.919820 time elapsed: 38.1138 learning rate: 0.001344, scenario: 0, slope: -2.4351260492860307, fluctuations: 0.0\n",
      "step: 30710 loss: 986.468281 time elapsed: 38.1288 learning rate: 0.001344, scenario: 0, slope: -2.332176019947656, fluctuations: 0.0\n",
      "step: 30720 loss: 967.951648 time elapsed: 38.1407 learning rate: 0.001344, scenario: 0, slope: -2.2403539309940634, fluctuations: 0.0\n",
      "step: 30730 loss: 949.872880 time elapsed: 38.1532 learning rate: 0.001344, scenario: 0, slope: -2.1520341584077536, fluctuations: 0.0\n",
      "step: 30740 loss: 932.579120 time elapsed: 38.1670 learning rate: 0.001344, scenario: 0, slope: -2.067723858995152, fluctuations: 0.0\n",
      "step: 30750 loss: 915.741574 time elapsed: 38.1808 learning rate: 0.001344, scenario: 0, slope: -1.9879805453568495, fluctuations: 0.0\n",
      "step: 30760 loss: 899.540815 time elapsed: 38.1946 learning rate: 0.001344, scenario: 0, slope: -1.9125308428998637, fluctuations: 0.0\n",
      "step: 30770 loss: 883.827789 time elapsed: 38.2071 learning rate: 0.001344, scenario: 0, slope: -1.8414527021041653, fluctuations: 0.0\n",
      "step: 30780 loss: 868.577234 time elapsed: 38.2198 learning rate: 0.001344, scenario: 0, slope: -1.7750080800659152, fluctuations: 0.0\n",
      "step: 30790 loss: 853.722251 time elapsed: 38.2318 learning rate: 0.001344, scenario: 0, slope: -1.7133866345224753, fluctuations: 0.0\n",
      "step: 30800 loss: 839.202641 time elapsed: 38.2436 learning rate: 0.001344, scenario: 0, slope: -1.6622271136662443, fluctuations: 0.0\n",
      "step: 30810 loss: 824.943799 time elapsed: 38.2560 learning rate: 0.001344, scenario: 0, slope: -1.6055076592455773, fluctuations: 0.0\n",
      "step: 30820 loss: 810.851055 time elapsed: 38.2680 learning rate: 0.001344, scenario: 0, slope: -1.558926308176952, fluctuations: 0.0\n",
      "step: 30830 loss: 796.894898 time elapsed: 38.2798 learning rate: 0.001344, scenario: 0, slope: -1.5182931052495958, fluctuations: 0.0\n",
      "step: 30840 loss: 783.072061 time elapsed: 38.2917 learning rate: 0.001344, scenario: 0, slope: -1.4827503556522428, fluctuations: 0.0\n",
      "step: 30850 loss: 769.463853 time elapsed: 38.3039 learning rate: 0.001344, scenario: 0, slope: -1.452799021168348, fluctuations: 0.0\n",
      "step: 30860 loss: 756.004083 time elapsed: 38.3159 learning rate: 0.001344, scenario: 0, slope: -1.4275659709707875, fluctuations: 0.0\n",
      "step: 30870 loss: 742.905747 time elapsed: 38.3276 learning rate: 0.001344, scenario: 0, slope: -1.4052459998494218, fluctuations: 0.0\n",
      "step: 30880 loss: 729.912518 time elapsed: 38.3414 learning rate: 0.001344, scenario: 0, slope: -1.3851490977882146, fluctuations: 0.0\n",
      "step: 30890 loss: 716.856861 time elapsed: 38.3557 learning rate: 0.001344, scenario: 0, slope: -1.3669384182584883, fluctuations: 0.0\n",
      "step: 30900 loss: 703.793905 time elapsed: 38.3691 learning rate: 0.001344, scenario: 0, slope: -1.3523324499091791, fluctuations: 0.0\n",
      "step: 30910 loss: 690.753849 time elapsed: 38.3837 learning rate: 0.001344, scenario: 0, slope: -1.3366967825016407, fluctuations: 0.0\n",
      "step: 30920 loss: 678.029253 time elapsed: 38.3974 learning rate: 0.001344, scenario: 0, slope: -1.3240329026830866, fluctuations: 0.0\n",
      "step: 30930 loss: 665.566398 time elapsed: 38.4108 learning rate: 0.001344, scenario: 0, slope: -1.3117875648307424, fluctuations: 0.0\n",
      "step: 30940 loss: 653.659825 time elapsed: 38.4229 learning rate: 0.001344, scenario: 0, slope: -1.2980094499964578, fluctuations: 0.0\n",
      "step: 30950 loss: 642.463274 time elapsed: 38.4363 learning rate: 0.001344, scenario: 0, slope: -1.2806306818294335, fluctuations: 0.0\n",
      "step: 30960 loss: 631.921863 time elapsed: 38.4494 learning rate: 0.001344, scenario: 0, slope: -1.2572916558305367, fluctuations: 0.0\n",
      "step: 30970 loss: 621.998427 time elapsed: 38.4639 learning rate: 0.001344, scenario: 0, slope: -1.2258291024641754, fluctuations: 0.0\n",
      "step: 30980 loss: 612.613585 time elapsed: 38.4767 learning rate: 0.001344, scenario: 0, slope: -1.1857912683475391, fluctuations: 0.0\n",
      "step: 30990 loss: 603.724200 time elapsed: 38.4890 learning rate: 0.001344, scenario: 0, slope: -1.1380876516366727, fluctuations: 0.0\n",
      "step: 31000 loss: 595.189212 time elapsed: 38.5020 learning rate: 0.001344, scenario: 0, slope: -1.0904821129640454, fluctuations: 0.0\n",
      "step: 31010 loss: 587.064777 time elapsed: 38.5154 learning rate: 0.001344, scenario: 0, slope: -1.0304184277111592, fluctuations: 0.0\n",
      "step: 31020 loss: 579.240947 time elapsed: 38.5284 learning rate: 0.001344, scenario: 0, slope: -0.9768990688202785, fluctuations: 0.0\n",
      "step: 31030 loss: 571.706034 time elapsed: 38.5417 learning rate: 0.001344, scenario: 0, slope: -0.9265863422328339, fluctuations: 0.0\n",
      "step: 31040 loss: 564.468087 time elapsed: 38.5548 learning rate: 0.001344, scenario: 0, slope: -0.8812109631666005, fluctuations: 0.0\n",
      "step: 31050 loss: 557.463011 time elapsed: 38.5690 learning rate: 0.001344, scenario: 0, slope: -0.8405257256505541, fluctuations: 0.0\n",
      "step: 31060 loss: 550.680613 time elapsed: 38.5830 learning rate: 0.001344, scenario: 0, slope: -0.8045790447905226, fluctuations: 0.0\n",
      "step: 31070 loss: 544.096813 time elapsed: 38.5965 learning rate: 0.001344, scenario: 0, slope: -0.7724898141890545, fluctuations: 0.0\n",
      "step: 31080 loss: 537.815106 time elapsed: 38.6102 learning rate: 0.001344, scenario: 0, slope: -0.7432799159741579, fluctuations: 0.0\n",
      "step: 31090 loss: 531.591798 time elapsed: 38.6228 learning rate: 0.001344, scenario: 0, slope: -0.716732406762218, fluctuations: 0.0\n",
      "step: 31100 loss: 525.576064 time elapsed: 38.6352 learning rate: 0.001344, scenario: 0, slope: -0.6943547587534773, fluctuations: 0.0\n",
      "step: 31110 loss: 519.821077 time elapsed: 38.6477 learning rate: 0.001344, scenario: 0, slope: -0.6691746325757346, fluctuations: 0.0\n",
      "step: 31120 loss: 514.113899 time elapsed: 38.6600 learning rate: 0.001344, scenario: 0, slope: -0.6478235087374872, fluctuations: 0.0\n",
      "step: 31130 loss: 508.658766 time elapsed: 38.6720 learning rate: 0.001344, scenario: 0, slope: -0.6279008751545788, fluctuations: 0.0\n",
      "step: 31140 loss: 503.280387 time elapsed: 38.6841 learning rate: 0.001344, scenario: 0, slope: -0.6091700526208874, fluctuations: 0.0\n",
      "step: 31150 loss: 498.127345 time elapsed: 38.6961 learning rate: 0.001344, scenario: 0, slope: -0.5911663256850073, fluctuations: 0.0\n",
      "step: 31160 loss: 493.094176 time elapsed: 38.7081 learning rate: 0.001344, scenario: 0, slope: -0.5734762309230436, fluctuations: 0.0\n",
      "step: 31170 loss: 488.093606 time elapsed: 38.7203 learning rate: 0.001344, scenario: 0, slope: -0.5573738800567368, fluctuations: 0.0\n",
      "step: 31180 loss: 483.292069 time elapsed: 38.7323 learning rate: 0.001344, scenario: 0, slope: -0.5420246221178732, fluctuations: 0.0\n",
      "step: 31190 loss: 478.462205 time elapsed: 38.7443 learning rate: 0.002164, scenario: 1, slope: -0.5277419541072972, fluctuations: 0.0\n",
      "step: 31200 loss: 474.789801 time elapsed: 38.7559 learning rate: 0.005103, scenario: 1, slope: -0.40047501538441493, fluctuations: 0.02\n",
      "step: 31210 loss: 2100.663806 time elapsed: 38.7681 learning rate: 0.007688, scenario: -1, slope: 2.5503115433022625, fluctuations: 0.04\n",
      "step: 31220 loss: 572.925180 time elapsed: 38.7800 learning rate: 0.002681, scenario: -1, slope: 1.801904199177979, fluctuations: 0.09\n",
      "step: 31230 loss: 458.413412 time elapsed: 38.7929 learning rate: 0.000935, scenario: -1, slope: 1.3864737644107092, fluctuations: 0.11\n",
      "step: 31240 loss: 451.626066 time elapsed: 38.8067 learning rate: 0.000326, scenario: -1, slope: 0.7797607628565875, fluctuations: 0.13\n",
      "step: 31250 loss: 442.057761 time elapsed: 38.8208 learning rate: 0.000114, scenario: -1, slope: 0.05192255859456058, fluctuations: 0.13\n",
      "step: 31260 loss: 440.289975 time elapsed: 38.8340 learning rate: 0.000257, scenario: 0, slope: -0.5313012564673667, fluctuations: 0.14\n",
      "step: 31270 loss: 438.870320 time elapsed: 38.8474 learning rate: 0.000257, scenario: 0, slope: -1.117182543246375, fluctuations: 0.14\n",
      "step: 31280 loss: 437.479840 time elapsed: 38.8605 learning rate: 0.000257, scenario: 0, slope: -1.7276521668681126, fluctuations: 0.14\n",
      "step: 31290 loss: 436.154012 time elapsed: 38.8732 learning rate: 0.000257, scenario: 0, slope: -2.4550268148371917, fluctuations: 0.14\n",
      "step: 31300 loss: 434.935617 time elapsed: 38.8872 learning rate: 0.000257, scenario: 0, slope: -2.72194605410234, fluctuations: 0.11\n",
      "step: 31310 loss: 433.795036 time elapsed: 38.9012 learning rate: 0.000257, scenario: 0, slope: -0.9158509143151311, fluctuations: 0.09\n",
      "step: 31320 loss: 432.699255 time elapsed: 38.9145 learning rate: 0.000342, scenario: 1, slope: -0.4243717830042432, fluctuations: 0.05\n",
      "step: 31330 loss: 430.718916 time elapsed: 38.9273 learning rate: 0.000888, scenario: 1, slope: -0.19737095759934078, fluctuations: 0.03\n",
      "step: 31340 loss: 426.052267 time elapsed: 38.9398 learning rate: 0.002302, scenario: 1, slope: -0.15316369709515248, fluctuations: 0.01\n",
      "step: 31350 loss: 415.926235 time elapsed: 38.9531 learning rate: 0.005972, scenario: 1, slope: -0.186143668198869, fluctuations: 0.01\n",
      "step: 31360 loss: 395.008417 time elapsed: 38.9666 learning rate: 0.015489, scenario: 1, slope: -0.29155669060570644, fluctuations: 0.0\n",
      "step: 31370 loss: 57042.952049 time elapsed: 38.9792 learning rate: 0.019094, scenario: -1, slope: 119.34201235707135, fluctuations: 0.01\n",
      "step: 31380 loss: 27944.755209 time elapsed: 38.9923 learning rate: 0.006658, scenario: -1, slope: 252.30382482192525, fluctuations: 0.04\n",
      "step: 31390 loss: 12070.657049 time elapsed: 39.0062 learning rate: 0.002321, scenario: -1, slope: 278.81159875430166, fluctuations: 0.05\n",
      "step: 31400 loss: 8950.418771 time elapsed: 39.0194 learning rate: 0.000899, scenario: -1, slope: 249.8579093505819, fluctuations: 0.05\n",
      "step: 31410 loss: 7928.722410 time elapsed: 39.0340 learning rate: 0.000314, scenario: -1, slope: 195.49193503887864, fluctuations: 0.05\n",
      "step: 31420 loss: 7648.964267 time elapsed: 39.0469 learning rate: 0.000109, scenario: -1, slope: 136.60611305852456, fluctuations: 0.05\n",
      "step: 31430 loss: 7563.015442 time elapsed: 39.0593 learning rate: 0.000038, scenario: -1, slope: 69.74959685119538, fluctuations: 0.05\n",
      "step: 31440 loss: 7534.590941 time elapsed: 39.0713 learning rate: 0.000015, scenario: 0, slope: -6.875413839280371, fluctuations: 0.05\n",
      "step: 31450 loss: 7519.734295 time elapsed: 39.0835 learning rate: 0.000015, scenario: 0, slope: -96.45060292800028, fluctuations: 0.05\n",
      "step: 31460 loss: 7505.078844 time elapsed: 39.0959 learning rate: 0.000015, scenario: 0, slope: -203.69940494494418, fluctuations: 0.05\n",
      "step: 31470 loss: 7490.582662 time elapsed: 39.1076 learning rate: 0.000015, scenario: 0, slope: -171.9325897893045, fluctuations: 0.04\n",
      "step: 31480 loss: 7476.221067 time elapsed: 39.1194 learning rate: 0.000015, scenario: 0, slope: -59.80034568435106, fluctuations: 0.01\n",
      "step: 31490 loss: 7461.976449 time elapsed: 39.1314 learning rate: 0.000015, scenario: 0, slope: -18.131771158009684, fluctuations: 0.0\n",
      "step: 31500 loss: 7447.397770 time elapsed: 39.1431 learning rate: 0.000020, scenario: 1, slope: -6.883676779014635, fluctuations: 0.0\n",
      "step: 31510 loss: 7420.195768 time elapsed: 39.1552 learning rate: 0.000051, scenario: 1, slope: -2.6408771832800175, fluctuations: 0.0\n",
      "step: 31520 loss: 7351.211535 time elapsed: 39.1669 learning rate: 0.000132, scenario: 1, slope: -1.9107784018580187, fluctuations: 0.0\n",
      "step: 31530 loss: 7177.944327 time elapsed: 39.1787 learning rate: 0.000343, scenario: 1, slope: -2.431085412808822, fluctuations: 0.0\n",
      "step: 31540 loss: 6753.998083 time elapsed: 39.1904 learning rate: 0.000890, scenario: 1, slope: -4.502120698958691, fluctuations: 0.0\n",
      "step: 31550 loss: 5863.974498 time elapsed: 39.2022 learning rate: 0.001576, scenario: 0, slope: -9.703975615969304, fluctuations: 0.0\n",
      "step: 31560 loss: 5082.975169 time elapsed: 39.2153 learning rate: 0.001576, scenario: 0, slope: -18.333627234308377, fluctuations: 0.0\n",
      "step: 31570 loss: 4532.614062 time elapsed: 39.2289 learning rate: 0.001576, scenario: 0, slope: -27.97538171300276, fluctuations: 0.0\n",
      "step: 31580 loss: 4077.664852 time elapsed: 39.2425 learning rate: 0.001576, scenario: 0, slope: -37.066713364379204, fluctuations: 0.0\n",
      "step: 31590 loss: 3704.638249 time elapsed: 39.2551 learning rate: 0.001576, scenario: 0, slope: -44.602082526960075, fluctuations: 0.0\n",
      "step: 31600 loss: 3404.757847 time elapsed: 39.2675 learning rate: 0.001576, scenario: 0, slope: -49.338313044339046, fluctuations: 0.0\n",
      "step: 31610 loss: 3164.896141 time elapsed: 39.2810 learning rate: 0.001576, scenario: 0, slope: -51.68762480660699, fluctuations: 0.0\n",
      "step: 31620 loss: 2960.619724 time elapsed: 39.2944 learning rate: 0.001576, scenario: 0, slope: -50.14454136229224, fluctuations: 0.0\n",
      "step: 31630 loss: 2782.407401 time elapsed: 39.3066 learning rate: 0.001576, scenario: 0, slope: -45.19012388888674, fluctuations: 0.0\n",
      "step: 31640 loss: 2626.249940 time elapsed: 39.3196 learning rate: 0.001576, scenario: 0, slope: -37.750676960996486, fluctuations: 0.0\n",
      "step: 31650 loss: 2488.584696 time elapsed: 39.3329 learning rate: 0.001576, scenario: 0, slope: -30.36477393963491, fluctuations: 0.0\n",
      "step: 31660 loss: 2364.759812 time elapsed: 39.3464 learning rate: 0.001576, scenario: 0, slope: -25.080724506331187, fluctuations: 0.0\n",
      "step: 31670 loss: 2247.986715 time elapsed: 39.3597 learning rate: 0.001576, scenario: 0, slope: -21.10741244496824, fluctuations: 0.0\n",
      "step: 31680 loss: 2128.782353 time elapsed: 39.3725 learning rate: 0.001576, scenario: 0, slope: -18.040304956308038, fluctuations: 0.0\n",
      "step: 31690 loss: 1987.971701 time elapsed: 39.3858 learning rate: 0.001576, scenario: 0, slope: -15.857547684933653, fluctuations: 0.0\n",
      "step: 31700 loss: 1804.082407 time elapsed: 39.3980 learning rate: 0.001576, scenario: 0, slope: -14.696497837216626, fluctuations: 0.0\n",
      "step: 31710 loss: 1657.247525 time elapsed: 39.4124 learning rate: 0.001576, scenario: 0, slope: -14.081855339371057, fluctuations: 0.0\n",
      "step: 31720 loss: 1542.223747 time elapsed: 39.4268 learning rate: 0.001576, scenario: 0, slope: -13.80161525952129, fluctuations: 0.0\n",
      "step: 31730 loss: 1458.442703 time elapsed: 39.4410 learning rate: 0.001576, scenario: 0, slope: -13.555322014599767, fluctuations: 0.0\n",
      "step: 31740 loss: 1383.000250 time elapsed: 39.4553 learning rate: 0.001576, scenario: 0, slope: -13.186193773025805, fluctuations: 0.0\n",
      "step: 31750 loss: 1323.211291 time elapsed: 39.4684 learning rate: 0.001576, scenario: 0, slope: -12.606787345636599, fluctuations: 0.0\n",
      "step: 31760 loss: 1270.754846 time elapsed: 39.4817 learning rate: 0.001576, scenario: 0, slope: -11.745181277369918, fluctuations: 0.0\n",
      "step: 31770 loss: 1225.229461 time elapsed: 39.4948 learning rate: 0.001576, scenario: 0, slope: -10.582290357600218, fluctuations: 0.0\n",
      "step: 31780 loss: 1186.407589 time elapsed: 39.5069 learning rate: 0.001576, scenario: 0, slope: -9.149648895060176, fluctuations: 0.0\n",
      "step: 31790 loss: 1152.958900 time elapsed: 39.5197 learning rate: 0.001576, scenario: 0, slope: -7.582199666944351, fluctuations: 0.0\n",
      "step: 31800 loss: 1123.775652 time elapsed: 39.5337 learning rate: 0.001576, scenario: 0, slope: -6.320230278849495, fluctuations: 0.0\n",
      "step: 31810 loss: 1097.675235 time elapsed: 39.5491 learning rate: 0.001576, scenario: 0, slope: -5.211212473998274, fluctuations: 0.0\n",
      "step: 31820 loss: 1073.906130 time elapsed: 39.5621 learning rate: 0.001576, scenario: 0, slope: -4.442284460873826, fluctuations: 0.0\n",
      "step: 31830 loss: 1052.198664 time elapsed: 39.5748 learning rate: 0.001576, scenario: 0, slope: -3.838413288053194, fluctuations: 0.0\n",
      "step: 31840 loss: 1032.243331 time elapsed: 39.5872 learning rate: 0.001576, scenario: 0, slope: -3.3441846212391684, fluctuations: 0.0\n",
      "step: 31850 loss: 1013.496594 time elapsed: 39.6005 learning rate: 0.001576, scenario: 0, slope: -2.945963914005607, fluctuations: 0.0\n",
      "step: 31860 loss: 995.687732 time elapsed: 39.6136 learning rate: 0.001576, scenario: 0, slope: -2.6226518149580045, fluctuations: 0.0\n",
      "step: 31870 loss: 978.699102 time elapsed: 39.6286 learning rate: 0.001576, scenario: 0, slope: -2.3661618484833196, fluctuations: 0.0\n",
      "step: 31880 loss: 962.495634 time elapsed: 39.6438 learning rate: 0.001576, scenario: 0, slope: -2.1638684233362193, fluctuations: 0.0\n",
      "step: 31890 loss: 946.891801 time elapsed: 39.6579 learning rate: 0.001576, scenario: 0, slope: -2.003708392513439, fluctuations: 0.0\n",
      "step: 31900 loss: 931.908971 time elapsed: 39.6712 learning rate: 0.001576, scenario: 0, slope: -1.886227731839938, fluctuations: 0.0\n",
      "step: 31910 loss: 917.498753 time elapsed: 39.6849 learning rate: 0.001576, scenario: 0, slope: -1.7679988661242698, fluctuations: 0.0\n",
      "step: 31920 loss: 903.611209 time elapsed: 39.6980 learning rate: 0.001576, scenario: 0, slope: -1.679020700822701, fluctuations: 0.0\n",
      "step: 31930 loss: 890.209306 time elapsed: 39.7118 learning rate: 0.001576, scenario: 0, slope: -1.6034426769873353, fluctuations: 0.0\n",
      "step: 31940 loss: 877.244930 time elapsed: 39.7249 learning rate: 0.001576, scenario: 0, slope: -1.537141484620939, fluctuations: 0.0\n",
      "step: 31950 loss: 864.718793 time elapsed: 39.7376 learning rate: 0.001576, scenario: 0, slope: -1.4769238046486064, fluctuations: 0.0\n",
      "step: 31960 loss: 852.699594 time elapsed: 39.7508 learning rate: 0.001576, scenario: 0, slope: -1.421511922066306, fluctuations: 0.0\n",
      "step: 31970 loss: 840.940089 time elapsed: 39.7636 learning rate: 0.001576, scenario: 0, slope: -1.369710762348148, fluctuations: 0.0\n",
      "step: 31980 loss: 829.492796 time elapsed: 39.7767 learning rate: 0.001576, scenario: 0, slope: -1.321954121080011, fluctuations: 0.0\n",
      "step: 31990 loss: 818.430123 time elapsed: 39.7889 learning rate: 0.001576, scenario: 0, slope: -1.2773878830109546, fluctuations: 0.0\n",
      "step: 32000 loss: 807.702946 time elapsed: 39.8008 learning rate: 0.001576, scenario: 0, slope: -1.239714223269984, fluctuations: 0.0\n",
      "step: 32010 loss: 797.285809 time elapsed: 39.8133 learning rate: 0.001576, scenario: 0, slope: -1.1964263738766698, fluctuations: 0.0\n",
      "step: 32020 loss: 787.154764 time elapsed: 39.8251 learning rate: 0.001576, scenario: 0, slope: -1.1593995915074522, fluctuations: 0.0\n",
      "step: 32030 loss: 777.299051 time elapsed: 39.8370 learning rate: 0.001576, scenario: 0, slope: -1.124391535126409, fluctuations: 0.0\n",
      "step: 32040 loss: 767.703567 time elapsed: 39.8507 learning rate: 0.001576, scenario: 0, slope: -1.0913186094904768, fluctuations: 0.0\n",
      "step: 32050 loss: 758.357127 time elapsed: 39.8646 learning rate: 0.001576, scenario: 0, slope: -1.0600400178049223, fluctuations: 0.0\n",
      "step: 32060 loss: 749.250892 time elapsed: 39.8783 learning rate: 0.001576, scenario: 0, slope: -1.030278792059362, fluctuations: 0.0\n",
      "step: 32070 loss: 740.379781 time elapsed: 39.8911 learning rate: 0.001576, scenario: 0, slope: -1.0013378784737956, fluctuations: 0.0\n",
      "step: 32080 loss: 731.741513 time elapsed: 39.9033 learning rate: 0.001576, scenario: 0, slope: -0.9741189459646195, fluctuations: 0.0\n",
      "step: 32090 loss: 723.332684 time elapsed: 39.9156 learning rate: 0.001576, scenario: 0, slope: -0.9481109485668873, fluctuations: 0.0\n",
      "step: 32100 loss: 715.143567 time elapsed: 39.9278 learning rate: 0.001576, scenario: 0, slope: -0.9255439435547407, fluctuations: 0.0\n",
      "step: 32110 loss: 707.157480 time elapsed: 39.9400 learning rate: 0.001576, scenario: 0, slope: -0.8989150740526157, fluctuations: 0.0\n",
      "step: 32120 loss: 699.405636 time elapsed: 39.9519 learning rate: 0.001576, scenario: 0, slope: -0.8755294787981086, fluctuations: 0.0\n",
      "step: 32130 loss: 691.739841 time elapsed: 39.9637 learning rate: 0.001576, scenario: 0, slope: -0.8523885117610784, fluctuations: 0.0\n",
      "step: 32140 loss: 684.303540 time elapsed: 39.9756 learning rate: 0.001576, scenario: 0, slope: -0.8308245257806033, fluctuations: 0.0\n",
      "step: 32150 loss: 676.947538 time elapsed: 39.9878 learning rate: 0.001576, scenario: 0, slope: -0.8105046556749381, fluctuations: 0.0\n",
      "step: 32160 loss: 669.749939 time elapsed: 39.9997 learning rate: 0.001576, scenario: 0, slope: -0.791402974253749, fluctuations: 0.0\n",
      "step: 32170 loss: 662.688872 time elapsed: 40.0114 learning rate: 0.001576, scenario: 0, slope: -0.7735887745393889, fluctuations: 0.0\n",
      "step: 32180 loss: 655.734551 time elapsed: 40.0230 learning rate: 0.001576, scenario: 0, slope: -0.7571086497043467, fluctuations: 0.0\n",
      "step: 32190 loss: 648.876960 time elapsed: 40.0348 learning rate: 0.001576, scenario: 0, slope: -0.7419684604781779, fluctuations: 0.0\n",
      "step: 32200 loss: 642.091816 time elapsed: 40.0464 learning rate: 0.001576, scenario: 0, slope: -0.7294896936871197, fluctuations: 0.0\n",
      "step: 32210 loss: 635.350000 time elapsed: 40.0601 learning rate: 0.001576, scenario: 0, slope: -0.7157817890040324, fluctuations: 0.0\n",
      "step: 32220 loss: 628.615065 time elapsed: 40.0733 learning rate: 0.001576, scenario: 0, slope: -0.7049269174851402, fluctuations: 0.0\n",
      "step: 32230 loss: 621.869048 time elapsed: 40.0878 learning rate: 0.001576, scenario: 0, slope: -0.6950736966783196, fluctuations: 0.0\n",
      "step: 32240 loss: 615.206082 time elapsed: 40.1030 learning rate: 0.001576, scenario: 0, slope: -0.6874213463926299, fluctuations: 0.0\n",
      "step: 32250 loss: 608.845579 time elapsed: 40.1161 learning rate: 0.001576, scenario: 0, slope: -0.6804989189614917, fluctuations: 0.0\n",
      "step: 32260 loss: 602.729721 time elapsed: 40.1293 learning rate: 0.001576, scenario: 0, slope: -0.6726885977780184, fluctuations: 0.0\n",
      "step: 32270 loss: 596.708357 time elapsed: 40.1426 learning rate: 0.001576, scenario: 0, slope: -0.6636110187932178, fluctuations: 0.0\n",
      "step: 32280 loss: 590.420115 time elapsed: 40.1555 learning rate: 0.002539, scenario: 1, slope: -0.6535647467694903, fluctuations: 0.0\n",
      "step: 32290 loss: 611.282888 time elapsed: 40.1686 learning rate: 0.006585, scenario: 1, slope: -0.6205257638693923, fluctuations: 0.02\n",
      "step: 32300 loss: 596.907672 time elapsed: 40.1814 learning rate: 0.015526, scenario: 1, slope: -0.509098769603093, fluctuations: 0.05\n",
      "step: 32310 loss: 5395.718383 time elapsed: 40.1952 learning rate: 0.012813, scenario: -1, slope: 14.322131984237357, fluctuations: 0.07\n",
      "step: 32320 loss: 1500.532444 time elapsed: 40.2078 learning rate: 0.004468, scenario: -1, slope: 19.805520448307373, fluctuations: 0.09\n",
      "step: 32330 loss: 836.023153 time elapsed: 40.2195 learning rate: 0.001558, scenario: -1, slope: 16.94077250747461, fluctuations: 0.11\n",
      "step: 32340 loss: 694.578806 time elapsed: 40.2317 learning rate: 0.000543, scenario: -1, slope: 12.51131814550974, fluctuations: 0.13\n",
      "step: 32350 loss: 649.708608 time elapsed: 40.2437 learning rate: 0.000189, scenario: -1, slope: 6.679936181608829, fluctuations: 0.13\n",
      "step: 32360 loss: 642.564433 time elapsed: 40.2554 learning rate: 0.000066, scenario: -1, slope: 1.2120923863139546, fluctuations: 0.13\n",
      "step: 32370 loss: 638.913841 time elapsed: 40.2694 learning rate: 0.000056, scenario: 0, slope: -4.457588768852467, fluctuations: 0.13\n",
      "step: 32380 loss: 636.348576 time elapsed: 40.2831 learning rate: 0.000056, scenario: 0, slope: -11.04432894525793, fluctuations: 0.13\n",
      "step: 32390 loss: 634.457899 time elapsed: 40.2966 learning rate: 0.000056, scenario: 0, slope: -15.989582273255188, fluctuations: 0.1\n",
      "step: 32400 loss: 632.796841 time elapsed: 40.3095 learning rate: 0.000056, scenario: 0, slope: -19.444711441731197, fluctuations: 0.07\n",
      "step: 32410 loss: 631.241276 time elapsed: 40.3223 learning rate: 0.000056, scenario: 0, slope: -9.980951240228245, fluctuations: 0.06\n",
      "step: 32420 loss: 629.784492 time elapsed: 40.3346 learning rate: 0.000056, scenario: 0, slope: -2.6374029497879365, fluctuations: 0.03\n",
      "step: 32430 loss: 628.410122 time elapsed: 40.3468 learning rate: 0.000068, scenario: 1, slope: -0.7647746452806776, fluctuations: 0.01\n",
      "step: 32440 loss: 626.137179 time elapsed: 40.3614 learning rate: 0.000177, scenario: 1, slope: -0.30233041699286495, fluctuations: 0.0\n",
      "step: 32450 loss: 620.776067 time elapsed: 40.3741 learning rate: 0.000460, scenario: 1, slope: -0.21125488782178561, fluctuations: 0.0\n",
      "step: 32460 loss: 609.075933 time elapsed: 40.3863 learning rate: 0.001192, scenario: 1, slope: -0.23158339576481118, fluctuations: 0.0\n",
      "step: 32470 loss: 585.489077 time elapsed: 40.3980 learning rate: 0.003092, scenario: 1, slope: -0.34580377894082903, fluctuations: 0.0\n",
      "step: 32480 loss: 544.987540 time elapsed: 40.4096 learning rate: 0.008020, scenario: 1, slope: -0.6061707879246095, fluctuations: 0.0\n",
      "step: 32490 loss: 499.616629 time elapsed: 40.4211 learning rate: 0.008020, scenario: 0, slope: -1.043699269582336, fluctuations: 0.0\n",
      "step: 32500 loss: 469.866264 time elapsed: 40.4328 learning rate: 0.008020, scenario: 0, slope: -1.497199923577274, fluctuations: 0.0\n",
      "step: 32510 loss: 446.319031 time elapsed: 40.4450 learning rate: 0.008020, scenario: 0, slope: -2.01536471986688, fluctuations: 0.0\n",
      "step: 32520 loss: 425.663108 time elapsed: 40.4573 learning rate: 0.008020, scenario: 0, slope: -2.4034609566487437, fluctuations: 0.0\n",
      "step: 32530 loss: 408.182178 time elapsed: 40.4691 learning rate: 0.008020, scenario: 0, slope: -2.667133351627232, fluctuations: 0.0\n",
      "step: 32540 loss: 392.648676 time elapsed: 40.4838 learning rate: 0.008020, scenario: 0, slope: -2.7740134446337814, fluctuations: 0.0\n",
      "step: 32550 loss: 378.526203 time elapsed: 40.4986 learning rate: 0.008020, scenario: 0, slope: -2.7115231708109033, fluctuations: 0.0\n",
      "step: 32560 loss: 365.546478 time elapsed: 40.5135 learning rate: 0.008020, scenario: 0, slope: -2.490587568941015, fluctuations: 0.0\n",
      "step: 32570 loss: 353.523751 time elapsed: 40.5263 learning rate: 0.008020, scenario: 0, slope: -2.1597667543151298, fluctuations: 0.0\n",
      "step: 32580 loss: 342.329262 time elapsed: 40.5387 learning rate: 0.008020, scenario: 0, slope: -1.818692516806698, fluctuations: 0.0\n",
      "step: 32590 loss: 331.867276 time elapsed: 40.5539 learning rate: 0.008020, scenario: 0, slope: -1.579871975277107, fluctuations: 0.0\n",
      "step: 32600 loss: 322.057037 time elapsed: 40.5664 learning rate: 0.008020, scenario: 0, slope: -1.4325949475428528, fluctuations: 0.0\n",
      "step: 32610 loss: 312.825213 time elapsed: 40.5802 learning rate: 0.008020, scenario: 0, slope: -1.2882652604909803, fluctuations: 0.0\n",
      "step: 32620 loss: 304.103846 time elapsed: 40.5925 learning rate: 0.008020, scenario: 0, slope: -1.1858791435439666, fluctuations: 0.0\n",
      "step: 32630 loss: 295.831890 time elapsed: 40.6047 learning rate: 0.008020, scenario: 0, slope: -1.1016213354039917, fluctuations: 0.0\n",
      "step: 32640 loss: 287.956627 time elapsed: 40.6169 learning rate: 0.008020, scenario: 0, slope: -1.029115586824841, fluctuations: 0.0\n",
      "step: 32650 loss: 280.433789 time elapsed: 40.6304 learning rate: 0.008020, scenario: 0, slope: -0.9657702370221981, fluctuations: 0.0\n",
      "step: 32660 loss: 273.227314 time elapsed: 40.6427 learning rate: 0.008020, scenario: 0, slope: -0.9099293266876567, fluctuations: 0.0\n",
      "step: 32670 loss: 266.309098 time elapsed: 40.6549 learning rate: 0.008020, scenario: 0, slope: -0.860485260688579, fluctuations: 0.0\n",
      "step: 32680 loss: 259.658250 time elapsed: 40.6688 learning rate: 0.008020, scenario: 0, slope: -0.8165852779915409, fluctuations: 0.0\n",
      "step: 32690 loss: 253.259435 time elapsed: 40.6828 learning rate: 0.008020, scenario: 0, slope: -0.7774623774550526, fluctuations: 0.0\n",
      "step: 32700 loss: 247.100668 time elapsed: 40.6961 learning rate: 0.008020, scenario: 0, slope: -0.7457257035312589, fluctuations: 0.0\n",
      "step: 32710 loss: 241.171349 time elapsed: 40.7098 learning rate: 0.008020, scenario: 0, slope: -0.7106280952248083, fluctuations: 0.0\n",
      "step: 32720 loss: 235.461040 time elapsed: 40.7220 learning rate: 0.008020, scenario: 0, slope: -0.6815749676302046, fluctuations: 0.0\n",
      "step: 32730 loss: 229.958859 time elapsed: 40.7341 learning rate: 0.008020, scenario: 0, slope: -0.6546857427515628, fluctuations: 0.0\n",
      "step: 32740 loss: 224.653095 time elapsed: 40.7459 learning rate: 0.008020, scenario: 0, slope: -0.6295461686173927, fluctuations: 0.0\n",
      "step: 32750 loss: 219.530514 time elapsed: 40.7580 learning rate: 0.008020, scenario: 0, slope: -0.6058659231270807, fluctuations: 0.0\n",
      "step: 32760 loss: 214.574512 time elapsed: 40.7699 learning rate: 0.008020, scenario: 0, slope: -0.5834794778736966, fluctuations: 0.0\n",
      "step: 32770 loss: 209.759519 time elapsed: 40.7819 learning rate: 0.008020, scenario: 0, slope: -0.562355096583506, fluctuations: 0.0\n",
      "step: 32780 loss: 205.029854 time elapsed: 40.7938 learning rate: 0.008020, scenario: 0, slope: -0.5426576502590328, fluctuations: 0.0\n",
      "step: 32790 loss: 200.184934 time elapsed: 40.8059 learning rate: 0.008020, scenario: 0, slope: -0.5251137669412206, fluctuations: 0.0\n",
      "step: 32800 loss: 194.477801 time elapsed: 40.8176 learning rate: 0.008020, scenario: 0, slope: -0.5137562838859515, fluctuations: 0.0\n",
      "step: 32810 loss: 188.863865 time elapsed: 40.8298 learning rate: 0.008020, scenario: 0, slope: -0.508807448322435, fluctuations: 0.0\n",
      "step: 32820 loss: 184.011169 time elapsed: 40.8416 learning rate: 0.008020, scenario: 0, slope: -0.5078511238598892, fluctuations: 0.0\n",
      "step: 32830 loss: 179.670134 time elapsed: 40.8532 learning rate: 0.008020, scenario: 0, slope: -0.5060927548925584, fluctuations: 0.0\n",
      "step: 32840 loss: 175.609534 time elapsed: 40.8647 learning rate: 0.008020, scenario: 0, slope: -0.5014926719403969, fluctuations: 0.0\n",
      "step: 32850 loss: 171.728111 time elapsed: 40.8764 learning rate: 0.008020, scenario: 0, slope: -0.49306895349547963, fluctuations: 0.0\n",
      "step: 32860 loss: 167.878877 time elapsed: 40.8881 learning rate: 0.008020, scenario: 0, slope: -0.4805700058599699, fluctuations: 0.0\n",
      "step: 32870 loss: 172.275542 time elapsed: 40.9025 learning rate: 0.008020, scenario: 0, slope: -0.4526838527781348, fluctuations: 0.01\n",
      "step: 32880 loss: 161.401325 time elapsed: 40.9174 learning rate: 0.008020, scenario: 0, slope: -0.43445148806277784, fluctuations: 0.05\n",
      "step: 32890 loss: 157.780240 time elapsed: 40.9321 learning rate: 0.008020, scenario: 0, slope: -0.40839648523641864, fluctuations: 0.07\n",
      "step: 32900 loss: 153.977253 time elapsed: 40.9451 learning rate: 0.008020, scenario: 0, slope: -0.38786842676733585, fluctuations: 0.09\n",
      "step: 32910 loss: 150.623794 time elapsed: 40.9584 learning rate: 0.008020, scenario: 0, slope: -0.37095993499909, fluctuations: 0.09\n",
      "step: 32920 loss: 147.467750 time elapsed: 40.9712 learning rate: 0.008020, scenario: 0, slope: -0.36128708674448473, fluctuations: 0.09\n",
      "step: 32930 loss: 144.401678 time elapsed: 40.9840 learning rate: 0.008020, scenario: 0, slope: -0.3537937228347884, fluctuations: 0.09\n",
      "step: 32940 loss: 141.846525 time elapsed: 40.9973 learning rate: 0.008020, scenario: 0, slope: -0.3465359870020183, fluctuations: 0.09\n",
      "step: 32950 loss: 141.180458 time elapsed: 41.0103 learning rate: 0.008020, scenario: 0, slope: -0.3341783606398287, fluctuations: 0.11\n",
      "step: 32960 loss: 136.916052 time elapsed: 41.0224 learning rate: 0.008020, scenario: 0, slope: -0.32333460379503876, fluctuations: 0.12\n",
      "step: 32970 loss: 133.509489 time elapsed: 41.0347 learning rate: 0.008020, scenario: 0, slope: -0.306167560566889, fluctuations: 0.12\n",
      "step: 32980 loss: 130.947442 time elapsed: 41.0469 learning rate: 0.008020, scenario: 0, slope: -0.29252000458063093, fluctuations: 0.08\n",
      "step: 32990 loss: 129.276412 time elapsed: 41.0589 learning rate: 0.008020, scenario: 0, slope: -0.28010318455626154, fluctuations: 0.06\n",
      "step: 33000 loss: 127.279932 time elapsed: 41.0705 learning rate: 0.008020, scenario: 0, slope: -0.2716592195361992, fluctuations: 0.05\n",
      "step: 33010 loss: 124.431829 time elapsed: 41.0833 learning rate: 0.008020, scenario: 0, slope: -0.2620525978881949, fluctuations: 0.07\n",
      "step: 33020 loss: 121.768007 time elapsed: 41.0960 learning rate: 0.008020, scenario: 0, slope: -0.25578703767414473, fluctuations: 0.08\n",
      "step: 33030 loss: 119.789633 time elapsed: 41.1098 learning rate: 0.008020, scenario: 0, slope: -0.2504349624712775, fluctuations: 0.08\n",
      "step: 33040 loss: 117.613232 time elapsed: 41.1232 learning rate: 0.008020, scenario: 0, slope: -0.24645309913905086, fluctuations: 0.08\n",
      "step: 33050 loss: 115.481428 time elapsed: 41.1357 learning rate: 0.008020, scenario: 0, slope: -0.23592400426036989, fluctuations: 0.06\n",
      "step: 33060 loss: 113.491083 time elapsed: 41.1485 learning rate: 0.008020, scenario: 0, slope: -0.2277490460926099, fluctuations: 0.04\n",
      "step: 33070 loss: 111.517183 time elapsed: 41.1608 learning rate: 0.008020, scenario: 0, slope: -0.2221611573363232, fluctuations: 0.03\n",
      "step: 33080 loss: 109.617546 time elapsed: 41.1728 learning rate: 0.008020, scenario: 0, slope: -0.2178680647069455, fluctuations: 0.03\n",
      "step: 33090 loss: 107.998988 time elapsed: 41.1843 learning rate: 0.008020, scenario: 0, slope: -0.21177416607365648, fluctuations: 0.03\n",
      "step: 33100 loss: 105.989424 time elapsed: 41.1960 learning rate: 0.008020, scenario: 0, slope: -0.205285577865284, fluctuations: 0.02\n",
      "step: 33110 loss: 104.205334 time elapsed: 41.2087 learning rate: 0.008020, scenario: 0, slope: -0.19664872662975438, fluctuations: 0.01\n",
      "step: 33120 loss: 102.465699 time elapsed: 41.2205 learning rate: 0.008020, scenario: 0, slope: -0.19179962396332947, fluctuations: 0.0\n",
      "step: 33130 loss: 100.721398 time elapsed: 41.2324 learning rate: 0.008020, scenario: 0, slope: -0.18723296838217124, fluctuations: 0.0\n",
      "step: 33140 loss: 99.013220 time elapsed: 41.2445 learning rate: 0.008020, scenario: 0, slope: -0.18363257801689825, fluctuations: 0.0\n",
      "step: 33150 loss: 97.353203 time elapsed: 41.2565 learning rate: 0.008020, scenario: 0, slope: -0.18061647452938634, fluctuations: 0.0\n",
      "step: 33160 loss: 95.737190 time elapsed: 41.2683 learning rate: 0.008020, scenario: 0, slope: -0.17775183638071398, fluctuations: 0.0\n",
      "step: 33170 loss: 94.149479 time elapsed: 41.2799 learning rate: 0.008020, scenario: 0, slope: -0.1752917992562487, fluctuations: 0.0\n",
      "step: 33180 loss: 92.589582 time elapsed: 41.2915 learning rate: 0.008020, scenario: 0, slope: -0.17300131669550803, fluctuations: 0.0\n",
      "step: 33190 loss: 91.061035 time elapsed: 41.3028 learning rate: 0.008020, scenario: 0, slope: -0.17039652364614577, fluctuations: 0.0\n",
      "step: 33200 loss: 89.561762 time elapsed: 41.3164 learning rate: 0.008020, scenario: 0, slope: -0.16600618798414718, fluctuations: 0.0\n",
      "step: 33210 loss: 88.092128 time elapsed: 41.3306 learning rate: 0.008020, scenario: 0, slope: -0.1613199674958119, fluctuations: 0.0\n",
      "step: 33220 loss: 86.654785 time elapsed: 41.3449 learning rate: 0.008020, scenario: 0, slope: -0.15766618184766082, fluctuations: 0.0\n",
      "step: 33230 loss: 85.275903 time elapsed: 41.3577 learning rate: 0.008020, scenario: 0, slope: -0.1543353098245268, fluctuations: 0.0\n",
      "step: 33240 loss: 84.082310 time elapsed: 41.3706 learning rate: 0.008020, scenario: 0, slope: -0.15055054715737912, fluctuations: 0.0\n",
      "step: 33250 loss: 82.521965 time elapsed: 41.3838 learning rate: 0.008020, scenario: 0, slope: -0.14726561156880869, fluctuations: 0.0\n",
      "step: 33260 loss: 81.328240 time elapsed: 41.3966 learning rate: 0.008020, scenario: 0, slope: -0.14400776765577378, fluctuations: 0.0\n",
      "step: 33270 loss: 79.932069 time elapsed: 41.4094 learning rate: 0.008020, scenario: 0, slope: -0.1411905442229495, fluctuations: 0.0\n",
      "step: 33280 loss: 78.739693 time elapsed: 41.4218 learning rate: 0.008020, scenario: 0, slope: -0.13818927630291183, fluctuations: 0.0\n",
      "step: 33290 loss: 77.439245 time elapsed: 41.4349 learning rate: 0.008020, scenario: 0, slope: -0.13549240437830656, fluctuations: 0.0\n",
      "step: 33300 loss: 76.274562 time elapsed: 41.4481 learning rate: 0.008020, scenario: 0, slope: -0.13312929239496898, fluctuations: 0.0\n",
      "step: 33310 loss: 75.186509 time elapsed: 41.4616 learning rate: 0.008020, scenario: 0, slope: -0.12992745445895246, fluctuations: 0.0\n",
      "step: 33320 loss: 73.996199 time elapsed: 41.4738 learning rate: 0.008020, scenario: 0, slope: -0.1269563351350706, fluctuations: 0.0\n",
      "step: 33330 loss: 72.829477 time elapsed: 41.4870 learning rate: 0.008020, scenario: 0, slope: -0.1245272731240125, fluctuations: 0.0\n",
      "step: 33340 loss: 71.739942 time elapsed: 41.5013 learning rate: 0.008020, scenario: 0, slope: -0.12151676695553715, fluctuations: 0.0\n",
      "step: 33350 loss: 70.688081 time elapsed: 41.5158 learning rate: 0.008020, scenario: 0, slope: -0.1186718409467878, fluctuations: 0.0\n",
      "step: 33360 loss: 70.085113 time elapsed: 41.5303 learning rate: 0.008020, scenario: 0, slope: -0.1149253499266899, fluctuations: 0.0\n",
      "step: 33370 loss: 68.832141 time elapsed: 41.5451 learning rate: 0.008020, scenario: 0, slope: -0.111440755617337, fluctuations: 0.01\n",
      "step: 33380 loss: 67.665036 time elapsed: 41.5581 learning rate: 0.008020, scenario: 0, slope: -0.10799325079357314, fluctuations: 0.02\n",
      "step: 33390 loss: 66.644686 time elapsed: 41.5701 learning rate: 0.008020, scenario: 0, slope: -0.10568875619282136, fluctuations: 0.02\n",
      "step: 33400 loss: 65.703201 time elapsed: 41.5820 learning rate: 0.008020, scenario: 0, slope: -0.10421984547795271, fluctuations: 0.02\n",
      "step: 33410 loss: 64.784966 time elapsed: 41.5942 learning rate: 0.008020, scenario: 0, slope: -0.10249195718503273, fluctuations: 0.02\n",
      "step: 33420 loss: 63.828460 time elapsed: 41.6065 learning rate: 0.008020, scenario: 0, slope: -0.10075104611477559, fluctuations: 0.02\n",
      "step: 33430 loss: 62.941748 time elapsed: 41.6193 learning rate: 0.008020, scenario: 0, slope: -0.09957111829554162, fluctuations: 0.02\n",
      "step: 33440 loss: 62.102882 time elapsed: 41.6319 learning rate: 0.008020, scenario: 0, slope: -0.0984104742680628, fluctuations: 0.02\n",
      "step: 33450 loss: 61.415547 time elapsed: 41.6437 learning rate: 0.008020, scenario: 0, slope: -0.09668651622749903, fluctuations: 0.02\n",
      "step: 33460 loss: 60.346529 time elapsed: 41.6555 learning rate: 0.008020, scenario: 0, slope: -0.09457521328764495, fluctuations: 0.01\n",
      "step: 33470 loss: 59.576029 time elapsed: 41.6674 learning rate: 0.008020, scenario: 0, slope: -0.0918645214341647, fluctuations: 0.01\n",
      "step: 33480 loss: 59.017570 time elapsed: 41.6791 learning rate: 0.008020, scenario: 0, slope: -0.08802870915836737, fluctuations: 0.0\n",
      "step: 33490 loss: 57.935689 time elapsed: 41.6907 learning rate: 0.008020, scenario: 0, slope: -0.08593358636639707, fluctuations: 0.0\n",
      "step: 33500 loss: 57.532856 time elapsed: 41.7019 learning rate: 0.008020, scenario: 0, slope: -0.08340005315851748, fluctuations: 0.01\n",
      "step: 33510 loss: 56.664835 time elapsed: 41.7140 learning rate: 0.008020, scenario: 0, slope: -0.08167054775377393, fluctuations: 0.01\n",
      "step: 33520 loss: 55.594131 time elapsed: 41.7272 learning rate: 0.008020, scenario: 0, slope: -0.0795442352250186, fluctuations: 0.02\n",
      "step: 33530 loss: 55.157930 time elapsed: 41.7408 learning rate: 0.008020, scenario: 0, slope: -0.07785535175341743, fluctuations: 0.03\n",
      "step: 33540 loss: 54.355171 time elapsed: 41.7545 learning rate: 0.008020, scenario: 0, slope: -0.07763499689238425, fluctuations: 0.03\n",
      "step: 33550 loss: 53.438523 time elapsed: 41.7672 learning rate: 0.008020, scenario: 0, slope: -0.07634649590335522, fluctuations: 0.03\n",
      "step: 33560 loss: 52.851029 time elapsed: 41.7795 learning rate: 0.008020, scenario: 0, slope: -0.07596448864280497, fluctuations: 0.03\n",
      "step: 33570 loss: 52.437806 time elapsed: 41.7917 learning rate: 0.008020, scenario: 0, slope: -0.07465020030261328, fluctuations: 0.03\n",
      "step: 33580 loss: 51.370348 time elapsed: 41.8038 learning rate: 0.008020, scenario: 0, slope: -0.07333280502682951, fluctuations: 0.03\n",
      "step: 33590 loss: 50.697376 time elapsed: 41.8162 learning rate: 0.008020, scenario: 0, slope: -0.07380746106406494, fluctuations: 0.03\n",
      "step: 33600 loss: 50.076023 time elapsed: 41.8289 learning rate: 0.008020, scenario: 0, slope: -0.0729600878951128, fluctuations: 0.02\n",
      "step: 33610 loss: 52.187056 time elapsed: 41.8423 learning rate: 0.008020, scenario: 0, slope: -0.06775627314654002, fluctuations: 0.02\n",
      "step: 33620 loss: 49.420262 time elapsed: 41.8549 learning rate: 0.008020, scenario: 0, slope: -0.0569801918056397, fluctuations: 0.02\n",
      "step: 33630 loss: 1053.014090 time elapsed: 41.8677 learning rate: 0.007353, scenario: -1, slope: 4.011573599269432, fluctuations: 0.02\n",
      "step: 33640 loss: 866.772789 time elapsed: 41.8811 learning rate: 0.002564, scenario: -1, slope: 7.076885367573855, fluctuations: 0.06\n",
      "step: 33650 loss: 353.088849 time elapsed: 41.8940 learning rate: 0.000894, scenario: -1, slope: 7.5117102852233995, fluctuations: 0.08\n",
      "step: 33660 loss: 226.868880 time elapsed: 41.9065 learning rate: 0.000312, scenario: -1, slope: 6.360296368659491, fluctuations: 0.09\n",
      "step: 33670 loss: 206.619381 time elapsed: 41.9195 learning rate: 0.000109, scenario: -1, slope: 4.657826299883009, fluctuations: 0.09\n",
      "step: 33680 loss: 199.099614 time elapsed: 41.9343 learning rate: 0.000038, scenario: -1, slope: 2.9087937662980057, fluctuations: 0.09\n",
      "step: 33690 loss: 196.941463 time elapsed: 41.9491 learning rate: 0.000013, scenario: -1, slope: 1.0317779008721932, fluctuations: 0.09\n",
      "step: 33700 loss: 196.329967 time elapsed: 41.9626 learning rate: 0.000008, scenario: 0, slope: -0.8606608034094788, fluctuations: 0.09\n",
      "step: 33710 loss: 195.909931 time elapsed: 41.9761 learning rate: 0.000008, scenario: 0, slope: -3.622584954714637, fluctuations: 0.09\n",
      "step: 33720 loss: 195.523672 time elapsed: 41.9885 learning rate: 0.000008, scenario: 0, slope: -6.278332680393064, fluctuations: 0.08\n",
      "step: 33730 loss: 195.160752 time elapsed: 42.0004 learning rate: 0.000008, scenario: 0, slope: -7.4780220035135505, fluctuations: 0.06\n",
      "step: 33740 loss: 194.814022 time elapsed: 42.0122 learning rate: 0.000008, scenario: 0, slope: -1.478279640568961, fluctuations: 0.03\n",
      "step: 33750 loss: 194.478541 time elapsed: 42.0241 learning rate: 0.000008, scenario: 0, slope: -0.41975205165056223, fluctuations: 0.01\n",
      "step: 33760 loss: 194.115038 time elapsed: 42.0356 learning rate: 0.000014, scenario: 1, slope: -0.13536710485224282, fluctuations: 0.0\n",
      "step: 33770 loss: 193.296532 time elapsed: 42.0473 learning rate: 0.000036, scenario: 1, slope: -0.06776432010388192, fluctuations: 0.0\n",
      "step: 33780 loss: 191.254644 time elapsed: 42.0596 learning rate: 0.000093, scenario: 1, slope: -0.049087417805285916, fluctuations: 0.0\n",
      "step: 33790 loss: 186.212811 time elapsed: 42.0714 learning rate: 0.000241, scenario: 1, slope: -0.06651588896403003, fluctuations: 0.0\n",
      "step: 33800 loss: 174.206494 time elapsed: 42.0834 learning rate: 0.000569, scenario: 1, slope: -0.11808420288699885, fluctuations: 0.0\n",
      "step: 33810 loss: 152.387342 time elapsed: 42.0959 learning rate: 0.000916, scenario: 0, slope: -0.2674985442414279, fluctuations: 0.0\n",
      "step: 33820 loss: 134.173877 time elapsed: 42.1080 learning rate: 0.000916, scenario: 0, slope: -0.48388419894314794, fluctuations: 0.0\n",
      "step: 33830 loss: 121.890619 time elapsed: 42.1199 learning rate: 0.000916, scenario: 0, slope: -0.7166437121763901, fluctuations: 0.0\n",
      "step: 33840 loss: 112.470234 time elapsed: 42.1315 learning rate: 0.000916, scenario: 0, slope: -0.9263348295248345, fluctuations: 0.0\n",
      "step: 33850 loss: 104.437997 time elapsed: 42.1440 learning rate: 0.000916, scenario: 0, slope: -1.0904835917016884, fluctuations: 0.0\n",
      "step: 33860 loss: 97.457640 time elapsed: 42.1573 learning rate: 0.000916, scenario: 0, slope: -1.1929546071949644, fluctuations: 0.0\n",
      "step: 33870 loss: 91.468526 time elapsed: 42.1708 learning rate: 0.000916, scenario: 0, slope: -1.221192546949663, fluctuations: 0.0\n",
      "step: 33880 loss: 86.394651 time elapsed: 42.1843 learning rate: 0.000916, scenario: 0, slope: -1.1684716190988313, fluctuations: 0.0\n",
      "step: 33890 loss: 82.147599 time elapsed: 42.1960 learning rate: 0.000916, scenario: 0, slope: -1.0387299033189896, fluctuations: 0.0\n",
      "step: 33900 loss: 78.642472 time elapsed: 42.2082 learning rate: 0.000916, scenario: 0, slope: -0.8773688812211256, fluctuations: 0.0\n",
      "step: 33910 loss: 75.782685 time elapsed: 42.2210 learning rate: 0.000916, scenario: 0, slope: -0.6904648038435575, fluctuations: 0.0\n",
      "step: 33920 loss: 73.457805 time elapsed: 42.2345 learning rate: 0.000916, scenario: 0, slope: -0.5725664317902858, fluctuations: 0.0\n",
      "step: 33930 loss: 71.557147 time elapsed: 42.2475 learning rate: 0.000916, scenario: 0, slope: -0.4825414057049762, fluctuations: 0.0\n",
      "step: 33940 loss: 69.984109 time elapsed: 42.2606 learning rate: 0.000916, scenario: 0, slope: -0.4057590946875576, fluctuations: 0.0\n",
      "step: 33950 loss: 68.661862 time elapsed: 42.2733 learning rate: 0.000916, scenario: 0, slope: -0.33849312368919, fluctuations: 0.0\n",
      "step: 33960 loss: 67.532031 time elapsed: 42.2859 learning rate: 0.000916, scenario: 0, slope: -0.28053304921669087, fluctuations: 0.0\n",
      "step: 33970 loss: 66.550986 time elapsed: 42.2989 learning rate: 0.000916, scenario: 0, slope: -0.23175575423877348, fluctuations: 0.0\n",
      "step: 33980 loss: 65.686289 time elapsed: 42.3130 learning rate: 0.000916, scenario: 0, slope: -0.1916309543702452, fluctuations: 0.0\n",
      "step: 33990 loss: 64.913835 time elapsed: 42.3261 learning rate: 0.000916, scenario: 0, slope: -0.15935874607241363, fluctuations: 0.0\n",
      "step: 34000 loss: 64.215640 time elapsed: 42.3395 learning rate: 0.000916, scenario: 0, slope: -0.13617190475571025, fluctuations: 0.0\n",
      "step: 34010 loss: 63.578150 time elapsed: 42.3555 learning rate: 0.000916, scenario: 0, slope: -0.11405005388152868, fluctuations: 0.0\n",
      "step: 34020 loss: 62.991015 time elapsed: 42.3700 learning rate: 0.000916, scenario: 0, slope: -0.09861668397967728, fluctuations: 0.0\n",
      "step: 34030 loss: 62.446204 time elapsed: 42.3839 learning rate: 0.000916, scenario: 0, slope: -0.0865536584529082, fluctuations: 0.0\n",
      "step: 34040 loss: 61.937381 time elapsed: 42.4002 learning rate: 0.000916, scenario: 0, slope: -0.0770273901901643, fluctuations: 0.0\n",
      "step: 34050 loss: 61.459470 time elapsed: 42.4137 learning rate: 0.000916, scenario: 0, slope: -0.06940598081662645, fluctuations: 0.0\n",
      "step: 34060 loss: 60.857742 time elapsed: 42.4275 learning rate: 0.002160, scenario: 1, slope: -0.06347120688199373, fluctuations: 0.0\n",
      "step: 34070 loss: 59.478435 time elapsed: 42.4397 learning rate: 0.005602, scenario: 1, slope: -0.06156392672147668, fluctuations: 0.0\n",
      "step: 34080 loss: 56.651046 time elapsed: 42.4513 learning rate: 0.010917, scenario: 0, slope: -0.06920623670883495, fluctuations: 0.0\n",
      "step: 34090 loss: 53.767456 time elapsed: 42.4638 learning rate: 0.010917, scenario: 0, slope: -0.0891243108110095, fluctuations: 0.0\n",
      "step: 34100 loss: 51.590589 time elapsed: 42.4759 learning rate: 0.010917, scenario: 0, slope: -0.11248108584944091, fluctuations: 0.0\n",
      "step: 34110 loss: 1564.953674 time elapsed: 42.4881 learning rate: 0.008356, scenario: -1, slope: 1.5573740801612532, fluctuations: 0.01\n",
      "step: 34120 loss: 102.061508 time elapsed: 42.5002 learning rate: 0.002914, scenario: -1, slope: 1.7984052475178087, fluctuations: 0.05\n",
      "step: 34130 loss: 90.052922 time elapsed: 42.5124 learning rate: 0.001016, scenario: -1, slope: 1.6102599790811476, fluctuations: 0.07\n",
      "step: 34140 loss: 60.210886 time elapsed: 42.5248 learning rate: 0.000354, scenario: -1, slope: 1.171203493923081, fluctuations: 0.09\n",
      "step: 34150 loss: 57.902385 time elapsed: 42.5369 learning rate: 0.000124, scenario: -1, slope: 0.6520636071647952, fluctuations: 0.1\n",
      "step: 34160 loss: 56.821417 time elapsed: 42.5502 learning rate: 0.000043, scenario: -1, slope: 0.1271526673990756, fluctuations: 0.1\n",
      "step: 34170 loss: 56.646902 time elapsed: 42.5646 learning rate: 0.000037, scenario: 0, slope: -0.36143705865637377, fluctuations: 0.1\n",
      "step: 34180 loss: 56.494702 time elapsed: 42.5784 learning rate: 0.000037, scenario: 0, slope: -0.8639307839859292, fluctuations: 0.1\n",
      "step: 34190 loss: 56.408285 time elapsed: 42.5924 learning rate: 0.000037, scenario: 0, slope: -1.4331210391900158, fluctuations: 0.1\n",
      "step: 34200 loss: 56.336259 time elapsed: 42.6043 learning rate: 0.000037, scenario: 0, slope: -2.051589049104724, fluctuations: 0.1\n",
      "step: 34210 loss: 56.263222 time elapsed: 42.6169 learning rate: 0.000037, scenario: 0, slope: -0.6353017296387198, fluctuations: 0.09\n",
      "step: 34220 loss: 56.193011 time elapsed: 42.6302 learning rate: 0.000037, scenario: 0, slope: -0.27293776394718633, fluctuations: 0.05\n",
      "step: 34230 loss: 56.125074 time elapsed: 42.6423 learning rate: 0.000041, scenario: 1, slope: -0.07526090060143677, fluctuations: 0.02\n",
      "step: 34240 loss: 56.018857 time elapsed: 42.6546 learning rate: 0.000105, scenario: 1, slope: -0.021155445405916257, fluctuations: 0.01\n",
      "step: 34250 loss: 55.753966 time elapsed: 42.6674 learning rate: 0.000272, scenario: 1, slope: -0.010870358657660745, fluctuations: 0.0\n",
      "step: 34260 loss: 55.128507 time elapsed: 42.6801 learning rate: 0.000707, scenario: 1, slope: -0.01138667607439937, fluctuations: 0.0\n",
      "step: 34270 loss: 53.809191 time elapsed: 42.6930 learning rate: 0.001833, scenario: 1, slope: -0.01756366476408413, fluctuations: 0.0\n",
      "step: 34280 loss: 51.592855 time elapsed: 42.7055 learning rate: 0.004755, scenario: 1, slope: -0.03210748586668295, fluctuations: 0.0\n",
      "step: 34290 loss: 49.300096 time elapsed: 42.7183 learning rate: 0.012333, scenario: 1, slope: -0.05545884254066076, fluctuations: 0.0\n",
      "step: 34300 loss: 46.461021 time elapsed: 42.7318 learning rate: 0.012333, scenario: 0, slope: -0.08172026145209589, fluctuations: 0.0\n",
      "step: 34310 loss: 44.510030 time elapsed: 42.7463 learning rate: 0.012333, scenario: 0, slope: -0.11856350648351746, fluctuations: 0.0\n",
      "step: 34320 loss: 376.556925 time elapsed: 42.7605 learning rate: 0.012949, scenario: -1, slope: 0.14506487705367416, fluctuations: 0.0\n",
      "step: 34330 loss: 324.329476 time elapsed: 42.7746 learning rate: 0.004515, scenario: -1, slope: 5.8065262253364995, fluctuations: 0.04\n",
      "step: 34340 loss: 188.365652 time elapsed: 42.7888 learning rate: 0.001574, scenario: -1, slope: 6.281619685471958, fluctuations: 0.07\n",
      "step: 34350 loss: 137.580436 time elapsed: 42.8038 learning rate: 0.000549, scenario: -1, slope: 5.185718789185798, fluctuations: 0.09\n",
      "step: 34360 loss: 102.431066 time elapsed: 42.8163 learning rate: 0.000191, scenario: -1, slope: 3.543351468319663, fluctuations: 0.1\n",
      "step: 34370 loss: 93.807111 time elapsed: 42.8286 learning rate: 0.000067, scenario: -1, slope: 1.7702354727195193, fluctuations: 0.1\n",
      "step: 34380 loss: 92.822479 time elapsed: 42.8403 learning rate: 0.000023, scenario: -1, slope: 0.09752171579587861, fluctuations: 0.1\n",
      "step: 34390 loss: 92.403506 time elapsed: 42.8518 learning rate: 0.000023, scenario: 0, slope: -1.6046471099090902, fluctuations: 0.1\n",
      "step: 34400 loss: 92.030761 time elapsed: 42.8635 learning rate: 0.000023, scenario: 0, slope: -3.280788970281635, fluctuations: 0.1\n",
      "step: 34410 loss: 91.728414 time elapsed: 42.8760 learning rate: 0.000023, scenario: 0, slope: -5.7176716104353815, fluctuations: 0.1\n",
      "step: 34420 loss: 91.462892 time elapsed: 42.8876 learning rate: 0.000023, scenario: 0, slope: -8.198277059784788, fluctuations: 0.1\n",
      "step: 34430 loss: 91.208471 time elapsed: 42.8995 learning rate: 0.000023, scenario: 0, slope: -2.073670597106198, fluctuations: 0.05\n",
      "step: 34440 loss: 90.956986 time elapsed: 42.9110 learning rate: 0.000023, scenario: 0, slope: -0.394123515436457, fluctuations: 0.03\n",
      "step: 34450 loss: 90.707447 time elapsed: 42.9229 learning rate: 0.000026, scenario: 1, slope: -0.12247542584174195, fluctuations: 0.01\n",
      "step: 34460 loss: 90.313711 time elapsed: 42.9348 learning rate: 0.000066, scenario: 1, slope: -0.05037631083060024, fluctuations: 0.0\n",
      "step: 34470 loss: 89.312714 time elapsed: 42.9465 learning rate: 0.000172, scenario: 1, slope: -0.032611183579631095, fluctuations: 0.0\n",
      "step: 34480 loss: 86.833125 time elapsed: 42.9582 learning rate: 0.000447, scenario: 1, slope: -0.04013009306562529, fluctuations: 0.0\n",
      "step: 34490 loss: 80.992557 time elapsed: 42.9702 learning rate: 0.001159, scenario: 1, slope: -0.06782250476975529, fluctuations: 0.0\n",
      "step: 34500 loss: 69.972337 time elapsed: 42.9819 learning rate: 0.001696, scenario: 0, slope: -0.1272530938641568, fluctuations: 0.0\n",
      "step: 34510 loss: 61.390007 time elapsed: 42.9954 learning rate: 0.001696, scenario: 0, slope: -0.24145471154815118, fluctuations: 0.0\n",
      "step: 34520 loss: 56.308421 time elapsed: 43.0083 learning rate: 0.001696, scenario: 0, slope: -0.3511435948777938, fluctuations: 0.0\n",
      "step: 34530 loss: 53.278483 time elapsed: 43.0229 learning rate: 0.001696, scenario: 0, slope: -0.44321919197957965, fluctuations: 0.0\n",
      "step: 34540 loss: 51.297860 time elapsed: 43.0352 learning rate: 0.001696, scenario: 0, slope: -0.5056529019974848, fluctuations: 0.0\n",
      "step: 34550 loss: 49.899000 time elapsed: 43.0472 learning rate: 0.001696, scenario: 0, slope: -0.5317742451858858, fluctuations: 0.0\n",
      "step: 34560 loss: 48.841954 time elapsed: 43.0593 learning rate: 0.001696, scenario: 0, slope: -0.5178830259459369, fluctuations: 0.0\n",
      "step: 34570 loss: 47.995546 time elapsed: 43.0715 learning rate: 0.001696, scenario: 0, slope: -0.4639788464619653, fluctuations: 0.0\n",
      "step: 34580 loss: 47.279552 time elapsed: 43.0837 learning rate: 0.001696, scenario: 0, slope: -0.3755774000824464, fluctuations: 0.0\n",
      "step: 34590 loss: 46.637912 time elapsed: 43.0956 learning rate: 0.001696, scenario: 0, slope: -0.2689642401381974, fluctuations: 0.0\n",
      "step: 34600 loss: 46.024318 time elapsed: 43.1079 learning rate: 0.001696, scenario: 0, slope: -0.18638548919705786, fluctuations: 0.0\n",
      "step: 34610 loss: 45.397439 time elapsed: 43.1210 learning rate: 0.001696, scenario: 0, slope: -0.12598894156638998, fluctuations: 0.0\n",
      "step: 34620 loss: 44.759516 time elapsed: 43.1341 learning rate: 0.001696, scenario: 0, slope: -0.09671323991056777, fluctuations: 0.0\n",
      "step: 34630 loss: 44.244042 time elapsed: 43.1478 learning rate: 0.001696, scenario: 0, slope: -0.08013422976035887, fluctuations: 0.0\n",
      "step: 34640 loss: 43.844935 time elapsed: 43.1603 learning rate: 0.001696, scenario: 0, slope: -0.06975980266811314, fluctuations: 0.0\n",
      "step: 34650 loss: 43.472548 time elapsed: 43.1735 learning rate: 0.001696, scenario: 0, slope: -0.06257853406963403, fluctuations: 0.0\n",
      "step: 34660 loss: 43.130821 time elapsed: 43.1873 learning rate: 0.001696, scenario: 0, slope: -0.05701326058072282, fluctuations: 0.0\n",
      "step: 34670 loss: 42.812057 time elapsed: 43.2020 learning rate: 0.001696, scenario: 0, slope: -0.05218481078639198, fluctuations: 0.0\n",
      "step: 34680 loss: 42.512500 time elapsed: 43.2167 learning rate: 0.001696, scenario: 0, slope: -0.04761559371991436, fluctuations: 0.0\n",
      "step: 34690 loss: 42.134304 time elapsed: 43.2309 learning rate: 0.003999, scenario: 1, slope: -0.043269744934014545, fluctuations: 0.0\n",
      "step: 34700 loss: 41.262158 time elapsed: 43.2442 learning rate: 0.009431, scenario: 1, slope: -0.040946075236395166, fluctuations: 0.0\n",
      "step: 34710 loss: 39.555154 time elapsed: 43.2574 learning rate: 0.024460, scenario: 1, slope: -0.04418187508776026, fluctuations: 0.0\n",
      "step: 34720 loss: 55055.671092 time elapsed: 43.2698 learning rate: 0.014299, scenario: -1, slope: 151.70252142144517, fluctuations: 0.01\n",
      "step: 34730 loss: 29434.907650 time elapsed: 43.2821 learning rate: 0.004986, scenario: -1, slope: 285.76357388389005, fluctuations: 0.03\n",
      "step: 34740 loss: 11498.113376 time elapsed: 43.2940 learning rate: 0.001738, scenario: -1, slope: 299.71986079826576, fluctuations: 0.05\n",
      "step: 34750 loss: 9435.023878 time elapsed: 43.3060 learning rate: 0.000606, scenario: -1, slope: 268.2989298747253, fluctuations: 0.06\n",
      "step: 34760 loss: 8492.012917 time elapsed: 43.3181 learning rate: 0.000211, scenario: -1, slope: 211.60958379932055, fluctuations: 0.06\n",
      "step: 34770 loss: 8310.355267 time elapsed: 43.3299 learning rate: 0.000074, scenario: -1, slope: 147.46134742469727, fluctuations: 0.06\n",
      "step: 34780 loss: 8248.311645 time elapsed: 43.3418 learning rate: 0.000026, scenario: -1, slope: 75.36494450394653, fluctuations: 0.06\n",
      "step: 34790 loss: 8226.866010 time elapsed: 43.3530 learning rate: 0.000010, scenario: 0, slope: -7.403904162319552, fluctuations: 0.06\n",
      "step: 34800 loss: 8215.564763 time elapsed: 43.3648 learning rate: 0.000010, scenario: 0, slope: -94.39796953162164, fluctuations: 0.06\n",
      "step: 34810 loss: 8204.435870 time elapsed: 43.3776 learning rate: 0.000010, scenario: 0, slope: -223.42390920874394, fluctuations: 0.06\n",
      "step: 34820 loss: 8193.475375 time elapsed: 43.3893 learning rate: 0.000010, scenario: 0, slope: -208.21413408973737, fluctuations: 0.04\n",
      "step: 34830 loss: 8182.668925 time elapsed: 43.4028 learning rate: 0.000010, scenario: 0, slope: -53.442559325115326, fluctuations: 0.02\n",
      "step: 34840 loss: 8172.000376 time elapsed: 43.4161 learning rate: 0.000010, scenario: 0, slope: -14.273131463216721, fluctuations: 0.01\n",
      "step: 34850 loss: 8159.654591 time elapsed: 43.4297 learning rate: 0.000019, scenario: 1, slope: -4.464646703143949, fluctuations: 0.0\n",
      "step: 34860 loss: 8130.395563 time elapsed: 43.4422 learning rate: 0.000050, scenario: 1, slope: -1.9560214471979491, fluctuations: 0.0\n",
      "step: 34870 loss: 8056.886896 time elapsed: 43.4546 learning rate: 0.000131, scenario: 1, slope: -1.5759263874074168, fluctuations: 0.0\n",
      "step: 34880 loss: 7875.902581 time elapsed: 43.4674 learning rate: 0.000339, scenario: 1, slope: -2.246230545550356, fluctuations: 0.0\n",
      "step: 34890 loss: 7438.048636 time elapsed: 43.4792 learning rate: 0.000878, scenario: 1, slope: -4.469756693735924, fluctuations: 0.0\n",
      "step: 34900 loss: 6471.878699 time elapsed: 43.4906 learning rate: 0.001711, scenario: 0, slope: -9.199692963896561, fluctuations: 0.0\n",
      "step: 34910 loss: 5458.800738 time elapsed: 43.5031 learning rate: 0.001711, scenario: 0, slope: -19.825586357994766, fluctuations: 0.0\n",
      "step: 34920 loss: 4703.115248 time elapsed: 43.5151 learning rate: 0.001711, scenario: 0, slope: -31.59710828327052, fluctuations: 0.0\n",
      "step: 34930 loss: 4127.914448 time elapsed: 43.5274 learning rate: 0.001711, scenario: 0, slope: -43.12775883026146, fluctuations: 0.0\n",
      "step: 34940 loss: 3673.210761 time elapsed: 43.5397 learning rate: 0.001711, scenario: 0, slope: -52.87128824836886, fluctuations: 0.0\n",
      "step: 34950 loss: 3300.594772 time elapsed: 43.5517 learning rate: 0.001711, scenario: 0, slope: -59.70898169725859, fluctuations: 0.0\n",
      "step: 34960 loss: 2992.966269 time elapsed: 43.5634 learning rate: 0.001711, scenario: 0, slope: -62.8425003667486, fluctuations: 0.0\n",
      "step: 34970 loss: 2746.780938 time elapsed: 43.5760 learning rate: 0.001711, scenario: 0, slope: -61.74992482578696, fluctuations: 0.0\n",
      "step: 34980 loss: 2548.166464 time elapsed: 43.5900 learning rate: 0.001711, scenario: 0, slope: -56.35452144356415, fluctuations: 0.0\n",
      "step: 34990 loss: 2382.390211 time elapsed: 43.6052 learning rate: 0.001711, scenario: 0, slope: -47.50573278606886, fluctuations: 0.0\n",
      "step: 35000 loss: 2241.611820 time elapsed: 43.6197 learning rate: 0.001711, scenario: 0, slope: -38.7075834162259, fluctuations: 0.0\n",
      "step: 35010 loss: 2121.595582 time elapsed: 43.6349 learning rate: 0.001711, scenario: 0, slope: -30.328777045990822, fluctuations: 0.0\n",
      "step: 35020 loss: 2018.306055 time elapsed: 43.6481 learning rate: 0.001711, scenario: 0, slope: -24.686630156096488, fluctuations: 0.0\n",
      "step: 35030 loss: 1927.349020 time elapsed: 43.6610 learning rate: 0.001711, scenario: 0, slope: -20.319174020571328, fluctuations: 0.0\n",
      "step: 35040 loss: 1846.642091 time elapsed: 43.6735 learning rate: 0.001711, scenario: 0, slope: -16.86912199092353, fluctuations: 0.0\n",
      "step: 35050 loss: 1774.175943 time elapsed: 43.6859 learning rate: 0.001711, scenario: 0, slope: -14.138140858258001, fluctuations: 0.0\n",
      "step: 35060 loss: 1707.882142 time elapsed: 43.6988 learning rate: 0.001711, scenario: 0, slope: -12.017845483409038, fluctuations: 0.0\n",
      "step: 35070 loss: 1646.720161 time elapsed: 43.7113 learning rate: 0.001711, scenario: 0, slope: -10.382725133099406, fluctuations: 0.0\n",
      "step: 35080 loss: 1589.827798 time elapsed: 43.7232 learning rate: 0.001711, scenario: 0, slope: -9.096452638648875, fluctuations: 0.0\n",
      "step: 35090 loss: 1536.634918 time elapsed: 43.7351 learning rate: 0.001711, scenario: 0, slope: -8.071778288540152, fluctuations: 0.0\n",
      "step: 35100 loss: 1486.724106 time elapsed: 43.7469 learning rate: 0.001711, scenario: 0, slope: -7.3275789878273025, fluctuations: 0.0\n",
      "step: 35110 loss: 1439.799285 time elapsed: 43.7593 learning rate: 0.001711, scenario: 0, slope: -6.596811838779913, fluctuations: 0.0\n",
      "step: 35120 loss: 1395.640226 time elapsed: 43.7711 learning rate: 0.001711, scenario: 0, slope: -6.059229992404683, fluctuations: 0.0\n",
      "step: 35130 loss: 1354.063954 time elapsed: 43.7832 learning rate: 0.001711, scenario: 0, slope: -5.6102342890937855, fluctuations: 0.0\n",
      "step: 35140 loss: 1314.892550 time elapsed: 43.7953 learning rate: 0.001711, scenario: 0, slope: -5.228213888643067, fluctuations: 0.0\n",
      "step: 35150 loss: 1277.934214 time elapsed: 43.8070 learning rate: 0.001711, scenario: 0, slope: -4.892820879125534, fluctuations: 0.0\n",
      "step: 35160 loss: 1242.985433 time elapsed: 43.8204 learning rate: 0.001711, scenario: 0, slope: -4.592538031889364, fluctuations: 0.0\n",
      "step: 35170 loss: 1209.845865 time elapsed: 43.8335 learning rate: 0.001711, scenario: 0, slope: -4.319707286361622, fluctuations: 0.0\n",
      "step: 35180 loss: 1178.333167 time elapsed: 43.8469 learning rate: 0.001711, scenario: 0, slope: -4.069916622009838, fluctuations: 0.0\n",
      "step: 35190 loss: 1148.289584 time elapsed: 43.8591 learning rate: 0.001711, scenario: 0, slope: -3.8406107103441065, fluctuations: 0.0\n",
      "step: 35200 loss: 1119.580715 time elapsed: 43.8711 learning rate: 0.001711, scenario: 0, slope: -3.650571679409353, fluctuations: 0.0\n",
      "step: 35210 loss: 1092.090834 time elapsed: 43.8846 learning rate: 0.001711, scenario: 0, slope: -3.4382768184964423, fluctuations: 0.0\n",
      "step: 35220 loss: 1065.717365 time elapsed: 43.8966 learning rate: 0.001711, scenario: 0, slope: -3.2634068058905914, fluctuations: 0.0\n",
      "step: 35230 loss: 1040.347148 time elapsed: 43.9081 learning rate: 0.001711, scenario: 0, slope: -3.1046876854772036, fluctuations: 0.0\n",
      "step: 35240 loss: 1012.338676 time elapsed: 43.9202 learning rate: 0.001711, scenario: 0, slope: -2.971621528146165, fluctuations: 0.0\n",
      "step: 35250 loss: 982.828657 time elapsed: 43.9321 learning rate: 0.001711, scenario: 0, slope: -2.8833402308158105, fluctuations: 0.01\n",
      "step: 35260 loss: 946.758437 time elapsed: 43.9438 learning rate: 0.001711, scenario: 0, slope: -2.836099265169427, fluctuations: 0.01\n",
      "step: 35270 loss: 904.777912 time elapsed: 43.9559 learning rate: 0.001711, scenario: 0, slope: -2.893166677236423, fluctuations: 0.01\n",
      "step: 35280 loss: 874.191727 time elapsed: 43.9677 learning rate: 0.001711, scenario: 0, slope: -2.9813736808520956, fluctuations: 0.01\n",
      "step: 35290 loss: 845.654720 time elapsed: 43.9795 learning rate: 0.001711, scenario: 0, slope: -3.0638615818293915, fluctuations: 0.01\n",
      "step: 35300 loss: 818.407471 time elapsed: 43.9907 learning rate: 0.001711, scenario: 0, slope: -3.1223906224573224, fluctuations: 0.01\n",
      "step: 35310 loss: 797.616984 time elapsed: 44.0028 learning rate: 0.001711, scenario: 0, slope: -3.138861171655846, fluctuations: 0.01\n",
      "step: 35320 loss: 778.705083 time elapsed: 44.0156 learning rate: 0.001711, scenario: 0, slope: -3.0778268195320835, fluctuations: 0.01\n",
      "step: 35330 loss: 760.991791 time elapsed: 44.0302 learning rate: 0.001711, scenario: 0, slope: -2.9347661989495024, fluctuations: 0.01\n",
      "step: 35340 loss: 744.105958 time elapsed: 44.0446 learning rate: 0.001711, scenario: 0, slope: -2.727887873949295, fluctuations: 0.0\n",
      "step: 35350 loss: 727.728670 time elapsed: 44.0584 learning rate: 0.001711, scenario: 0, slope: -2.475844086195531, fluctuations: 0.0\n",
      "step: 35360 loss: 711.616694 time elapsed: 44.0716 learning rate: 0.001711, scenario: 0, slope: -2.2006366512458295, fluctuations: 0.0\n",
      "step: 35370 loss: 695.446060 time elapsed: 44.0859 learning rate: 0.001711, scenario: 0, slope: -1.9999337057899291, fluctuations: 0.0\n",
      "step: 35380 loss: 678.887058 time elapsed: 44.0990 learning rate: 0.001711, scenario: 0, slope: -1.8507780180241924, fluctuations: 0.0\n",
      "step: 35390 loss: 662.073594 time elapsed: 44.1122 learning rate: 0.001711, scenario: 0, slope: -1.744202744900786, fluctuations: 0.0\n",
      "step: 35400 loss: 645.189676 time elapsed: 44.1243 learning rate: 0.001711, scenario: 0, slope: -1.6909588111976745, fluctuations: 0.0\n",
      "step: 35410 loss: 627.880676 time elapsed: 44.1373 learning rate: 0.001711, scenario: 0, slope: -1.6649113718498374, fluctuations: 0.0\n",
      "step: 35420 loss: 610.580715 time elapsed: 44.1499 learning rate: 0.001711, scenario: 0, slope: -1.6620373484062663, fluctuations: 0.0\n",
      "step: 35430 loss: 594.373360 time elapsed: 44.1621 learning rate: 0.001711, scenario: 0, slope: -1.6669363522137033, fluctuations: 0.0\n",
      "step: 35440 loss: 579.511989 time elapsed: 44.1742 learning rate: 0.001711, scenario: 0, slope: -1.6681730942994517, fluctuations: 0.0\n",
      "step: 35450 loss: 565.841178 time elapsed: 44.1863 learning rate: 0.001711, scenario: 0, slope: -1.6567667987400672, fluctuations: 0.0\n",
      "step: 35460 loss: 553.098073 time elapsed: 44.1980 learning rate: 0.001711, scenario: 0, slope: -1.6274312238159978, fluctuations: 0.0\n",
      "step: 35470 loss: 541.053977 time elapsed: 44.2096 learning rate: 0.001711, scenario: 0, slope: -1.5789141520052743, fluctuations: 0.0\n",
      "step: 35480 loss: 529.565191 time elapsed: 44.2233 learning rate: 0.001711, scenario: 0, slope: -1.514082808269905, fluctuations: 0.0\n",
      "step: 35490 loss: 518.491873 time elapsed: 44.2368 learning rate: 0.001711, scenario: 0, slope: -1.4382739797421975, fluctuations: 0.0\n",
      "step: 35500 loss: 506.044304 time elapsed: 44.2502 learning rate: 0.001711, scenario: 0, slope: -1.36905236187654, fluctuations: 0.0\n",
      "step: 35510 loss: 488.329322 time elapsed: 44.2643 learning rate: 0.001711, scenario: 0, slope: -1.3111164767102255, fluctuations: 0.0\n",
      "step: 35520 loss: 474.143747 time elapsed: 44.2763 learning rate: 0.001711, scenario: 0, slope: -1.2964499683060637, fluctuations: 0.0\n",
      "step: 35530 loss: 463.023450 time elapsed: 44.2884 learning rate: 0.001711, scenario: 0, slope: -1.2925379610940184, fluctuations: 0.0\n",
      "step: 35540 loss: 453.046000 time elapsed: 44.3003 learning rate: 0.001711, scenario: 0, slope: -1.2862629538449681, fluctuations: 0.0\n",
      "step: 35550 loss: 443.727598 time elapsed: 44.3118 learning rate: 0.001711, scenario: 0, slope: -1.2716644662190961, fluctuations: 0.0\n",
      "step: 35560 loss: 435.069449 time elapsed: 44.3231 learning rate: 0.001711, scenario: 0, slope: -1.2434197400982365, fluctuations: 0.0\n",
      "step: 35570 loss: 426.997008 time elapsed: 44.3348 learning rate: 0.001711, scenario: 0, slope: -1.1977517825338344, fluctuations: 0.0\n",
      "step: 35580 loss: 419.329638 time elapsed: 44.3467 learning rate: 0.001711, scenario: 0, slope: -1.1328542124445715, fluctuations: 0.0\n",
      "step: 35590 loss: 411.998473 time elapsed: 44.3585 learning rate: 0.001711, scenario: 0, slope: -1.0481628395885645, fluctuations: 0.0\n",
      "step: 35600 loss: 404.970152 time elapsed: 44.3699 learning rate: 0.001711, scenario: 0, slope: -0.9598723868964509, fluctuations: 0.0\n",
      "step: 35610 loss: 398.218428 time elapsed: 44.3824 learning rate: 0.001711, scenario: 0, slope: -0.8634306061608739, fluctuations: 0.0\n",
      "step: 35620 loss: 391.723710 time elapsed: 44.3946 learning rate: 0.001711, scenario: 0, slope: -0.8071004553654522, fluctuations: 0.0\n",
      "step: 35630 loss: 385.467602 time elapsed: 44.4064 learning rate: 0.001711, scenario: 0, slope: -0.7635821992784303, fluctuations: 0.0\n",
      "step: 35640 loss: 379.432580 time elapsed: 44.4183 learning rate: 0.001711, scenario: 0, slope: -0.7260335602932173, fluctuations: 0.0\n",
      "step: 35650 loss: 373.602447 time elapsed: 44.4317 learning rate: 0.001711, scenario: 0, slope: -0.6938517938941444, fluctuations: 0.0\n",
      "step: 35660 loss: 367.962248 time elapsed: 44.4447 learning rate: 0.001711, scenario: 0, slope: -0.6658388668365998, fluctuations: 0.0\n",
      "step: 35670 loss: 362.498162 time elapsed: 44.4581 learning rate: 0.001711, scenario: 0, slope: -0.640595337586536, fluctuations: 0.0\n",
      "step: 35680 loss: 357.197462 time elapsed: 44.4709 learning rate: 0.001711, scenario: 0, slope: -0.6173628586416515, fluctuations: 0.0\n",
      "step: 35690 loss: 352.048428 time elapsed: 44.4836 learning rate: 0.001711, scenario: 0, slope: -0.5958598446136449, fluctuations: 0.0\n",
      "step: 35700 loss: 347.040249 time elapsed: 44.4956 learning rate: 0.001711, scenario: 0, slope: -0.5778497173890641, fluctuations: 0.0\n",
      "step: 35710 loss: 342.162931 time elapsed: 44.5081 learning rate: 0.001711, scenario: 0, slope: -0.5574306178876771, fluctuations: 0.0\n",
      "step: 35720 loss: 337.407200 time elapsed: 44.5202 learning rate: 0.001711, scenario: 0, slope: -0.540277626873422, fluctuations: 0.0\n",
      "step: 35730 loss: 332.764440 time elapsed: 44.5320 learning rate: 0.001711, scenario: 0, slope: -0.5243620889322386, fluctuations: 0.0\n",
      "step: 35740 loss: 328.226632 time elapsed: 44.5449 learning rate: 0.001711, scenario: 0, slope: -0.5095893951742236, fluctuations: 0.0\n",
      "step: 35750 loss: 323.786342 time elapsed: 44.5576 learning rate: 0.001711, scenario: 0, slope: -0.49587309705018395, fluctuations: 0.0\n",
      "step: 35760 loss: 319.436733 time elapsed: 44.5709 learning rate: 0.001711, scenario: 0, slope: -0.48313398293984255, fluctuations: 0.0\n",
      "step: 35770 loss: 315.171608 time elapsed: 44.5848 learning rate: 0.001711, scenario: 0, slope: -0.4712990621624348, fluctuations: 0.0\n",
      "step: 35780 loss: 310.985499 time elapsed: 44.5981 learning rate: 0.001711, scenario: 0, slope: -0.46029991207297666, fluctuations: 0.0\n",
      "step: 35790 loss: 306.873777 time elapsed: 44.6112 learning rate: 0.001711, scenario: 0, slope: -0.450070371671468, fluctuations: 0.0\n",
      "step: 35800 loss: 302.832786 time elapsed: 44.6234 learning rate: 0.001711, scenario: 0, slope: -0.4414668397311295, fluctuations: 0.0\n",
      "step: 35810 loss: 298.859978 time elapsed: 44.6391 learning rate: 0.001711, scenario: 0, slope: -0.4316506516659125, fluctuations: 0.0\n",
      "step: 35820 loss: 294.954002 time elapsed: 44.6534 learning rate: 0.001711, scenario: 0, slope: -0.4233151298211714, fluctuations: 0.0\n",
      "step: 35830 loss: 291.114736 time elapsed: 44.6674 learning rate: 0.001711, scenario: 0, slope: -0.4154540644109129, fluctuations: 0.0\n",
      "step: 35840 loss: 287.343189 time elapsed: 44.6821 learning rate: 0.001711, scenario: 0, slope: -0.40797590870997724, fluctuations: 0.0\n",
      "step: 35850 loss: 283.641264 time elapsed: 44.6945 learning rate: 0.001711, scenario: 0, slope: -0.40078203564981624, fluctuations: 0.0\n",
      "step: 35860 loss: 280.011374 time elapsed: 44.7062 learning rate: 0.001711, scenario: 0, slope: -0.39377022687324836, fluctuations: 0.0\n",
      "step: 35870 loss: 276.455958 time elapsed: 44.7181 learning rate: 0.001711, scenario: 0, slope: -0.38684049306778984, fluctuations: 0.0\n",
      "step: 35880 loss: 272.976977 time elapsed: 44.7302 learning rate: 0.001711, scenario: 0, slope: -0.3799027260677255, fluctuations: 0.0\n",
      "step: 35890 loss: 269.575497 time elapsed: 44.7419 learning rate: 0.001711, scenario: 0, slope: -0.37288505791744647, fluctuations: 0.0\n",
      "step: 35900 loss: 266.251452 time elapsed: 44.7533 learning rate: 0.001711, scenario: 0, slope: -0.3664620684016887, fluctuations: 0.0\n",
      "step: 35910 loss: 263.003614 time elapsed: 44.7659 learning rate: 0.001711, scenario: 0, slope: -0.35845595161705684, fluctuations: 0.0\n",
      "step: 35920 loss: 259.829773 time elapsed: 44.7776 learning rate: 0.001711, scenario: 0, slope: -0.3510448756405668, fluctuations: 0.0\n",
      "step: 35930 loss: 256.727033 time elapsed: 44.7894 learning rate: 0.001711, scenario: 0, slope: -0.34355206267504607, fluctuations: 0.0\n",
      "step: 35940 loss: 253.692158 time elapsed: 44.8008 learning rate: 0.001711, scenario: 0, slope: -0.3360419675819988, fluctuations: 0.0\n",
      "step: 35950 loss: 250.721884 time elapsed: 44.8126 learning rate: 0.001711, scenario: 0, slope: -0.3285893293831935, fluctuations: 0.0\n",
      "step: 35960 loss: 247.813156 time elapsed: 44.8242 learning rate: 0.001711, scenario: 0, slope: -0.3212681767905302, fluctuations: 0.0\n",
      "step: 35970 loss: 244.963287 time elapsed: 44.8358 learning rate: 0.001711, scenario: 0, slope: -0.3141420252537765, fluctuations: 0.0\n",
      "step: 35980 loss: 242.170038 time elapsed: 44.8485 learning rate: 0.001711, scenario: 0, slope: -0.3072568433419896, fluctuations: 0.0\n",
      "step: 35990 loss: 239.431653 time elapsed: 44.8622 learning rate: 0.001711, scenario: 0, slope: -0.30063760562379327, fluctuations: 0.0\n",
      "step: 36000 loss: 236.746864 time elapsed: 44.8758 learning rate: 0.001711, scenario: 0, slope: -0.29491141641151936, fluctuations: 0.0\n",
      "step: 36010 loss: 234.114867 time elapsed: 44.8897 learning rate: 0.001711, scenario: 0, slope: -0.28819504283700076, fluctuations: 0.0\n",
      "step: 36020 loss: 231.535295 time elapsed: 44.9023 learning rate: 0.001711, scenario: 0, slope: -0.28232977273925763, fluctuations: 0.0\n",
      "step: 36030 loss: 229.008162 time elapsed: 44.9142 learning rate: 0.001711, scenario: 0, slope: -0.27665566280712084, fluctuations: 0.0\n",
      "step: 36040 loss: 226.533786 time elapsed: 44.9270 learning rate: 0.001711, scenario: 0, slope: -0.27113121390109024, fluctuations: 0.0\n",
      "step: 36050 loss: 224.112681 time elapsed: 44.9395 learning rate: 0.001711, scenario: 0, slope: -0.26571407128142127, fluctuations: 0.0\n",
      "step: 36060 loss: 221.745408 time elapsed: 44.9525 learning rate: 0.001711, scenario: 0, slope: -0.2603641639775433, fluctuations: 0.0\n",
      "step: 36070 loss: 219.432402 time elapsed: 44.9657 learning rate: 0.001711, scenario: 0, slope: -0.25504645380483926, fluctuations: 0.0\n",
      "step: 36080 loss: 217.173790 time elapsed: 44.9785 learning rate: 0.001711, scenario: 0, slope: -0.24973347340647825, fluctuations: 0.0\n",
      "step: 36090 loss: 214.969238 time elapsed: 44.9911 learning rate: 0.001711, scenario: 0, slope: -0.24440763973957397, fluctuations: 0.0\n",
      "step: 36100 loss: 212.817853 time elapsed: 45.0039 learning rate: 0.001711, scenario: 0, slope: -0.23959829598442162, fluctuations: 0.0\n",
      "step: 36110 loss: 210.718177 time elapsed: 45.0169 learning rate: 0.001882, scenario: 1, slope: -0.2337066523980565, fluctuations: 0.0\n",
      "step: 36120 loss: 207.469495 time elapsed: 45.0302 learning rate: 0.004882, scenario: 1, slope: -0.230714991396798, fluctuations: 0.0\n",
      "step: 36130 loss: 1545.486892 time elapsed: 45.0435 learning rate: 0.005075, scenario: -1, slope: 1.4315867218872091, fluctuations: 0.0\n",
      "step: 36140 loss: 342.833086 time elapsed: 45.0580 learning rate: 0.001769, scenario: -1, slope: 0.9662903629771804, fluctuations: 0.05\n",
      "step: 36150 loss: 210.870647 time elapsed: 45.0725 learning rate: 0.000617, scenario: -1, slope: 0.8975989175571932, fluctuations: 0.07\n",
      "step: 36160 loss: 210.952493 time elapsed: 45.0874 learning rate: 0.000215, scenario: -1, slope: 0.619540103423998, fluctuations: 0.08\n",
      "step: 36170 loss: 205.911577 time elapsed: 45.1015 learning rate: 0.000075, scenario: -1, slope: 0.3190093116728927, fluctuations: 0.09\n",
      "step: 36180 loss: 205.728484 time elapsed: 45.1142 learning rate: 0.000026, scenario: -1, slope: 0.024305159380138695, fluctuations: 0.1\n",
      "step: 36190 loss: 205.240931 time elapsed: 45.1264 learning rate: 0.000049, scenario: 0, slope: -0.2786180076753181, fluctuations: 0.1\n",
      "step: 36200 loss: 205.031028 time elapsed: 45.1383 learning rate: 0.000049, scenario: 0, slope: -0.5517362869017483, fluctuations: 0.1\n",
      "step: 36210 loss: 204.805323 time elapsed: 45.1508 learning rate: 0.000049, scenario: 0, slope: -0.9162705868596746, fluctuations: 0.1\n",
      "step: 36220 loss: 204.597410 time elapsed: 45.1627 learning rate: 0.000049, scenario: 0, slope: -1.317516490823916, fluctuations: 0.1\n",
      "step: 36230 loss: 204.407299 time elapsed: 45.1747 learning rate: 0.000049, scenario: 0, slope: -0.5791157209568718, fluctuations: 0.09\n",
      "step: 36240 loss: 204.218622 time elapsed: 45.1865 learning rate: 0.000054, scenario: 0, slope: -0.23714822676413244, fluctuations: 0.05\n",
      "step: 36250 loss: 203.927168 time elapsed: 45.1982 learning rate: 0.000140, scenario: 1, slope: -0.07931755906234572, fluctuations: 0.03\n",
      "step: 36260 loss: 203.210770 time elapsed: 45.2093 learning rate: 0.000362, scenario: 1, slope: -0.039207738232017335, fluctuations: 0.01\n",
      "step: 36270 loss: 201.558962 time elapsed: 45.2209 learning rate: 0.000939, scenario: 1, slope: -0.03120178145833976, fluctuations: 0.01\n",
      "step: 36280 loss: 198.270685 time elapsed: 45.2328 learning rate: 0.002435, scenario: 1, slope: -0.047327406915519184, fluctuations: 0.0\n",
      "step: 36290 loss: 192.814877 time elapsed: 45.2442 learning rate: 0.006315, scenario: 1, slope: -0.08284342394969547, fluctuations: 0.0\n",
      "step: 36300 loss: 183.741580 time elapsed: 45.2558 learning rate: 0.014891, scenario: 1, slope: -0.14110363354854938, fluctuations: 0.0\n",
      "step: 36310 loss: 5965.119527 time elapsed: 45.2688 learning rate: 0.006731, scenario: -1, slope: 46.87830915306797, fluctuations: 0.03\n",
      "step: 36320 loss: 3775.591655 time elapsed: 45.2820 learning rate: 0.002347, scenario: -1, slope: 71.7091267057669, fluctuations: 0.05\n",
      "step: 36330 loss: 2150.202893 time elapsed: 45.2956 learning rate: 0.000818, scenario: -1, slope: 68.29044850712403, fluctuations: 0.06\n",
      "step: 36340 loss: 1841.643331 time elapsed: 45.3090 learning rate: 0.000285, scenario: -1, slope: 56.66019683154018, fluctuations: 0.07\n",
      "step: 36350 loss: 1657.567907 time elapsed: 45.3213 learning rate: 0.000099, scenario: -1, slope: 40.482954321717756, fluctuations: 0.07\n",
      "step: 36360 loss: 1629.136160 time elapsed: 45.3335 learning rate: 0.000035, scenario: -1, slope: 23.35892428590074, fluctuations: 0.07\n",
      "step: 36370 loss: 1620.790318 time elapsed: 45.3456 learning rate: 0.000012, scenario: -1, slope: 4.799552571252016, fluctuations: 0.07\n",
      "step: 36380 loss: 1617.377176 time elapsed: 45.3570 learning rate: 0.000010, scenario: 0, slope: -16.180487376599764, fluctuations: 0.07\n",
      "step: 36390 loss: 1614.243116 time elapsed: 45.3687 learning rate: 0.000010, scenario: 0, slope: -40.964603030831235, fluctuations: 0.07\n",
      "step: 36400 loss: 1611.163441 time elapsed: 45.3803 learning rate: 0.000010, scenario: 0, slope: -68.2091895003474, fluctuations: 0.07\n",
      "step: 36410 loss: 1608.136118 time elapsed: 45.3935 learning rate: 0.000010, scenario: 0, slope: -42.99652388765637, fluctuations: 0.03\n",
      "step: 36420 loss: 1605.156484 time elapsed: 45.4065 learning rate: 0.000010, scenario: 0, slope: -9.4489186025935, fluctuations: 0.02\n",
      "step: 36430 loss: 1602.219420 time elapsed: 45.4186 learning rate: 0.000010, scenario: 0, slope: -2.7177253613493773, fluctuations: 0.01\n",
      "step: 36440 loss: 1598.825741 time elapsed: 45.4302 learning rate: 0.000019, scenario: 1, slope: -0.8390782818604446, fluctuations: 0.0\n",
      "step: 36450 loss: 1590.811743 time elapsed: 45.4415 learning rate: 0.000050, scenario: 1, slope: -0.4041875360100869, fluctuations: 0.0\n",
      "step: 36460 loss: 1570.894119 time elapsed: 45.4526 learning rate: 0.000128, scenario: 1, slope: -0.398711450996028, fluctuations: 0.0\n",
      "step: 36470 loss: 1523.126980 time elapsed: 45.4657 learning rate: 0.000333, scenario: 1, slope: -0.6073578642518977, fluctuations: 0.0\n",
      "step: 36480 loss: 1415.472438 time elapsed: 45.4792 learning rate: 0.000864, scenario: 1, slope: -1.182427814758882, fluctuations: 0.0\n",
      "step: 36490 loss: 1227.343129 time elapsed: 45.4929 learning rate: 0.001265, scenario: 0, slope: -2.4474377516657113, fluctuations: 0.0\n",
      "step: 36500 loss: 1062.061486 time elapsed: 45.5064 learning rate: 0.001265, scenario: 0, slope: -4.143141923128833, fluctuations: 0.0\n",
      "step: 36510 loss: 931.663813 time elapsed: 45.5204 learning rate: 0.001265, scenario: 0, slope: -6.485524272023806, fluctuations: 0.0\n",
      "step: 36520 loss: 835.506329 time elapsed: 45.5325 learning rate: 0.001265, scenario: 0, slope: -8.489597565999782, fluctuations: 0.0\n",
      "step: 36530 loss: 762.418108 time elapsed: 45.5445 learning rate: 0.001265, scenario: 0, slope: -10.080413371308879, fluctuations: 0.0\n",
      "step: 36540 loss: 704.148341 time elapsed: 45.5567 learning rate: 0.001265, scenario: 0, slope: -11.066041447587093, fluctuations: 0.0\n",
      "step: 36550 loss: 656.007038 time elapsed: 45.5690 learning rate: 0.001265, scenario: 0, slope: -11.32379694599616, fluctuations: 0.0\n",
      "step: 36560 loss: 614.141433 time elapsed: 45.5809 learning rate: 0.001265, scenario: 0, slope: -10.80801017219066, fluctuations: 0.0\n",
      "step: 36570 loss: 573.653629 time elapsed: 45.5932 learning rate: 0.001265, scenario: 0, slope: -9.590518438769756, fluctuations: 0.0\n",
      "step: 36580 loss: 536.793616 time elapsed: 45.6052 learning rate: 0.001265, scenario: 0, slope: -7.94453599679743, fluctuations: 0.0\n",
      "step: 36590 loss: 512.564712 time elapsed: 45.6176 learning rate: 0.001265, scenario: 0, slope: -6.369189380410805, fluctuations: 0.0\n",
      "step: 36600 loss: 492.809870 time elapsed: 45.6302 learning rate: 0.001265, scenario: 0, slope: -5.268200808388794, fluctuations: 0.0\n",
      "step: 36610 loss: 476.199273 time elapsed: 45.6440 learning rate: 0.001265, scenario: 0, slope: -4.262854211638409, fluctuations: 0.0\n",
      "step: 36620 loss: 461.917296 time elapsed: 45.6567 learning rate: 0.001265, scenario: 0, slope: -3.576196649876188, fluctuations: 0.0\n",
      "step: 36630 loss: 449.285081 time elapsed: 45.6700 learning rate: 0.001265, scenario: 0, slope: -3.018628340962529, fluctuations: 0.0\n",
      "step: 36640 loss: 437.995439 time elapsed: 45.6844 learning rate: 0.001265, scenario: 0, slope: -2.5451012404377624, fluctuations: 0.0\n",
      "step: 36650 loss: 427.814258 time elapsed: 45.6988 learning rate: 0.001265, scenario: 0, slope: -2.132022241362945, fluctuations: 0.0\n",
      "step: 36660 loss: 418.557348 time elapsed: 45.7120 learning rate: 0.001265, scenario: 0, slope: -1.7707161118822001, fluctuations: 0.0\n",
      "step: 36670 loss: 410.066672 time elapsed: 45.7263 learning rate: 0.001265, scenario: 0, slope: -1.4734974682249236, fluctuations: 0.0\n",
      "step: 36680 loss: 402.208889 time elapsed: 45.7388 learning rate: 0.001265, scenario: 0, slope: -1.2688446502981803, fluctuations: 0.0\n",
      "step: 36690 loss: 394.872435 time elapsed: 45.7514 learning rate: 0.001265, scenario: 0, slope: -1.1227784658298143, fluctuations: 0.0\n",
      "step: 36700 loss: 387.962150 time elapsed: 45.7631 learning rate: 0.001265, scenario: 0, slope: -1.0196231760216947, fluctuations: 0.0\n",
      "step: 36710 loss: 381.395680 time elapsed: 45.7755 learning rate: 0.001265, scenario: 0, slope: -0.9180630094301537, fluctuations: 0.0\n",
      "step: 36720 loss: 375.100155 time elapsed: 45.7873 learning rate: 0.001265, scenario: 0, slope: -0.8432429603739666, fluctuations: 0.0\n",
      "step: 36730 loss: 369.011405 time elapsed: 45.7989 learning rate: 0.001265, scenario: 0, slope: -0.7816953564912006, fluctuations: 0.0\n",
      "step: 36740 loss: 363.081085 time elapsed: 45.8107 learning rate: 0.001265, scenario: 0, slope: -0.7312954228256264, fluctuations: 0.0\n",
      "step: 36750 loss: 357.300498 time elapsed: 45.8224 learning rate: 0.001265, scenario: 0, slope: -0.6902617091230144, fluctuations: 0.0\n",
      "step: 36760 loss: 351.726400 time elapsed: 45.8338 learning rate: 0.001265, scenario: 0, slope: -0.6567752840468737, fluctuations: 0.0\n",
      "step: 36770 loss: 346.418023 time elapsed: 45.8456 learning rate: 0.001265, scenario: 0, slope: -0.6287624393243807, fluctuations: 0.0\n",
      "step: 36780 loss: 341.329049 time elapsed: 45.8574 learning rate: 0.001265, scenario: 0, slope: -0.6043901735470304, fluctuations: 0.0\n",
      "step: 36790 loss: 336.411467 time elapsed: 45.8689 learning rate: 0.001265, scenario: 0, slope: -0.5824186552697143, fluctuations: 0.0\n",
      "step: 36800 loss: 331.654397 time elapsed: 45.8805 learning rate: 0.001265, scenario: 0, slope: -0.5639407221117833, fluctuations: 0.0\n",
      "step: 36810 loss: 327.039371 time elapsed: 45.8937 learning rate: 0.001265, scenario: 0, slope: -0.5423493707884172, fluctuations: 0.0\n",
      "step: 36820 loss: 322.552013 time elapsed: 45.9068 learning rate: 0.001265, scenario: 0, slope: -0.5233616130037498, fluctuations: 0.0\n",
      "step: 36830 loss: 318.179252 time elapsed: 45.9198 learning rate: 0.001265, scenario: 0, slope: -0.5050141724017871, fluctuations: 0.0\n",
      "step: 36840 loss: 313.910739 time elapsed: 45.9334 learning rate: 0.001265, scenario: 0, slope: -0.4876445612917319, fluctuations: 0.0\n",
      "step: 36850 loss: 309.738243 time elapsed: 45.9452 learning rate: 0.001265, scenario: 0, slope: -0.4717533828872113, fluctuations: 0.0\n",
      "step: 36860 loss: 305.655293 time elapsed: 45.9568 learning rate: 0.001265, scenario: 0, slope: -0.45764953117742474, fluctuations: 0.0\n",
      "step: 36870 loss: 301.657031 time elapsed: 45.9689 learning rate: 0.001265, scenario: 0, slope: -0.44511470907336553, fluctuations: 0.0\n",
      "step: 36880 loss: 297.740174 time elapsed: 45.9809 learning rate: 0.001265, scenario: 0, slope: -0.4337334372802677, fluctuations: 0.0\n",
      "step: 36890 loss: 293.902896 time elapsed: 45.9934 learning rate: 0.001265, scenario: 0, slope: -0.42330348490574227, fluctuations: 0.0\n",
      "step: 36900 loss: 290.144613 time elapsed: 46.0054 learning rate: 0.001265, scenario: 0, slope: -0.4145837519651591, fluctuations: 0.0\n",
      "step: 36910 loss: 286.465442 time elapsed: 46.0188 learning rate: 0.001265, scenario: 0, slope: -0.4045860114149058, fluctuations: 0.0\n",
      "step: 36920 loss: 282.864998 time elapsed: 46.0324 learning rate: 0.001265, scenario: 0, slope: -0.3959619669819233, fluctuations: 0.0\n",
      "step: 36930 loss: 279.339829 time elapsed: 46.0455 learning rate: 0.001265, scenario: 0, slope: -0.3876528474445011, fluctuations: 0.0\n",
      "step: 36940 loss: 275.876537 time elapsed: 46.0577 learning rate: 0.001265, scenario: 0, slope: -0.3796053598099358, fluctuations: 0.0\n",
      "step: 36950 loss: 272.424798 time elapsed: 46.0705 learning rate: 0.001265, scenario: 0, slope: -0.3719420068961332, fluctuations: 0.0\n",
      "step: 36960 loss: 268.711640 time elapsed: 46.0837 learning rate: 0.001265, scenario: 0, slope: -0.3655244462879376, fluctuations: 0.0\n",
      "step: 36970 loss: 263.343631 time elapsed: 46.0986 learning rate: 0.001265, scenario: 0, slope: -0.3663040940448441, fluctuations: 0.0\n",
      "step: 36980 loss: 259.910764 time elapsed: 46.1131 learning rate: 0.001265, scenario: 0, slope: -0.3712633715197847, fluctuations: 0.0\n",
      "step: 36990 loss: 256.571885 time elapsed: 46.1281 learning rate: 0.001265, scenario: 0, slope: -0.3749035288174858, fluctuations: 0.0\n",
      "step: 37000 loss: 253.508968 time elapsed: 46.1432 learning rate: 0.001265, scenario: 0, slope: -0.37559126229646794, fluctuations: 0.0\n",
      "step: 37010 loss: 250.554122 time elapsed: 46.1575 learning rate: 0.001265, scenario: 0, slope: -0.3725160772623395, fluctuations: 0.0\n",
      "step: 37020 loss: 247.715938 time elapsed: 46.1712 learning rate: 0.001265, scenario: 0, slope: -0.36560248374049986, fluctuations: 0.0\n",
      "step: 37030 loss: 244.970092 time elapsed: 46.1845 learning rate: 0.001265, scenario: 0, slope: -0.3546380076178215, fluctuations: 0.0\n",
      "step: 37040 loss: 242.303528 time elapsed: 46.1964 learning rate: 0.001265, scenario: 0, slope: -0.3396255597055293, fluctuations: 0.0\n",
      "step: 37050 loss: 239.713154 time elapsed: 46.2080 learning rate: 0.001265, scenario: 0, slope: -0.32082053118766124, fluctuations: 0.0\n",
      "step: 37060 loss: 237.192522 time elapsed: 46.2194 learning rate: 0.001265, scenario: 0, slope: -0.299437921341321, fluctuations: 0.0\n",
      "step: 37070 loss: 234.736836 time elapsed: 46.2312 learning rate: 0.001265, scenario: 0, slope: -0.2830489376319053, fluctuations: 0.0\n",
      "step: 37080 loss: 232.341441 time elapsed: 46.2426 learning rate: 0.001265, scenario: 0, slope: -0.27254112923908996, fluctuations: 0.0\n",
      "step: 37090 loss: 230.001855 time elapsed: 46.2556 learning rate: 0.001265, scenario: 0, slope: -0.2640517276720177, fluctuations: 0.0\n",
      "step: 37100 loss: 227.714004 time elapsed: 46.2685 learning rate: 0.001265, scenario: 0, slope: -0.25720731782831346, fluctuations: 0.0\n",
      "step: 37110 loss: 225.474228 time elapsed: 46.2824 learning rate: 0.001531, scenario: 1, slope: -0.24963726239724096, fluctuations: 0.0\n",
      "step: 37120 loss: 221.649442 time elapsed: 46.2962 learning rate: 0.003970, scenario: 1, slope: -0.24675963121126918, fluctuations: 0.0\n",
      "step: 37130 loss: 215.154727 time elapsed: 46.3098 learning rate: 0.003970, scenario: 0, slope: -0.26230811802154225, fluctuations: 0.0\n",
      "step: 37140 loss: 208.986607 time elapsed: 46.3233 learning rate: 0.003970, scenario: 0, slope: -0.29616140054355716, fluctuations: 0.0\n",
      "step: 37150 loss: 203.189004 time elapsed: 46.3368 learning rate: 0.003970, scenario: 0, slope: -0.34137862139783487, fluctuations: 0.0\n",
      "step: 37160 loss: 197.722671 time elapsed: 46.3510 learning rate: 0.003970, scenario: 0, slope: -0.39158959001206445, fluctuations: 0.0\n",
      "step: 37170 loss: 192.537597 time elapsed: 46.3637 learning rate: 0.003970, scenario: 0, slope: -0.4410395450161442, fluctuations: 0.0\n",
      "step: 37180 loss: 187.582560 time elapsed: 46.3764 learning rate: 0.003970, scenario: 0, slope: -0.484569012138033, fluctuations: 0.0\n",
      "step: 37190 loss: 182.822172 time elapsed: 46.3888 learning rate: 0.003970, scenario: 0, slope: -0.5175168457845769, fluctuations: 0.0\n",
      "step: 37200 loss: 178.276620 time elapsed: 46.4005 learning rate: 0.003970, scenario: 0, slope: -0.5344779277373942, fluctuations: 0.0\n",
      "step: 37210 loss: 173.998928 time elapsed: 46.4161 learning rate: 0.003970, scenario: 0, slope: -0.5339506327237592, fluctuations: 0.0\n",
      "step: 37220 loss: 169.849789 time elapsed: 46.4289 learning rate: 0.003970, scenario: 0, slope: -0.5127797509917638, fluctuations: 0.0\n",
      "step: 37230 loss: 165.921167 time elapsed: 46.4411 learning rate: 0.003970, scenario: 0, slope: -0.4878231063495138, fluctuations: 0.0\n",
      "step: 37240 loss: 162.184905 time elapsed: 46.4534 learning rate: 0.003970, scenario: 0, slope: -0.4647791422610203, fluctuations: 0.0\n",
      "step: 37250 loss: 158.578772 time elapsed: 46.4667 learning rate: 0.003970, scenario: 0, slope: -0.4433779053956493, fluctuations: 0.0\n",
      "step: 37260 loss: 155.078224 time elapsed: 46.4799 learning rate: 0.003970, scenario: 0, slope: -0.4234921113861934, fluctuations: 0.0\n",
      "step: 37270 loss: 153.599640 time elapsed: 46.4926 learning rate: 0.003970, scenario: 0, slope: -0.40302832470417477, fluctuations: 0.01\n",
      "step: 37280 loss: 148.767554 time elapsed: 46.5049 learning rate: 0.003970, scenario: 0, slope: -0.38494536519885525, fluctuations: 0.05\n",
      "step: 37290 loss: 145.751832 time elapsed: 46.5173 learning rate: 0.003970, scenario: 0, slope: -0.3689134861003923, fluctuations: 0.09\n",
      "step: 37300 loss: 141.975239 time elapsed: 46.5311 learning rate: 0.003970, scenario: 0, slope: -0.35587796810861094, fluctuations: 0.1\n",
      "step: 37310 loss: 138.703676 time elapsed: 46.5455 learning rate: 0.003970, scenario: 0, slope: -0.3447759810484513, fluctuations: 0.1\n",
      "step: 37320 loss: 135.799879 time elapsed: 46.5598 learning rate: 0.003970, scenario: 0, slope: -0.3365088278547935, fluctuations: 0.1\n",
      "step: 37330 loss: 133.026787 time elapsed: 46.5719 learning rate: 0.003970, scenario: 0, slope: -0.32921677182240994, fluctuations: 0.1\n",
      "step: 37340 loss: 130.398049 time elapsed: 46.5840 learning rate: 0.003970, scenario: 0, slope: -0.3216628435271221, fluctuations: 0.1\n",
      "step: 37350 loss: 127.893488 time elapsed: 46.5960 learning rate: 0.003970, scenario: 0, slope: -0.3131946580642548, fluctuations: 0.1\n",
      "step: 37360 loss: 125.554513 time elapsed: 46.6082 learning rate: 0.003970, scenario: 0, slope: -0.3031151654797823, fluctuations: 0.1\n",
      "step: 37370 loss: 123.284615 time elapsed: 46.6203 learning rate: 0.003970, scenario: 0, slope: -0.2877643648390966, fluctuations: 0.09\n",
      "step: 37380 loss: 121.049280 time elapsed: 46.6321 learning rate: 0.003970, scenario: 0, slope: -0.2736792669617171, fluctuations: 0.04\n",
      "step: 37390 loss: 118.911412 time elapsed: 46.6441 learning rate: 0.003970, scenario: 0, slope: -0.260812460736412, fluctuations: 0.0\n",
      "step: 37400 loss: 116.818550 time elapsed: 46.6558 learning rate: 0.003970, scenario: 0, slope: -0.24868485588622147, fluctuations: 0.0\n",
      "step: 37410 loss: 114.991071 time elapsed: 46.6688 learning rate: 0.003970, scenario: 0, slope: -0.23643319323408057, fluctuations: 0.0\n",
      "step: 37420 loss: 112.974685 time elapsed: 46.6817 learning rate: 0.003970, scenario: 0, slope: -0.2265618170946883, fluctuations: 0.0\n",
      "step: 37430 loss: 111.015748 time elapsed: 46.6958 learning rate: 0.003970, scenario: 0, slope: -0.21804070524209188, fluctuations: 0.0\n",
      "step: 37440 loss: 109.200426 time elapsed: 46.7090 learning rate: 0.003970, scenario: 0, slope: -0.21052673995968718, fluctuations: 0.0\n",
      "step: 37450 loss: 107.451733 time elapsed: 46.7246 learning rate: 0.003970, scenario: 0, slope: -0.20382339134186203, fluctuations: 0.0\n",
      "step: 37460 loss: 105.714029 time elapsed: 46.7394 learning rate: 0.003970, scenario: 0, slope: -0.19677380483257584, fluctuations: 0.0\n",
      "step: 37470 loss: 104.133154 time elapsed: 46.7553 learning rate: 0.003970, scenario: 0, slope: -0.1907168723047786, fluctuations: 0.0\n",
      "step: 37480 loss: 102.480027 time elapsed: 46.7709 learning rate: 0.003970, scenario: 0, slope: -0.1847848118201344, fluctuations: 0.0\n",
      "step: 37490 loss: 100.918584 time elapsed: 46.7862 learning rate: 0.003970, scenario: 0, slope: -0.17929847266137358, fluctuations: 0.0\n",
      "step: 37500 loss: 99.405610 time elapsed: 46.8006 learning rate: 0.003970, scenario: 0, slope: -0.17458070306268592, fluctuations: 0.0\n",
      "step: 37510 loss: 98.366009 time elapsed: 46.8160 learning rate: 0.003970, scenario: 0, slope: -0.16826786407397776, fluctuations: 0.01\n",
      "step: 37520 loss: 96.757630 time elapsed: 46.8289 learning rate: 0.003970, scenario: 0, slope: -0.16202620201734722, fluctuations: 0.03\n",
      "step: 37530 loss: 95.184252 time elapsed: 46.8413 learning rate: 0.003970, scenario: 0, slope: -0.1569858958218443, fluctuations: 0.05\n",
      "step: 37540 loss: 93.712182 time elapsed: 46.8540 learning rate: 0.003970, scenario: 0, slope: -0.1526857568692468, fluctuations: 0.05\n",
      "step: 37550 loss: 92.626536 time elapsed: 46.8665 learning rate: 0.003970, scenario: 0, slope: -0.14909026226125568, fluctuations: 0.07\n",
      "step: 37560 loss: 91.102434 time elapsed: 46.8787 learning rate: 0.003970, scenario: 0, slope: -0.1450872997418666, fluctuations: 0.1\n",
      "step: 37570 loss: 90.231001 time elapsed: 46.8907 learning rate: 0.003970, scenario: 0, slope: -0.14116489043057204, fluctuations: 0.13\n",
      "step: 37580 loss: 88.702405 time elapsed: 46.9032 learning rate: 0.003970, scenario: 0, slope: -0.13760308616834271, fluctuations: 0.16\n",
      "step: 37590 loss: 87.457933 time elapsed: 46.9150 learning rate: 0.003970, scenario: 0, slope: -0.13475022947003434, fluctuations: 0.18\n",
      "step: 37600 loss: 86.287730 time elapsed: 46.9284 learning rate: 0.003970, scenario: 0, slope: -0.1323370315042991, fluctuations: 0.18\n",
      "step: 37610 loss: 85.104853 time elapsed: 46.9425 learning rate: 0.003970, scenario: 0, slope: -0.12932711326864177, fluctuations: 0.16\n",
      "step: 37620 loss: 84.145272 time elapsed: 46.9560 learning rate: 0.003970, scenario: 0, slope: -0.12532587524213856, fluctuations: 0.14\n",
      "step: 37630 loss: 83.155788 time elapsed: 46.9696 learning rate: 0.003970, scenario: 0, slope: -0.12133324590496049, fluctuations: 0.15\n",
      "step: 37640 loss: 81.954486 time elapsed: 46.9823 learning rate: 0.003970, scenario: 0, slope: -0.11727639358551128, fluctuations: 0.17\n",
      "step: 37650 loss: 80.937620 time elapsed: 46.9940 learning rate: 0.003970, scenario: 0, slope: -0.11394098370270529, fluctuations: 0.16\n",
      "step: 37660 loss: 79.936605 time elapsed: 47.0058 learning rate: 0.003970, scenario: 0, slope: -0.11150950684190385, fluctuations: 0.12\n",
      "step: 37670 loss: 78.951682 time elapsed: 47.0175 learning rate: 0.003970, scenario: 0, slope: -0.10885239097197633, fluctuations: 0.09\n",
      "step: 37680 loss: 78.028247 time elapsed: 47.0295 learning rate: 0.003970, scenario: 0, slope: -0.10598917805841403, fluctuations: 0.07\n",
      "step: 37690 loss: 77.421027 time elapsed: 47.0416 learning rate: 0.003970, scenario: 0, slope: -0.10310275648188405, fluctuations: 0.07\n",
      "step: 37700 loss: 76.697083 time elapsed: 47.0531 learning rate: 0.003970, scenario: 0, slope: -0.09929804027888206, fluctuations: 0.08\n",
      "step: 37710 loss: 75.452177 time elapsed: 47.0654 learning rate: 0.003970, scenario: 0, slope: -0.0972533333381497, fluctuations: 0.11\n",
      "step: 37720 loss: 74.396440 time elapsed: 47.0774 learning rate: 0.003970, scenario: 0, slope: -0.09603582938928354, fluctuations: 0.11\n",
      "step: 37730 loss: 73.496224 time elapsed: 47.0893 learning rate: 0.003970, scenario: 0, slope: -0.09465250517797925, fluctuations: 0.08\n",
      "step: 37740 loss: 72.626973 time elapsed: 47.1015 learning rate: 0.003970, scenario: 0, slope: -0.09266213086566207, fluctuations: 0.07\n",
      "step: 37750 loss: 71.867173 time elapsed: 47.1137 learning rate: 0.003970, scenario: 0, slope: -0.09154590158232634, fluctuations: 0.06\n",
      "step: 37760 loss: 70.993290 time elapsed: 47.1260 learning rate: 0.003970, scenario: 0, slope: -0.09031258259865718, fluctuations: 0.06\n",
      "step: 37770 loss: 70.137571 time elapsed: 47.1382 learning rate: 0.003970, scenario: 0, slope: -0.08972057700357847, fluctuations: 0.06\n",
      "step: 37780 loss: 69.392106 time elapsed: 47.1520 learning rate: 0.003970, scenario: 0, slope: -0.08939374822271036, fluctuations: 0.06\n",
      "step: 37790 loss: 68.552275 time elapsed: 47.1668 learning rate: 0.003970, scenario: 0, slope: -0.08644788332612957, fluctuations: 0.05\n",
      "step: 37800 loss: 67.726077 time elapsed: 47.1811 learning rate: 0.003970, scenario: 0, slope: -0.08367386304786387, fluctuations: 0.05\n",
      "step: 37810 loss: 66.958288 time elapsed: 47.1949 learning rate: 0.003970, scenario: 0, slope: -0.08217312546958364, fluctuations: 0.03\n",
      "step: 37820 loss: 66.187309 time elapsed: 47.2068 learning rate: 0.003970, scenario: 0, slope: -0.08134607578932541, fluctuations: 0.03\n",
      "step: 37830 loss: 65.364528 time elapsed: 47.2200 learning rate: 0.003970, scenario: 0, slope: -0.08076088376725873, fluctuations: 0.03\n",
      "step: 37840 loss: 64.621835 time elapsed: 47.2326 learning rate: 0.003970, scenario: 0, slope: -0.08042562756145893, fluctuations: 0.03\n",
      "step: 37850 loss: 64.041588 time elapsed: 47.2457 learning rate: 0.003970, scenario: 0, slope: -0.07971827677309762, fluctuations: 0.03\n",
      "step: 37860 loss: 63.413088 time elapsed: 47.2582 learning rate: 0.003970, scenario: 0, slope: -0.07712765602010661, fluctuations: 0.05\n",
      "step: 37870 loss: 62.471756 time elapsed: 47.2709 learning rate: 0.003970, scenario: 0, slope: -0.07630128893714362, fluctuations: 0.06\n",
      "step: 37880 loss: 61.811820 time elapsed: 47.2838 learning rate: 0.003970, scenario: 0, slope: -0.07571375649201954, fluctuations: 0.06\n",
      "step: 37890 loss: 61.060343 time elapsed: 47.2961 learning rate: 0.003970, scenario: 0, slope: -0.07367829437133225, fluctuations: 0.05\n",
      "step: 37900 loss: 60.370902 time elapsed: 47.3079 learning rate: 0.003970, scenario: 0, slope: -0.07288391189071627, fluctuations: 0.03\n",
      "step: 37910 loss: 61.289897 time elapsed: 47.3201 learning rate: 0.003970, scenario: 0, slope: -0.07019373510035187, fluctuations: 0.03\n",
      "step: 37920 loss: 59.695682 time elapsed: 47.3326 learning rate: 0.003970, scenario: 0, slope: -0.0674185624828214, fluctuations: 0.05\n",
      "step: 37930 loss: 58.471535 time elapsed: 47.3462 learning rate: 0.003970, scenario: 0, slope: -0.06684770623792259, fluctuations: 0.08\n",
      "step: 37940 loss: 57.724932 time elapsed: 47.3596 learning rate: 0.003970, scenario: 0, slope: -0.06753255907923432, fluctuations: 0.1\n",
      "step: 37950 loss: 57.069142 time elapsed: 47.3727 learning rate: 0.003970, scenario: 0, slope: -0.06842990557063731, fluctuations: 0.1\n",
      "step: 37960 loss: 56.381046 time elapsed: 47.3851 learning rate: 0.003970, scenario: 0, slope: -0.06766938296516674, fluctuations: 0.08\n",
      "step: 37970 loss: 55.697772 time elapsed: 47.3975 learning rate: 0.003970, scenario: 0, slope: -0.06811775288207982, fluctuations: 0.07\n",
      "step: 37980 loss: 55.021863 time elapsed: 47.4098 learning rate: 0.003970, scenario: 0, slope: -0.06887925190587915, fluctuations: 0.07\n",
      "step: 37990 loss: 54.348594 time elapsed: 47.4221 learning rate: 0.003970, scenario: 0, slope: -0.07010878182852753, fluctuations: 0.07\n",
      "step: 38000 loss: 54.539188 time elapsed: 47.4357 learning rate: 0.003970, scenario: 0, slope: -0.07033134698001893, fluctuations: 0.07\n",
      "step: 38010 loss: 53.637665 time elapsed: 47.4485 learning rate: 0.003970, scenario: 0, slope: -0.06769161673859607, fluctuations: 0.07\n",
      "step: 38020 loss: 52.389914 time elapsed: 47.4608 learning rate: 0.003970, scenario: 0, slope: -0.06463023866459831, fluctuations: 0.07\n",
      "step: 38030 loss: 51.530122 time elapsed: 47.4723 learning rate: 0.003970, scenario: 0, slope: -0.06516389962772384, fluctuations: 0.05\n",
      "step: 38040 loss: 50.859582 time elapsed: 47.4844 learning rate: 0.003970, scenario: 0, slope: -0.0668238804817857, fluctuations: 0.03\n",
      "step: 38050 loss: 50.138021 time elapsed: 47.4964 learning rate: 0.003970, scenario: 0, slope: -0.0685100989902008, fluctuations: 0.03\n",
      "step: 38060 loss: 49.478410 time elapsed: 47.5083 learning rate: 0.003970, scenario: 0, slope: -0.07009236568714422, fluctuations: 0.03\n",
      "step: 38070 loss: 48.917685 time elapsed: 47.5203 learning rate: 0.003970, scenario: 0, slope: -0.07128019514577345, fluctuations: 0.03\n",
      "step: 38080 loss: 49.179012 time elapsed: 47.5319 learning rate: 0.003970, scenario: 0, slope: -0.06765754032861516, fluctuations: 0.04\n",
      "step: 38090 loss: 47.959206 time elapsed: 47.5443 learning rate: 0.003970, scenario: 0, slope: -0.06767496106144051, fluctuations: 0.05\n",
      "step: 38100 loss: 47.055075 time elapsed: 47.5577 learning rate: 0.003970, scenario: 0, slope: -0.06687826238387666, fluctuations: 0.07\n",
      "step: 38110 loss: 46.519708 time elapsed: 47.5720 learning rate: 0.003970, scenario: 0, slope: -0.06303018977733406, fluctuations: 0.05\n",
      "step: 38120 loss: 45.890354 time elapsed: 47.5856 learning rate: 0.003970, scenario: 0, slope: -0.06201413266593177, fluctuations: 0.04\n",
      "step: 38130 loss: 45.343417 time elapsed: 47.5989 learning rate: 0.003970, scenario: 0, slope: -0.06148570364708088, fluctuations: 0.04\n",
      "step: 38140 loss: 44.795425 time elapsed: 47.6127 learning rate: 0.003970, scenario: 0, slope: -0.06129379154192458, fluctuations: 0.04\n",
      "step: 38150 loss: 44.247833 time elapsed: 47.6258 learning rate: 0.003970, scenario: 0, slope: -0.06145755645565497, fluctuations: 0.04\n",
      "step: 38160 loss: 43.708992 time elapsed: 47.6381 learning rate: 0.003970, scenario: 0, slope: -0.06192311248430335, fluctuations: 0.04\n",
      "step: 38170 loss: 43.192819 time elapsed: 47.6512 learning rate: 0.003970, scenario: 0, slope: -0.06252024748746378, fluctuations: 0.04\n",
      "step: 38180 loss: 42.704712 time elapsed: 47.6641 learning rate: 0.003970, scenario: 0, slope: -0.05741771506040035, fluctuations: 0.03\n",
      "step: 38190 loss: 42.705246 time elapsed: 47.6768 learning rate: 0.003970, scenario: 0, slope: -0.05447801495633802, fluctuations: 0.01\n",
      "step: 38200 loss: 41.845997 time elapsed: 47.6892 learning rate: 0.003970, scenario: 0, slope: -0.04934896653470143, fluctuations: 0.01\n",
      "step: 38210 loss: 41.600123 time elapsed: 47.7031 learning rate: 0.003970, scenario: 0, slope: -0.04735555066463039, fluctuations: 0.02\n",
      "step: 38220 loss: 40.923852 time elapsed: 47.7159 learning rate: 0.003970, scenario: 0, slope: -0.046600210387130955, fluctuations: 0.04\n",
      "step: 38230 loss: 40.477913 time elapsed: 47.7290 learning rate: 0.003970, scenario: 0, slope: -0.046247542470447, fluctuations: 0.04\n",
      "step: 38240 loss: 40.075010 time elapsed: 47.7415 learning rate: 0.003970, scenario: 0, slope: -0.04592808735403811, fluctuations: 0.04\n",
      "step: 38250 loss: 39.672717 time elapsed: 47.7558 learning rate: 0.003970, scenario: 0, slope: -0.045688685570359606, fluctuations: 0.04\n",
      "step: 38260 loss: 39.280489 time elapsed: 47.7701 learning rate: 0.003970, scenario: 0, slope: -0.04560321832572272, fluctuations: 0.04\n",
      "step: 38270 loss: 38.891051 time elapsed: 47.7841 learning rate: 0.003970, scenario: 0, slope: -0.045735128136041706, fluctuations: 0.04\n",
      "step: 38280 loss: 45.857275 time elapsed: 47.7975 learning rate: 0.004367, scenario: 1, slope: -0.041231721176590214, fluctuations: 0.04\n",
      "step: 38290 loss: 184.915535 time elapsed: 47.8112 learning rate: 0.003604, scenario: -1, slope: 0.5535359710937389, fluctuations: 0.06\n",
      "step: 38300 loss: 61.666084 time elapsed: 47.8234 learning rate: 0.001396, scenario: -1, slope: 0.5339732324031989, fluctuations: 0.09\n",
      "step: 38310 loss: 41.881875 time elapsed: 47.8362 learning rate: 0.000487, scenario: -1, slope: 0.4397813988161538, fluctuations: 0.1\n",
      "step: 38320 loss: 40.058256 time elapsed: 47.8482 learning rate: 0.000170, scenario: -1, slope: 0.28489250139782757, fluctuations: 0.1\n",
      "step: 38330 loss: 39.617965 time elapsed: 47.8600 learning rate: 0.000059, scenario: -1, slope: 0.14045662700860248, fluctuations: 0.11\n",
      "step: 38340 loss: 39.526965 time elapsed: 47.8720 learning rate: 0.000024, scenario: 1, slope: -0.005448953267531353, fluctuations: 0.11\n",
      "step: 38350 loss: 39.461092 time elapsed: 47.8839 learning rate: 0.000029, scenario: 0, slope: -0.14611789146943532, fluctuations: 0.11\n",
      "step: 38360 loss: 39.422737 time elapsed: 47.8961 learning rate: 0.000029, scenario: 0, slope: -0.2951807781265234, fluctuations: 0.11\n",
      "step: 38370 loss: 39.391221 time elapsed: 47.9080 learning rate: 0.000029, scenario: 0, slope: -0.4681670996949265, fluctuations: 0.11\n",
      "step: 38380 loss: 39.360358 time elapsed: 47.9196 learning rate: 0.000029, scenario: 0, slope: -0.6518776562343677, fluctuations: 0.1\n",
      "step: 38390 loss: 39.332589 time elapsed: 47.9315 learning rate: 0.000029, scenario: 0, slope: -0.148795746036507, fluctuations: 0.08\n",
      "step: 38400 loss: 39.305752 time elapsed: 47.9427 learning rate: 0.000032, scenario: 1, slope: -0.06005621434519729, fluctuations: 0.04\n",
      "step: 38410 loss: 39.264546 time elapsed: 47.9548 learning rate: 0.000084, scenario: 1, slope: -0.012027226231420433, fluctuations: 0.02\n",
      "step: 38420 loss: 39.165351 time elapsed: 47.9667 learning rate: 0.000217, scenario: 1, slope: -0.005671430050744332, fluctuations: 0.01\n",
      "step: 38430 loss: 38.941844 time elapsed: 47.9796 learning rate: 0.000562, scenario: 1, slope: -0.004523120021898503, fluctuations: 0.0\n",
      "step: 38440 loss: 38.505185 time elapsed: 47.9933 learning rate: 0.001458, scenario: 1, slope: -0.006601767178848758, fluctuations: 0.0\n",
      "step: 38450 loss: 37.912543 time elapsed: 48.0063 learning rate: 0.003782, scenario: 1, slope: -0.01102097711636585, fluctuations: 0.0\n",
      "step: 38460 loss: 37.332975 time elapsed: 48.0192 learning rate: 0.009810, scenario: 1, slope: -0.017316774496831618, fluctuations: 0.0\n",
      "step: 38470 loss: 23899.399387 time elapsed: 48.0317 learning rate: 0.008095, scenario: -1, slope: 84.1568815850118, fluctuations: 0.01\n",
      "step: 38480 loss: 10511.354075 time elapsed: 48.0443 learning rate: 0.002823, scenario: -1, slope: 141.67582656942224, fluctuations: 0.03\n",
      "step: 38490 loss: 5335.825008 time elapsed: 48.0565 learning rate: 0.000984, scenario: -1, slope: 143.1973162768002, fluctuations: 0.05\n",
      "step: 38500 loss: 3251.655195 time elapsed: 48.0694 learning rate: 0.000381, scenario: -1, slope: 120.25478910588572, fluctuations: 0.05\n",
      "step: 38510 loss: 3074.621225 time elapsed: 48.0821 learning rate: 0.000133, scenario: -1, slope: 86.65797439762235, fluctuations: 0.05\n",
      "step: 38520 loss: 2936.170952 time elapsed: 48.0953 learning rate: 0.000046, scenario: -1, slope: 53.14644850353633, fluctuations: 0.05\n",
      "step: 38530 loss: 2898.295597 time elapsed: 48.1088 learning rate: 0.000016, scenario: -1, slope: 16.534735158924683, fluctuations: 0.05\n",
      "step: 38540 loss: 2886.183866 time elapsed: 48.1221 learning rate: 0.000011, scenario: 0, slope: -24.19240449143696, fluctuations: 0.05\n",
      "step: 38550 loss: 2876.875563 time elapsed: 48.1352 learning rate: 0.000011, scenario: 0, slope: -70.69793254823635, fluctuations: 0.05\n",
      "step: 38560 loss: 2868.089219 time elapsed: 48.1480 learning rate: 0.000011, scenario: 0, slope: -125.3117455142482, fluctuations: 0.05\n",
      "step: 38570 loss: 2859.702084 time elapsed: 48.1611 learning rate: 0.000011, scenario: 0, slope: -102.30237939215586, fluctuations: 0.03\n",
      "step: 38580 loss: 2851.641347 time elapsed: 48.1748 learning rate: 0.000011, scenario: 0, slope: -25.798205890874964, fluctuations: 0.01\n",
      "step: 38590 loss: 2843.856275 time elapsed: 48.1891 learning rate: 0.000011, scenario: 0, slope: -6.847626202407286, fluctuations: 0.0\n",
      "step: 38600 loss: 2836.076848 time elapsed: 48.2037 learning rate: 0.000014, scenario: 1, slope: -2.827278405492876, fluctuations: 0.0\n",
      "step: 38610 loss: 2821.948043 time elapsed: 48.2180 learning rate: 0.000037, scenario: 1, slope: -1.3459761903659586, fluctuations: 0.0\n",
      "step: 38620 loss: 2787.673539 time elapsed: 48.2314 learning rate: 0.000095, scenario: 1, slope: -1.01859959794968, fluctuations: 0.0\n",
      "step: 38630 loss: 2707.627491 time elapsed: 48.2441 learning rate: 0.000246, scenario: 1, slope: -1.2777954186776588, fluctuations: 0.0\n",
      "step: 38640 loss: 2526.998346 time elapsed: 48.2564 learning rate: 0.000639, scenario: 1, slope: -2.1880238768588693, fluctuations: 0.0\n",
      "step: 38650 loss: 2208.096501 time elapsed: 48.2680 learning rate: 0.000935, scenario: 0, slope: -4.2769065918461635, fluctuations: 0.0\n",
      "step: 38660 loss: 1937.302378 time elapsed: 48.2797 learning rate: 0.000935, scenario: 0, slope: -7.428222617786244, fluctuations: 0.0\n",
      "step: 38670 loss: 1735.443067 time elapsed: 48.2921 learning rate: 0.000935, scenario: 0, slope: -10.893467459604523, fluctuations: 0.0\n",
      "step: 38680 loss: 1577.252854 time elapsed: 48.3056 learning rate: 0.000935, scenario: 0, slope: -14.109746509918494, fluctuations: 0.0\n",
      "step: 38690 loss: 1448.460664 time elapsed: 48.3191 learning rate: 0.000935, scenario: 0, slope: -16.68605087213702, fluctuations: 0.0\n",
      "step: 38700 loss: 1340.678490 time elapsed: 48.3323 learning rate: 0.000935, scenario: 0, slope: -18.214009698917547, fluctuations: 0.0\n",
      "step: 38710 loss: 1248.618983 time elapsed: 48.3443 learning rate: 0.000935, scenario: 0, slope: -18.830244579154844, fluctuations: 0.0\n",
      "step: 38720 loss: 1168.754374 time elapsed: 48.3559 learning rate: 0.000935, scenario: 0, slope: -18.09551327560535, fluctuations: 0.0\n",
      "step: 38730 loss: 1098.578394 time elapsed: 48.3687 learning rate: 0.000935, scenario: 0, slope: -16.20723371360152, fluctuations: 0.0\n",
      "step: 38740 loss: 1036.275264 time elapsed: 48.3810 learning rate: 0.000935, scenario: 0, slope: -13.57156811623752, fluctuations: 0.0\n",
      "step: 38750 loss: 980.482785 time elapsed: 48.3940 learning rate: 0.000935, scenario: 0, slope: -11.110656239836208, fluctuations: 0.0\n",
      "step: 38760 loss: 930.154202 time elapsed: 48.4101 learning rate: 0.000935, scenario: 0, slope: -9.321991668279699, fluctuations: 0.0\n",
      "step: 38770 loss: 884.456620 time elapsed: 48.4252 learning rate: 0.000935, scenario: 0, slope: -8.001473216203722, fluctuations: 0.0\n",
      "step: 38780 loss: 842.707768 time elapsed: 48.4426 learning rate: 0.000935, scenario: 0, slope: -6.981617926423359, fluctuations: 0.0\n",
      "step: 38790 loss: 804.332926 time elapsed: 48.4583 learning rate: 0.000935, scenario: 0, slope: -6.169982916778877, fluctuations: 0.0\n",
      "step: 38800 loss: 768.832775 time elapsed: 48.4725 learning rate: 0.000935, scenario: 0, slope: -5.569619232665481, fluctuations: 0.0\n",
      "step: 38810 loss: 735.756515 time elapsed: 48.4877 learning rate: 0.000935, scenario: 0, slope: -4.962230544466181, fluctuations: 0.0\n",
      "step: 38820 loss: 704.677389 time elapsed: 48.5036 learning rate: 0.000935, scenario: 0, slope: -4.504364743360361, fluctuations: 0.0\n",
      "step: 38830 loss: 675.168504 time elapsed: 48.5167 learning rate: 0.000935, scenario: 0, slope: -4.11872243158839, fluctuations: 0.0\n",
      "step: 38840 loss: 646.778494 time elapsed: 48.5300 learning rate: 0.000935, scenario: 0, slope: -3.793969372225408, fluctuations: 0.0\n",
      "step: 38850 loss: 619.010619 time elapsed: 48.5428 learning rate: 0.000935, scenario: 0, slope: -3.5228583002308373, fluctuations: 0.0\n",
      "step: 38860 loss: 591.322016 time elapsed: 48.5562 learning rate: 0.000935, scenario: 0, slope: -3.3012924650969278, fluctuations: 0.0\n",
      "step: 38870 loss: 563.191078 time elapsed: 48.5706 learning rate: 0.000935, scenario: 0, slope: -3.127502280300706, fluctuations: 0.0\n",
      "step: 38880 loss: 534.347372 time elapsed: 48.5839 learning rate: 0.000935, scenario: 0, slope: -3.000606697035472, fluctuations: 0.0\n",
      "step: 38890 loss: 505.219581 time elapsed: 48.5988 learning rate: 0.000935, scenario: 0, slope: -2.917520662818016, fluctuations: 0.0\n",
      "step: 38900 loss: 477.261032 time elapsed: 48.6134 learning rate: 0.000935, scenario: 0, slope: -2.8720981726955, fluctuations: 0.0\n",
      "step: 38910 loss: 452.317787 time elapsed: 48.6287 learning rate: 0.000935, scenario: 0, slope: -2.832872239916344, fluctuations: 0.0\n",
      "step: 38920 loss: 431.304587 time elapsed: 48.6430 learning rate: 0.000935, scenario: 0, slope: -2.7866326645208317, fluctuations: 0.0\n",
      "step: 38930 loss: 413.973422 time elapsed: 48.6554 learning rate: 0.000935, scenario: 0, slope: -2.7079378388374393, fluctuations: 0.0\n",
      "step: 38940 loss: 399.612659 time elapsed: 48.6685 learning rate: 0.000935, scenario: 0, slope: -2.5837078690845616, fluctuations: 0.0\n",
      "step: 38950 loss: 387.459569 time elapsed: 48.6805 learning rate: 0.000935, scenario: 0, slope: -2.410474867634692, fluctuations: 0.0\n",
      "step: 38960 loss: 376.885017 time elapsed: 48.6924 learning rate: 0.000935, scenario: 0, slope: -2.1940294417389326, fluctuations: 0.0\n",
      "step: 38970 loss: 367.462749 time elapsed: 48.7045 learning rate: 0.000935, scenario: 0, slope: -1.9483367498672708, fluctuations: 0.0\n",
      "step: 38980 loss: 358.920731 time elapsed: 48.7160 learning rate: 0.000935, scenario: 0, slope: -1.6937554176490144, fluctuations: 0.0\n",
      "step: 38990 loss: 351.074740 time elapsed: 48.7278 learning rate: 0.000935, scenario: 0, slope: -1.4533641213907842, fluctuations: 0.0\n",
      "step: 39000 loss: 343.791903 time elapsed: 48.7394 learning rate: 0.000935, scenario: 0, slope: -1.265099772134108, fluctuations: 0.0\n",
      "step: 39010 loss: 336.972850 time elapsed: 48.7517 learning rate: 0.000935, scenario: 0, slope: -1.080522531595514, fluctuations: 0.0\n",
      "step: 39020 loss: 330.541204 time elapsed: 48.7637 learning rate: 0.000935, scenario: 0, slope: -0.9534379773866183, fluctuations: 0.0\n",
      "step: 39030 loss: 324.436919 time elapsed: 48.7757 learning rate: 0.000935, scenario: 0, slope: -0.856983285235286, fluctuations: 0.0\n",
      "step: 39040 loss: 318.611960 time elapsed: 48.7876 learning rate: 0.000935, scenario: 0, slope: -0.7828832761153074, fluctuations: 0.0\n",
      "step: 39050 loss: 313.027347 time elapsed: 48.8009 learning rate: 0.000935, scenario: 0, slope: -0.7244947970841998, fluctuations: 0.0\n",
      "step: 39060 loss: 307.651076 time elapsed: 48.8143 learning rate: 0.000935, scenario: 0, slope: -0.6772551384762604, fluctuations: 0.0\n",
      "step: 39070 loss: 302.456633 time elapsed: 48.8278 learning rate: 0.000935, scenario: 0, slope: -0.6382432829841206, fluctuations: 0.0\n",
      "step: 39080 loss: 297.421902 time elapsed: 48.8417 learning rate: 0.000935, scenario: 0, slope: -0.6055328114168824, fluctuations: 0.0\n",
      "step: 39090 loss: 292.528268 time elapsed: 48.8564 learning rate: 0.000935, scenario: 0, slope: -0.5777741476610855, fluctuations: 0.0\n",
      "step: 39100 loss: 287.759656 time elapsed: 48.8704 learning rate: 0.000935, scenario: 0, slope: -0.556205336766304, fluctuations: 0.0\n",
      "step: 39110 loss: 283.100987 time elapsed: 48.8832 learning rate: 0.000935, scenario: 0, slope: -0.53342050024165, fluctuations: 0.0\n",
      "step: 39120 loss: 278.534408 time elapsed: 48.8953 learning rate: 0.000935, scenario: 0, slope: -0.5155435276158116, fluctuations: 0.0\n",
      "step: 39130 loss: 274.026070 time elapsed: 48.9085 learning rate: 0.000935, scenario: 0, slope: -0.4999999637921024, fluctuations: 0.0\n",
      "step: 39140 loss: 269.452895 time elapsed: 48.9212 learning rate: 0.000935, scenario: 0, slope: -0.4868438173263172, fluctuations: 0.0\n",
      "step: 39150 loss: 263.606939 time elapsed: 48.9331 learning rate: 0.000935, scenario: 0, slope: -0.4787695842559735, fluctuations: 0.0\n",
      "step: 39160 loss: 247.268886 time elapsed: 48.9449 learning rate: 0.000935, scenario: 0, slope: -0.5218315351836875, fluctuations: 0.0\n",
      "step: 39170 loss: 237.244208 time elapsed: 48.9570 learning rate: 0.000935, scenario: 0, slope: -0.5901025993768261, fluctuations: 0.02\n",
      "step: 39180 loss: 231.917844 time elapsed: 48.9692 learning rate: 0.000935, scenario: 0, slope: -0.6612475265591775, fluctuations: 0.02\n",
      "step: 39190 loss: 227.424259 time elapsed: 48.9813 learning rate: 0.000935, scenario: 0, slope: -0.7098597174087643, fluctuations: 0.02\n",
      "step: 39200 loss: 223.348400 time elapsed: 48.9944 learning rate: 0.000935, scenario: 0, slope: -0.7327103939940388, fluctuations: 0.02\n",
      "step: 39210 loss: 219.623927 time elapsed: 49.0091 learning rate: 0.000935, scenario: 0, slope: -0.7323338500490758, fluctuations: 0.02\n",
      "step: 39220 loss: 216.086182 time elapsed: 49.0228 learning rate: 0.000935, scenario: 0, slope: -0.7043940239556853, fluctuations: 0.02\n",
      "step: 39230 loss: 212.683246 time elapsed: 49.0368 learning rate: 0.000935, scenario: 0, slope: -0.6495367016355388, fluctuations: 0.02\n",
      "step: 39240 loss: 209.403504 time elapsed: 49.0518 learning rate: 0.000935, scenario: 0, slope: -0.5670648177143702, fluctuations: 0.02\n",
      "step: 39250 loss: 206.222724 time elapsed: 49.0652 learning rate: 0.000935, scenario: 0, slope: -0.4589374082744412, fluctuations: 0.02\n",
      "step: 39260 loss: 203.133395 time elapsed: 49.0780 learning rate: 0.000935, scenario: 0, slope: -0.3867798625630498, fluctuations: 0.01\n",
      "step: 39270 loss: 200.127330 time elapsed: 49.0901 learning rate: 0.000935, scenario: 0, slope: -0.3581815558600027, fluctuations: 0.0\n",
      "step: 39280 loss: 197.200059 time elapsed: 49.1027 learning rate: 0.000935, scenario: 0, slope: -0.34000359294938653, fluctuations: 0.0\n",
      "step: 39290 loss: 194.348119 time elapsed: 49.1166 learning rate: 0.000935, scenario: 0, slope: -0.3262598076109904, fluctuations: 0.0\n",
      "step: 39300 loss: 191.569303 time elapsed: 49.1301 learning rate: 0.000935, scenario: 0, slope: -0.3163707283567969, fluctuations: 0.0\n",
      "step: 39310 loss: 188.862031 time elapsed: 49.1437 learning rate: 0.000935, scenario: 0, slope: -0.3059000443804297, fluctuations: 0.0\n",
      "step: 39320 loss: 186.225177 time elapsed: 49.1560 learning rate: 0.000935, scenario: 0, slope: -0.29730840491899124, fluctuations: 0.0\n",
      "step: 39330 loss: 183.657827 time elapsed: 49.1693 learning rate: 0.000935, scenario: 0, slope: -0.28929452870929273, fluctuations: 0.0\n",
      "step: 39340 loss: 181.159157 time elapsed: 49.1820 learning rate: 0.000935, scenario: 0, slope: -0.28166117961949344, fluctuations: 0.0\n",
      "step: 39350 loss: 178.728334 time elapsed: 49.1951 learning rate: 0.000935, scenario: 0, slope: -0.27429508854005735, fluctuations: 0.0\n",
      "step: 39360 loss: 176.364466 time elapsed: 49.2102 learning rate: 0.000935, scenario: 0, slope: -0.2671180313365841, fluctuations: 0.0\n",
      "step: 39370 loss: 174.066583 time elapsed: 49.2244 learning rate: 0.000935, scenario: 0, slope: -0.260083272374431, fluctuations: 0.0\n",
      "step: 39380 loss: 171.833633 time elapsed: 49.2388 learning rate: 0.000935, scenario: 0, slope: -0.25316298001909576, fluctuations: 0.0\n",
      "step: 39390 loss: 169.664489 time elapsed: 49.2533 learning rate: 0.000935, scenario: 0, slope: -0.2463434470308993, fluctuations: 0.0\n",
      "step: 39400 loss: 167.557952 time elapsed: 49.2661 learning rate: 0.000935, scenario: 0, slope: -0.2402881792561299, fluctuations: 0.0\n",
      "step: 39410 loss: 165.512757 time elapsed: 49.2789 learning rate: 0.000935, scenario: 0, slope: -0.23299463738542278, fluctuations: 0.0\n",
      "step: 39420 loss: 163.527577 time elapsed: 49.2912 learning rate: 0.000935, scenario: 0, slope: -0.22647152351530322, fluctuations: 0.0\n",
      "step: 39430 loss: 161.601022 time elapsed: 49.3028 learning rate: 0.000935, scenario: 0, slope: -0.22005722578043374, fluctuations: 0.0\n",
      "step: 39440 loss: 159.731640 time elapsed: 49.3148 learning rate: 0.000935, scenario: 0, slope: -0.21375868417873922, fluctuations: 0.0\n",
      "step: 39450 loss: 157.917918 time elapsed: 49.3266 learning rate: 0.000935, scenario: 0, slope: -0.20758283879355519, fluctuations: 0.0\n",
      "step: 39460 loss: 156.158288 time elapsed: 49.3386 learning rate: 0.000935, scenario: 0, slope: -0.2015364027234427, fluctuations: 0.0\n",
      "step: 39470 loss: 154.451131 time elapsed: 49.3506 learning rate: 0.000935, scenario: 0, slope: -0.19562580233848098, fluctuations: 0.0\n",
      "step: 39480 loss: 152.794784 time elapsed: 49.3625 learning rate: 0.000935, scenario: 0, slope: -0.18985717120776283, fluctuations: 0.0\n",
      "step: 39490 loss: 151.187548 time elapsed: 49.3742 learning rate: 0.000935, scenario: 0, slope: -0.18423632703760479, fluctuations: 0.0\n",
      "step: 39500 loss: 149.627700 time elapsed: 49.3860 learning rate: 0.000935, scenario: 0, slope: -0.17930842907682012, fluctuations: 0.0\n",
      "step: 39510 loss: 148.113500 time elapsed: 49.3982 learning rate: 0.000935, scenario: 0, slope: -0.17345927101350694, fluctuations: 0.0\n",
      "step: 39520 loss: 146.643205 time elapsed: 49.4101 learning rate: 0.000935, scenario: 0, slope: -0.16831237276584599, fluctuations: 0.0\n",
      "step: 39530 loss: 145.215074 time elapsed: 49.4218 learning rate: 0.000935, scenario: 0, slope: -0.1633316503053568, fluctuations: 0.0\n",
      "step: 39540 loss: 143.784945 time elapsed: 49.4349 learning rate: 0.001369, scenario: 1, slope: -0.15855312132195806, fluctuations: 0.0\n",
      "step: 39550 loss: 140.959885 time elapsed: 49.4487 learning rate: 0.003552, scenario: 1, slope: -0.15753979690404363, fluctuations: 0.0\n",
      "step: 39560 loss: 136.303564 time elapsed: 49.4617 learning rate: 0.003552, scenario: 0, slope: -0.17084204468536346, fluctuations: 0.0\n",
      "step: 39570 loss: 132.063372 time elapsed: 49.4755 learning rate: 0.003552, scenario: 0, slope: -0.19725695534537885, fluctuations: 0.0\n",
      "step: 39580 loss: 128.236965 time elapsed: 49.4877 learning rate: 0.003552, scenario: 0, slope: -0.23082181532572169, fluctuations: 0.0\n",
      "step: 39590 loss: 124.757552 time elapsed: 49.4994 learning rate: 0.003552, scenario: 0, slope: -0.26638252323715234, fluctuations: 0.0\n",
      "step: 39600 loss: 121.562836 time elapsed: 49.5107 learning rate: 0.003552, scenario: 0, slope: -0.29644477470189906, fluctuations: 0.0\n",
      "step: 39610 loss: 118.599957 time elapsed: 49.5230 learning rate: 0.003552, scenario: 0, slope: -0.3265342829195503, fluctuations: 0.0\n",
      "step: 39620 loss: 115.824197 time elapsed: 49.5347 learning rate: 0.003552, scenario: 0, slope: -0.3441213678531263, fluctuations: 0.0\n",
      "step: 39630 loss: 113.196219 time elapsed: 49.5463 learning rate: 0.003552, scenario: 0, slope: -0.349490304876515, fluctuations: 0.0\n",
      "step: 39640 loss: 110.680069 time elapsed: 49.5579 learning rate: 0.003552, scenario: 0, slope: -0.3402232642135297, fluctuations: 0.0\n",
      "step: 39650 loss: 108.244928 time elapsed: 49.5690 learning rate: 0.003552, scenario: 0, slope: -0.31819897964358157, fluctuations: 0.0\n",
      "step: 39660 loss: 105.877215 time elapsed: 49.5806 learning rate: 0.003552, scenario: 0, slope: -0.2960003082052732, fluctuations: 0.0\n",
      "step: 39670 loss: 103.604251 time elapsed: 49.5923 learning rate: 0.003552, scenario: 0, slope: -0.27797570614458084, fluctuations: 0.0\n",
      "step: 39680 loss: 101.476100 time elapsed: 49.6037 learning rate: 0.003552, scenario: 0, slope: -0.26310130553272243, fluctuations: 0.0\n",
      "step: 39690 loss: 99.473351 time elapsed: 49.6155 learning rate: 0.003552, scenario: 0, slope: -0.250293160617589, fluctuations: 0.0\n",
      "step: 39700 loss: 97.548206 time elapsed: 49.6273 learning rate: 0.003552, scenario: 0, slope: -0.23995547400896292, fluctuations: 0.0\n",
      "step: 39710 loss: 95.688409 time elapsed: 49.6402 learning rate: 0.003552, scenario: 0, slope: -0.22837746944201792, fluctuations: 0.0\n",
      "step: 39720 loss: 93.891604 time elapsed: 49.6539 learning rate: 0.003552, scenario: 0, slope: -0.21854004100601102, fluctuations: 0.0\n",
      "step: 39730 loss: 92.160369 time elapsed: 49.6673 learning rate: 0.003552, scenario: 0, slope: -0.20914633501346536, fluctuations: 0.0\n",
      "step: 39740 loss: 90.500842 time elapsed: 49.6811 learning rate: 0.003552, scenario: 0, slope: -0.20009420732590616, fluctuations: 0.0\n",
      "step: 39750 loss: 88.920151 time elapsed: 49.6942 learning rate: 0.003552, scenario: 0, slope: -0.19139491425421049, fluctuations: 0.0\n",
      "step: 39760 loss: 87.421399 time elapsed: 49.7059 learning rate: 0.003552, scenario: 0, slope: -0.18316248961898499, fluctuations: 0.0\n",
      "step: 39770 loss: 86.000149 time elapsed: 49.7188 learning rate: 0.003552, scenario: 0, slope: -0.1754855636293434, fluctuations: 0.0\n",
      "step: 39780 loss: 84.645642 time elapsed: 49.7321 learning rate: 0.003552, scenario: 0, slope: -0.16819965084264998, fluctuations: 0.0\n",
      "step: 39790 loss: 83.342227 time elapsed: 49.7490 learning rate: 0.003552, scenario: 0, slope: -0.16102823761499704, fluctuations: 0.0\n",
      "step: 39800 loss: 82.056499 time elapsed: 49.7619 learning rate: 0.003552, scenario: 0, slope: -0.15469986610163744, fluctuations: 0.0\n",
      "step: 39810 loss: 80.604551 time elapsed: 49.7754 learning rate: 0.003552, scenario: 0, slope: -0.14781847165750833, fluctuations: 0.0\n",
      "step: 39820 loss: 79.164190 time elapsed: 49.7878 learning rate: 0.003552, scenario: 0, slope: -0.14374213103114566, fluctuations: 0.0\n",
      "step: 39830 loss: 77.946469 time elapsed: 49.8006 learning rate: 0.003552, scenario: 0, slope: -0.14015131914807483, fluctuations: 0.0\n",
      "step: 39840 loss: 76.817795 time elapsed: 49.8131 learning rate: 0.003552, scenario: 0, slope: -0.1366675166277404, fluctuations: 0.0\n",
      "step: 39850 loss: 75.727033 time elapsed: 49.8254 learning rate: 0.003552, scenario: 0, slope: -0.13315159559637776, fluctuations: 0.0\n",
      "step: 39860 loss: 74.672217 time elapsed: 49.8397 learning rate: 0.003552, scenario: 0, slope: -0.1294612169788198, fluctuations: 0.0\n",
      "step: 39870 loss: 73.648385 time elapsed: 49.8546 learning rate: 0.003552, scenario: 0, slope: -0.12544830194005452, fluctuations: 0.0\n",
      "step: 39880 loss: 72.670358 time elapsed: 49.8692 learning rate: 0.003552, scenario: 0, slope: -0.12094924420977017, fluctuations: 0.0\n",
      "step: 39890 loss: 71.681421 time elapsed: 49.8839 learning rate: 0.003552, scenario: 0, slope: -0.1157943661634178, fluctuations: 0.0\n",
      "step: 39900 loss: 70.732450 time elapsed: 49.8976 learning rate: 0.003552, scenario: 0, slope: -0.11087284956437105, fluctuations: 0.0\n",
      "step: 39910 loss: 69.808190 time elapsed: 49.9122 learning rate: 0.003552, scenario: 0, slope: -0.1049871217865827, fluctuations: 0.0\n",
      "step: 39920 loss: 68.903501 time elapsed: 49.9255 learning rate: 0.003552, scenario: 0, slope: -0.10154199996924175, fluctuations: 0.0\n",
      "step: 39930 loss: 68.019157 time elapsed: 49.9379 learning rate: 0.003552, scenario: 0, slope: -0.0987279013423691, fluctuations: 0.0\n",
      "step: 39940 loss: 67.155092 time elapsed: 49.9506 learning rate: 0.003552, scenario: 0, slope: -0.09621648806386796, fluctuations: 0.0\n",
      "step: 39950 loss: 66.310794 time elapsed: 49.9632 learning rate: 0.003552, scenario: 0, slope: -0.09390387599376285, fluctuations: 0.0\n",
      "step: 39960 loss: 65.485238 time elapsed: 49.9758 learning rate: 0.003552, scenario: 0, slope: -0.09172994594248074, fluctuations: 0.0\n",
      "step: 39970 loss: 64.690758 time elapsed: 49.9879 learning rate: 0.003552, scenario: 0, slope: -0.08964084360470417, fluctuations: 0.0\n",
      "step: 39980 loss: 63.910626 time elapsed: 50.0009 learning rate: 0.003552, scenario: 0, slope: -0.08726202205662191, fluctuations: 0.0\n",
      "step: 39990 loss: 63.126572 time elapsed: 50.0138 learning rate: 0.003552, scenario: 0, slope: -0.0851064713641024, fluctuations: 0.0\n",
      "step: 40000 loss: 62.373874 time elapsed: 50.0264 learning rate: 0.003552, scenario: 0, slope: -0.08338160608226335, fluctuations: 0.0\n",
      "step: 40010 loss: 61.624605 time elapsed: 50.0390 learning rate: 0.003552, scenario: 0, slope: -0.0814199219047299, fluctuations: 0.0\n",
      "step: 40020 loss: 60.898017 time elapsed: 50.0530 learning rate: 0.003552, scenario: 0, slope: -0.0797295475406171, fluctuations: 0.0\n",
      "step: 40030 loss: 60.184953 time elapsed: 50.0679 learning rate: 0.003552, scenario: 0, slope: -0.07812436074975113, fluctuations: 0.0\n",
      "step: 40040 loss: 59.510915 time elapsed: 50.0826 learning rate: 0.003552, scenario: 0, slope: -0.07655305979753398, fluctuations: 0.0\n",
      "step: 40050 loss: 58.801076 time elapsed: 50.0969 learning rate: 0.003552, scenario: 0, slope: -0.07471724204708667, fluctuations: 0.0\n",
      "step: 40060 loss: 58.114758 time elapsed: 50.1105 learning rate: 0.003552, scenario: 0, slope: -0.07334527808734216, fluctuations: 0.0\n",
      "step: 40070 loss: 57.443416 time elapsed: 50.1230 learning rate: 0.003552, scenario: 0, slope: -0.07213833433125384, fluctuations: 0.0\n",
      "step: 40080 loss: 56.811673 time elapsed: 50.1367 learning rate: 0.003552, scenario: 0, slope: -0.07064537322639694, fluctuations: 0.0\n",
      "step: 40090 loss: 56.192505 time elapsed: 50.1491 learning rate: 0.003552, scenario: 0, slope: -0.06924661759859939, fluctuations: 0.0\n",
      "step: 40100 loss: 55.468245 time elapsed: 50.1620 learning rate: 0.003552, scenario: 0, slope: -0.06843630296138341, fluctuations: 0.0\n",
      "step: 40110 loss: 55.120538 time elapsed: 50.1752 learning rate: 0.003552, scenario: 0, slope: -0.06703820283734147, fluctuations: 0.01\n",
      "step: 40120 loss: 54.164017 time elapsed: 50.1875 learning rate: 0.003552, scenario: 0, slope: -0.06625388490073161, fluctuations: 0.02\n",
      "step: 40130 loss: 53.564268 time elapsed: 50.1997 learning rate: 0.003552, scenario: 0, slope: -0.06586789723820483, fluctuations: 0.02\n",
      "step: 40140 loss: 52.897807 time elapsed: 50.2117 learning rate: 0.003552, scenario: 0, slope: -0.06571123943793387, fluctuations: 0.02\n",
      "step: 40150 loss: 52.364321 time elapsed: 50.2236 learning rate: 0.003552, scenario: 0, slope: -0.06474439990393206, fluctuations: 0.02\n",
      "step: 40160 loss: 51.627234 time elapsed: 50.2351 learning rate: 0.003552, scenario: 0, slope: -0.06472856283201212, fluctuations: 0.02\n",
      "step: 40170 loss: 50.968528 time elapsed: 50.2470 learning rate: 0.003552, scenario: 0, slope: -0.063738804568531, fluctuations: 0.03\n",
      "step: 40180 loss: 50.495250 time elapsed: 50.2592 learning rate: 0.003552, scenario: 0, slope: -0.06332896150608584, fluctuations: 0.05\n",
      "step: 40190 loss: 49.797502 time elapsed: 50.2725 learning rate: 0.003552, scenario: 0, slope: -0.06325804215335606, fluctuations: 0.06\n",
      "step: 40200 loss: 49.211244 time elapsed: 50.2857 learning rate: 0.003552, scenario: 0, slope: -0.06343806146947639, fluctuations: 0.06\n",
      "step: 40210 loss: 48.550365 time elapsed: 50.2995 learning rate: 0.003552, scenario: 0, slope: -0.06292724936611144, fluctuations: 0.05\n",
      "step: 40220 loss: 49.217923 time elapsed: 50.3131 learning rate: 0.003552, scenario: 0, slope: -0.0609754036553255, fluctuations: 0.04\n",
      "step: 40230 loss: 48.036680 time elapsed: 50.3258 learning rate: 0.003552, scenario: 0, slope: -0.05885263926425574, fluctuations: 0.06\n",
      "step: 40240 loss: 47.081604 time elapsed: 50.3386 learning rate: 0.003552, scenario: 0, slope: -0.05814475979381275, fluctuations: 0.09\n",
      "step: 40250 loss: 46.471904 time elapsed: 50.3507 learning rate: 0.003552, scenario: 0, slope: -0.057146736598107414, fluctuations: 0.11\n",
      "step: 40260 loss: 45.983819 time elapsed: 50.3629 learning rate: 0.003552, scenario: 0, slope: -0.056640425571751765, fluctuations: 0.12\n",
      "step: 40270 loss: 45.475917 time elapsed: 50.3752 learning rate: 0.003552, scenario: 0, slope: -0.054887473443043226, fluctuations: 0.11\n",
      "step: 40280 loss: 44.955458 time elapsed: 50.3871 learning rate: 0.003552, scenario: 0, slope: -0.05380292284243558, fluctuations: 0.09\n",
      "step: 40290 loss: 44.445759 time elapsed: 50.3990 learning rate: 0.003552, scenario: 0, slope: -0.05362942890155766, fluctuations: 0.08\n",
      "step: 40300 loss: 44.159143 time elapsed: 50.4108 learning rate: 0.003552, scenario: 0, slope: -0.05339986235539211, fluctuations: 0.08\n",
      "step: 40310 loss: 44.232003 time elapsed: 50.4230 learning rate: 0.003552, scenario: 0, slope: -0.05084699725159585, fluctuations: 0.09\n",
      "step: 40320 loss: 43.350065 time elapsed: 50.4349 learning rate: 0.003552, scenario: 0, slope: -0.04991974116187096, fluctuations: 0.1\n",
      "step: 40330 loss: 97.541806 time elapsed: 50.4465 learning rate: 0.006922, scenario: 1, slope: -0.0011503283949478096, fluctuations: 0.09\n",
      "step: 40340 loss: 68.998431 time elapsed: 50.4591 learning rate: 0.002560, scenario: -1, slope: 0.20287403318390437, fluctuations: 0.1\n",
      "step: 40350 loss: 50.026920 time elapsed: 50.4735 learning rate: 0.000893, scenario: -1, slope: 0.2000050359812286, fluctuations: 0.11\n",
      "step: 40360 loss: 46.787218 time elapsed: 50.4877 learning rate: 0.000311, scenario: -1, slope: 0.16599678595836095, fluctuations: 0.13\n",
      "step: 40370 loss: 46.541482 time elapsed: 50.5013 learning rate: 0.000109, scenario: -1, slope: 0.11661020632567091, fluctuations: 0.14\n",
      "step: 40380 loss: 46.407739 time elapsed: 50.5150 learning rate: 0.000038, scenario: -1, slope: 0.06221667384998771, fluctuations: 0.14\n",
      "step: 40390 loss: 46.373663 time elapsed: 50.5276 learning rate: 0.000013, scenario: -1, slope: 0.006498764309341115, fluctuations: 0.14\n",
      "step: 40400 loss: 46.359434 time elapsed: 50.5394 learning rate: 0.000022, scenario: 0, slope: -0.05059691153067223, fluctuations: 0.14\n",
      "step: 40410 loss: 46.339853 time elapsed: 50.5519 learning rate: 0.000022, scenario: 0, slope: -0.1161647210690956, fluctuations: 0.12\n",
      "step: 40420 loss: 46.322822 time elapsed: 50.5641 learning rate: 0.000022, scenario: 0, slope: -0.18181808068425995, fluctuations: 0.1\n",
      "step: 40430 loss: 46.307128 time elapsed: 50.5761 learning rate: 0.000022, scenario: 0, slope: -0.2932053374342602, fluctuations: 0.09\n",
      "step: 40440 loss: 46.291136 time elapsed: 50.5883 learning rate: 0.000033, scenario: 1, slope: -0.043882103778871376, fluctuations: 0.06\n",
      "step: 40450 loss: 46.258695 time elapsed: 50.6002 learning rate: 0.000084, scenario: 1, slope: -0.010586233144184668, fluctuations: 0.03\n",
      "step: 40460 loss: 46.176928 time elapsed: 50.6120 learning rate: 0.000219, scenario: 1, slope: -0.004051554020347976, fluctuations: 0.01\n",
      "step: 40470 loss: 45.975457 time elapsed: 50.6238 learning rate: 0.000567, scenario: 1, slope: -0.0031429370570957263, fluctuations: 0.0\n",
      "step: 40480 loss: 45.483895 time elapsed: 50.6357 learning rate: 0.001471, scenario: 1, slope: -0.005260168234293417, fluctuations: 0.0\n",
      "step: 40490 loss: 44.210004 time elapsed: 50.6478 learning rate: 0.003816, scenario: 1, slope: -0.011690679240087555, fluctuations: 0.0\n",
      "step: 40500 loss: 42.287592 time elapsed: 50.6596 learning rate: 0.008998, scenario: 1, slope: -0.024861858323653188, fluctuations: 0.0\n",
      "step: 40510 loss: 33823.084139 time elapsed: 50.6723 learning rate: 0.007425, scenario: -1, slope: 42.328916951746244, fluctuations: 0.02\n",
      "step: 40520 loss: 10583.402161 time elapsed: 50.6841 learning rate: 0.002589, scenario: -1, slope: 130.19189123188508, fluctuations: 0.04\n",
      "step: 40530 loss: 4922.462030 time elapsed: 50.6962 learning rate: 0.000903, scenario: -1, slope: 134.8640125847497, fluctuations: 0.06\n",
      "step: 40540 loss: 2914.235729 time elapsed: 50.7091 learning rate: 0.000315, scenario: -1, slope: 111.36777888412797, fluctuations: 0.06\n",
      "step: 40550 loss: 2386.502772 time elapsed: 50.7224 learning rate: 0.000110, scenario: -1, slope: 81.50537891232439, fluctuations: 0.06\n",
      "step: 40560 loss: 2282.706380 time elapsed: 50.7358 learning rate: 0.000038, scenario: -1, slope: 49.575780070656414, fluctuations: 0.06\n",
      "step: 40570 loss: 2252.762264 time elapsed: 50.7487 learning rate: 0.000013, scenario: -1, slope: 15.52415967983403, fluctuations: 0.06\n",
      "step: 40580 loss: 2241.916973 time elapsed: 50.7608 learning rate: 0.000009, scenario: 0, slope: -21.97688287051255, fluctuations: 0.06\n",
      "step: 40590 loss: 2233.052023 time elapsed: 50.7727 learning rate: 0.000009, scenario: 0, slope: -64.84683700125167, fluctuations: 0.06\n",
      "step: 40600 loss: 2224.375581 time elapsed: 50.7845 learning rate: 0.000009, scenario: 0, slope: -110.24640899191046, fluctuations: 0.06\n",
      "step: 40610 loss: 2215.872136 time elapsed: 50.7975 learning rate: 0.000009, scenario: 0, slope: -109.99879645305002, fluctuations: 0.04\n",
      "step: 40620 loss: 2207.526616 time elapsed: 50.8090 learning rate: 0.000009, scenario: 0, slope: -30.699082326822403, fluctuations: 0.01\n",
      "step: 40630 loss: 2199.325245 time elapsed: 50.8211 learning rate: 0.000009, scenario: 0, slope: -9.041119842884312, fluctuations: 0.0\n",
      "step: 40640 loss: 2191.255886 time elapsed: 50.8331 learning rate: 0.000010, scenario: 1, slope: -2.724333567296649, fluctuations: 0.0\n",
      "step: 40650 loss: 2178.628343 time elapsed: 50.8450 learning rate: 0.000025, scenario: 1, slope: -1.2037675614925314, fluctuations: 0.0\n",
      "step: 40660 loss: 2146.907117 time elapsed: 50.8569 learning rate: 0.000065, scenario: 1, slope: -0.9863669585867813, fluctuations: 0.0\n",
      "step: 40670 loss: 2070.294997 time elapsed: 50.8685 learning rate: 0.000168, scenario: 1, slope: -1.240700711523935, fluctuations: 0.0\n",
      "step: 40680 loss: 1899.686013 time elapsed: 50.8799 learning rate: 0.000436, scenario: 1, slope: -2.1024121209415862, fluctuations: 0.0\n",
      "step: 40690 loss: 1651.296434 time elapsed: 50.8914 learning rate: 0.000479, scenario: 0, slope: -3.9521289626781027, fluctuations: 0.0\n",
      "step: 40700 loss: 1462.732437 time elapsed: 50.9030 learning rate: 0.000479, scenario: 0, slope: -6.178825086479153, fluctuations: 0.0\n",
      "step: 40710 loss: 1303.441443 time elapsed: 50.9175 learning rate: 0.000479, scenario: 0, slope: -9.079882856937354, fluctuations: 0.0\n",
      "step: 40720 loss: 1167.765177 time elapsed: 50.9313 learning rate: 0.000479, scenario: 0, slope: -11.561621776829723, fluctuations: 0.0\n",
      "step: 40730 loss: 1068.598730 time elapsed: 50.9450 learning rate: 0.000479, scenario: 0, slope: -13.523008497314962, fluctuations: 0.0\n",
      "step: 40740 loss: 992.808402 time elapsed: 50.9584 learning rate: 0.000479, scenario: 0, slope: -14.691505712190029, fluctuations: 0.0\n",
      "step: 40750 loss: 934.628455 time elapsed: 50.9703 learning rate: 0.000479, scenario: 0, slope: -14.8710661541755, fluctuations: 0.0\n",
      "step: 40760 loss: 887.645120 time elapsed: 50.9824 learning rate: 0.000479, scenario: 0, slope: -13.999656193151383, fluctuations: 0.0\n",
      "step: 40770 loss: 848.316579 time elapsed: 50.9942 learning rate: 0.000479, scenario: 0, slope: -12.198600383383152, fluctuations: 0.0\n",
      "step: 40780 loss: 814.377588 time elapsed: 51.0059 learning rate: 0.000479, scenario: 0, slope: -9.902920299359119, fluctuations: 0.0\n",
      "step: 40790 loss: 783.957811 time elapsed: 51.0171 learning rate: 0.000479, scenario: 0, slope: -7.887284184056889, fluctuations: 0.0\n",
      "step: 40800 loss: 754.734776 time elapsed: 51.0287 learning rate: 0.000479, scenario: 0, slope: -6.445006987310974, fluctuations: 0.0\n",
      "step: 40810 loss: 721.839959 time elapsed: 51.0408 learning rate: 0.000479, scenario: 0, slope: -5.077769436082816, fluctuations: 0.0\n",
      "step: 40820 loss: 686.224973 time elapsed: 51.0524 learning rate: 0.000479, scenario: 0, slope: -4.267138460801691, fluctuations: 0.0\n",
      "step: 40830 loss: 663.204831 time elapsed: 51.0643 learning rate: 0.000479, scenario: 0, slope: -3.734220121807341, fluctuations: 0.0\n",
      "step: 40840 loss: 643.118103 time elapsed: 51.0760 learning rate: 0.000479, scenario: 0, slope: -3.359476494051457, fluctuations: 0.0\n",
      "step: 40850 loss: 625.979021 time elapsed: 51.0878 learning rate: 0.000479, scenario: 0, slope: -3.0701475761805423, fluctuations: 0.0\n",
      "step: 40860 loss: 610.664461 time elapsed: 51.0988 learning rate: 0.000479, scenario: 0, slope: -2.818250108915854, fluctuations: 0.0\n",
      "step: 40870 loss: 596.733456 time elapsed: 51.1112 learning rate: 0.000479, scenario: 0, slope: -2.577567053169006, fluctuations: 0.0\n",
      "step: 40880 loss: 583.902448 time elapsed: 51.1238 learning rate: 0.000479, scenario: 0, slope: -2.3329770468805537, fluctuations: 0.0\n",
      "step: 40890 loss: 571.945942 time elapsed: 51.1379 learning rate: 0.000479, scenario: 0, slope: -2.077840030895459, fluctuations: 0.0\n",
      "step: 40900 loss: 560.720757 time elapsed: 51.1514 learning rate: 0.000479, scenario: 0, slope: -1.8417183968010056, fluctuations: 0.0\n",
      "step: 40910 loss: 550.108906 time elapsed: 51.1656 learning rate: 0.000479, scenario: 0, slope: -1.5716195104245385, fluctuations: 0.0\n",
      "step: 40920 loss: 540.017815 time elapsed: 51.1780 learning rate: 0.000479, scenario: 0, slope: -1.3992720675797143, fluctuations: 0.0\n",
      "step: 40930 loss: 530.370983 time elapsed: 51.1898 learning rate: 0.000479, scenario: 0, slope: -1.2823537686850448, fluctuations: 0.0\n",
      "step: 40940 loss: 521.104498 time elapsed: 51.2017 learning rate: 0.000479, scenario: 0, slope: -1.1888495938484167, fluctuations: 0.0\n",
      "step: 40950 loss: 512.163659 time elapsed: 51.2140 learning rate: 0.000479, scenario: 0, slope: -1.114574965026772, fluctuations: 0.0\n",
      "step: 40960 loss: 503.500387 time elapsed: 51.2258 learning rate: 0.000479, scenario: 0, slope: -1.0530298601093282, fluctuations: 0.0\n",
      "step: 40970 loss: 495.071338 time elapsed: 51.2372 learning rate: 0.000479, scenario: 0, slope: -1.0015312529883145, fluctuations: 0.0\n",
      "step: 40980 loss: 486.836491 time elapsed: 51.2489 learning rate: 0.000479, scenario: 0, slope: -0.957978399985405, fluctuations: 0.0\n",
      "step: 40990 loss: 478.758092 time elapsed: 51.2606 learning rate: 0.000479, scenario: 0, slope: -0.9210185285714346, fluctuations: 0.0\n",
      "step: 41000 loss: 470.800147 time elapsed: 51.2721 learning rate: 0.000479, scenario: 0, slope: -0.8926019402293486, fluctuations: 0.0\n",
      "step: 41010 loss: 462.928716 time elapsed: 51.2846 learning rate: 0.000479, scenario: 0, slope: -0.8633306258735812, fluctuations: 0.0\n",
      "step: 41020 loss: 455.113416 time elapsed: 51.2961 learning rate: 0.000479, scenario: 0, slope: -0.8413865390663224, fluctuations: 0.0\n",
      "step: 41030 loss: 447.330619 time elapsed: 51.3078 learning rate: 0.000479, scenario: 0, slope: -0.8234607684170265, fluctuations: 0.0\n",
      "step: 41040 loss: 439.568567 time elapsed: 51.3205 learning rate: 0.000479, scenario: 0, slope: -0.8091741880344445, fluctuations: 0.0\n",
      "step: 41050 loss: 431.833637 time elapsed: 51.3338 learning rate: 0.000479, scenario: 0, slope: -0.7980856638362329, fluctuations: 0.0\n",
      "step: 41060 loss: 424.155093 time elapsed: 51.3470 learning rate: 0.000479, scenario: 0, slope: -0.7895944485095843, fluctuations: 0.0\n",
      "step: 41070 loss: 416.583804 time elapsed: 51.3601 learning rate: 0.000479, scenario: 0, slope: -0.7828636171076208, fluctuations: 0.0\n",
      "step: 41080 loss: 409.181277 time elapsed: 51.3735 learning rate: 0.000479, scenario: 0, slope: -0.7768108550619651, fluctuations: 0.0\n",
      "step: 41090 loss: 402.001007 time elapsed: 51.3856 learning rate: 0.000479, scenario: 0, slope: -0.7702078862236241, fluctuations: 0.0\n",
      "step: 41100 loss: 395.071564 time elapsed: 51.3973 learning rate: 0.000479, scenario: 0, slope: -0.7628213928197577, fluctuations: 0.0\n",
      "step: 41110 loss: 388.391329 time elapsed: 51.4092 learning rate: 0.000479, scenario: 0, slope: -0.7509598954313608, fluctuations: 0.0\n",
      "step: 41120 loss: 381.935792 time elapsed: 51.4207 learning rate: 0.000479, scenario: 0, slope: -0.7370389085845149, fluctuations: 0.0\n",
      "step: 41130 loss: 375.669230 time elapsed: 51.4325 learning rate: 0.000479, scenario: 0, slope: -0.7202728423083935, fluctuations: 0.0\n",
      "step: 41140 loss: 369.553529 time elapsed: 51.4443 learning rate: 0.000479, scenario: 0, slope: -0.701330778544813, fluctuations: 0.0\n",
      "step: 41150 loss: 363.555376 time elapsed: 51.4559 learning rate: 0.000479, scenario: 0, slope: -0.681265527587738, fluctuations: 0.0\n",
      "step: 41160 loss: 357.660618 time elapsed: 51.4676 learning rate: 0.000479, scenario: 0, slope: -0.6612701119811158, fluctuations: 0.0\n",
      "step: 41170 loss: 351.903351 time elapsed: 51.4797 learning rate: 0.000479, scenario: 0, slope: -0.6422876514461815, fluctuations: 0.0\n",
      "step: 41180 loss: 346.385002 time elapsed: 51.4917 learning rate: 0.000479, scenario: 0, slope: -0.6245243271310823, fluctuations: 0.0\n",
      "step: 41190 loss: 341.199956 time elapsed: 51.5033 learning rate: 0.000479, scenario: 0, slope: -0.6072676771451884, fluctuations: 0.0\n",
      "step: 41200 loss: 336.315072 time elapsed: 51.5175 learning rate: 0.000479, scenario: 0, slope: -0.5913295368960906, fluctuations: 0.0\n",
      "step: 41210 loss: 331.644228 time elapsed: 51.5331 learning rate: 0.000479, scenario: 0, slope: -0.5706826622646289, fluctuations: 0.0\n",
      "step: 41220 loss: 327.150042 time elapsed: 51.5469 learning rate: 0.000479, scenario: 0, slope: -0.5506278773538966, fluctuations: 0.0\n",
      "step: 41230 loss: 322.814424 time elapsed: 51.5606 learning rate: 0.000479, scenario: 0, slope: -0.5294676443516453, fluctuations: 0.0\n",
      "step: 41240 loss: 318.621217 time elapsed: 51.5740 learning rate: 0.000479, scenario: 0, slope: -0.5076157514922962, fluctuations: 0.0\n",
      "step: 41250 loss: 314.557272 time elapsed: 51.5871 learning rate: 0.000479, scenario: 0, slope: -0.485781480963248, fluctuations: 0.0\n",
      "step: 41260 loss: 310.611940 time elapsed: 51.5987 learning rate: 0.000479, scenario: 0, slope: -0.4649047765907361, fluctuations: 0.0\n",
      "step: 41270 loss: 306.776201 time elapsed: 51.6099 learning rate: 0.000479, scenario: 0, slope: -0.44593909518265484, fluctuations: 0.0\n",
      "step: 41280 loss: 303.042199 time elapsed: 51.6216 learning rate: 0.000479, scenario: 0, slope: -0.42941074597598233, fluctuations: 0.0\n",
      "step: 41290 loss: 299.403008 time elapsed: 51.6335 learning rate: 0.000479, scenario: 0, slope: -0.4150647037272786, fluctuations: 0.0\n",
      "step: 41300 loss: 295.852479 time elapsed: 51.6460 learning rate: 0.000479, scenario: 0, slope: -0.403468866517029, fluctuations: 0.0\n",
      "step: 41310 loss: 292.385090 time elapsed: 51.6585 learning rate: 0.000479, scenario: 0, slope: -0.39052178515996283, fluctuations: 0.0\n",
      "step: 41320 loss: 288.995830 time elapsed: 51.6702 learning rate: 0.000479, scenario: 0, slope: -0.3797278028394015, fluctuations: 0.0\n",
      "step: 41330 loss: 285.680106 time elapsed: 51.6824 learning rate: 0.000479, scenario: 0, slope: -0.3697278613522614, fluctuations: 0.0\n",
      "step: 41340 loss: 282.433660 time elapsed: 51.6943 learning rate: 0.000479, scenario: 0, slope: -0.3604184652687488, fluctuations: 0.0\n",
      "step: 41350 loss: 279.252493 time elapsed: 51.7061 learning rate: 0.000479, scenario: 0, slope: -0.35172061165460133, fluctuations: 0.0\n",
      "step: 41360 loss: 276.132804 time elapsed: 51.7176 learning rate: 0.000479, scenario: 0, slope: -0.34357219629424257, fluctuations: 0.0\n",
      "step: 41370 loss: 273.070919 time elapsed: 51.7304 learning rate: 0.000479, scenario: 0, slope: -0.3359233234909189, fluctuations: 0.0\n",
      "step: 41380 loss: 270.063236 time elapsed: 51.7435 learning rate: 0.000479, scenario: 0, slope: -0.3287340247287124, fluctuations: 0.0\n",
      "step: 41390 loss: 267.106153 time elapsed: 51.7561 learning rate: 0.000479, scenario: 0, slope: -0.3219729908949997, fluctuations: 0.0\n",
      "step: 41400 loss: 264.195999 time elapsed: 51.7691 learning rate: 0.000479, scenario: 0, slope: -0.31623459701385925, fluctuations: 0.0\n",
      "step: 41410 loss: 261.328961 time elapsed: 51.7830 learning rate: 0.000479, scenario: 0, slope: -0.30964863192183395, fluctuations: 0.0\n",
      "step: 41420 loss: 258.500991 time elapsed: 51.7948 learning rate: 0.000479, scenario: 0, slope: -0.3040595306156542, fluctuations: 0.0\n",
      "step: 41430 loss: 255.707718 time elapsed: 51.8065 learning rate: 0.000479, scenario: 0, slope: -0.2988469789357433, fluctuations: 0.0\n",
      "step: 41440 loss: 252.944351 time elapsed: 51.8181 learning rate: 0.000479, scenario: 0, slope: -0.29401590330181315, fluctuations: 0.0\n",
      "step: 41450 loss: 250.205576 time elapsed: 51.8298 learning rate: 0.000479, scenario: 0, slope: -0.2895789323620223, fluctuations: 0.0\n",
      "step: 41460 loss: 247.485490 time elapsed: 51.8413 learning rate: 0.000479, scenario: 0, slope: -0.28555679529127503, fluctuations: 0.0\n",
      "step: 41470 loss: 244.777573 time elapsed: 51.8530 learning rate: 0.000479, scenario: 0, slope: -0.28197846541811633, fluctuations: 0.0\n",
      "step: 41480 loss: 242.074767 time elapsed: 51.8648 learning rate: 0.000479, scenario: 0, slope: -0.2788806459564378, fluctuations: 0.0\n",
      "step: 41490 loss: 239.369702 time elapsed: 51.8764 learning rate: 0.000479, scenario: 0, slope: -0.2763060030965021, fluctuations: 0.0\n",
      "step: 41500 loss: 236.655155 time elapsed: 51.8882 learning rate: 0.000479, scenario: 0, slope: -0.274473259601499, fluctuations: 0.0\n",
      "step: 41510 loss: 233.924750 time elapsed: 51.9011 learning rate: 0.000479, scenario: 0, slope: -0.27290127376370876, fluctuations: 0.0\n",
      "step: 41520 loss: 231.173855 time elapsed: 51.9131 learning rate: 0.000479, scenario: 0, slope: -0.2721380601462196, fluctuations: 0.0\n",
      "step: 41530 loss: 228.400510 time elapsed: 51.9248 learning rate: 0.000479, scenario: 0, slope: -0.2720097198164603, fluctuations: 0.0\n",
      "step: 41540 loss: 225.606110 time elapsed: 51.9367 learning rate: 0.000479, scenario: 0, slope: -0.27247677834851275, fluctuations: 0.0\n",
      "step: 41550 loss: 222.795587 time elapsed: 51.9496 learning rate: 0.000479, scenario: 0, slope: -0.27344984130467, fluctuations: 0.0\n",
      "step: 41560 loss: 219.976969 time elapsed: 51.9633 learning rate: 0.000479, scenario: 0, slope: -0.27478538531246977, fluctuations: 0.0\n",
      "step: 41570 loss: 217.160443 time elapsed: 51.9767 learning rate: 0.000479, scenario: 0, slope: -0.2762905669830049, fluctuations: 0.0\n",
      "step: 41580 loss: 214.357171 time elapsed: 51.9900 learning rate: 0.000479, scenario: 0, slope: -0.27773768608256344, fluctuations: 0.0\n",
      "step: 41590 loss: 211.578168 time elapsed: 52.0031 learning rate: 0.000479, scenario: 0, slope: -0.2788864836623233, fluctuations: 0.0\n",
      "step: 41600 loss: 208.833393 time elapsed: 52.0147 learning rate: 0.000479, scenario: 0, slope: -0.27947747266832723, fluctuations: 0.0\n",
      "step: 41610 loss: 206.131138 time elapsed: 52.0266 learning rate: 0.000479, scenario: 0, slope: -0.27942297188315257, fluctuations: 0.0\n",
      "step: 41620 loss: 203.477586 time elapsed: 52.0383 learning rate: 0.000479, scenario: 0, slope: -0.27849743671428434, fluctuations: 0.0\n",
      "step: 41630 loss: 200.876354 time elapsed: 52.0500 learning rate: 0.000479, scenario: 0, slope: -0.2766810043568095, fluctuations: 0.0\n",
      "step: 41640 loss: 198.327626 time elapsed: 52.0619 learning rate: 0.000479, scenario: 0, slope: -0.27399985666698057, fluctuations: 0.0\n",
      "step: 41650 loss: 195.825898 time elapsed: 52.0736 learning rate: 0.000479, scenario: 0, slope: -0.2705630092694353, fluctuations: 0.0\n",
      "step: 41660 loss: 193.353344 time elapsed: 52.0854 learning rate: 0.000479, scenario: 0, slope: -0.2665807727610631, fluctuations: 0.0\n",
      "step: 41670 loss: 190.856829 time elapsed: 52.0974 learning rate: 0.000479, scenario: 0, slope: -0.26244976036792605, fluctuations: 0.0\n",
      "step: 41680 loss: 188.152826 time elapsed: 52.1094 learning rate: 0.000479, scenario: 0, slope: -0.2591132423313139, fluctuations: 0.0\n",
      "step: 41690 loss: 184.592281 time elapsed: 52.1213 learning rate: 0.000479, scenario: 0, slope: -0.25952626534408824, fluctuations: 0.0\n",
      "step: 41700 loss: 180.694323 time elapsed: 52.1332 learning rate: 0.000479, scenario: 0, slope: -0.26727927194412276, fluctuations: 0.0\n",
      "step: 41710 loss: 178.428060 time elapsed: 52.1454 learning rate: 0.000479, scenario: 0, slope: -0.27613932173418765, fluctuations: 0.0\n",
      "step: 41720 loss: 176.320537 time elapsed: 52.1581 learning rate: 0.000479, scenario: 0, slope: -0.2805526038156633, fluctuations: 0.0\n",
      "step: 41730 loss: 174.283569 time elapsed: 52.1715 learning rate: 0.000479, scenario: 0, slope: -0.280679672307889, fluctuations: 0.0\n",
      "step: 41740 loss: 172.383543 time elapsed: 52.1845 learning rate: 0.000479, scenario: 0, slope: -0.2760409831574304, fluctuations: 0.0\n",
      "step: 41750 loss: 170.576145 time elapsed: 52.1982 learning rate: 0.000479, scenario: 0, slope: -0.2665369251086905, fluctuations: 0.0\n",
      "step: 41760 loss: 168.846287 time elapsed: 52.2116 learning rate: 0.000479, scenario: 0, slope: -0.25226813910946244, fluctuations: 0.0\n",
      "step: 41770 loss: 167.183056 time elapsed: 52.2231 learning rate: 0.000479, scenario: 0, slope: -0.23367117536112414, fluctuations: 0.0\n",
      "step: 41780 loss: 165.580054 time elapsed: 52.2351 learning rate: 0.000479, scenario: 0, slope: -0.2120274763308377, fluctuations: 0.0\n",
      "step: 41790 loss: 164.031274 time elapsed: 52.2464 learning rate: 0.000479, scenario: 0, slope: -0.19146192484263427, fluctuations: 0.0\n",
      "step: 41800 loss: 162.531777 time elapsed: 52.2581 learning rate: 0.000479, scenario: 0, slope: -0.18080774841634348, fluctuations: 0.0\n",
      "step: 41810 loss: 160.423067 time elapsed: 52.2702 learning rate: 0.001244, scenario: 1, slope: -0.1727926189503914, fluctuations: 0.0\n",
      "step: 41820 loss: 155.332357 time elapsed: 52.2821 learning rate: 0.002666, scenario: 0, slope: -0.17848746261663306, fluctuations: 0.0\n",
      "step: 41830 loss: 148.631907 time elapsed: 52.2938 learning rate: 0.002666, scenario: 0, slope: -0.20959348591371887, fluctuations: 0.0\n",
      "step: 41840 loss: 142.839139 time elapsed: 52.3052 learning rate: 0.002666, scenario: 0, slope: -0.2585586271990322, fluctuations: 0.0\n",
      "step: 41850 loss: 137.808949 time elapsed: 52.3170 learning rate: 0.002666, scenario: 0, slope: -0.31518275011994906, fluctuations: 0.0\n",
      "step: 41860 loss: 133.400770 time elapsed: 52.3286 learning rate: 0.002666, scenario: 0, slope: -0.3710155384102541, fluctuations: 0.0\n",
      "step: 41870 loss: 129.493024 time elapsed: 52.3404 learning rate: 0.002666, scenario: 0, slope: -0.4191075426733711, fluctuations: 0.0\n",
      "step: 41880 loss: 125.987597 time elapsed: 52.3523 learning rate: 0.002666, scenario: 0, slope: -0.4537336040085868, fluctuations: 0.0\n",
      "step: 41890 loss: 122.808628 time elapsed: 52.3643 learning rate: 0.002666, scenario: 0, slope: -0.4701280086665109, fluctuations: 0.0\n",
      "step: 41900 loss: 119.898688 time elapsed: 52.3776 learning rate: 0.002666, scenario: 0, slope: -0.46596231838654645, fluctuations: 0.0\n",
      "step: 41910 loss: 117.214710 time elapsed: 52.3919 learning rate: 0.002666, scenario: 0, slope: -0.4340321511675331, fluctuations: 0.0\n",
      "step: 41920 loss: 114.724224 time elapsed: 52.4052 learning rate: 0.002666, scenario: 0, slope: -0.38900757065299363, fluctuations: 0.0\n",
      "step: 41930 loss: 112.402186 time elapsed: 52.4185 learning rate: 0.002666, scenario: 0, slope: -0.34851052109606695, fluctuations: 0.0\n",
      "step: 41940 loss: 110.228537 time elapsed: 52.4305 learning rate: 0.002666, scenario: 0, slope: -0.31527548608790923, fluctuations: 0.0\n",
      "step: 41950 loss: 108.186552 time elapsed: 52.4426 learning rate: 0.002666, scenario: 0, slope: -0.28780832316468813, fluctuations: 0.0\n",
      "step: 41960 loss: 106.261820 time elapsed: 52.4542 learning rate: 0.002666, scenario: 0, slope: -0.2648281804103336, fluctuations: 0.0\n",
      "step: 41970 loss: 104.441553 time elapsed: 52.4683 learning rate: 0.002666, scenario: 0, slope: -0.2453187929061377, fluctuations: 0.0\n",
      "step: 41980 loss: 102.714050 time elapsed: 52.4809 learning rate: 0.002666, scenario: 0, slope: -0.228518000477884, fluctuations: 0.0\n",
      "step: 41990 loss: 101.068356 time elapsed: 52.4929 learning rate: 0.002666, scenario: 0, slope: -0.21387578556533152, fluctuations: 0.0\n",
      "step: 42000 loss: 99.494156 time elapsed: 52.5043 learning rate: 0.002666, scenario: 0, slope: -0.20222159958117675, fluctuations: 0.0\n",
      "step: 42010 loss: 97.981847 time elapsed: 52.5167 learning rate: 0.002666, scenario: 0, slope: -0.18964135363695864, fluctuations: 0.0\n",
      "step: 42020 loss: 96.522658 time elapsed: 52.5288 learning rate: 0.002666, scenario: 0, slope: -0.17959489475814314, fluctuations: 0.0\n",
      "step: 42030 loss: 95.108787 time elapsed: 52.5406 learning rate: 0.002666, scenario: 0, slope: -0.17073028231296203, fluctuations: 0.0\n",
      "step: 42040 loss: 93.733525 time elapsed: 52.5525 learning rate: 0.002666, scenario: 0, slope: -0.16294000060692843, fluctuations: 0.0\n",
      "step: 42050 loss: 92.391455 time elapsed: 52.5651 learning rate: 0.002666, scenario: 0, slope: -0.1561301454887615, fluctuations: 0.0\n",
      "step: 42060 loss: 91.078735 time elapsed: 52.5787 learning rate: 0.002666, scenario: 0, slope: -0.15020900512329052, fluctuations: 0.0\n",
      "step: 42070 loss: 89.793444 time elapsed: 52.5921 learning rate: 0.002666, scenario: 0, slope: -0.1450777356023441, fluctuations: 0.0\n",
      "step: 42080 loss: 88.535707 time elapsed: 52.6053 learning rate: 0.002666, scenario: 0, slope: -0.14062347450915408, fluctuations: 0.0\n",
      "step: 42090 loss: 87.307221 time elapsed: 52.6182 learning rate: 0.002666, scenario: 0, slope: -0.1367171793276649, fluctuations: 0.0\n",
      "step: 42100 loss: 86.109946 time elapsed: 52.6548 learning rate: 0.002666, scenario: 0, slope: -0.1335546297893771, fluctuations: 0.0\n",
      "step: 42110 loss: 84.944561 time elapsed: 52.6689 learning rate: 0.002666, scenario: 0, slope: -0.12999352011749166, fluctuations: 0.0\n",
      "step: 42120 loss: 83.809770 time elapsed: 52.6810 learning rate: 0.002666, scenario: 0, slope: -0.12692562235509827, fluctuations: 0.0\n",
      "step: 42130 loss: 82.702842 time elapsed: 52.6929 learning rate: 0.002666, scenario: 0, slope: -0.12393664671834985, fluctuations: 0.0\n",
      "step: 42140 loss: 81.620614 time elapsed: 52.7043 learning rate: 0.002666, scenario: 0, slope: -0.1209884181301942, fluctuations: 0.0\n",
      "step: 42150 loss: 80.560141 time elapsed: 52.7162 learning rate: 0.002666, scenario: 0, slope: -0.11808071092630348, fluctuations: 0.0\n",
      "step: 42160 loss: 79.518945 time elapsed: 52.7281 learning rate: 0.002666, scenario: 0, slope: -0.11524323783936512, fluctuations: 0.0\n",
      "step: 42170 loss: 78.495081 time elapsed: 52.7400 learning rate: 0.002666, scenario: 0, slope: -0.11252352151652958, fluctuations: 0.0\n",
      "step: 42180 loss: 77.487185 time elapsed: 52.7516 learning rate: 0.002666, scenario: 0, slope: -0.10997127306741376, fluctuations: 0.0\n",
      "step: 42190 loss: 76.494495 time elapsed: 52.7632 learning rate: 0.002666, scenario: 0, slope: -0.1076218883392117, fluctuations: 0.0\n",
      "step: 42200 loss: 75.516865 time elapsed: 52.7754 learning rate: 0.002666, scenario: 0, slope: -0.10568903424406279, fluctuations: 0.0\n",
      "step: 42210 loss: 74.554730 time elapsed: 52.7895 learning rate: 0.002666, scenario: 0, slope: -0.10353964671339135, fluctuations: 0.0\n",
      "step: 42220 loss: 73.609018 time elapsed: 52.8031 learning rate: 0.002666, scenario: 0, slope: -0.10174676487227985, fluctuations: 0.0\n",
      "step: 42230 loss: 72.680985 time elapsed: 52.8169 learning rate: 0.002666, scenario: 0, slope: -0.10005482103966581, fluctuations: 0.0\n",
      "step: 42240 loss: 71.771986 time elapsed: 52.8305 learning rate: 0.002666, scenario: 0, slope: -0.09841116905245977, fluctuations: 0.0\n",
      "step: 42250 loss: 70.883228 time elapsed: 52.8427 learning rate: 0.002666, scenario: 0, slope: -0.09676729061196032, fluctuations: 0.0\n",
      "step: 42260 loss: 70.015562 time elapsed: 52.8547 learning rate: 0.002666, scenario: 0, slope: -0.09508302858438154, fluctuations: 0.0\n",
      "step: 42270 loss: 69.169371 time elapsed: 52.8668 learning rate: 0.002666, scenario: 0, slope: -0.09333001037822078, fluctuations: 0.0\n",
      "step: 42280 loss: 68.344556 time elapsed: 52.8784 learning rate: 0.002666, scenario: 0, slope: -0.09149391252910555, fluctuations: 0.0\n",
      "step: 42290 loss: 67.540606 time elapsed: 52.8904 learning rate: 0.002666, scenario: 0, slope: -0.0895750564355122, fluctuations: 0.0\n",
      "step: 42300 loss: 66.756699 time elapsed: 52.9026 learning rate: 0.002666, scenario: 0, slope: -0.08778840987641245, fluctuations: 0.0\n",
      "step: 42310 loss: 65.991822 time elapsed: 52.9153 learning rate: 0.002666, scenario: 0, slope: -0.08555421194080355, fluctuations: 0.0\n",
      "step: 42320 loss: 65.244870 time elapsed: 52.9272 learning rate: 0.002666, scenario: 0, slope: -0.08350657181330824, fluctuations: 0.0\n",
      "step: 42330 loss: 64.514717 time elapsed: 52.9393 learning rate: 0.002666, scenario: 0, slope: -0.08147609746415954, fluctuations: 0.0\n",
      "step: 42340 loss: 63.800252 time elapsed: 52.9510 learning rate: 0.002666, scenario: 0, slope: -0.07949219191161802, fluctuations: 0.0\n",
      "step: 42350 loss: 63.100393 time elapsed: 52.9627 learning rate: 0.002666, scenario: 0, slope: -0.07757893266830937, fluctuations: 0.0\n",
      "step: 42360 loss: 62.414074 time elapsed: 52.9743 learning rate: 0.002666, scenario: 0, slope: -0.07575383023695985, fluctuations: 0.0\n",
      "step: 42370 loss: 61.740245 time elapsed: 52.9858 learning rate: 0.002666, scenario: 0, slope: -0.07402802803675862, fluctuations: 0.0\n",
      "step: 42380 loss: 61.077870 time elapsed: 52.9992 learning rate: 0.002666, scenario: 0, slope: -0.07240742975746363, fluctuations: 0.0\n",
      "step: 42390 loss: 60.425926 time elapsed: 53.0124 learning rate: 0.002666, scenario: 0, slope: -0.07089420380254893, fluctuations: 0.0\n",
      "step: 42400 loss: 59.783385 time elapsed: 53.0256 learning rate: 0.002666, scenario: 0, slope: -0.06962408895148145, fluctuations: 0.0\n",
      "step: 42410 loss: 59.149175 time elapsed: 53.0400 learning rate: 0.002666, scenario: 0, slope: -0.06818882961591695, fluctuations: 0.0\n",
      "step: 42420 loss: 58.522168 time elapsed: 53.0535 learning rate: 0.002666, scenario: 0, slope: -0.06699508349557283, fluctuations: 0.0\n",
      "step: 42430 loss: 57.901247 time elapsed: 53.0670 learning rate: 0.002666, scenario: 0, slope: -0.06590704665251731, fluctuations: 0.0\n",
      "step: 42440 loss: 57.285648 time elapsed: 53.0795 learning rate: 0.002666, scenario: 0, slope: -0.06492441891173815, fluctuations: 0.0\n",
      "step: 42450 loss: 56.675868 time elapsed: 53.0917 learning rate: 0.002666, scenario: 0, slope: -0.06404224464015028, fluctuations: 0.0\n",
      "step: 42460 loss: 56.074821 time elapsed: 53.1038 learning rate: 0.002666, scenario: 0, slope: -0.06324125439882489, fluctuations: 0.0\n",
      "step: 42470 loss: 55.486584 time elapsed: 53.1155 learning rate: 0.002666, scenario: 0, slope: -0.062480111352604964, fluctuations: 0.0\n",
      "step: 42480 loss: 54.912455 time elapsed: 53.1270 learning rate: 0.002666, scenario: 0, slope: -0.06170739778867139, fluctuations: 0.0\n",
      "step: 42490 loss: 54.351371 time elapsed: 53.1391 learning rate: 0.002666, scenario: 0, slope: -0.060882098054956424, fluctuations: 0.0\n",
      "step: 42500 loss: 53.801373 time elapsed: 53.1502 learning rate: 0.002666, scenario: 0, slope: -0.06007553622418186, fluctuations: 0.0\n",
      "step: 42510 loss: 53.255091 time elapsed: 53.1629 learning rate: 0.003548, scenario: 1, slope: -0.05900857308203485, fluctuations: 0.0\n",
      "step: 42520 loss: 52.281472 time elapsed: 53.1749 learning rate: 0.006285, scenario: 0, slope: -0.05905038880104867, fluctuations: 0.0\n",
      "step: 42530 loss: 415.331045 time elapsed: 53.1869 learning rate: 0.003858, scenario: -1, slope: 1.6454232321282845, fluctuations: 0.02\n",
      "step: 42540 loss: 76.464258 time elapsed: 53.1998 learning rate: 0.001345, scenario: -1, slope: 1.864272850169956, fluctuations: 0.06\n",
      "step: 42550 loss: 60.426992 time elapsed: 53.2136 learning rate: 0.000469, scenario: -1, slope: 1.580324191990042, fluctuations: 0.08\n",
      "step: 42560 loss: 60.164560 time elapsed: 53.2267 learning rate: 0.000164, scenario: -1, slope: 1.0783650253304313, fluctuations: 0.09\n",
      "step: 42570 loss: 58.270854 time elapsed: 53.2404 learning rate: 0.000057, scenario: -1, slope: 0.5792219957239387, fluctuations: 0.1\n",
      "step: 42580 loss: 57.807055 time elapsed: 53.2537 learning rate: 0.000020, scenario: -1, slope: 0.06051420339974147, fluctuations: 0.1\n",
      "step: 42590 loss: 57.728221 time elapsed: 53.2658 learning rate: 0.000019, scenario: 0, slope: -0.42761032772704843, fluctuations: 0.11\n",
      "step: 42600 loss: 57.591025 time elapsed: 53.2774 learning rate: 0.000019, scenario: 0, slope: -0.901608962199821, fluctuations: 0.11\n",
      "step: 42610 loss: 57.497278 time elapsed: 53.2898 learning rate: 0.000019, scenario: 0, slope: -1.5504780840749628, fluctuations: 0.11\n",
      "step: 42620 loss: 57.427578 time elapsed: 53.3018 learning rate: 0.000019, scenario: 0, slope: -2.281206387983498, fluctuations: 0.11\n",
      "step: 42630 loss: 57.360003 time elapsed: 53.3136 learning rate: 0.000019, scenario: 0, slope: -0.9043353794903702, fluctuations: 0.08\n",
      "step: 42640 loss: 57.292831 time elapsed: 53.3251 learning rate: 0.000019, scenario: 0, slope: -0.1883558011224245, fluctuations: 0.05\n",
      "step: 42650 loss: 57.225321 time elapsed: 53.3369 learning rate: 0.000028, scenario: 1, slope: -0.05398848248253346, fluctuations: 0.03\n",
      "step: 42660 loss: 57.090183 time elapsed: 53.3488 learning rate: 0.000072, scenario: 1, slope: -0.020053065353918882, fluctuations: 0.02\n",
      "step: 42670 loss: 56.755187 time elapsed: 53.3603 learning rate: 0.000186, scenario: 1, slope: -0.00926678755303789, fluctuations: 0.01\n",
      "step: 42680 loss: 55.974182 time elapsed: 53.3718 learning rate: 0.000483, scenario: 1, slope: -0.01229883663650022, fluctuations: 0.0\n",
      "step: 42690 loss: 54.388126 time elapsed: 53.3834 learning rate: 0.001252, scenario: 1, slope: -0.020583663109880797, fluctuations: 0.0\n",
      "step: 42700 loss: 52.011075 time elapsed: 53.3948 learning rate: 0.002951, scenario: 1, slope: -0.03564417395213781, fluctuations: 0.0\n",
      "step: 42710 loss: 49.948483 time elapsed: 53.4075 learning rate: 0.006327, scenario: 0, slope: -0.06269910274463318, fluctuations: 0.0\n",
      "step: 42720 loss: 48.588681 time elapsed: 53.4200 learning rate: 0.006327, scenario: 0, slope: -0.08873539327704777, fluctuations: 0.0\n",
      "step: 42730 loss: 47.157024 time elapsed: 53.4336 learning rate: 0.006327, scenario: 0, slope: -0.11298759281241268, fluctuations: 0.0\n",
      "step: 42740 loss: 45.983960 time elapsed: 53.4468 learning rate: 0.006327, scenario: 0, slope: -0.1330382502718901, fluctuations: 0.0\n",
      "step: 42750 loss: 44.851471 time elapsed: 53.4596 learning rate: 0.006327, scenario: 0, slope: -0.1467017473273046, fluctuations: 0.0\n",
      "step: 42760 loss: 43.768197 time elapsed: 53.4728 learning rate: 0.006327, scenario: 0, slope: -0.15269710387021784, fluctuations: 0.0\n",
      "step: 42770 loss: 42.754202 time elapsed: 53.4848 learning rate: 0.006327, scenario: 0, slope: -0.15034071193336562, fluctuations: 0.0\n",
      "step: 42780 loss: 41.812041 time elapsed: 53.4967 learning rate: 0.006327, scenario: 0, slope: -0.1403745989821034, fluctuations: 0.0\n",
      "step: 42790 loss: 40.936313 time elapsed: 53.5083 learning rate: 0.006327, scenario: 0, slope: -0.12612368770415225, fluctuations: 0.0\n",
      "step: 42800 loss: 40.118135 time elapsed: 53.5198 learning rate: 0.006327, scenario: 0, slope: -0.11440967256308006, fluctuations: 0.0\n",
      "step: 42810 loss: 39.352684 time elapsed: 53.5320 learning rate: 0.006327, scenario: 0, slope: -0.10483874250981572, fluctuations: 0.0\n",
      "step: 42820 loss: 38.635884 time elapsed: 53.5434 learning rate: 0.006327, scenario: 0, slope: -0.0977311432305621, fluctuations: 0.0\n",
      "step: 42830 loss: 37.962478 time elapsed: 53.5551 learning rate: 0.006327, scenario: 0, slope: -0.09142275813503471, fluctuations: 0.0\n",
      "step: 42840 loss: 37.325418 time elapsed: 53.5667 learning rate: 0.006327, scenario: 0, slope: -0.08568719989387087, fluctuations: 0.0\n",
      "step: 42850 loss: 36.716563 time elapsed: 53.5790 learning rate: 0.006327, scenario: 0, slope: -0.08016703310172739, fluctuations: 0.0\n",
      "step: 42860 loss: 36.125983 time elapsed: 53.5906 learning rate: 0.006327, scenario: 0, slope: -0.0751572053371593, fluctuations: 0.0\n",
      "step: 42870 loss: 35.543890 time elapsed: 53.6024 learning rate: 0.006327, scenario: 0, slope: -0.0707854676938228, fluctuations: 0.0\n",
      "step: 42880 loss: 34.983147 time elapsed: 53.6140 learning rate: 0.006327, scenario: 0, slope: -0.06705948537952748, fluctuations: 0.0\n",
      "step: 42890 loss: 34.459348 time elapsed: 53.6266 learning rate: 0.006327, scenario: 0, slope: -0.06383421466585826, fluctuations: 0.0\n",
      "step: 42900 loss: 33.961957 time elapsed: 53.6395 learning rate: 0.006327, scenario: 0, slope: -0.061234119190458694, fluctuations: 0.0\n",
      "step: 42910 loss: 33.484804 time elapsed: 53.6533 learning rate: 0.006327, scenario: 0, slope: -0.05836510383411563, fluctuations: 0.0\n",
      "step: 42920 loss: 33.661686 time elapsed: 53.6667 learning rate: 0.006327, scenario: 0, slope: -0.0519870077481124, fluctuations: 0.01\n",
      "step: 42930 loss: 33.611730 time elapsed: 53.6803 learning rate: 0.006327, scenario: 0, slope: -0.04714648401393365, fluctuations: 0.03\n",
      "step: 42940 loss: 32.193240 time elapsed: 53.6922 learning rate: 0.006327, scenario: 0, slope: -0.04492602335246957, fluctuations: 0.04\n",
      "step: 42950 loss: 31.810127 time elapsed: 53.7039 learning rate: 0.006327, scenario: 0, slope: -0.04296917916269913, fluctuations: 0.05\n",
      "step: 42960 loss: 31.442853 time elapsed: 53.7152 learning rate: 0.006327, scenario: 0, slope: -0.041661959327541555, fluctuations: 0.06\n",
      "step: 42970 loss: 31.584506 time elapsed: 53.7267 learning rate: 0.006327, scenario: 0, slope: -0.04110500224715273, fluctuations: 0.06\n",
      "step: 42980 loss: 30.803170 time elapsed: 53.7386 learning rate: 0.006327, scenario: 0, slope: -0.04111823960683754, fluctuations: 0.07\n",
      "step: 42990 loss: 30.232294 time elapsed: 53.7505 learning rate: 0.006327, scenario: 0, slope: -0.040037151250084634, fluctuations: 0.08\n",
      "step: 43000 loss: 31.240295 time elapsed: 53.7623 learning rate: 0.006327, scenario: 0, slope: -0.038259573929393074, fluctuations: 0.08\n",
      "step: 43010 loss: 31.163708 time elapsed: 53.7742 learning rate: 0.006327, scenario: 0, slope: -0.03855041148924388, fluctuations: 0.09\n",
      "step: 43020 loss: 29.730367 time elapsed: 53.7860 learning rate: 0.006327, scenario: 0, slope: -0.034762924183910957, fluctuations: 0.09\n",
      "step: 43030 loss: 13139.606338 time elapsed: 53.7978 learning rate: 0.006446, scenario: -1, slope: 10.713385924225491, fluctuations: 0.07\n",
      "step: 43040 loss: 1451.658691 time elapsed: 53.8099 learning rate: 0.002247, scenario: -1, slope: 13.211868462972758, fluctuations: 0.1\n",
      "step: 43050 loss: 378.784063 time elapsed: 53.8222 learning rate: 0.000784, scenario: -1, slope: 13.027459252351912, fluctuations: 0.11\n",
      "step: 43060 loss: 185.520822 time elapsed: 53.8352 learning rate: 0.000273, scenario: -1, slope: 10.214593641349937, fluctuations: 0.11\n",
      "step: 43070 loss: 173.733463 time elapsed: 53.8488 learning rate: 0.000095, scenario: -1, slope: 7.027494403370439, fluctuations: 0.11\n",
      "step: 43080 loss: 155.884430 time elapsed: 53.8620 learning rate: 0.000033, scenario: -1, slope: 3.395759889763551, fluctuations: 0.11\n",
      "step: 43090 loss: 153.798040 time elapsed: 53.8752 learning rate: 0.000013, scenario: 0, slope: -0.08721229455720773, fluctuations: 0.1\n",
      "step: 43100 loss: 152.779304 time elapsed: 53.8879 learning rate: 0.000013, scenario: 0, slope: -3.211523142464197, fluctuations: 0.09\n",
      "step: 43110 loss: 151.742717 time elapsed: 53.9005 learning rate: 0.000013, scenario: 0, slope: -7.280842944304177, fluctuations: 0.08\n",
      "step: 43120 loss: 150.731479 time elapsed: 53.9132 learning rate: 0.000013, scenario: 0, slope: -11.876210641187587, fluctuations: 0.08\n",
      "step: 43130 loss: 149.753746 time elapsed: 53.9253 learning rate: 0.000013, scenario: 0, slope: -13.53996486092406, fluctuations: 0.07\n",
      "step: 43140 loss: 148.801599 time elapsed: 53.9376 learning rate: 0.000013, scenario: 0, slope: -2.5223568327950776, fluctuations: 0.04\n",
      "step: 43150 loss: 147.866301 time elapsed: 53.9497 learning rate: 0.000013, scenario: 0, slope: -0.7492249778864266, fluctuations: 0.02\n",
      "step: 43160 loss: 146.942330 time elapsed: 53.9617 learning rate: 0.000013, scenario: 0, slope: -0.2676471517324681, fluctuations: 0.01\n",
      "step: 43170 loss: 145.968413 time elapsed: 53.9737 learning rate: 0.000021, scenario: 1, slope: -0.13093597257121709, fluctuations: 0.0\n",
      "step: 43180 loss: 143.860395 time elapsed: 53.9853 learning rate: 0.000054, scenario: 1, slope: -0.10159810954393468, fluctuations: 0.0\n",
      "step: 43190 loss: 138.564011 time elapsed: 53.9971 learning rate: 0.000139, scenario: 1, slope: -0.11615648833378192, fluctuations: 0.0\n",
      "step: 43200 loss: 126.052205 time elapsed: 54.0088 learning rate: 0.000272, scenario: 0, slope: -0.16340860524593523, fluctuations: 0.0\n",
      "step: 43210 loss: 111.234872 time elapsed: 54.0210 learning rate: 0.000272, scenario: 0, slope: -0.2895632776530586, fluctuations: 0.0\n",
      "step: 43220 loss: 98.870889 time elapsed: 54.0332 learning rate: 0.000272, scenario: 0, slope: -0.44494171517678793, fluctuations: 0.0\n",
      "step: 43230 loss: 88.650906 time elapsed: 54.0468 learning rate: 0.000272, scenario: 0, slope: -0.6105788004294119, fluctuations: 0.0\n",
      "step: 43240 loss: 80.235435 time elapsed: 54.0605 learning rate: 0.000272, scenario: 0, slope: -0.7635544886103737, fluctuations: 0.0\n",
      "step: 43250 loss: 73.318724 time elapsed: 54.0740 learning rate: 0.000272, scenario: 0, slope: -0.885054838784247, fluctuations: 0.0\n",
      "step: 43260 loss: 67.633015 time elapsed: 54.0880 learning rate: 0.000272, scenario: 0, slope: -0.9597848521552289, fluctuations: 0.0\n",
      "step: 43270 loss: 62.952851 time elapsed: 54.1014 learning rate: 0.000272, scenario: 0, slope: -0.9754524890800188, fluctuations: 0.0\n",
      "step: 43280 loss: 59.091489 time elapsed: 54.1148 learning rate: 0.000272, scenario: 0, slope: -0.9255768084142522, fluctuations: 0.0\n",
      "step: 43290 loss: 55.895299 time elapsed: 54.1271 learning rate: 0.000272, scenario: 0, slope: -0.8170433415215469, fluctuations: 0.0\n",
      "step: 43300 loss: 53.238460 time elapsed: 54.1386 learning rate: 0.000272, scenario: 0, slope: -0.6941611333670239, fluctuations: 0.0\n",
      "step: 43310 loss: 51.018329 time elapsed: 54.1506 learning rate: 0.000272, scenario: 0, slope: -0.5615331003803062, fluctuations: 0.0\n",
      "step: 43320 loss: 49.151558 time elapsed: 54.1638 learning rate: 0.000272, scenario: 0, slope: -0.4631372580643801, fluctuations: 0.0\n",
      "step: 43330 loss: 47.570779 time elapsed: 54.1758 learning rate: 0.000272, scenario: 0, slope: -0.38262640132058223, fluctuations: 0.0\n",
      "step: 43340 loss: 46.221781 time elapsed: 54.1878 learning rate: 0.000272, scenario: 0, slope: -0.3170307747517109, fluctuations: 0.0\n",
      "step: 43350 loss: 45.061105 time elapsed: 54.1997 learning rate: 0.000272, scenario: 0, slope: -0.26372771006346074, fluctuations: 0.0\n",
      "step: 43360 loss: 44.054003 time elapsed: 54.2115 learning rate: 0.000272, scenario: 0, slope: -0.22047657625623077, fluctuations: 0.0\n",
      "step: 43370 loss: 43.172738 time elapsed: 54.2231 learning rate: 0.000272, scenario: 0, slope: -0.1854049076654396, fluctuations: 0.0\n",
      "step: 43380 loss: 42.395172 time elapsed: 54.2350 learning rate: 0.000272, scenario: 0, slope: -0.156961514703301, fluctuations: 0.0\n",
      "step: 43390 loss: 41.703608 time elapsed: 54.2482 learning rate: 0.000272, scenario: 0, slope: -0.13386934097756195, fluctuations: 0.0\n",
      "step: 43400 loss: 41.083866 time elapsed: 54.2615 learning rate: 0.000272, scenario: 0, slope: -0.11679344715181642, fluctuations: 0.0\n",
      "step: 43410 loss: 40.524538 time elapsed: 54.2758 learning rate: 0.000272, scenario: 0, slope: -0.09975184722848912, fluctuations: 0.0\n",
      "step: 43420 loss: 40.016406 time elapsed: 54.2897 learning rate: 0.000272, scenario: 0, slope: -0.087188569355895, fluctuations: 0.0\n",
      "step: 43430 loss: 39.551977 time elapsed: 54.3028 learning rate: 0.000272, scenario: 0, slope: -0.07684089059428054, fluctuations: 0.0\n",
      "step: 43440 loss: 39.125131 time elapsed: 54.3150 learning rate: 0.000272, scenario: 0, slope: -0.06826764198125256, fluctuations: 0.0\n",
      "step: 43450 loss: 38.730833 time elapsed: 54.3270 learning rate: 0.000272, scenario: 0, slope: -0.061117941298114174, fluctuations: 0.0\n",
      "step: 43460 loss: 38.364916 time elapsed: 54.3384 learning rate: 0.000272, scenario: 0, slope: -0.05511357864163369, fluctuations: 0.0\n",
      "step: 43470 loss: 38.023911 time elapsed: 54.3503 learning rate: 0.000272, scenario: 0, slope: -0.0500343170677505, fluctuations: 0.0\n",
      "step: 43480 loss: 37.704908 time elapsed: 54.3623 learning rate: 0.000272, scenario: 0, slope: -0.04570582830934923, fluctuations: 0.0\n",
      "step: 43490 loss: 37.405453 time elapsed: 54.3739 learning rate: 0.000272, scenario: 0, slope: -0.04198994345217719, fluctuations: 0.0\n",
      "step: 43500 loss: 37.029478 time elapsed: 54.3854 learning rate: 0.000582, scenario: 1, slope: -0.03918051669941023, fluctuations: 0.0\n",
      "step: 43510 loss: 36.243411 time elapsed: 54.3976 learning rate: 0.001511, scenario: 1, slope: -0.037935403348117576, fluctuations: 0.0\n",
      "step: 43520 loss: 34.662666 time elapsed: 54.4090 learning rate: 0.002944, scenario: 0, slope: -0.042053650039367674, fluctuations: 0.0\n",
      "step: 43530 loss: 33.173849 time elapsed: 54.4209 learning rate: 0.002944, scenario: 0, slope: -0.05252566052734965, fluctuations: 0.0\n",
      "step: 43540 loss: 32.147780 time elapsed: 54.4325 learning rate: 0.002944, scenario: 0, slope: -0.06526848075326823, fluctuations: 0.0\n",
      "step: 43550 loss: 31.385048 time elapsed: 54.4454 learning rate: 0.002944, scenario: 0, slope: -0.07730594090981253, fluctuations: 0.0\n",
      "step: 43560 loss: 30.788700 time elapsed: 54.4588 learning rate: 0.002944, scenario: 0, slope: -0.08678816408895892, fluctuations: 0.0\n",
      "step: 43570 loss: 30.305984 time elapsed: 54.4718 learning rate: 0.002944, scenario: 0, slope: -0.0924716287579626, fluctuations: 0.0\n",
      "step: 43580 loss: 29.904849 time elapsed: 54.4851 learning rate: 0.002944, scenario: 0, slope: -0.09347910355593259, fluctuations: 0.0\n",
      "step: 43590 loss: 29.564060 time elapsed: 54.4996 learning rate: 0.002944, scenario: 0, slope: -0.08917387252445694, fluctuations: 0.0\n",
      "step: 43600 loss: 29.268737 time elapsed: 54.5135 learning rate: 0.002944, scenario: 0, slope: -0.08047553006718182, fluctuations: 0.0\n",
      "step: 43610 loss: 29.008169 time elapsed: 54.5264 learning rate: 0.002944, scenario: 0, slope: -0.06518860406059536, fluctuations: 0.0\n",
      "step: 43620 loss: 28.774578 time elapsed: 54.5382 learning rate: 0.002944, scenario: 0, slope: -0.05125011973527912, fluctuations: 0.0\n",
      "step: 43630 loss: 28.562222 time elapsed: 54.5498 learning rate: 0.002944, scenario: 0, slope: -0.04155433449439051, fluctuations: 0.0\n",
      "step: 43640 loss: 28.366870 time elapsed: 54.5616 learning rate: 0.002944, scenario: 0, slope: -0.03482781764672478, fluctuations: 0.0\n",
      "step: 43650 loss: 28.180052 time elapsed: 54.5735 learning rate: 0.004311, scenario: 1, slope: -0.02993162107373202, fluctuations: 0.0\n",
      "step: 43660 loss: 27.830120 time elapsed: 54.5854 learning rate: 0.011180, scenario: 1, slope: -0.02669744033217288, fluctuations: 0.0\n",
      "step: 43670 loss: 5030.979839 time elapsed: 54.5971 learning rate: 0.020589, scenario: -1, slope: 2.976566703475316, fluctuations: 0.0\n",
      "step: 43680 loss: 50380.739333 time elapsed: 54.6090 learning rate: 0.007179, scenario: -1, slope: 466.07965278182354, fluctuations: 0.01\n",
      "step: 43690 loss: 18983.685946 time elapsed: 54.6207 learning rate: 0.002503, scenario: -1, slope: 510.1488048389259, fluctuations: 0.02\n",
      "step: 43700 loss: 12331.643403 time elapsed: 54.6326 learning rate: 0.000970, scenario: -1, slope: 454.3443572347372, fluctuations: 0.02\n",
      "step: 43710 loss: 10536.827980 time elapsed: 54.6449 learning rate: 0.000338, scenario: -1, slope: 348.84730592093035, fluctuations: 0.02\n",
      "step: 43720 loss: 9971.491616 time elapsed: 54.6579 learning rate: 0.000118, scenario: -1, slope: 235.8182141583739, fluctuations: 0.02\n",
      "step: 43730 loss: 9781.216828 time elapsed: 54.6720 learning rate: 0.000041, scenario: -1, slope: 110.1908639699406, fluctuations: 0.02\n",
      "step: 43740 loss: 9717.597199 time elapsed: 54.6854 learning rate: 0.000018, scenario: 0, slope: -28.37162600009046, fluctuations: 0.02\n",
      "step: 43750 loss: 9680.963181 time elapsed: 54.6990 learning rate: 0.000018, scenario: 0, slope: -181.62753599892855, fluctuations: 0.02\n",
      "step: 43760 loss: 9645.016577 time elapsed: 54.7130 learning rate: 0.000018, scenario: 0, slope: -352.2400396808277, fluctuations: 0.02\n",
      "step: 43770 loss: 9609.594245 time elapsed: 54.7250 learning rate: 0.000018, scenario: 0, slope: -662.3603640564809, fluctuations: 0.01\n",
      "step: 43780 loss: 9574.592496 time elapsed: 54.7371 learning rate: 0.000018, scenario: 0, slope: -141.64961718949817, fluctuations: 0.0\n",
      "step: 43790 loss: 9539.937166 time elapsed: 54.7488 learning rate: 0.000018, scenario: 0, slope: -36.6648646475832, fluctuations: 0.0\n",
      "step: 43800 loss: 9505.569845 time elapsed: 54.7606 learning rate: 0.000018, scenario: 0, slope: -13.89080802199419, fluctuations: 0.0\n",
      "step: 43810 loss: 9455.864936 time elapsed: 54.7726 learning rate: 0.000046, scenario: 1, slope: -5.960387732051296, fluctuations: 0.0\n",
      "step: 43820 loss: 9329.148486 time elapsed: 54.7844 learning rate: 0.000119, scenario: 1, slope: -4.343934286650688, fluctuations: 0.0\n",
      "step: 43830 loss: 9005.248097 time elapsed: 54.7963 learning rate: 0.000309, scenario: 1, slope: -5.1416164925994, fluctuations: 0.0\n",
      "step: 43840 loss: 8141.488052 time elapsed: 54.8083 learning rate: 0.000801, scenario: 1, slope: -9.013935306788444, fluctuations: 0.0\n",
      "step: 43850 loss: 6493.278706 time elapsed: 54.8199 learning rate: 0.000881, scenario: 0, slope: -19.475571627358033, fluctuations: 0.0\n",
      "step: 43860 loss: 5353.597280 time elapsed: 54.8318 learning rate: 0.000881, scenario: 0, slope: -34.49906463925357, fluctuations: 0.0\n",
      "step: 43870 loss: 4621.170086 time elapsed: 54.8436 learning rate: 0.000881, scenario: 0, slope: -50.001525600382905, fluctuations: 0.0\n",
      "step: 43880 loss: 4039.306887 time elapsed: 54.8553 learning rate: 0.000881, scenario: 0, slope: -63.49599753233881, fluctuations: 0.0\n",
      "step: 43890 loss: 3542.999671 time elapsed: 54.8672 learning rate: 0.000881, scenario: 0, slope: -73.78975770784467, fluctuations: 0.0\n",
      "step: 43900 loss: 3172.464795 time elapsed: 54.8796 learning rate: 0.000881, scenario: 0, slope: -79.28061173432653, fluctuations: 0.0\n",
      "step: 43910 loss: 2919.479733 time elapsed: 54.8934 learning rate: 0.000881, scenario: 0, slope: -79.92916939086817, fluctuations: 0.0\n",
      "step: 43920 loss: 2733.568750 time elapsed: 54.9067 learning rate: 0.000881, scenario: 0, slope: -74.16816203658706, fluctuations: 0.0\n",
      "step: 43930 loss: 2570.395033 time elapsed: 54.9202 learning rate: 0.000881, scenario: 0, slope: -62.86806683425815, fluctuations: 0.0\n",
      "step: 43940 loss: 2417.607908 time elapsed: 54.9338 learning rate: 0.000881, scenario: 0, slope: -48.296226002102074, fluctuations: 0.0\n",
      "step: 43950 loss: 2262.148954 time elapsed: 54.9465 learning rate: 0.000881, scenario: 0, slope: -36.4039276886662, fluctuations: 0.0\n",
      "step: 43960 loss: 2105.643435 time elapsed: 54.9587 learning rate: 0.000881, scenario: 0, slope: -28.601301462920173, fluctuations: 0.0\n",
      "step: 43970 loss: 1976.554909 time elapsed: 54.9707 learning rate: 0.000881, scenario: 0, slope: -23.175772263001125, fluctuations: 0.0\n",
      "step: 43980 loss: 1869.024024 time elapsed: 54.9826 learning rate: 0.000881, scenario: 0, slope: -19.090975360063, fluctuations: 0.0\n",
      "step: 43990 loss: 1778.939971 time elapsed: 54.9941 learning rate: 0.000881, scenario: 0, slope: -16.267567006688832, fluctuations: 0.0\n",
      "step: 44000 loss: 1703.628303 time elapsed: 55.0057 learning rate: 0.000881, scenario: 0, slope: -14.535681012681852, fluctuations: 0.0\n",
      "step: 44010 loss: 1638.273562 time elapsed: 55.0184 learning rate: 0.000881, scenario: 0, slope: -12.967676998936474, fluctuations: 0.0\n",
      "step: 44020 loss: 1579.807607 time elapsed: 55.0301 learning rate: 0.000881, scenario: 0, slope: -11.655654681773626, fluctuations: 0.0\n",
      "step: 44030 loss: 1526.690761 time elapsed: 55.0415 learning rate: 0.000881, scenario: 0, slope: -10.329007039872872, fluctuations: 0.0\n",
      "step: 44040 loss: 1477.616721 time elapsed: 55.0527 learning rate: 0.000881, scenario: 0, slope: -8.998860197677963, fluctuations: 0.0\n",
      "step: 44050 loss: 1431.675478 time elapsed: 55.0646 learning rate: 0.000881, scenario: 0, slope: -7.75101192430267, fluctuations: 0.0\n",
      "step: 44060 loss: 1387.534533 time elapsed: 55.0764 learning rate: 0.000881, scenario: 0, slope: -6.730501660649437, fluctuations: 0.0\n",
      "step: 44070 loss: 1345.378026 time elapsed: 55.0899 learning rate: 0.000881, scenario: 0, slope: -5.955680512922677, fluctuations: 0.0\n",
      "step: 44080 loss: 1303.997016 time elapsed: 55.1031 learning rate: 0.000881, scenario: 0, slope: -5.381131551778303, fluctuations: 0.0\n",
      "step: 44090 loss: 1259.881166 time elapsed: 55.1168 learning rate: 0.000881, scenario: 0, slope: -4.97201056783054, fluctuations: 0.0\n",
      "step: 44100 loss: 1196.472291 time elapsed: 55.1300 learning rate: 0.000881, scenario: 0, slope: -4.7513008422189085, fluctuations: 0.0\n",
      "step: 44110 loss: 1125.079804 time elapsed: 55.1431 learning rate: 0.000881, scenario: 0, slope: -4.779692674275401, fluctuations: 0.0\n",
      "step: 44120 loss: 1078.391307 time elapsed: 55.1549 learning rate: 0.000881, scenario: 0, slope: -4.8902535965107585, fluctuations: 0.0\n",
      "step: 44130 loss: 1039.085459 time elapsed: 55.1666 learning rate: 0.000881, scenario: 0, slope: -4.976164460477969, fluctuations: 0.0\n",
      "step: 44140 loss: 1007.267157 time elapsed: 55.1792 learning rate: 0.000881, scenario: 0, slope: -4.991357545617812, fluctuations: 0.0\n",
      "step: 44150 loss: 978.443108 time elapsed: 55.1912 learning rate: 0.000881, scenario: 0, slope: -4.907691262437198, fluctuations: 0.0\n",
      "step: 44160 loss: 952.093368 time elapsed: 55.2035 learning rate: 0.000881, scenario: 0, slope: -4.715873090937766, fluctuations: 0.0\n",
      "step: 44170 loss: 928.006899 time elapsed: 55.2155 learning rate: 0.000881, scenario: 0, slope: -4.4110223110340625, fluctuations: 0.0\n",
      "step: 44180 loss: 906.235144 time elapsed: 55.2274 learning rate: 0.000881, scenario: 0, slope: -3.9929717798478244, fluctuations: 0.0\n",
      "step: 44190 loss: 886.523544 time elapsed: 55.2389 learning rate: 0.000881, scenario: 0, slope: -3.477118439961078, fluctuations: 0.0\n",
      "step: 44200 loss: 868.278045 time elapsed: 55.2503 learning rate: 0.000881, scenario: 0, slope: -2.9904357672592967, fluctuations: 0.0\n",
      "step: 44210 loss: 851.313362 time elapsed: 55.2626 learning rate: 0.000881, scenario: 0, slope: -2.5953609751972855, fluctuations: 0.0\n",
      "step: 44220 loss: 835.467023 time elapsed: 55.2745 learning rate: 0.000881, scenario: 0, slope: -2.3410736939812544, fluctuations: 0.0\n",
      "step: 44230 loss: 820.582899 time elapsed: 55.2864 learning rate: 0.000881, scenario: 0, slope: -2.1340034496701605, fluctuations: 0.0\n",
      "step: 44240 loss: 806.535761 time elapsed: 55.3003 learning rate: 0.000881, scenario: 0, slope: -1.9593611464357765, fluctuations: 0.0\n",
      "step: 44250 loss: 793.219843 time elapsed: 55.3142 learning rate: 0.000881, scenario: 0, slope: -1.8072240351456381, fluctuations: 0.0\n",
      "step: 44260 loss: 780.538988 time elapsed: 55.3277 learning rate: 0.000881, scenario: 0, slope: -1.6761688220541944, fluctuations: 0.0\n",
      "step: 44270 loss: 768.409643 time elapsed: 55.3405 learning rate: 0.000881, scenario: 0, slope: -1.5645331084236358, fluctuations: 0.0\n",
      "step: 44280 loss: 756.757525 time elapsed: 55.3540 learning rate: 0.000881, scenario: 0, slope: -1.4700200718922707, fluctuations: 0.0\n",
      "step: 44290 loss: 745.511848 time elapsed: 55.3661 learning rate: 0.000881, scenario: 0, slope: -1.388482711501983, fluctuations: 0.0\n",
      "step: 44300 loss: 734.485015 time elapsed: 55.3778 learning rate: 0.000881, scenario: 0, slope: -1.3245321029662827, fluctuations: 0.0\n",
      "step: 44310 loss: 720.445081 time elapsed: 55.3906 learning rate: 0.000881, scenario: 0, slope: -1.2647402085356652, fluctuations: 0.0\n",
      "step: 44320 loss: 706.789174 time elapsed: 55.4037 learning rate: 0.000881, scenario: 0, slope: -1.2411366407594722, fluctuations: 0.0\n",
      "step: 44330 loss: 694.331848 time elapsed: 55.4171 learning rate: 0.000881, scenario: 0, slope: -1.2317585525378092, fluctuations: 0.0\n",
      "step: 44340 loss: 682.655483 time elapsed: 55.4294 learning rate: 0.000881, scenario: 0, slope: -1.229046529179821, fluctuations: 0.0\n",
      "step: 44350 loss: 671.306003 time elapsed: 55.4410 learning rate: 0.000881, scenario: 0, slope: -1.2282725726357817, fluctuations: 0.0\n",
      "step: 44360 loss: 660.090754 time elapsed: 55.4529 learning rate: 0.000881, scenario: 0, slope: -1.2266389973006437, fluctuations: 0.0\n",
      "step: 44370 loss: 648.898261 time elapsed: 55.4649 learning rate: 0.000881, scenario: 0, slope: -1.221882851486133, fluctuations: 0.0\n",
      "step: 44380 loss: 637.949756 time elapsed: 55.4767 learning rate: 0.000881, scenario: 0, slope: -1.211476403747065, fluctuations: 0.0\n",
      "step: 44390 loss: 627.518910 time elapsed: 55.4888 learning rate: 0.000881, scenario: 0, slope: -1.1909313859691144, fluctuations: 0.0\n",
      "step: 44400 loss: 617.417377 time elapsed: 55.5022 learning rate: 0.000881, scenario: 0, slope: -1.1610541203439413, fluctuations: 0.0\n",
      "step: 44410 loss: 607.614154 time elapsed: 55.5168 learning rate: 0.000881, scenario: 0, slope: -1.115364828353348, fluctuations: 0.0\n",
      "step: 44420 loss: 598.118069 time elapsed: 55.5303 learning rate: 0.000881, scenario: 0, slope: -1.0847905802267797, fluctuations: 0.0\n",
      "step: 44430 loss: 588.926429 time elapsed: 55.5439 learning rate: 0.000881, scenario: 0, slope: -1.0568464326496605, fluctuations: 0.0\n",
      "step: 44440 loss: 580.050759 time elapsed: 55.5571 learning rate: 0.000881, scenario: 0, slope: -1.0282925290687008, fluctuations: 0.0\n",
      "step: 44450 loss: 571.508883 time elapsed: 55.5693 learning rate: 0.000881, scenario: 0, slope: -0.997643654367728, fluctuations: 0.0\n",
      "step: 44460 loss: 563.313795 time elapsed: 55.5812 learning rate: 0.000881, scenario: 0, slope: -0.9651716707880462, fluctuations: 0.0\n",
      "step: 44470 loss: 555.470648 time elapsed: 55.5932 learning rate: 0.000881, scenario: 0, slope: -0.9319732551291319, fluctuations: 0.0\n",
      "step: 44480 loss: 547.976406 time elapsed: 55.6055 learning rate: 0.000881, scenario: 0, slope: -0.8993644348790202, fluctuations: 0.0\n",
      "step: 44490 loss: 540.821688 time elapsed: 55.6196 learning rate: 0.000881, scenario: 0, slope: -0.8665138179595483, fluctuations: 0.0\n",
      "step: 44500 loss: 533.980097 time elapsed: 55.6318 learning rate: 0.000881, scenario: 0, slope: -0.8367360061488697, fluctuations: 0.0\n",
      "step: 44510 loss: 527.419427 time elapsed: 55.6446 learning rate: 0.000881, scenario: 0, slope: -0.8001040808481946, fluctuations: 0.0\n",
      "step: 44520 loss: 521.110220 time elapsed: 55.6565 learning rate: 0.000881, scenario: 0, slope: -0.7670478699251959, fluctuations: 0.0\n",
      "step: 44530 loss: 515.024441 time elapsed: 55.6683 learning rate: 0.000881, scenario: 0, slope: -0.7348421306249066, fluctuations: 0.0\n",
      "step: 44540 loss: 509.134143 time elapsed: 55.6807 learning rate: 0.000881, scenario: 0, slope: -0.7040980678509589, fluctuations: 0.0\n",
      "step: 44550 loss: 503.415918 time elapsed: 55.6941 learning rate: 0.000881, scenario: 0, slope: -0.675336630697942, fluctuations: 0.0\n",
      "step: 44560 loss: 497.849005 time elapsed: 55.7074 learning rate: 0.000881, scenario: 0, slope: -0.6489207241141461, fluctuations: 0.0\n",
      "step: 44570 loss: 492.416355 time elapsed: 55.7208 learning rate: 0.000881, scenario: 0, slope: -0.6250358648744283, fluctuations: 0.0\n",
      "step: 44580 loss: 487.105670 time elapsed: 55.7346 learning rate: 0.000881, scenario: 0, slope: -0.6036977040648112, fluctuations: 0.0\n",
      "step: 44590 loss: 481.910857 time elapsed: 55.7481 learning rate: 0.000881, scenario: 0, slope: -0.5845945447163208, fluctuations: 0.0\n",
      "step: 44600 loss: 476.805532 time elapsed: 55.7612 learning rate: 0.000881, scenario: 0, slope: -0.5693423883163417, fluctuations: 0.0\n",
      "step: 44610 loss: 471.790297 time elapsed: 55.7734 learning rate: 0.000881, scenario: 0, slope: -0.5528375718113044, fluctuations: 0.0\n",
      "step: 44620 loss: 466.859125 time elapsed: 55.7850 learning rate: 0.000881, scenario: 0, slope: -0.5395650791380953, fluctuations: 0.0\n",
      "step: 44630 loss: 462.005587 time elapsed: 55.7968 learning rate: 0.000881, scenario: 0, slope: -0.5276794087360976, fluctuations: 0.0\n",
      "step: 44640 loss: 457.223129 time elapsed: 55.8086 learning rate: 0.000881, scenario: 0, slope: -0.5169671906677782, fluctuations: 0.0\n",
      "step: 44650 loss: 452.507290 time elapsed: 55.8210 learning rate: 0.000881, scenario: 0, slope: -0.5072487407907276, fluctuations: 0.0\n",
      "step: 44660 loss: 447.853919 time elapsed: 55.8333 learning rate: 0.000881, scenario: 0, slope: -0.4983764983760657, fluctuations: 0.0\n",
      "step: 44670 loss: 442.544696 time elapsed: 55.8453 learning rate: 0.001717, scenario: 1, slope: -0.4912168682602147, fluctuations: 0.0\n",
      "step: 44680 loss: 435.047141 time elapsed: 55.8572 learning rate: 0.003680, scenario: 0, slope: -0.4925061627284473, fluctuations: 0.03\n",
      "step: 44690 loss: 413.270396 time elapsed: 55.8695 learning rate: 0.003680, scenario: 0, slope: -0.5505069762540945, fluctuations: 0.06\n",
      "step: 44700 loss: 398.317641 time elapsed: 55.8815 learning rate: 0.003680, scenario: 0, slope: -0.648609885926934, fluctuations: 0.09\n",
      "step: 44710 loss: 381.258062 time elapsed: 55.8941 learning rate: 0.003680, scenario: 0, slope: -0.835621565372924, fluctuations: 0.1\n",
      "step: 44720 loss: 366.084948 time elapsed: 55.9072 learning rate: 0.003680, scenario: 0, slope: -1.0097282291817768, fluctuations: 0.1\n",
      "step: 44730 loss: 351.734858 time elapsed: 55.9213 learning rate: 0.003680, scenario: 0, slope: -1.1677146701546561, fluctuations: 0.1\n",
      "step: 44740 loss: 337.882111 time elapsed: 55.9351 learning rate: 0.003680, scenario: 0, slope: -1.304215117423807, fluctuations: 0.1\n",
      "step: 44750 loss: 324.696378 time elapsed: 55.9498 learning rate: 0.003680, scenario: 0, slope: -1.4113559216433298, fluctuations: 0.1\n",
      "step: 44760 loss: 312.137200 time elapsed: 55.9648 learning rate: 0.003680, scenario: 0, slope: -1.4761150636002047, fluctuations: 0.1\n",
      "step: 44770 loss: 300.001938 time elapsed: 55.9776 learning rate: 0.003680, scenario: 0, slope: -1.4788183155252717, fluctuations: 0.1\n",
      "step: 44780 loss: 288.583851 time elapsed: 55.9897 learning rate: 0.003680, scenario: 0, slope: -1.4240878788296314, fluctuations: 0.06\n",
      "step: 44790 loss: 277.829738 time elapsed: 56.0019 learning rate: 0.003680, scenario: 0, slope: -1.3504140987417137, fluctuations: 0.03\n",
      "step: 44800 loss: 267.689366 time elapsed: 56.0135 learning rate: 0.003680, scenario: 0, slope: -1.2978742136968915, fluctuations: 0.0\n",
      "step: 44810 loss: 260.437195 time elapsed: 56.0264 learning rate: 0.003680, scenario: 0, slope: -1.2268675900305692, fluctuations: 0.0\n",
      "step: 44820 loss: 252.354895 time elapsed: 56.0392 learning rate: 0.003680, scenario: 0, slope: -1.1554400670712073, fluctuations: 0.02\n",
      "step: 44830 loss: 240.980742 time elapsed: 56.0512 learning rate: 0.003680, scenario: 0, slope: -1.0976207028821008, fluctuations: 0.05\n",
      "step: 44840 loss: 232.739830 time elapsed: 56.0634 learning rate: 0.003680, scenario: 0, slope: -1.0360962652657968, fluctuations: 0.05\n",
      "step: 44850 loss: 224.865275 time elapsed: 56.0757 learning rate: 0.003680, scenario: 0, slope: -0.9819076537692543, fluctuations: 0.05\n",
      "step: 44860 loss: 217.895624 time elapsed: 56.0876 learning rate: 0.003680, scenario: 0, slope: -0.9310292060029749, fluctuations: 0.05\n",
      "step: 44870 loss: 211.310414 time elapsed: 56.0991 learning rate: 0.003680, scenario: 0, slope: -0.8828682633725659, fluctuations: 0.05\n",
      "step: 44880 loss: 205.219554 time elapsed: 56.1117 learning rate: 0.003680, scenario: 0, slope: -0.8288174816848821, fluctuations: 0.06\n",
      "step: 44890 loss: 200.070006 time elapsed: 56.1260 learning rate: 0.003680, scenario: 0, slope: -0.7826784665063512, fluctuations: 0.08\n",
      "step: 44900 loss: 194.992685 time elapsed: 56.1394 learning rate: 0.003680, scenario: 0, slope: -0.7413810708101394, fluctuations: 0.09\n",
      "step: 44910 loss: 189.709127 time elapsed: 56.1536 learning rate: 0.003680, scenario: 0, slope: -0.6876607888959467, fluctuations: 0.09\n",
      "step: 44920 loss: 184.201190 time elapsed: 56.1677 learning rate: 0.003680, scenario: 0, slope: -0.6360744686057358, fluctuations: 0.06\n",
      "step: 44930 loss: 187.619604 time elapsed: 56.1817 learning rate: 0.003680, scenario: 0, slope: -0.5910762424069447, fluctuations: 0.04\n",
      "step: 44940 loss: 179.913298 time elapsed: 56.1943 learning rate: 0.003680, scenario: 0, slope: -0.534231281527302, fluctuations: 0.06\n",
      "step: 44950 loss: 173.023488 time elapsed: 56.2063 learning rate: 0.003680, scenario: 0, slope: -0.5052862922797625, fluctuations: 0.09\n",
      "step: 44960 loss: 168.087091 time elapsed: 56.2184 learning rate: 0.003680, scenario: 0, slope: -0.4844422536239623, fluctuations: 0.12\n",
      "step: 44970 loss: 163.894396 time elapsed: 56.2306 learning rate: 0.003680, scenario: 0, slope: -0.46825255469285143, fluctuations: 0.13\n",
      "step: 44980 loss: 160.044652 time elapsed: 56.2427 learning rate: 0.003680, scenario: 0, slope: -0.44529858660092647, fluctuations: 0.12\n",
      "step: 44990 loss: 156.601146 time elapsed: 56.2551 learning rate: 0.003680, scenario: 0, slope: -0.4304098148057802, fluctuations: 0.1\n",
      "step: 45000 loss: 153.383810 time elapsed: 56.2673 learning rate: 0.003680, scenario: 0, slope: -0.42048878656918726, fluctuations: 0.09\n",
      "step: 45010 loss: 150.322832 time elapsed: 56.2802 learning rate: 0.003680, scenario: 0, slope: -0.40866598122977255, fluctuations: 0.09\n",
      "step: 45020 loss: 147.381655 time elapsed: 56.2925 learning rate: 0.003680, scenario: 0, slope: -0.4005443517973884, fluctuations: 0.09\n",
      "step: 45030 loss: 144.861362 time elapsed: 56.3043 learning rate: 0.003680, scenario: 0, slope: -0.39555704165729205, fluctuations: 0.08\n",
      "step: 45040 loss: 141.910892 time elapsed: 56.3173 learning rate: 0.003680, scenario: 0, slope: -0.3216480042367394, fluctuations: 0.07\n",
      "step: 45050 loss: 139.157784 time elapsed: 56.3309 learning rate: 0.003680, scenario: 0, slope: -0.2946388214933936, fluctuations: 0.06\n",
      "step: 45060 loss: 136.517579 time elapsed: 56.3448 learning rate: 0.003680, scenario: 0, slope: -0.28424872469529516, fluctuations: 0.05\n",
      "step: 45070 loss: 133.988034 time elapsed: 56.3592 learning rate: 0.003680, scenario: 0, slope: -0.27613826950170617, fluctuations: 0.07\n",
      "step: 45080 loss: 131.584614 time elapsed: 56.3738 learning rate: 0.003680, scenario: 0, slope: -0.27237641660366857, fluctuations: 0.07\n",
      "step: 45090 loss: 129.339621 time elapsed: 56.3869 learning rate: 0.003680, scenario: 0, slope: -0.27046996049757166, fluctuations: 0.07\n",
      "step: 45100 loss: 127.133796 time elapsed: 56.3991 learning rate: 0.003680, scenario: 0, slope: -0.2693871475486758, fluctuations: 0.07\n",
      "step: 45110 loss: 125.092274 time elapsed: 56.4118 learning rate: 0.003680, scenario: 0, slope: -0.26815037430773236, fluctuations: 0.07\n",
      "step: 45120 loss: 123.146879 time elapsed: 56.4237 learning rate: 0.003680, scenario: 0, slope: -0.26705246731919324, fluctuations: 0.07\n",
      "step: 45130 loss: 121.299358 time elapsed: 56.4355 learning rate: 0.003680, scenario: 0, slope: -0.2656966972951936, fluctuations: 0.07\n",
      "step: 45140 loss: 119.531237 time elapsed: 56.4476 learning rate: 0.003680, scenario: 0, slope: -0.2345528898336561, fluctuations: 0.06\n",
      "step: 45150 loss: 117.822960 time elapsed: 56.4595 learning rate: 0.003680, scenario: 0, slope: -0.21498368337887735, fluctuations: 0.04\n",
      "step: 45160 loss: 116.163660 time elapsed: 56.4718 learning rate: 0.003680, scenario: 0, slope: -0.20240559658083254, fluctuations: 0.02\n",
      "step: 45170 loss: 120.776804 time elapsed: 56.4835 learning rate: 0.003680, scenario: 0, slope: -0.18367719687549136, fluctuations: 0.0\n",
      "step: 45180 loss: 116.059569 time elapsed: 56.4958 learning rate: 0.003680, scenario: 0, slope: -0.15812625073732714, fluctuations: 0.02\n",
      "step: 45190 loss: 112.666954 time elapsed: 56.5077 learning rate: 0.003680, scenario: 0, slope: -0.1516774785261811, fluctuations: 0.03\n",
      "step: 45200 loss: 111.078499 time elapsed: 56.5220 learning rate: 0.003680, scenario: 0, slope: -0.14878137006958217, fluctuations: 0.05\n",
      "step: 45210 loss: 113.336127 time elapsed: 56.5365 learning rate: 0.003680, scenario: 0, slope: -0.1456901093481599, fluctuations: 0.05\n",
      "step: 45220 loss: 106.446800 time elapsed: 56.5502 learning rate: 0.003680, scenario: 0, slope: -0.1433011909383225, fluctuations: 0.07\n",
      "step: 45230 loss: 104.670940 time elapsed: 56.5637 learning rate: 0.003680, scenario: 0, slope: -0.1509544383102947, fluctuations: 0.09\n",
      "step: 45240 loss: 103.456356 time elapsed: 56.5775 learning rate: 0.003680, scenario: 0, slope: -0.16196967878484148, fluctuations: 0.11\n",
      "step: 45250 loss: 101.889778 time elapsed: 56.5905 learning rate: 0.003680, scenario: 0, slope: -0.1741991532458601, fluctuations: 0.11\n",
      "step: 45260 loss: 100.577928 time elapsed: 56.6023 learning rate: 0.003680, scenario: 0, slope: -0.18555270688914802, fluctuations: 0.11\n",
      "step: 45270 loss: 99.388593 time elapsed: 56.6146 learning rate: 0.003680, scenario: 0, slope: -0.1853745553647554, fluctuations: 0.11\n",
      "step: 45280 loss: 98.250663 time elapsed: 56.6266 learning rate: 0.003680, scenario: 0, slope: -0.1651245662252548, fluctuations: 0.09\n",
      "step: 45290 loss: 97.390310 time elapsed: 56.6388 learning rate: 0.003680, scenario: 0, slope: -0.15850987866474406, fluctuations: 0.07\n",
      "step: 45300 loss: 99.798848 time elapsed: 56.6504 learning rate: 0.003680, scenario: 0, slope: -0.1421380081963329, fluctuations: 0.07\n",
      "step: 45310 loss: 95.029531 time elapsed: 56.6629 learning rate: 0.003680, scenario: 0, slope: -0.12929986059927964, fluctuations: 0.07\n",
      "step: 45320 loss: 94.476527 time elapsed: 56.6746 learning rate: 0.003680, scenario: 0, slope: -0.10980885773762193, fluctuations: 0.07\n",
      "step: 45330 loss: 93.313714 time elapsed: 56.6866 learning rate: 0.003680, scenario: 0, slope: -0.10488399314449694, fluctuations: 0.07\n",
      "step: 45340 loss: 92.224786 time elapsed: 56.6984 learning rate: 0.003680, scenario: 0, slope: -0.10432041741868532, fluctuations: 0.05\n",
      "step: 45350 loss: 91.225385 time elapsed: 56.7102 learning rate: 0.003680, scenario: 0, slope: -0.10458567762189803, fluctuations: 0.05\n",
      "step: 45360 loss: 90.340566 time elapsed: 56.7225 learning rate: 0.003680, scenario: 0, slope: -0.10545948771216249, fluctuations: 0.05\n",
      "step: 45370 loss: 89.823239 time elapsed: 56.7348 learning rate: 0.003680, scenario: 0, slope: -0.10621084137344682, fluctuations: 0.05\n",
      "step: 45380 loss: 90.383704 time elapsed: 56.7481 learning rate: 0.003680, scenario: 0, slope: -0.0999929149416025, fluctuations: 0.06\n",
      "step: 45390 loss: 88.487936 time elapsed: 56.7617 learning rate: 0.003680, scenario: 0, slope: -0.10117828344169029, fluctuations: 0.07\n",
      "step: 45400 loss: 96.599767 time elapsed: 56.7753 learning rate: 0.005388, scenario: 1, slope: -0.09113169045084017, fluctuations: 0.06\n",
      "step: 45410 loss: 426.093642 time elapsed: 56.7901 learning rate: 0.002435, scenario: -1, slope: 2.2627504358610158, fluctuations: 0.08\n",
      "step: 45420 loss: 131.627683 time elapsed: 56.8023 learning rate: 0.000849, scenario: -1, slope: 2.36470540458213, fluctuations: 0.08\n",
      "step: 45430 loss: 110.482153 time elapsed: 56.8142 learning rate: 0.000296, scenario: -1, slope: 1.9142849239449315, fluctuations: 0.09\n",
      "step: 45440 loss: 96.146062 time elapsed: 56.8262 learning rate: 0.000103, scenario: -1, slope: 1.2689827310776614, fluctuations: 0.1\n",
      "step: 45450 loss: 93.848693 time elapsed: 56.8380 learning rate: 0.000036, scenario: -1, slope: 0.5667432585122774, fluctuations: 0.1\n",
      "step: 45460 loss: 93.206385 time elapsed: 56.8495 learning rate: 0.000016, scenario: 0, slope: -0.10080052661205605, fluctuations: 0.1\n",
      "step: 45470 loss: 93.043721 time elapsed: 56.8616 learning rate: 0.000016, scenario: 0, slope: -0.7831934797807575, fluctuations: 0.1\n",
      "step: 45480 loss: 92.961182 time elapsed: 56.8736 learning rate: 0.000016, scenario: 0, slope: -1.4319405395402662, fluctuations: 0.09\n",
      "step: 45490 loss: 92.896249 time elapsed: 56.8853 learning rate: 0.000016, scenario: 0, slope: -2.1206975711497846, fluctuations: 0.08\n",
      "step: 45500 loss: 92.833886 time elapsed: 56.8970 learning rate: 0.000016, scenario: 0, slope: -2.924642769834916, fluctuations: 0.08\n",
      "step: 45510 loss: 92.774744 time elapsed: 56.9092 learning rate: 0.000016, scenario: 0, slope: -0.7043533378946495, fluctuations: 0.05\n",
      "step: 45520 loss: 92.718662 time elapsed: 56.9209 learning rate: 0.000016, scenario: 0, slope: -0.16143352444991932, fluctuations: 0.03\n",
      "step: 45530 loss: 92.645945 time elapsed: 56.9326 learning rate: 0.000039, scenario: 1, slope: -0.04375499849165829, fluctuations: 0.01\n",
      "step: 45540 loss: 92.468238 time elapsed: 56.9441 learning rate: 0.000100, scenario: 1, slope: -0.017355201727231106, fluctuations: 0.0\n",
      "step: 45550 loss: 92.047653 time elapsed: 56.9575 learning rate: 0.000259, scenario: 1, slope: -0.00987307889534648, fluctuations: 0.0\n",
      "step: 45560 loss: 91.175330 time elapsed: 56.9711 learning rate: 0.000673, scenario: 1, slope: -0.012783636803681565, fluctuations: 0.0\n",
      "step: 45570 loss: 89.827024 time elapsed: 56.9847 learning rate: 0.001746, scenario: 1, slope: -0.021795592566768086, fluctuations: 0.0\n",
      "step: 45580 loss: 88.475615 time elapsed: 56.9981 learning rate: 0.004528, scenario: 1, slope: -0.035825879535715505, fluctuations: 0.0\n",
      "step: 45590 loss: 86.166167 time elapsed: 57.0120 learning rate: 0.011743, scenario: 1, slope: -0.05504016646803321, fluctuations: 0.0\n",
      "step: 45600 loss: 9539.404445 time elapsed: 57.0242 learning rate: 0.010768, scenario: -1, slope: 61.346146845175376, fluctuations: 0.01\n",
      "step: 45610 loss: 8734.213991 time elapsed: 57.0367 learning rate: 0.003755, scenario: -1, slope: 80.66666853429277, fluctuations: 0.05\n",
      "step: 45620 loss: 3525.579677 time elapsed: 57.0483 learning rate: 0.001309, scenario: -1, slope: 87.0888305808346, fluctuations: 0.06\n",
      "step: 45630 loss: 2757.685367 time elapsed: 57.0600 learning rate: 0.000456, scenario: -1, slope: 76.59005711270719, fluctuations: 0.06\n",
      "step: 45640 loss: 2585.210140 time elapsed: 57.0719 learning rate: 0.000159, scenario: -1, slope: 61.15609279455667, fluctuations: 0.06\n",
      "step: 45650 loss: 2510.082515 time elapsed: 57.0837 learning rate: 0.000055, scenario: -1, slope: 43.299579676398814, fluctuations: 0.06\n",
      "step: 45660 loss: 2485.877210 time elapsed: 57.0956 learning rate: 0.000019, scenario: -1, slope: 22.768180534042248, fluctuations: 0.06\n",
      "step: 45670 loss: 2478.487592 time elapsed: 57.1075 learning rate: 0.000007, scenario: 0, slope: -1.1963879701048157, fluctuations: 0.06\n",
      "step: 45680 loss: 2474.813631 time elapsed: 57.1193 learning rate: 0.000007, scenario: 0, slope: -29.879974823742675, fluctuations: 0.06\n",
      "step: 45690 loss: 2471.292197 time elapsed: 57.1309 learning rate: 0.000007, scenario: 0, slope: -65.24917686674918, fluctuations: 0.06\n",
      "step: 45700 loss: 2467.887205 time elapsed: 57.1425 learning rate: 0.000007, scenario: 0, slope: -58.07662534288295, fluctuations: 0.04\n",
      "step: 45710 loss: 2464.578665 time elapsed: 57.1547 learning rate: 0.000007, scenario: 0, slope: -16.048696277829098, fluctuations: 0.01\n",
      "step: 45720 loss: 2461.353092 time elapsed: 57.1685 learning rate: 0.000007, scenario: 0, slope: -4.758212073208021, fluctuations: 0.0\n",
      "step: 45730 loss: 2457.424218 time elapsed: 57.1818 learning rate: 0.000016, scenario: 1, slope: -1.4373396454751743, fluctuations: 0.0\n",
      "step: 45740 loss: 2448.005023 time elapsed: 57.1954 learning rate: 0.000042, scenario: 1, slope: -0.6949458510447738, fluctuations: 0.0\n",
      "step: 45750 loss: 2425.305776 time elapsed: 57.2090 learning rate: 0.000108, scenario: 1, slope: -0.5073277726960161, fluctuations: 0.0\n",
      "step: 45760 loss: 2372.679134 time elapsed: 57.2216 learning rate: 0.000280, scenario: 1, slope: -0.6948321386828231, fluctuations: 0.0\n",
      "step: 45770 loss: 2249.511608 time elapsed: 57.2341 learning rate: 0.000727, scenario: 1, slope: -1.3322217295419958, fluctuations: 0.0\n",
      "step: 45780 loss: 1994.941990 time elapsed: 57.2459 learning rate: 0.001559, scenario: 0, slope: -2.8524128143314504, fluctuations: 0.0\n",
      "step: 45790 loss: 1719.606173 time elapsed: 57.2581 learning rate: 0.001559, scenario: 0, slope: -5.515446268703613, fluctuations: 0.0\n",
      "step: 45800 loss: 1516.502688 time elapsed: 57.2700 learning rate: 0.001559, scenario: 0, slope: -8.361254210607282, fluctuations: 0.0\n",
      "step: 45810 loss: 1360.331677 time elapsed: 57.2826 learning rate: 0.001559, scenario: 0, slope: -11.774793942729552, fluctuations: 0.0\n",
      "step: 45820 loss: 1235.982947 time elapsed: 57.2942 learning rate: 0.001559, scenario: 0, slope: -14.381812078500907, fluctuations: 0.0\n",
      "step: 45830 loss: 1134.050346 time elapsed: 57.3058 learning rate: 0.001559, scenario: 0, slope: -16.20264321753555, fluctuations: 0.0\n",
      "step: 45840 loss: 1048.176699 time elapsed: 57.3177 learning rate: 0.001559, scenario: 0, slope: -17.02733974156133, fluctuations: 0.0\n",
      "step: 45850 loss: 974.305701 time elapsed: 57.3292 learning rate: 0.001559, scenario: 0, slope: -16.739780890236734, fluctuations: 0.0\n",
      "step: 45860 loss: 909.204336 time elapsed: 57.3411 learning rate: 0.001559, scenario: 0, slope: -15.348251651629312, fluctuations: 0.0\n",
      "step: 45870 loss: 848.884170 time elapsed: 57.3524 learning rate: 0.001559, scenario: 0, slope: -13.10149330656832, fluctuations: 0.0\n",
      "step: 45880 loss: 796.496973 time elapsed: 57.3643 learning rate: 0.001559, scenario: 0, slope: -10.68794169245093, fluctuations: 0.0\n",
      "step: 45890 loss: 750.010210 time elapsed: 57.3776 learning rate: 0.001559, scenario: 0, slope: -8.864568972170272, fluctuations: 0.0\n",
      "step: 45900 loss: 707.184690 time elapsed: 57.3912 learning rate: 0.001559, scenario: 0, slope: -7.657627582065865, fluctuations: 0.0\n",
      "step: 45910 loss: 666.277294 time elapsed: 57.4052 learning rate: 0.001559, scenario: 0, slope: -6.555047334484992, fluctuations: 0.0\n",
      "step: 45920 loss: 626.816602 time elapsed: 57.4186 learning rate: 0.001559, scenario: 0, slope: -5.80554991938948, fluctuations: 0.0\n",
      "step: 45930 loss: 591.961042 time elapsed: 57.4316 learning rate: 0.001559, scenario: 0, slope: -5.216758759883818, fluctuations: 0.0\n",
      "step: 45940 loss: 559.900320 time elapsed: 57.4452 learning rate: 0.001559, scenario: 0, slope: -4.737082829043654, fluctuations: 0.0\n",
      "step: 45950 loss: 531.286117 time elapsed: 57.4575 learning rate: 0.001559, scenario: 0, slope: -4.32782198084992, fluctuations: 0.0\n",
      "step: 45960 loss: 505.496414 time elapsed: 57.4693 learning rate: 0.001559, scenario: 0, slope: -3.963500231078914, fluctuations: 0.0\n",
      "step: 45970 loss: 481.699626 time elapsed: 57.4810 learning rate: 0.001559, scenario: 0, slope: -3.638012734146772, fluctuations: 0.0\n",
      "step: 45980 loss: 459.228896 time elapsed: 57.4929 learning rate: 0.001559, scenario: 0, slope: -3.3472145889519287, fluctuations: 0.0\n",
      "step: 45990 loss: 437.389156 time elapsed: 57.5047 learning rate: 0.001559, scenario: 0, slope: -3.0797740037241597, fluctuations: 0.0\n",
      "step: 46000 loss: 417.227003 time elapsed: 57.5167 learning rate: 0.001559, scenario: 0, slope: -2.855386917303702, fluctuations: 0.0\n",
      "step: 46010 loss: 398.947919 time elapsed: 57.5294 learning rate: 0.001559, scenario: 0, slope: -2.5979876940208806, fluctuations: 0.0\n",
      "step: 46020 loss: 381.037762 time elapsed: 57.5409 learning rate: 0.001559, scenario: 0, slope: -2.4011379249969274, fluctuations: 0.0\n",
      "step: 46030 loss: 367.014441 time elapsed: 57.5530 learning rate: 0.001559, scenario: 0, slope: -2.2228473309016126, fluctuations: 0.0\n",
      "step: 46040 loss: 354.717253 time elapsed: 57.5650 learning rate: 0.001559, scenario: 0, slope: -2.0535965121424464, fluctuations: 0.0\n",
      "step: 46050 loss: 343.491155 time elapsed: 57.5774 learning rate: 0.001559, scenario: 0, slope: -1.8893866036055018, fluctuations: 0.0\n",
      "step: 46060 loss: 333.151020 time elapsed: 57.5913 learning rate: 0.001559, scenario: 0, slope: -1.7269894061407858, fluctuations: 0.0\n",
      "step: 46070 loss: 323.519746 time elapsed: 57.6052 learning rate: 0.001559, scenario: 0, slope: -1.565678220997966, fluctuations: 0.0\n",
      "step: 46080 loss: 314.522962 time elapsed: 57.6186 learning rate: 0.001559, scenario: 0, slope: -1.4094333544784374, fluctuations: 0.0\n",
      "step: 46090 loss: 306.021601 time elapsed: 57.6321 learning rate: 0.001559, scenario: 0, slope: -1.2650807210659787, fluctuations: 0.0\n",
      "step: 46100 loss: 298.018525 time elapsed: 57.6445 learning rate: 0.001559, scenario: 0, slope: -1.1513769019901041, fluctuations: 0.0\n",
      "step: 46110 loss: 290.288523 time elapsed: 57.6571 learning rate: 0.001559, scenario: 0, slope: -1.031605919691062, fluctuations: 0.0\n",
      "step: 46120 loss: 283.042332 time elapsed: 57.6691 learning rate: 0.001559, scenario: 0, slope: -0.9529079331877517, fluctuations: 0.0\n",
      "step: 46130 loss: 275.897958 time elapsed: 57.6804 learning rate: 0.001559, scenario: 0, slope: -0.8924683519651796, fluctuations: 0.0\n",
      "step: 46140 loss: 269.002744 time elapsed: 57.6919 learning rate: 0.001559, scenario: 0, slope: -0.8415194208701934, fluctuations: 0.0\n",
      "step: 46150 loss: 262.274522 time elapsed: 57.7039 learning rate: 0.001559, scenario: 0, slope: -0.7986966701069574, fluctuations: 0.0\n",
      "step: 46160 loss: 255.585609 time elapsed: 57.7155 learning rate: 0.001559, scenario: 0, slope: -0.7631820356854275, fluctuations: 0.0\n",
      "step: 46170 loss: 248.777169 time elapsed: 57.7277 learning rate: 0.001559, scenario: 0, slope: -0.733899558284583, fluctuations: 0.0\n",
      "step: 46180 loss: 240.869429 time elapsed: 57.7397 learning rate: 0.001559, scenario: 0, slope: -0.716096368762034, fluctuations: 0.0\n",
      "step: 46190 loss: 233.128671 time elapsed: 57.7515 learning rate: 0.001559, scenario: 0, slope: -0.7088923515453966, fluctuations: 0.0\n",
      "step: 46200 loss: 225.472520 time elapsed: 57.7632 learning rate: 0.001559, scenario: 0, slope: -0.7094573683211562, fluctuations: 0.0\n",
      "step: 46210 loss: 217.766692 time elapsed: 57.7755 learning rate: 0.001559, scenario: 0, slope: -0.7174279809158324, fluctuations: 0.0\n",
      "step: 46220 loss: 210.248261 time elapsed: 57.7877 learning rate: 0.001559, scenario: 0, slope: -0.7285004326696172, fluctuations: 0.0\n",
      "step: 46230 loss: 204.142878 time elapsed: 57.8007 learning rate: 0.001559, scenario: 0, slope: -0.7360112694653795, fluctuations: 0.0\n",
      "step: 46240 loss: 197.692797 time elapsed: 57.8143 learning rate: 0.001559, scenario: 0, slope: -0.7373745076785819, fluctuations: 0.0\n",
      "step: 46250 loss: 191.751335 time elapsed: 57.8280 learning rate: 0.001559, scenario: 0, slope: -0.7299713465437262, fluctuations: 0.0\n",
      "step: 46260 loss: 186.063663 time elapsed: 57.8415 learning rate: 0.001559, scenario: 0, slope: -0.7128701058292273, fluctuations: 0.0\n",
      "step: 46270 loss: 180.695037 time elapsed: 57.8551 learning rate: 0.001559, scenario: 0, slope: -0.6846559384700602, fluctuations: 0.0\n",
      "step: 46280 loss: 175.779345 time elapsed: 57.8674 learning rate: 0.001559, scenario: 0, slope: -0.6514116735939763, fluctuations: 0.0\n",
      "step: 46290 loss: 171.290533 time elapsed: 57.8792 learning rate: 0.001559, scenario: 0, slope: -0.6156518663626586, fluctuations: 0.0\n",
      "step: 46300 loss: 167.201653 time elapsed: 57.8907 learning rate: 0.001559, scenario: 0, slope: -0.5809607071776732, fluctuations: 0.0\n",
      "step: 46310 loss: 163.545521 time elapsed: 57.9029 learning rate: 0.001559, scenario: 0, slope: -0.5389163709126744, fluctuations: 0.0\n",
      "step: 46320 loss: 160.177765 time elapsed: 57.9145 learning rate: 0.001559, scenario: 0, slope: -0.5029455539159254, fluctuations: 0.0\n",
      "step: 46330 loss: 157.137456 time elapsed: 57.9264 learning rate: 0.001559, scenario: 0, slope: -0.46655902764776147, fluctuations: 0.0\n",
      "step: 46340 loss: 154.238165 time elapsed: 57.9382 learning rate: 0.001559, scenario: 0, slope: -0.43032309113453077, fluctuations: 0.0\n",
      "step: 46350 loss: 151.671016 time elapsed: 57.9499 learning rate: 0.001559, scenario: 0, slope: -0.3947798022289724, fluctuations: 0.0\n",
      "step: 46360 loss: 149.194997 time elapsed: 57.9616 learning rate: 0.001559, scenario: 0, slope: -0.3608700691680139, fluctuations: 0.0\n",
      "step: 46370 loss: 146.990664 time elapsed: 57.9735 learning rate: 0.001559, scenario: 0, slope: -0.3296563371728046, fluctuations: 0.0\n",
      "step: 46380 loss: 144.919095 time elapsed: 57.9854 learning rate: 0.001559, scenario: 0, slope: -0.3019516260309957, fluctuations: 0.0\n",
      "step: 46390 loss: 143.180184 time elapsed: 57.9973 learning rate: 0.001559, scenario: 0, slope: -0.27724579204071376, fluctuations: 0.0\n",
      "step: 46400 loss: 141.355615 time elapsed: 58.0104 learning rate: 0.001559, scenario: 0, slope: -0.25671433765957635, fluctuations: 0.0\n",
      "step: 46410 loss: 139.628447 time elapsed: 58.0243 learning rate: 0.001559, scenario: 0, slope: -0.2347886507912261, fluctuations: 0.0\n",
      "step: 46420 loss: 138.066129 time elapsed: 58.0376 learning rate: 0.001559, scenario: 0, slope: -0.2170710249375901, fluctuations: 0.0\n",
      "step: 46430 loss: 136.596850 time elapsed: 58.0509 learning rate: 0.001559, scenario: 0, slope: -0.2011061774218321, fluctuations: 0.0\n",
      "step: 46440 loss: 135.187436 time elapsed: 58.0641 learning rate: 0.001559, scenario: 0, slope: -0.1869716878795224, fluctuations: 0.0\n",
      "step: 46450 loss: 133.803453 time elapsed: 58.0767 learning rate: 0.001559, scenario: 0, slope: -0.1747098304618101, fluctuations: 0.0\n",
      "step: 46460 loss: 132.246552 time elapsed: 58.0887 learning rate: 0.001559, scenario: 0, slope: -0.16494751787824558, fluctuations: 0.0\n",
      "step: 46470 loss: 130.919944 time elapsed: 58.1009 learning rate: 0.001559, scenario: 0, slope: -0.15751795864184473, fluctuations: 0.0\n",
      "step: 46480 loss: 129.903424 time elapsed: 58.1127 learning rate: 0.001559, scenario: 0, slope: -0.1510381735568726, fluctuations: 0.0\n",
      "step: 46490 loss: 128.471768 time elapsed: 58.1243 learning rate: 0.001559, scenario: 0, slope: -0.14479168648896823, fluctuations: 0.0\n",
      "step: 46500 loss: 130.061966 time elapsed: 58.1362 learning rate: 0.002511, scenario: 1, slope: -0.13964772314872811, fluctuations: 0.0\n",
      "step: 46510 loss: 162.775687 time elapsed: 58.1486 learning rate: 0.004624, scenario: -1, slope: 0.022586465402347437, fluctuations: 0.04\n",
      "step: 46520 loss: 132.046795 time elapsed: 58.1599 learning rate: 0.001612, scenario: -1, slope: 0.07123085147126436, fluctuations: 0.07\n",
      "step: 46530 loss: 123.322731 time elapsed: 58.1715 learning rate: 0.000562, scenario: -1, slope: 0.027692112505002496, fluctuations: 0.09\n",
      "step: 46540 loss: 122.576396 time elapsed: 58.1831 learning rate: 0.000513, scenario: 1, slope: -0.02058596198481869, fluctuations: 0.11\n",
      "step: 46550 loss: 121.880389 time elapsed: 58.1946 learning rate: 0.001331, scenario: 1, slope: -0.06626076272330629, fluctuations: 0.13\n",
      "step: 46560 loss: 120.555180 time elapsed: 58.2073 learning rate: 0.003451, scenario: 1, slope: -0.11531823612869091, fluctuations: 0.14\n",
      "step: 46570 loss: 118.038724 time elapsed: 58.2211 learning rate: 0.004594, scenario: 0, slope: -0.17016475358014738, fluctuations: 0.14\n",
      "step: 46580 loss: 115.480683 time elapsed: 58.2344 learning rate: 0.004594, scenario: 0, slope: -0.23346352055020234, fluctuations: 0.14\n",
      "step: 46590 loss: 112.996038 time elapsed: 58.2479 learning rate: 0.004594, scenario: 0, slope: -0.3094958141068988, fluctuations: 0.14\n",
      "step: 46600 loss: 110.627147 time elapsed: 58.2607 learning rate: 0.004594, scenario: 0, slope: -0.3973826159006233, fluctuations: 0.14\n",
      "step: 46610 loss: 108.316714 time elapsed: 58.2738 learning rate: 0.004594, scenario: 0, slope: -0.2883739060157061, fluctuations: 0.09\n",
      "step: 46620 loss: 105.967680 time elapsed: 58.2859 learning rate: 0.004594, scenario: 0, slope: -0.20325003170046105, fluctuations: 0.07\n",
      "step: 46630 loss: 103.896746 time elapsed: 58.2979 learning rate: 0.004594, scenario: 0, slope: -0.21488839273028862, fluctuations: 0.05\n",
      "step: 46640 loss: 101.663496 time elapsed: 58.3096 learning rate: 0.004594, scenario: 0, slope: -0.2284853048832728, fluctuations: 0.05\n",
      "step: 46650 loss: 99.397761 time elapsed: 58.3215 learning rate: 0.004594, scenario: 0, slope: -0.23421075284081497, fluctuations: 0.03\n",
      "step: 46660 loss: 97.973824 time elapsed: 58.3332 learning rate: 0.004594, scenario: 0, slope: -0.23034408951345078, fluctuations: 0.03\n",
      "step: 46670 loss: 95.704907 time elapsed: 58.3451 learning rate: 0.004594, scenario: 0, slope: -0.2235677335642721, fluctuations: 0.04\n",
      "step: 46680 loss: 94.045870 time elapsed: 58.3567 learning rate: 0.004594, scenario: 0, slope: -0.21688417329155332, fluctuations: 0.04\n",
      "step: 46690 loss: 92.650499 time elapsed: 58.3688 learning rate: 0.004594, scenario: 0, slope: -0.20889007222382233, fluctuations: 0.06\n",
      "step: 46700 loss: 90.334826 time elapsed: 58.3804 learning rate: 0.004594, scenario: 0, slope: -0.20215849243735864, fluctuations: 0.07\n",
      "step: 46710 loss: 89.103418 time elapsed: 58.3930 learning rate: 0.004594, scenario: 0, slope: -0.19281319898056368, fluctuations: 0.07\n",
      "step: 46720 loss: 87.292843 time elapsed: 58.4051 learning rate: 0.004594, scenario: 0, slope: -0.18539112890407589, fluctuations: 0.07\n",
      "step: 46730 loss: 85.883046 time elapsed: 58.4175 learning rate: 0.004594, scenario: 0, slope: -0.173770505050069, fluctuations: 0.07\n",
      "step: 46740 loss: 84.704171 time elapsed: 58.4309 learning rate: 0.004594, scenario: 0, slope: -0.16616233802688093, fluctuations: 0.07\n",
      "step: 46750 loss: 83.320093 time elapsed: 58.4443 learning rate: 0.004594, scenario: 0, slope: -0.1598230980499995, fluctuations: 0.08\n",
      "step: 46760 loss: 81.704783 time elapsed: 58.4580 learning rate: 0.004594, scenario: 0, slope: -0.15400417943133993, fluctuations: 0.07\n",
      "step: 46770 loss: 82.266351 time elapsed: 58.4718 learning rate: 0.004594, scenario: 0, slope: -0.1467506770906138, fluctuations: 0.06\n",
      "step: 46780 loss: 80.158005 time elapsed: 58.4850 learning rate: 0.004594, scenario: 0, slope: -0.13864220146899095, fluctuations: 0.06\n",
      "step: 46790 loss: 78.585605 time elapsed: 58.4973 learning rate: 0.004594, scenario: 0, slope: -0.13310561055883038, fluctuations: 0.06\n",
      "step: 46800 loss: 77.016581 time elapsed: 58.5091 learning rate: 0.004594, scenario: 0, slope: -0.13143693972536707, fluctuations: 0.06\n",
      "step: 46810 loss: 76.523600 time elapsed: 58.5233 learning rate: 0.004594, scenario: 0, slope: -0.12770489955115286, fluctuations: 0.06\n",
      "step: 46820 loss: 75.645159 time elapsed: 58.5362 learning rate: 0.004594, scenario: 0, slope: -0.12280845505757075, fluctuations: 0.07\n",
      "step: 46830 loss: 73.613151 time elapsed: 58.5484 learning rate: 0.004594, scenario: 0, slope: -0.11852125968910425, fluctuations: 0.06\n",
      "step: 46840 loss: 76.496741 time elapsed: 58.5601 learning rate: 0.004594, scenario: 0, slope: -0.11231331126772873, fluctuations: 0.04\n",
      "step: 46850 loss: 75.093408 time elapsed: 58.5719 learning rate: 0.004594, scenario: 0, slope: -0.09718322121528375, fluctuations: 0.05\n",
      "step: 46860 loss: 70.748590 time elapsed: 58.5835 learning rate: 0.004594, scenario: 0, slope: -0.0996689602543073, fluctuations: 0.07\n",
      "step: 46870 loss: 70.307784 time elapsed: 58.5956 learning rate: 0.004594, scenario: 0, slope: -0.1005598639306708, fluctuations: 0.09\n",
      "step: 46880 loss: 69.057521 time elapsed: 58.6073 learning rate: 0.004594, scenario: 0, slope: -0.09967548106922303, fluctuations: 0.08\n",
      "step: 46890 loss: 67.925510 time elapsed: 58.6195 learning rate: 0.004594, scenario: 0, slope: -0.10289494118541262, fluctuations: 0.07\n",
      "step: 46900 loss: 75.298188 time elapsed: 58.6336 learning rate: 0.004594, scenario: 0, slope: -0.09789717193026519, fluctuations: 0.07\n",
      "step: 46910 loss: 66.752045 time elapsed: 58.6474 learning rate: 0.004594, scenario: 0, slope: -0.08507898045805386, fluctuations: 0.09\n",
      "step: 46920 loss: 66.651494 time elapsed: 58.6609 learning rate: 0.004594, scenario: 0, slope: -0.08451734271426842, fluctuations: 0.1\n",
      "step: 46930 loss: 64.649402 time elapsed: 58.6744 learning rate: 0.004594, scenario: 0, slope: -0.09380227550095703, fluctuations: 0.11\n",
      "step: 46940 loss: 63.950450 time elapsed: 58.6880 learning rate: 0.004594, scenario: 0, slope: -0.09884568582927708, fluctuations: 0.12\n",
      "step: 46950 loss: 62.992561 time elapsed: 58.7005 learning rate: 0.004594, scenario: 0, slope: -0.08844080534312589, fluctuations: 0.11\n",
      "step: 46960 loss: 64.173669 time elapsed: 58.7129 learning rate: 0.004594, scenario: 0, slope: -0.08833649614946418, fluctuations: 0.1\n",
      "step: 46970 loss: 1097.727498 time elapsed: 58.7249 learning rate: 0.006355, scenario: -1, slope: 0.829899964550368, fluctuations: 0.09\n",
      "step: 46980 loss: 209.578927 time elapsed: 58.7371 learning rate: 0.002216, scenario: -1, slope: 1.3116677865478017, fluctuations: 0.12\n",
      "step: 46990 loss: 114.030849 time elapsed: 58.7489 learning rate: 0.000773, scenario: -1, slope: 1.368261318040362, fluctuations: 0.14\n",
      "step: 47000 loss: 85.516621 time elapsed: 58.7607 learning rate: 0.000299, scenario: -1, slope: 1.1825868833628839, fluctuations: 0.16\n",
      "step: 47010 loss: 81.002950 time elapsed: 58.7734 learning rate: 0.000104, scenario: -1, slope: 0.7849616761347578, fluctuations: 0.15\n",
      "step: 47020 loss: 78.620606 time elapsed: 58.7851 learning rate: 0.000036, scenario: -1, slope: 0.3869870991688276, fluctuations: 0.13\n",
      "step: 47030 loss: 78.425545 time elapsed: 58.7972 learning rate: 0.000015, scenario: 1, slope: -0.003950059101981687, fluctuations: 0.12\n",
      "step: 47040 loss: 78.289901 time elapsed: 58.8091 learning rate: 0.000018, scenario: 0, slope: -0.36296068950792043, fluctuations: 0.1\n",
      "step: 47050 loss: 78.143223 time elapsed: 58.8210 learning rate: 0.000018, scenario: 0, slope: -0.8234414276659365, fluctuations: 0.1\n",
      "step: 47060 loss: 78.011396 time elapsed: 58.8346 learning rate: 0.000018, scenario: 0, slope: -1.390507037565423, fluctuations: 0.1\n",
      "step: 47070 loss: 77.889419 time elapsed: 58.8483 learning rate: 0.000018, scenario: 0, slope: -1.7120439683700353, fluctuations: 0.08\n",
      "step: 47080 loss: 77.770773 time elapsed: 58.8616 learning rate: 0.000018, scenario: 0, slope: -0.4164439848941887, fluctuations: 0.04\n",
      "step: 47090 loss: 77.653510 time elapsed: 58.8748 learning rate: 0.000020, scenario: 1, slope: -0.10220477506149608, fluctuations: 0.02\n",
      "step: 47100 loss: 77.469303 time elapsed: 58.8886 learning rate: 0.000047, scenario: 1, slope: -0.03500002873046213, fluctuations: 0.01\n",
      "step: 47110 loss: 77.044810 time elapsed: 58.9016 learning rate: 0.000121, scenario: 1, slope: -0.01792715064926898, fluctuations: 0.0\n",
      "step: 47120 loss: 76.020914 time elapsed: 58.9134 learning rate: 0.000314, scenario: 1, slope: -0.017941807398951605, fluctuations: 0.0\n",
      "step: 47130 loss: 73.734705 time elapsed: 58.9252 learning rate: 0.000815, scenario: 1, slope: -0.029236504380019842, fluctuations: 0.0\n",
      "step: 47140 loss: 69.508093 time elapsed: 58.9371 learning rate: 0.002114, scenario: 1, slope: -0.05529247802111706, fluctuations: 0.0\n",
      "step: 47150 loss: 65.058286 time elapsed: 58.9485 learning rate: 0.003405, scenario: 0, slope: -0.09976638305443847, fluctuations: 0.0\n",
      "step: 47160 loss: 62.752460 time elapsed: 58.9600 learning rate: 0.003405, scenario: 0, slope: -0.1479667514680396, fluctuations: 0.0\n",
      "step: 47170 loss: 61.175038 time elapsed: 58.9717 learning rate: 0.003405, scenario: 0, slope: -0.19017958298610477, fluctuations: 0.0\n",
      "step: 47180 loss: 60.260843 time elapsed: 58.9835 learning rate: 0.003405, scenario: 0, slope: -0.22010997881525807, fluctuations: 0.0\n",
      "step: 47190 loss: 59.543638 time elapsed: 58.9953 learning rate: 0.003405, scenario: 0, slope: -0.23440777964606418, fluctuations: 0.0\n",
      "step: 47200 loss: 58.924983 time elapsed: 59.0068 learning rate: 0.003405, scenario: 0, slope: -0.2328081870026046, fluctuations: 0.0\n",
      "step: 47210 loss: 58.373118 time elapsed: 59.0189 learning rate: 0.003405, scenario: 0, slope: -0.2121568251044612, fluctuations: 0.0\n",
      "step: 47220 loss: 57.853598 time elapsed: 59.0305 learning rate: 0.003405, scenario: 0, slope: -0.1776800263439423, fluctuations: 0.0\n",
      "step: 47230 loss: 57.354734 time elapsed: 59.0429 learning rate: 0.003405, scenario: 0, slope: -0.13454448355980897, fluctuations: 0.0\n",
      "step: 47240 loss: 56.868399 time elapsed: 59.0560 learning rate: 0.003405, scenario: 0, slope: -0.09505601114896746, fluctuations: 0.0\n",
      "step: 47250 loss: 56.391243 time elapsed: 59.0697 learning rate: 0.003405, scenario: 0, slope: -0.07202679637305537, fluctuations: 0.0\n",
      "step: 47260 loss: 55.907019 time elapsed: 59.0836 learning rate: 0.004985, scenario: 1, slope: -0.059875274122365346, fluctuations: 0.0\n",
      "step: 47270 loss: 54.933681 time elapsed: 59.0969 learning rate: 0.012929, scenario: 1, slope: -0.05517225469548439, fluctuations: 0.0\n",
      "step: 47280 loss: 120984.301541 time elapsed: 59.1101 learning rate: 0.015939, scenario: -1, slope: 94.9698794994571, fluctuations: 0.01\n",
      "step: 47290 loss: 27263.418117 time elapsed: 59.1249 learning rate: 0.005558, scenario: -1, slope: 231.01067927106445, fluctuations: 0.03\n",
      "step: 47300 loss: 10589.674396 time elapsed: 59.1372 learning rate: 0.002153, scenario: -1, slope: 249.85788960013187, fluctuations: 0.05\n",
      "step: 47310 loss: 7877.716603 time elapsed: 59.1502 learning rate: 0.000751, scenario: -1, slope: 227.61616364235365, fluctuations: 0.06\n",
      "step: 47320 loss: 7260.635418 time elapsed: 59.1620 learning rate: 0.000262, scenario: -1, slope: 183.83679134432953, fluctuations: 0.06\n",
      "step: 47330 loss: 6954.561261 time elapsed: 59.1745 learning rate: 0.000091, scenario: -1, slope: 132.43752197892906, fluctuations: 0.06\n",
      "step: 47340 loss: 6879.786829 time elapsed: 59.1867 learning rate: 0.000032, scenario: -1, slope: 74.07967305711007, fluctuations: 0.06\n",
      "step: 47350 loss: 6855.676143 time elapsed: 59.1983 learning rate: 0.000011, scenario: -1, slope: 6.9044388706450155, fluctuations: 0.06\n",
      "step: 47360 loss: 6844.527497 time elapsed: 59.2105 learning rate: 0.000011, scenario: 0, slope: -72.3886090775748, fluctuations: 0.06\n",
      "step: 47370 loss: 6833.829496 time elapsed: 59.2226 learning rate: 0.000011, scenario: 0, slope: -168.7479010868401, fluctuations: 0.06\n",
      "step: 47380 loss: 6823.365340 time elapsed: 59.2343 learning rate: 0.000011, scenario: 0, slope: -159.97878395228975, fluctuations: 0.05\n",
      "step: 47390 loss: 6813.076610 time elapsed: 59.2466 learning rate: 0.000011, scenario: 0, slope: -53.04668301572262, fluctuations: 0.02\n",
      "step: 47400 loss: 6802.926628 time elapsed: 59.2594 learning rate: 0.000011, scenario: 0, slope: -18.18870544204729, fluctuations: 0.01\n",
      "step: 47410 loss: 6792.252042 time elapsed: 59.2735 learning rate: 0.000018, scenario: 1, slope: -5.522370731118867, fluctuations: 0.0\n",
      "step: 47420 loss: 6769.184972 time elapsed: 59.2872 learning rate: 0.000046, scenario: 1, slope: -2.2065686924073717, fluctuations: 0.0\n",
      "step: 47430 loss: 6711.062510 time elapsed: 59.3007 learning rate: 0.000120, scenario: 1, slope: -1.4819968186712356, fluctuations: 0.0\n",
      "step: 47440 loss: 6569.119438 time elapsed: 59.3141 learning rate: 0.000312, scenario: 1, slope: -1.9129120849066312, fluctuations: 0.0\n",
      "step: 47450 loss: 6238.849602 time elapsed: 59.3260 learning rate: 0.000809, scenario: 1, slope: -3.5910164721052746, fluctuations: 0.0\n",
      "step: 47460 loss: 5514.111224 time elapsed: 59.3376 learning rate: 0.001734, scenario: 0, slope: -7.695182660185077, fluctuations: 0.0\n",
      "step: 47470 loss: 4645.808222 time elapsed: 59.3497 learning rate: 0.001734, scenario: 0, slope: -15.407777206964138, fluctuations: 0.0\n",
      "step: 47480 loss: 3989.670587 time elapsed: 59.3616 learning rate: 0.001734, scenario: 0, slope: -25.026369571741807, fluctuations: 0.0\n",
      "step: 47490 loss: 3492.569249 time elapsed: 59.3734 learning rate: 0.001734, scenario: 0, slope: -34.6743022079871, fluctuations: 0.0\n",
      "step: 47500 loss: 3102.161623 time elapsed: 59.3851 learning rate: 0.001734, scenario: 0, slope: -42.24505012082561, fluctuations: 0.0\n",
      "step: 47510 loss: 2784.637006 time elapsed: 59.3973 learning rate: 0.001734, scenario: 0, slope: -49.016021641851935, fluctuations: 0.0\n",
      "step: 47520 loss: 2545.597187 time elapsed: 59.4089 learning rate: 0.001734, scenario: 0, slope: -51.95781140159481, fluctuations: 0.0\n",
      "step: 47530 loss: 2359.641416 time elapsed: 59.4208 learning rate: 0.001734, scenario: 0, slope: -51.30982031364104, fluctuations: 0.0\n",
      "step: 47540 loss: 2190.864990 time elapsed: 59.4323 learning rate: 0.001734, scenario: 0, slope: -47.073986562726034, fluctuations: 0.0\n",
      "step: 47550 loss: 2013.841875 time elapsed: 59.4441 learning rate: 0.001734, scenario: 0, slope: -40.07596731776958, fluctuations: 0.0\n",
      "step: 47560 loss: 1874.199411 time elapsed: 59.4575 learning rate: 0.001734, scenario: 0, slope: -32.13970331854555, fluctuations: 0.0\n",
      "step: 47570 loss: 1729.272433 time elapsed: 59.4708 learning rate: 0.001734, scenario: 0, slope: -25.908542228795735, fluctuations: 0.0\n",
      "step: 47580 loss: 1580.020248 time elapsed: 59.4839 learning rate: 0.001734, scenario: 0, slope: -21.6219837578476, fluctuations: 0.0\n",
      "step: 47590 loss: 1455.634103 time elapsed: 59.4972 learning rate: 0.001734, scenario: 0, slope: -18.64572441157001, fluctuations: 0.0\n",
      "step: 47600 loss: 1374.006419 time elapsed: 59.5101 learning rate: 0.001734, scenario: 0, slope: -16.640993983240822, fluctuations: 0.0\n",
      "step: 47610 loss: 1312.777131 time elapsed: 59.5239 learning rate: 0.001734, scenario: 0, slope: -14.703642775715998, fluctuations: 0.0\n",
      "step: 47620 loss: 1259.756608 time elapsed: 59.5361 learning rate: 0.001734, scenario: 0, slope: -13.176572293426306, fluctuations: 0.0\n",
      "step: 47630 loss: 1213.501712 time elapsed: 59.5484 learning rate: 0.001734, scenario: 0, slope: -11.647822766614293, fluctuations: 0.0\n",
      "step: 47640 loss: 1172.199947 time elapsed: 59.5604 learning rate: 0.001734, scenario: 0, slope: -10.048568511170313, fluctuations: 0.0\n",
      "step: 47650 loss: 1135.051929 time elapsed: 59.5724 learning rate: 0.001734, scenario: 0, slope: -8.546523673510327, fluctuations: 0.0\n",
      "step: 47660 loss: 1101.724135 time elapsed: 59.5841 learning rate: 0.001734, scenario: 0, slope: -7.127396288229037, fluctuations: 0.0\n",
      "step: 47670 loss: 1071.653659 time elapsed: 59.5959 learning rate: 0.001734, scenario: 0, slope: -5.789326387283541, fluctuations: 0.0\n",
      "step: 47680 loss: 1044.240372 time elapsed: 59.6079 learning rate: 0.001734, scenario: 0, slope: -4.782988130218682, fluctuations: 0.0\n",
      "step: 47690 loss: 1019.000322 time elapsed: 59.6198 learning rate: 0.001734, scenario: 0, slope: -4.1018943622849715, fluctuations: 0.0\n",
      "step: 47700 loss: 995.557435 time elapsed: 59.6315 learning rate: 0.001734, scenario: 0, slope: -3.684897103300514, fluctuations: 0.0\n",
      "step: 47710 loss: 973.620037 time elapsed: 59.6434 learning rate: 0.001734, scenario: 0, slope: -3.2791890005488056, fluctuations: 0.0\n",
      "step: 47720 loss: 952.960674 time elapsed: 59.6553 learning rate: 0.001734, scenario: 0, slope: -2.9760628249832988, fluctuations: 0.0\n",
      "step: 47730 loss: 933.390261 time elapsed: 59.6675 learning rate: 0.001734, scenario: 0, slope: -2.720753888413499, fluctuations: 0.0\n",
      "step: 47740 loss: 914.741112 time elapsed: 59.6812 learning rate: 0.001734, scenario: 0, slope: -2.5052278518199738, fluctuations: 0.0\n",
      "step: 47750 loss: 896.858257 time elapsed: 59.6946 learning rate: 0.001734, scenario: 0, slope: -2.3249519591410728, fluctuations: 0.0\n",
      "step: 47760 loss: 879.593912 time elapsed: 59.7081 learning rate: 0.001734, scenario: 0, slope: -2.174803044659355, fluctuations: 0.0\n",
      "step: 47770 loss: 862.802937 time elapsed: 59.7211 learning rate: 0.001734, scenario: 0, slope: -2.0495559163569075, fluctuations: 0.0\n",
      "step: 47780 loss: 846.338862 time elapsed: 59.7342 learning rate: 0.001734, scenario: 0, slope: -1.945151318273964, fluctuations: 0.0\n",
      "step: 47790 loss: 830.051240 time elapsed: 59.7458 learning rate: 0.001734, scenario: 0, slope: -1.858884900999748, fluctuations: 0.0\n",
      "step: 47800 loss: 813.785823 time elapsed: 59.7577 learning rate: 0.001734, scenario: 0, slope: -1.7953707484366, fluctuations: 0.0\n",
      "step: 47810 loss: 797.389889 time elapsed: 59.7700 learning rate: 0.001734, scenario: 0, slope: -1.7349275859183761, fluctuations: 0.0\n",
      "step: 47820 loss: 780.725102 time elapsed: 59.7815 learning rate: 0.001734, scenario: 0, slope: -1.6959103200783343, fluctuations: 0.0\n",
      "step: 47830 loss: 763.687730 time elapsed: 59.7936 learning rate: 0.001734, scenario: 0, slope: -1.6717123045872122, fluctuations: 0.0\n",
      "step: 47840 loss: 746.228475 time elapsed: 59.8054 learning rate: 0.001734, scenario: 0, slope: -1.661756637616328, fluctuations: 0.0\n",
      "step: 47850 loss: 728.354289 time elapsed: 59.8168 learning rate: 0.001734, scenario: 0, slope: -1.6649320250430677, fluctuations: 0.0\n",
      "step: 47860 loss: 710.097092 time elapsed: 59.8283 learning rate: 0.001734, scenario: 0, slope: -1.6794782405953095, fluctuations: 0.0\n",
      "step: 47870 loss: 691.462741 time elapsed: 59.8396 learning rate: 0.001734, scenario: 0, slope: -1.7031867360439905, fluctuations: 0.0\n",
      "step: 47880 loss: 672.401437 time elapsed: 59.8510 learning rate: 0.001734, scenario: 0, slope: -1.73384812608017, fluctuations: 0.0\n",
      "step: 47890 loss: 652.819047 time elapsed: 59.8628 learning rate: 0.001734, scenario: 0, slope: -1.769698152879966, fluctuations: 0.0\n",
      "step: 47900 loss: 632.613854 time elapsed: 59.8747 learning rate: 0.001734, scenario: 0, slope: -1.8055022941657557, fluctuations: 0.0\n",
      "step: 47910 loss: 611.757495 time elapsed: 59.8886 learning rate: 0.001734, scenario: 0, slope: -1.8532817926963459, fluctuations: 0.0\n",
      "step: 47920 loss: 590.452289 time elapsed: 59.9017 learning rate: 0.001734, scenario: 0, slope: -1.8998461437971068, fluctuations: 0.0\n",
      "step: 47930 loss: 569.266354 time elapsed: 59.9153 learning rate: 0.001734, scenario: 0, slope: -1.9468513578347506, fluctuations: 0.0\n",
      "step: 47940 loss: 549.017355 time elapsed: 59.9285 learning rate: 0.001734, scenario: 0, slope: -1.988516761263301, fluctuations: 0.0\n",
      "step: 47950 loss: 530.356304 time elapsed: 59.9422 learning rate: 0.001734, scenario: 0, slope: -2.016228646842805, fluctuations: 0.0\n",
      "step: 47960 loss: 513.433265 time elapsed: 59.9544 learning rate: 0.001734, scenario: 0, slope: -2.021277004091576, fluctuations: 0.0\n",
      "step: 47970 loss: 497.998866 time elapsed: 59.9664 learning rate: 0.001734, scenario: 0, slope: -1.9978882932857516, fluctuations: 0.0\n",
      "step: 47980 loss: 483.731728 time elapsed: 59.9783 learning rate: 0.001734, scenario: 0, slope: -1.944536613111959, fluctuations: 0.0\n",
      "step: 47990 loss: 470.424699 time elapsed: 59.9900 learning rate: 0.001734, scenario: 0, slope: -1.8636611903843576, fluctuations: 0.0\n",
      "step: 48000 loss: 457.986696 time elapsed: 60.0012 learning rate: 0.001734, scenario: 0, slope: -1.7719764146348136, fluctuations: 0.0\n",
      "step: 48010 loss: 446.387219 time elapsed: 60.0135 learning rate: 0.001734, scenario: 0, slope: -1.6445870501492303, fluctuations: 0.0\n",
      "step: 48020 loss: 435.618481 time elapsed: 60.0252 learning rate: 0.001734, scenario: 0, slope: -1.524177022280943, fluctuations: 0.0\n",
      "step: 48030 loss: 425.678670 time elapsed: 60.0368 learning rate: 0.001734, scenario: 0, slope: -1.4083819246754683, fluctuations: 0.0\n",
      "step: 48040 loss: 416.561715 time elapsed: 60.0485 learning rate: 0.001734, scenario: 0, slope: -1.3022950636788302, fluctuations: 0.0\n",
      "step: 48050 loss: 408.246128 time elapsed: 60.0601 learning rate: 0.001734, scenario: 0, slope: -1.2063313919845633, fluctuations: 0.0\n",
      "step: 48060 loss: 400.684206 time elapsed: 60.0716 learning rate: 0.001734, scenario: 0, slope: -1.1178780369548604, fluctuations: 0.0\n",
      "step: 48070 loss: 393.797931 time elapsed: 60.0838 learning rate: 0.001734, scenario: 0, slope: -1.0341326242872417, fluctuations: 0.0\n",
      "step: 48080 loss: 387.486271 time elapsed: 60.0973 learning rate: 0.001734, scenario: 0, slope: -0.9537662290896697, fluctuations: 0.0\n",
      "step: 48090 loss: 381.640982 time elapsed: 60.1105 learning rate: 0.001734, scenario: 0, slope: -0.8769618823366852, fluctuations: 0.0\n",
      "step: 48100 loss: 376.162449 time elapsed: 60.1238 learning rate: 0.001734, scenario: 0, slope: -0.811739780378461, fluctuations: 0.0\n",
      "step: 48110 loss: 370.968964 time elapsed: 60.1379 learning rate: 0.001734, scenario: 0, slope: -0.73847203674296, fluctuations: 0.0\n",
      "step: 48120 loss: 365.998622 time elapsed: 60.1519 learning rate: 0.001734, scenario: 0, slope: -0.6792352778305597, fluctuations: 0.0\n",
      "step: 48130 loss: 361.206693 time elapsed: 60.1646 learning rate: 0.001734, scenario: 0, slope: -0.6278083029044138, fluctuations: 0.0\n",
      "step: 48140 loss: 356.561525 time elapsed: 60.1769 learning rate: 0.001734, scenario: 0, slope: -0.5844072481963397, fluctuations: 0.0\n",
      "step: 48150 loss: 352.040739 time elapsed: 60.1888 learning rate: 0.001734, scenario: 0, slope: -0.5486776061870257, fluctuations: 0.0\n",
      "step: 48160 loss: 347.628300 time elapsed: 60.2004 learning rate: 0.001734, scenario: 0, slope: -0.5197799070877004, fluctuations: 0.0\n",
      "step: 48170 loss: 343.312484 time elapsed: 60.2118 learning rate: 0.001734, scenario: 0, slope: -0.4965748907096326, fluctuations: 0.0\n",
      "step: 48180 loss: 339.084521 time elapsed: 60.2241 learning rate: 0.001734, scenario: 0, slope: -0.47784644740148857, fluctuations: 0.0\n",
      "step: 48190 loss: 334.937703 time elapsed: 60.2362 learning rate: 0.001734, scenario: 0, slope: -0.46248532316523777, fluctuations: 0.0\n",
      "step: 48200 loss: 330.866801 time elapsed: 60.2479 learning rate: 0.001734, scenario: 0, slope: -0.45078779441045685, fluctuations: 0.0\n",
      "step: 48210 loss: 326.867657 time elapsed: 60.2605 learning rate: 0.001734, scenario: 0, slope: -0.4384737432750185, fluctuations: 0.0\n",
      "step: 48220 loss: 322.936900 time elapsed: 60.2724 learning rate: 0.001734, scenario: 0, slope: -0.42865542047278593, fluctuations: 0.0\n",
      "step: 48230 loss: 319.071721 time elapsed: 60.2842 learning rate: 0.001734, scenario: 0, slope: -0.41979212028065577, fluctuations: 0.0\n",
      "step: 48240 loss: 315.269698 time elapsed: 60.2968 learning rate: 0.001734, scenario: 0, slope: -0.41164715183502176, fluctuations: 0.0\n",
      "step: 48250 loss: 311.528652 time elapsed: 60.3103 learning rate: 0.001734, scenario: 0, slope: -0.4040553497250888, fluctuations: 0.0\n",
      "step: 48260 loss: 307.846520 time elapsed: 60.3239 learning rate: 0.001734, scenario: 0, slope: -0.3969006118061449, fluctuations: 0.0\n",
      "step: 48270 loss: 304.221255 time elapsed: 60.3369 learning rate: 0.001734, scenario: 0, slope: -0.39010122833235966, fluctuations: 0.0\n",
      "step: 48280 loss: 300.650736 time elapsed: 60.3505 learning rate: 0.001734, scenario: 0, slope: -0.38360051234778536, fluctuations: 0.0\n",
      "step: 48290 loss: 297.132698 time elapsed: 60.3632 learning rate: 0.001734, scenario: 0, slope: -0.3773607833406499, fluctuations: 0.0\n",
      "step: 48300 loss: 293.664668 time elapsed: 60.3755 learning rate: 0.001734, scenario: 0, slope: -0.37194918128044846, fluctuations: 0.0\n",
      "step: 48310 loss: 290.243905 time elapsed: 60.3879 learning rate: 0.001734, scenario: 0, slope: -0.36558592339539703, fluctuations: 0.0\n",
      "step: 48320 loss: 286.867324 time elapsed: 60.3999 learning rate: 0.001734, scenario: 0, slope: -0.3600403233446613, fluctuations: 0.0\n",
      "step: 48330 loss: 283.531406 time elapsed: 60.4117 learning rate: 0.001734, scenario: 0, slope: -0.35473150835496053, fluctuations: 0.0\n",
      "step: 48340 loss: 280.232043 time elapsed: 60.4232 learning rate: 0.001734, scenario: 0, slope: -0.3496768003204798, fluctuations: 0.0\n",
      "step: 48350 loss: 276.964294 time elapsed: 60.4348 learning rate: 0.001734, scenario: 0, slope: -0.3449022644828945, fluctuations: 0.0\n",
      "step: 48360 loss: 273.721974 time elapsed: 60.4466 learning rate: 0.001734, scenario: 0, slope: -0.34044450831804424, fluctuations: 0.0\n",
      "step: 48370 loss: 270.496930 time elapsed: 60.4584 learning rate: 0.001734, scenario: 0, slope: -0.33635487018122995, fluctuations: 0.0\n",
      "step: 48380 loss: 267.277704 time elapsed: 60.4704 learning rate: 0.001734, scenario: 0, slope: -0.33270783087370553, fluctuations: 0.0\n",
      "step: 48390 loss: 264.046972 time elapsed: 60.4821 learning rate: 0.001734, scenario: 0, slope: -0.3296174163373498, fluctuations: 0.0\n",
      "step: 48400 loss: 260.776598 time elapsed: 60.4936 learning rate: 0.001734, scenario: 0, slope: -0.32746302317918435, fluctuations: 0.0\n",
      "step: 48410 loss: 257.419940 time elapsed: 60.5063 learning rate: 0.001734, scenario: 0, slope: -0.3259792024692856, fluctuations: 0.0\n",
      "step: 48420 loss: 253.921598 time elapsed: 60.5199 learning rate: 0.001734, scenario: 0, slope: -0.3262463242445449, fluctuations: 0.0\n",
      "step: 48430 loss: 250.404512 time elapsed: 60.5334 learning rate: 0.001734, scenario: 0, slope: -0.3283152498623758, fluctuations: 0.0\n",
      "step: 48440 loss: 247.295152 time elapsed: 60.5471 learning rate: 0.001734, scenario: 0, slope: -0.3303915313174427, fluctuations: 0.0\n",
      "step: 48450 loss: 244.295415 time elapsed: 60.5606 learning rate: 0.001734, scenario: 0, slope: -0.33081467309167334, fluctuations: 0.0\n",
      "step: 48460 loss: 241.360209 time elapsed: 60.5758 learning rate: 0.001734, scenario: 0, slope: -0.32933548288885495, fluctuations: 0.0\n",
      "step: 48470 loss: 238.536443 time elapsed: 60.5881 learning rate: 0.001734, scenario: 0, slope: -0.3257626613506637, fluctuations: 0.0\n",
      "step: 48480 loss: 235.805754 time elapsed: 60.6004 learning rate: 0.001734, scenario: 0, slope: -0.31974043545524117, fluctuations: 0.0\n",
      "step: 48490 loss: 233.114368 time elapsed: 60.6122 learning rate: 0.001734, scenario: 0, slope: -0.3116190531437814, fluctuations: 0.0\n",
      "step: 48500 loss: 230.466134 time elapsed: 60.6237 learning rate: 0.001734, scenario: 0, slope: -0.3029299838713102, fluctuations: 0.0\n",
      "step: 48510 loss: 227.854699 time elapsed: 60.6366 learning rate: 0.001734, scenario: 0, slope: -0.2915288745152513, fluctuations: 0.0\n",
      "step: 48520 loss: 225.275249 time elapsed: 60.6484 learning rate: 0.001734, scenario: 0, slope: -0.28188761467143497, fluctuations: 0.0\n",
      "step: 48530 loss: 222.724496 time elapsed: 60.6605 learning rate: 0.001734, scenario: 0, slope: -0.2745518887831241, fluctuations: 0.0\n",
      "step: 48540 loss: 220.198929 time elapsed: 60.6726 learning rate: 0.001734, scenario: 0, slope: -0.2690375006969864, fluctuations: 0.0\n",
      "step: 48550 loss: 217.695250 time elapsed: 60.6850 learning rate: 0.001734, scenario: 0, slope: -0.2642854698072969, fluctuations: 0.0\n",
      "step: 48560 loss: 215.210766 time elapsed: 60.6965 learning rate: 0.001734, scenario: 0, slope: -0.2603535047855977, fluctuations: 0.0\n",
      "step: 48570 loss: 212.743711 time elapsed: 60.7082 learning rate: 0.001734, scenario: 0, slope: -0.25722435400943416, fluctuations: 0.0\n",
      "step: 48580 loss: 210.294085 time elapsed: 60.7214 learning rate: 0.001734, scenario: 0, slope: -0.25444180427109, fluctuations: 0.0\n",
      "step: 48590 loss: 207.864802 time elapsed: 60.7341 learning rate: 0.001734, scenario: 0, slope: -0.2519802826074575, fluctuations: 0.0\n",
      "step: 48600 loss: 205.463024 time elapsed: 60.7468 learning rate: 0.001734, scenario: 0, slope: -0.24993565555368116, fluctuations: 0.0\n",
      "step: 48610 loss: 203.100560 time elapsed: 60.7607 learning rate: 0.001734, scenario: 0, slope: -0.2475161996493861, fluctuations: 0.0\n",
      "step: 48620 loss: 200.790163 time elapsed: 60.7750 learning rate: 0.001734, scenario: 0, slope: -0.2451993019257601, fluctuations: 0.0\n",
      "step: 48630 loss: 198.535659 time elapsed: 60.7876 learning rate: 0.001734, scenario: 0, slope: -0.2425760976197303, fluctuations: 0.0\n",
      "step: 48640 loss: 196.326443 time elapsed: 60.7998 learning rate: 0.001734, scenario: 0, slope: -0.23953665252929657, fluctuations: 0.0\n",
      "step: 48650 loss: 194.149866 time elapsed: 60.8119 learning rate: 0.001734, scenario: 0, slope: -0.2360936289504582, fluctuations: 0.0\n",
      "step: 48660 loss: 192.002015 time elapsed: 60.8233 learning rate: 0.001734, scenario: 0, slope: -0.23233898670128705, fluctuations: 0.0\n",
      "step: 48670 loss: 189.883328 time elapsed: 60.8352 learning rate: 0.001734, scenario: 0, slope: -0.22839567227419832, fluctuations: 0.0\n",
      "step: 48680 loss: 187.794333 time elapsed: 60.8470 learning rate: 0.001734, scenario: 0, slope: -0.22440215587793355, fluctuations: 0.0\n",
      "step: 48690 loss: 185.735319 time elapsed: 60.8588 learning rate: 0.001734, scenario: 0, slope: -0.22050007026305968, fluctuations: 0.0\n",
      "step: 48700 loss: 183.706595 time elapsed: 60.8703 learning rate: 0.001734, scenario: 0, slope: -0.2171664491641699, fluctuations: 0.0\n",
      "step: 48710 loss: 181.708459 time elapsed: 60.8828 learning rate: 0.001734, scenario: 0, slope: -0.21339125813378734, fluctuations: 0.0\n",
      "step: 48720 loss: 179.741080 time elapsed: 60.8946 learning rate: 0.001734, scenario: 0, slope: -0.21021731363214, fluctuations: 0.0\n",
      "step: 48730 loss: 177.804442 time elapsed: 60.9060 learning rate: 0.001734, scenario: 0, slope: -0.20718037745632029, fluctuations: 0.0\n",
      "step: 48740 loss: 175.898255 time elapsed: 60.9189 learning rate: 0.001734, scenario: 0, slope: -0.20417221684179437, fluctuations: 0.0\n",
      "step: 48750 loss: 174.033499 time elapsed: 60.9328 learning rate: 0.001734, scenario: 0, slope: -0.20114068864309184, fluctuations: 0.0\n",
      "step: 48760 loss: 172.343345 time elapsed: 60.9462 learning rate: 0.001734, scenario: 0, slope: -0.1968455790157863, fluctuations: 0.02\n",
      "step: 48770 loss: 170.436549 time elapsed: 60.9596 learning rate: 0.001734, scenario: 0, slope: -0.19340258995028872, fluctuations: 0.03\n",
      "step: 48780 loss: 167.740787 time elapsed: 60.9733 learning rate: 0.001734, scenario: 0, slope: -0.1945699333779492, fluctuations: 0.04\n",
      "step: 48790 loss: 165.807912 time elapsed: 60.9861 learning rate: 0.001734, scenario: 0, slope: -0.19671382355127456, fluctuations: 0.04\n",
      "step: 48800 loss: 164.024087 time elapsed: 60.9982 learning rate: 0.001734, scenario: 0, slope: -0.19774629790546255, fluctuations: 0.04\n",
      "step: 48810 loss: 162.315907 time elapsed: 61.0106 learning rate: 0.001734, scenario: 0, slope: -0.19765344010262104, fluctuations: 0.04\n",
      "step: 48820 loss: 160.653946 time elapsed: 61.0222 learning rate: 0.001734, scenario: 0, slope: -0.1962557080010823, fluctuations: 0.04\n",
      "step: 48830 loss: 159.022621 time elapsed: 61.0338 learning rate: 0.001734, scenario: 0, slope: -0.19356827163203835, fluctuations: 0.04\n",
      "step: 48840 loss: 157.419809 time elapsed: 61.0453 learning rate: 0.001734, scenario: 0, slope: -0.18950250331453797, fluctuations: 0.04\n",
      "step: 48850 loss: 155.843862 time elapsed: 61.0573 learning rate: 0.001734, scenario: 0, slope: -0.1838818138630232, fluctuations: 0.04\n",
      "step: 48860 loss: 154.292868 time elapsed: 61.0690 learning rate: 0.001734, scenario: 0, slope: -0.17572715128511718, fluctuations: 0.02\n",
      "step: 48870 loss: 152.668383 time elapsed: 61.0804 learning rate: 0.002792, scenario: 1, slope: -0.16725221981743996, fluctuations: 0.01\n",
      "step: 48880 loss: 171.837926 time elapsed: 61.0923 learning rate: 0.002816, scenario: -1, slope: 0.07745580597785412, fluctuations: 0.02\n",
      "step: 48890 loss: 164.052699 time elapsed: 61.1043 learning rate: 0.000982, scenario: -1, slope: 0.15782540924739097, fluctuations: 0.05\n",
      "step: 48900 loss: 150.439455 time elapsed: 61.1160 learning rate: 0.000380, scenario: -1, slope: 0.13185167490326202, fluctuations: 0.07\n",
      "step: 48910 loss: 149.350037 time elapsed: 61.1285 learning rate: 0.000133, scenario: -1, slope: 0.0483409966118476, fluctuations: 0.08\n",
      "step: 48920 loss: 149.135437 time elapsed: 61.1422 learning rate: 0.000099, scenario: 1, slope: -0.02369691084627398, fluctuations: 0.09\n",
      "step: 48930 loss: 148.905749 time elapsed: 61.1555 learning rate: 0.000257, scenario: 1, slope: -0.09260673032504328, fluctuations: 0.1\n",
      "step: 48940 loss: 148.114859 time elapsed: 61.1684 learning rate: 0.000606, scenario: 0, slope: -0.16234434514833324, fluctuations: 0.11\n",
      "step: 48950 loss: 147.337334 time elapsed: 61.1819 learning rate: 0.000606, scenario: 0, slope: -0.23861309850495527, fluctuations: 0.11\n",
      "step: 48960 loss: 146.635460 time elapsed: 61.1960 learning rate: 0.000606, scenario: 0, slope: -0.3253414857165149, fluctuations: 0.11\n",
      "step: 48970 loss: 146.005601 time elapsed: 61.2084 learning rate: 0.000606, scenario: 0, slope: -0.4328866167710858, fluctuations: 0.11\n",
      "step: 48980 loss: 145.388131 time elapsed: 61.2206 learning rate: 0.000806, scenario: 0, slope: -0.2886587116678792, fluctuations: 0.08\n",
      "step: 48990 loss: 144.362492 time elapsed: 61.2327 learning rate: 0.002091, scenario: 1, slope: -0.10872058371731518, fluctuations: 0.05\n",
      "step: 49000 loss: 141.909806 time elapsed: 61.2446 learning rate: 0.004931, scenario: 1, slope: -0.07423020863107802, fluctuations: 0.04\n",
      "step: 49010 loss: 770.747348 time elapsed: 61.2574 learning rate: 0.011099, scenario: -1, slope: 0.34340564962805614, fluctuations: 0.03\n",
      "step: 49020 loss: 962.185854 time elapsed: 61.2694 learning rate: 0.003870, scenario: -1, slope: 16.215732052449667, fluctuations: 0.06\n",
      "step: 49030 loss: 920.583644 time elapsed: 61.2814 learning rate: 0.001349, scenario: -1, slope: 18.768137134856985, fluctuations: 0.07\n",
      "step: 49040 loss: 563.294126 time elapsed: 61.2935 learning rate: 0.000470, scenario: -1, slope: 16.39219777910231, fluctuations: 0.08\n",
      "step: 49050 loss: 401.622949 time elapsed: 61.3050 learning rate: 0.000164, scenario: -1, slope: 11.675150560937658, fluctuations: 0.08\n",
      "step: 49060 loss: 379.967019 time elapsed: 61.3165 learning rate: 0.000057, scenario: -1, slope: 7.340103509367729, fluctuations: 0.09\n",
      "step: 49070 loss: 370.140929 time elapsed: 61.3280 learning rate: 0.000020, scenario: -1, slope: 2.5076332185098593, fluctuations: 0.09\n",
      "step: 49080 loss: 367.971452 time elapsed: 61.3403 learning rate: 0.000012, scenario: 0, slope: -2.511922576771705, fluctuations: 0.09\n",
      "step: 49090 loss: 366.675670 time elapsed: 61.3534 learning rate: 0.000012, scenario: 0, slope: -8.071203236171325, fluctuations: 0.09\n",
      "step: 49100 loss: 365.557389 time elapsed: 61.3671 learning rate: 0.000012, scenario: 0, slope: -13.930173733598991, fluctuations: 0.09\n",
      "step: 49110 loss: 364.536465 time elapsed: 61.3813 learning rate: 0.000012, scenario: 0, slope: -22.448861739985464, fluctuations: 0.09\n",
      "step: 49120 loss: 363.572106 time elapsed: 61.3948 learning rate: 0.000012, scenario: 0, slope: -6.044428543285682, fluctuations: 0.05\n",
      "step: 49130 loss: 362.642506 time elapsed: 61.4080 learning rate: 0.000012, scenario: 0, slope: -1.9682163477259094, fluctuations: 0.02\n",
      "step: 49140 loss: 361.735526 time elapsed: 61.4207 learning rate: 0.000013, scenario: 1, slope: -0.49671114141084094, fluctuations: 0.01\n",
      "step: 49150 loss: 360.319798 time elapsed: 61.4324 learning rate: 0.000034, scenario: 1, slope: -0.23807980253468608, fluctuations: 0.0\n",
      "step: 49160 loss: 356.756509 time elapsed: 61.4442 learning rate: 0.000087, scenario: 1, slope: -0.13496415378510396, fluctuations: 0.0\n",
      "step: 49170 loss: 348.011296 time elapsed: 61.4560 learning rate: 0.000226, scenario: 1, slope: -0.14638727852131636, fluctuations: 0.0\n",
      "step: 49180 loss: 327.819155 time elapsed: 61.4679 learning rate: 0.000586, scenario: 1, slope: -0.24244448595620416, fluctuations: 0.0\n",
      "step: 49190 loss: 290.260450 time elapsed: 61.4794 learning rate: 0.001039, scenario: 0, slope: -0.4753738580300713, fluctuations: 0.0\n",
      "step: 49200 loss: 260.355417 time elapsed: 61.4909 learning rate: 0.001039, scenario: 0, slope: -0.7982866303761016, fluctuations: 0.0\n",
      "step: 49210 loss: 241.157414 time elapsed: 61.5033 learning rate: 0.001039, scenario: 0, slope: -1.2186832098947409, fluctuations: 0.0\n",
      "step: 49220 loss: 227.026697 time elapsed: 61.5148 learning rate: 0.001039, scenario: 0, slope: -1.554488019222344, fluctuations: 0.0\n",
      "step: 49230 loss: 215.475378 time elapsed: 61.5264 learning rate: 0.001039, scenario: 0, slope: -1.8072510457130428, fluctuations: 0.0\n",
      "step: 49240 loss: 205.616804 time elapsed: 61.5380 learning rate: 0.001039, scenario: 0, slope: -1.9518836500404348, fluctuations: 0.0\n",
      "step: 49250 loss: 196.852805 time elapsed: 61.5506 learning rate: 0.001039, scenario: 0, slope: -1.9705815483509146, fluctuations: 0.0\n",
      "step: 49260 loss: 189.187943 time elapsed: 61.5644 learning rate: 0.001039, scenario: 0, slope: -1.8571238810323387, fluctuations: 0.0\n",
      "step: 49270 loss: 182.444590 time elapsed: 61.5788 learning rate: 0.001039, scenario: 0, slope: -1.6216351533737945, fluctuations: 0.0\n",
      "step: 49280 loss: 176.991402 time elapsed: 61.5927 learning rate: 0.001039, scenario: 0, slope: -1.3118757452446748, fluctuations: 0.0\n",
      "step: 49290 loss: 172.291457 time elapsed: 61.6060 learning rate: 0.001039, scenario: 0, slope: -1.0345148344034887, fluctuations: 0.0\n",
      "step: 49300 loss: 168.259628 time elapsed: 61.6193 learning rate: 0.001039, scenario: 0, slope: -0.8706691699907274, fluctuations: 0.0\n",
      "step: 49310 loss: 164.763395 time elapsed: 61.6319 learning rate: 0.001039, scenario: 0, slope: -0.7298139271021897, fluctuations: 0.0\n",
      "step: 49320 loss: 161.673572 time elapsed: 61.6440 learning rate: 0.001039, scenario: 0, slope: -0.6285338467962529, fluctuations: 0.0\n",
      "step: 49330 loss: 158.908066 time elapsed: 61.6555 learning rate: 0.001039, scenario: 0, slope: -0.5422421773324824, fluctuations: 0.0\n",
      "step: 49340 loss: 156.401773 time elapsed: 61.6670 learning rate: 0.001039, scenario: 0, slope: -0.46751283044524555, fluctuations: 0.0\n",
      "step: 49350 loss: 154.106941 time elapsed: 61.6788 learning rate: 0.001039, scenario: 0, slope: -0.4036674559913137, fluctuations: 0.0\n",
      "step: 49360 loss: 151.988697 time elapsed: 61.6906 learning rate: 0.001039, scenario: 0, slope: -0.3514332531270423, fluctuations: 0.0\n",
      "step: 49370 loss: 150.020528 time elapsed: 61.7023 learning rate: 0.001039, scenario: 0, slope: -0.30925542812511475, fluctuations: 0.0\n",
      "step: 49380 loss: 148.182167 time elapsed: 61.7137 learning rate: 0.001039, scenario: 0, slope: -0.27597102713444177, fluctuations: 0.0\n",
      "step: 49390 loss: 146.457620 time elapsed: 61.7259 learning rate: 0.001039, scenario: 0, slope: -0.24915989642269404, fluctuations: 0.0\n",
      "step: 49400 loss: 144.834005 time elapsed: 61.7382 learning rate: 0.001039, scenario: 0, slope: -0.22934575505178997, fluctuations: 0.0\n",
      "step: 49410 loss: 143.300653 time elapsed: 61.7513 learning rate: 0.001039, scenario: 0, slope: -0.20931085871498833, fluctuations: 0.0\n",
      "step: 49420 loss: 141.848468 time elapsed: 61.7644 learning rate: 0.001039, scenario: 0, slope: -0.19411105868459128, fluctuations: 0.0\n",
      "step: 49430 loss: 140.469447 time elapsed: 61.7784 learning rate: 0.001039, scenario: 0, slope: -0.18108432127481539, fluctuations: 0.0\n",
      "step: 49440 loss: 139.156320 time elapsed: 61.7922 learning rate: 0.001039, scenario: 0, slope: -0.16975220668185684, fluctuations: 0.0\n",
      "step: 49450 loss: 137.902215 time elapsed: 61.8057 learning rate: 0.001039, scenario: 0, slope: -0.15977784799925987, fluctuations: 0.0\n",
      "step: 49460 loss: 136.697363 time elapsed: 61.8190 learning rate: 0.001257, scenario: 1, slope: -0.1509287939222993, fluctuations: 0.0\n",
      "step: 49470 loss: 134.655511 time elapsed: 61.8320 learning rate: 0.003260, scenario: 1, slope: -0.14497609392113198, fluctuations: 0.0\n",
      "step: 49480 loss: 129.955687 time elapsed: 61.8445 learning rate: 0.006353, scenario: 0, slope: -0.15174805911956996, fluctuations: 0.0\n",
      "step: 49490 loss: 124.361005 time elapsed: 61.8567 learning rate: 0.006353, scenario: 0, slope: -0.17918468694346942, fluctuations: 0.01\n",
      "step: 49500 loss: 118.861879 time elapsed: 61.8682 learning rate: 0.006353, scenario: 0, slope: -0.21921065217199648, fluctuations: 0.02\n",
      "step: 49510 loss: 115.053853 time elapsed: 61.8803 learning rate: 0.006353, scenario: 0, slope: -0.276463560983004, fluctuations: 0.02\n",
      "step: 49520 loss: 111.670232 time elapsed: 61.8918 learning rate: 0.006353, scenario: 0, slope: -0.3236741929549074, fluctuations: 0.02\n",
      "step: 49530 loss: 108.695932 time elapsed: 61.9036 learning rate: 0.006353, scenario: 0, slope: -0.3611898844784532, fluctuations: 0.02\n",
      "step: 49540 loss: 105.917207 time elapsed: 61.9154 learning rate: 0.006353, scenario: 0, slope: -0.3862463999748438, fluctuations: 0.03\n",
      "step: 49550 loss: 103.442192 time elapsed: 61.9271 learning rate: 0.006353, scenario: 0, slope: -0.39338900917013336, fluctuations: 0.04\n",
      "step: 49560 loss: 102.948907 time elapsed: 61.9387 learning rate: 0.006353, scenario: 0, slope: -0.3796309126703003, fluctuations: 0.04\n",
      "step: 49570 loss: 98.991387 time elapsed: 61.9506 learning rate: 0.006353, scenario: 0, slope: -0.35258465109346737, fluctuations: 0.08\n",
      "step: 49580 loss: 96.451453 time elapsed: 61.9626 learning rate: 0.006353, scenario: 0, slope: -0.3106472057600592, fluctuations: 0.11\n",
      "step: 49590 loss: 95.889661 time elapsed: 61.9754 learning rate: 0.006353, scenario: 0, slope: -0.2787010252764802, fluctuations: 0.12\n",
      "step: 49600 loss: 93.815772 time elapsed: 61.9888 learning rate: 0.006353, scenario: 0, slope: -0.26435179935257164, fluctuations: 0.15\n",
      "step: 49610 loss: 94.243587 time elapsed: 62.0035 learning rate: 0.006353, scenario: 0, slope: -0.2152556805021456, fluctuations: 0.19\n",
      "step: 49620 loss: 90.313106 time elapsed: 62.0172 learning rate: 0.006353, scenario: 0, slope: -0.19547310626743541, fluctuations: 0.22\n",
      "step: 49630 loss: 87.292860 time elapsed: 62.0305 learning rate: 0.006353, scenario: 0, slope: -0.18851027323474992, fluctuations: 0.24\n",
      "step: 49640 loss: 85.376091 time elapsed: 62.0436 learning rate: 0.006353, scenario: 0, slope: -0.18764636394589085, fluctuations: 0.26\n",
      "step: 49650 loss: 85.728154 time elapsed: 62.0559 learning rate: 0.006353, scenario: 0, slope: -0.1863223062515773, fluctuations: 0.27\n",
      "step: 49660 loss: 82.808001 time elapsed: 62.0674 learning rate: 0.006353, scenario: 0, slope: -0.18348157344086347, fluctuations: 0.3\n",
      "step: 49670 loss: 79.319768 time elapsed: 62.0792 learning rate: 0.006353, scenario: 0, slope: -0.18846808203311965, fluctuations: 0.3\n",
      "step: 49680 loss: 77.892787 time elapsed: 62.0909 learning rate: 0.006353, scenario: 0, slope: -0.2037155364827824, fluctuations: 0.28\n",
      "step: 49690 loss: 74.482934 time elapsed: 62.1028 learning rate: 0.006353, scenario: 0, slope: -0.19977155962808354, fluctuations: 0.28\n",
      "step: 49700 loss: 75.091602 time elapsed: 62.1147 learning rate: 0.006353, scenario: 0, slope: -0.2225913414023378, fluctuations: 0.29\n",
      "step: 49710 loss: 68.531445 time elapsed: 62.1270 learning rate: 0.006353, scenario: 0, slope: -0.22013712560529586, fluctuations: 0.27\n",
      "step: 49720 loss: 65.127278 time elapsed: 62.1387 learning rate: 0.006353, scenario: 0, slope: -0.23340772452608385, fluctuations: 0.26\n",
      "step: 49730 loss: 64.616357 time elapsed: 62.1505 learning rate: 0.006353, scenario: 0, slope: -0.2466284761563459, fluctuations: 0.25\n",
      "step: 49740 loss: 63.503049 time elapsed: 62.1621 learning rate: 0.006353, scenario: 0, slope: -0.25702697345106423, fluctuations: 0.24\n",
      "step: 49750 loss: 62.025629 time elapsed: 62.1740 learning rate: 0.006353, scenario: 0, slope: -0.25977296954350676, fluctuations: 0.26\n",
      "step: 49760 loss: 59.919677 time elapsed: 62.1858 learning rate: 0.006353, scenario: 0, slope: -0.24690616967032183, fluctuations: 0.27\n",
      "step: 49770 loss: 59.146382 time elapsed: 62.1995 learning rate: 0.006353, scenario: 0, slope: -0.2309990078944793, fluctuations: 0.28\n",
      "step: 49780 loss: 57.913386 time elapsed: 62.2132 learning rate: 0.006353, scenario: 0, slope: -0.20732057544321855, fluctuations: 0.26\n",
      "step: 49790 loss: 57.163873 time elapsed: 62.2266 learning rate: 0.006353, scenario: 0, slope: -0.170261337806487, fluctuations: 0.22\n",
      "step: 49800 loss: 57.719893 time elapsed: 62.2398 learning rate: 0.006353, scenario: 0, slope: -0.13207959650433834, fluctuations: 0.2\n",
      "step: 49810 loss: 56.660483 time elapsed: 62.2537 learning rate: 0.006353, scenario: 0, slope: -0.10738924799660711, fluctuations: 0.19\n",
      "step: 49820 loss: 54.840613 time elapsed: 62.2660 learning rate: 0.006353, scenario: 0, slope: -0.0961974870875488, fluctuations: 0.2\n",
      "step: 49830 loss: 54.137138 time elapsed: 62.2785 learning rate: 0.006353, scenario: 0, slope: -0.0890970636285814, fluctuations: 0.19\n",
      "step: 49840 loss: 53.609803 time elapsed: 62.2902 learning rate: 0.006353, scenario: 0, slope: -0.08716652978354526, fluctuations: 0.17\n",
      "step: 49850 loss: 52.960525 time elapsed: 62.3021 learning rate: 0.006353, scenario: 0, slope: -0.0807039717292448, fluctuations: 0.14\n",
      "step: 49860 loss: 55.407559 time elapsed: 62.3136 learning rate: 0.006353, scenario: 0, slope: -0.07040796042749699, fluctuations: 0.1\n",
      "step: 49870 loss: 51.758530 time elapsed: 62.3256 learning rate: 0.006353, scenario: 0, slope: -0.06784851442957378, fluctuations: 0.08\n",
      "step: 49880 loss: 50.960167 time elapsed: 62.3372 learning rate: 0.006353, scenario: 0, slope: -0.06978564283008327, fluctuations: 0.09\n",
      "step: 49890 loss: 50.471861 time elapsed: 62.3491 learning rate: 0.006353, scenario: 0, slope: -0.07438266483812897, fluctuations: 0.1\n",
      "step: 49900 loss: 49.763262 time elapsed: 62.3607 learning rate: 0.006353, scenario: 0, slope: -0.06724182770316693, fluctuations: 0.09\n",
      "step: 49910 loss: 54.895872 time elapsed: 62.3732 learning rate: 0.006988, scenario: 1, slope: -0.05624282578803899, fluctuations: 0.07\n",
      "step: 49920 loss: 327.142089 time elapsed: 62.3853 learning rate: 0.004718, scenario: -1, slope: 2.2401460075154644, fluctuations: 0.08\n",
      "step: 49930 loss: 170.816956 time elapsed: 62.3988 learning rate: 0.001645, scenario: -1, slope: 2.6287612681982715, fluctuations: 0.11\n",
      "step: 49940 loss: 85.161090 time elapsed: 62.4126 learning rate: 0.000574, scenario: -1, slope: 2.2476672142951286, fluctuations: 0.12\n",
      "step: 49950 loss: 67.716602 time elapsed: 62.4262 learning rate: 0.000200, scenario: -1, slope: 1.5679868885003245, fluctuations: 0.13\n",
      "step: 49960 loss: 64.770041 time elapsed: 62.4394 learning rate: 0.000070, scenario: -1, slope: 0.818601231227251, fluctuations: 0.12\n",
      "step: 49970 loss: 63.729626 time elapsed: 62.4534 learning rate: 0.000024, scenario: -1, slope: 0.12380277140879921, fluctuations: 0.11\n",
      "step: 49980 loss: 63.417135 time elapsed: 62.4665 learning rate: 0.000023, scenario: 0, slope: -0.5485615686055272, fluctuations: 0.1\n",
      "step: 49990 loss: 63.248009 time elapsed: 62.4786 learning rate: 0.000023, scenario: 0, slope: -1.2204207312674311, fluctuations: 0.09\n",
      "step: 50000 loss: 63.135650 time elapsed: 62.4902 learning rate: 0.000023, scenario: 0, slope: -1.9580522154159046, fluctuations: 0.09\n",
      "step: 50010 loss: 63.031682 time elapsed: 62.5025 learning rate: 0.000023, scenario: 0, slope: -3.0485140271571316, fluctuations: 0.09\n",
      "step: 50020 loss: 62.929562 time elapsed: 62.5143 learning rate: 0.000023, scenario: 0, slope: -1.3806520486777814, fluctuations: 0.06\n",
      "step: 50030 loss: 62.830548 time elapsed: 62.5261 learning rate: 0.000023, scenario: 0, slope: -0.2717178570906108, fluctuations: 0.03\n",
      "step: 50040 loss: 62.733785 time elapsed: 62.5379 learning rate: 0.000028, scenario: 1, slope: -0.07174700178956704, fluctuations: 0.01\n",
      "step: 50050 loss: 62.566982 time elapsed: 62.5495 learning rate: 0.000072, scenario: 1, slope: -0.024767496322868576, fluctuations: 0.0\n",
      "step: 50060 loss: 62.147949 time elapsed: 62.5613 learning rate: 0.000188, scenario: 1, slope: -0.015500966239385053, fluctuations: 0.0\n",
      "step: 50070 loss: 61.148344 time elapsed: 62.5729 learning rate: 0.000488, scenario: 1, slope: -0.016565461195988087, fluctuations: 0.0\n",
      "step: 50080 loss: 59.105932 time elapsed: 62.5862 learning rate: 0.001265, scenario: 1, slope: -0.026834790914744465, fluctuations: 0.0\n",
      "step: 50090 loss: 56.084035 time elapsed: 62.5986 learning rate: 0.003281, scenario: 1, slope: -0.04865224427084495, fluctuations: 0.0\n",
      "step: 50100 loss: 52.853550 time elapsed: 62.6121 learning rate: 0.004804, scenario: 0, slope: -0.07857096752401599, fluctuations: 0.0\n",
      "step: 50110 loss: 51.076952 time elapsed: 62.6262 learning rate: 0.004804, scenario: 0, slope: -0.1184236772667855, fluctuations: 0.0\n",
      "step: 50120 loss: 49.777460 time elapsed: 62.6397 learning rate: 0.004804, scenario: 0, slope: -0.1501253261400093, fluctuations: 0.0\n",
      "step: 50130 loss: 48.876058 time elapsed: 62.6534 learning rate: 0.004804, scenario: 0, slope: -0.1727968882342447, fluctuations: 0.0\n",
      "step: 50140 loss: 48.159756 time elapsed: 62.6670 learning rate: 0.004804, scenario: 0, slope: -0.18407941177155393, fluctuations: 0.0\n",
      "step: 50150 loss: 47.567294 time elapsed: 62.6796 learning rate: 0.004804, scenario: 0, slope: -0.18244738910915215, fluctuations: 0.0\n",
      "step: 50160 loss: 47.043042 time elapsed: 62.6923 learning rate: 0.004804, scenario: 0, slope: -0.16785649981137044, fluctuations: 0.0\n",
      "step: 50170 loss: 46.559025 time elapsed: 62.7046 learning rate: 0.004804, scenario: 0, slope: -0.14242767371887588, fluctuations: 0.0\n",
      "step: 50180 loss: 46.101618 time elapsed: 62.7166 learning rate: 0.004804, scenario: 0, slope: -0.111740614838841, fluctuations: 0.0\n",
      "step: 50190 loss: 45.664074 time elapsed: 62.7285 learning rate: 0.004804, scenario: 0, slope: -0.08396060530748123, fluctuations: 0.0\n",
      "step: 50200 loss: 45.241792 time elapsed: 62.7403 learning rate: 0.004804, scenario: 0, slope: -0.06760045742091397, fluctuations: 0.0\n",
      "step: 50210 loss: 44.832236 time elapsed: 62.7528 learning rate: 0.004804, scenario: 0, slope: -0.05610754033405171, fluctuations: 0.0\n",
      "step: 50220 loss: 44.433808 time elapsed: 62.7647 learning rate: 0.004804, scenario: 0, slope: -0.05018372941440688, fluctuations: 0.0\n",
      "step: 50230 loss: 43.913720 time elapsed: 62.7766 learning rate: 0.011328, scenario: 1, slope: -0.04660345455287183, fluctuations: 0.0\n",
      "step: 50240 loss: 49.556906 time elapsed: 62.7887 learning rate: 0.029382, scenario: 1, slope: -0.04281316338633388, fluctuations: 0.0\n",
      "step: 50250 loss: 165287.264485 time elapsed: 62.8006 learning rate: 0.010866, scenario: -1, slope: 664.9678077770086, fluctuations: 0.02\n",
      "step: 50260 loss: 103494.709620 time elapsed: 62.8124 learning rate: 0.003789, scenario: -1, slope: 1205.4768916189964, fluctuations: 0.03\n",
      "step: 50270 loss: 68567.208586 time elapsed: 62.8251 learning rate: 0.001321, scenario: -1, slope: 1354.1130322431218, fluctuations: 0.03\n",
      "step: 50280 loss: 55406.803046 time elapsed: 62.8389 learning rate: 0.000461, scenario: -1, slope: 1274.4457822368436, fluctuations: 0.03\n",
      "step: 50290 loss: 51249.439473 time elapsed: 62.8528 learning rate: 0.000161, scenario: -1, slope: 1089.4079585106865, fluctuations: 0.03\n",
      "step: 50300 loss: 49982.590443 time elapsed: 62.8658 learning rate: 0.000062, scenario: -1, slope: 863.3806090338206, fluctuations: 0.03\n",
      "step: 50310 loss: 49532.791558 time elapsed: 62.8794 learning rate: 0.000022, scenario: -1, slope: 516.967650310798, fluctuations: 0.03\n",
      "step: 50320 loss: 49381.482243 time elapsed: 62.8922 learning rate: 0.000008, scenario: -1, slope: 130.0232526287607, fluctuations: 0.03\n",
      "step: 50330 loss: 49320.422032 time elapsed: 62.9045 learning rate: 0.000006, scenario: 0, slope: -336.4709434047125, fluctuations: 0.03\n",
      "step: 50340 loss: 49264.897168 time elapsed: 62.9162 learning rate: 0.000006, scenario: 0, slope: -898.8997224210971, fluctuations: 0.03\n",
      "step: 50350 loss: 49210.533513 time elapsed: 62.9283 learning rate: 0.000006, scenario: 0, slope: -656.2978746732113, fluctuations: 0.0\n",
      "step: 50360 loss: 49157.179272 time elapsed: 62.9397 learning rate: 0.000006, scenario: 0, slope: -248.45404180058458, fluctuations: 0.0\n",
      "step: 50370 loss: 49104.727156 time elapsed: 62.9515 learning rate: 0.000006, scenario: 0, slope: -80.71758703525478, fluctuations: 0.0\n",
      "step: 50380 loss: 49040.317916 time elapsed: 62.9629 learning rate: 0.000013, scenario: 1, slope: -27.567569806793625, fluctuations: 0.0\n",
      "step: 50390 loss: 48882.884018 time elapsed: 62.9744 learning rate: 0.000034, scenario: 1, slope: -12.043070532470843, fluctuations: 0.0\n",
      "step: 50400 loss: 48484.410877 time elapsed: 62.9863 learning rate: 0.000080, scenario: 1, slope: -8.710752457706816, fluctuations: 0.0\n",
      "step: 50410 loss: 47579.669325 time elapsed: 62.9987 learning rate: 0.000208, scenario: 1, slope: -11.720619250806621, fluctuations: 0.0\n",
      "step: 50420 loss: 45461.033061 time elapsed: 63.0104 learning rate: 0.000540, scenario: 1, slope: -22.723349270434433, fluctuations: 0.0\n",
      "step: 50430 loss: 41019.304825 time elapsed: 63.0219 learning rate: 0.001402, scenario: 1, slope: -48.88162559432799, fluctuations: 0.0\n",
      "step: 50440 loss: 34678.236797 time elapsed: 63.0341 learning rate: 0.001402, scenario: 0, slope: -99.60921928501114, fluctuations: 0.0\n",
      "step: 50450 loss: 28677.932626 time elapsed: 63.0477 learning rate: 0.001402, scenario: 0, slope: -169.02191468293285, fluctuations: 0.0\n",
      "step: 50460 loss: 21481.306694 time elapsed: 63.0608 learning rate: 0.001402, scenario: 0, slope: -255.34107671312293, fluctuations: 0.0\n",
      "step: 50470 loss: 17149.198335 time elapsed: 63.0745 learning rate: 0.001402, scenario: 0, slope: -341.97621839806914, fluctuations: 0.0\n",
      "step: 50480 loss: 13949.972359 time elapsed: 63.0880 learning rate: 0.001402, scenario: 0, slope: -411.7499719105019, fluctuations: 0.0\n",
      "step: 50490 loss: 11184.618479 time elapsed: 63.1013 learning rate: 0.001402, scenario: 0, slope: -458.01556676946313, fluctuations: 0.0\n",
      "step: 50500 loss: 9630.942078 time elapsed: 63.1131 learning rate: 0.001402, scenario: 0, slope: -473.1594641230318, fluctuations: 0.0\n",
      "step: 50510 loss: 8511.338460 time elapsed: 63.1255 learning rate: 0.001402, scenario: 0, slope: -453.139056256339, fluctuations: 0.0\n",
      "step: 50520 loss: 7706.600508 time elapsed: 63.1373 learning rate: 0.001402, scenario: 0, slope: -401.024459867845, fluctuations: 0.0\n",
      "step: 50530 loss: 7071.545132 time elapsed: 63.1489 learning rate: 0.001402, scenario: 0, slope: -327.99642847695384, fluctuations: 0.0\n",
      "step: 50540 loss: 6546.479030 time elapsed: 63.1608 learning rate: 0.001402, scenario: 0, slope: -253.03630135995613, fluctuations: 0.0\n",
      "step: 50550 loss: 6095.316822 time elapsed: 63.1728 learning rate: 0.001402, scenario: 0, slope: -183.65566301403993, fluctuations: 0.0\n",
      "step: 50560 loss: 5693.362671 time elapsed: 63.1846 learning rate: 0.001402, scenario: 0, slope: -132.3539869338981, fluctuations: 0.0\n",
      "step: 50570 loss: 5332.396020 time elapsed: 63.1966 learning rate: 0.001402, scenario: 0, slope: -97.8036391418636, fluctuations: 0.0\n",
      "step: 50580 loss: 5015.057566 time elapsed: 63.2084 learning rate: 0.001402, scenario: 0, slope: -72.512747986927, fluctuations: 0.0\n",
      "step: 50590 loss: 4742.005914 time elapsed: 63.2201 learning rate: 0.001402, scenario: 0, slope: -56.52968389051407, fluctuations: 0.0\n",
      "step: 50600 loss: 4511.801820 time elapsed: 63.2317 learning rate: 0.001402, scenario: 0, slope: -47.49410605412336, fluctuations: 0.0\n",
      "step: 50610 loss: 4315.211902 time elapsed: 63.2451 learning rate: 0.001402, scenario: 0, slope: -39.64459424890159, fluctuations: 0.0\n",
      "step: 50620 loss: 4146.836530 time elapsed: 63.2586 learning rate: 0.001402, scenario: 0, slope: -34.26839936937072, fluctuations: 0.0\n",
      "step: 50630 loss: 4001.078088 time elapsed: 63.2721 learning rate: 0.001402, scenario: 0, slope: -29.81615283935394, fluctuations: 0.0\n",
      "step: 50640 loss: 3872.873881 time elapsed: 63.2858 learning rate: 0.001402, scenario: 0, slope: -25.934389563636103, fluctuations: 0.0\n",
      "step: 50650 loss: 3756.813162 time elapsed: 63.2993 learning rate: 0.001402, scenario: 0, slope: -22.475818567503737, fluctuations: 0.0\n",
      "step: 50660 loss: 3647.337946 time elapsed: 63.3131 learning rate: 0.001402, scenario: 0, slope: -19.44283065032496, fluctuations: 0.0\n",
      "step: 50670 loss: 3549.223782 time elapsed: 63.3255 learning rate: 0.001402, scenario: 0, slope: -16.857819192783207, fluctuations: 0.0\n",
      "step: 50680 loss: 3457.784314 time elapsed: 63.3377 learning rate: 0.001402, scenario: 0, slope: -14.732659839065276, fluctuations: 0.0\n",
      "step: 50690 loss: 3370.979153 time elapsed: 63.3495 learning rate: 0.001402, scenario: 0, slope: -13.041231726889563, fluctuations: 0.0\n",
      "step: 50700 loss: 3287.517256 time elapsed: 63.3611 learning rate: 0.001402, scenario: 0, slope: -11.830152349779214, fluctuations: 0.0\n",
      "step: 50710 loss: 3206.499362 time elapsed: 63.3732 learning rate: 0.001402, scenario: 0, slope: -10.676333823976277, fluctuations: 0.0\n",
      "step: 50720 loss: 3123.008385 time elapsed: 63.3850 learning rate: 0.001402, scenario: 0, slope: -9.881926822149989, fluctuations: 0.0\n",
      "step: 50730 loss: 3045.616516 time elapsed: 63.3966 learning rate: 0.001402, scenario: 0, slope: -9.292005741505184, fluctuations: 0.0\n",
      "step: 50740 loss: 2976.079575 time elapsed: 63.4087 learning rate: 0.001402, scenario: 0, slope: -8.805130818136698, fluctuations: 0.0\n",
      "step: 50750 loss: 2909.818802 time elapsed: 63.4207 learning rate: 0.001402, scenario: 0, slope: -8.375724558345741, fluctuations: 0.0\n",
      "step: 50760 loss: 2843.196612 time elapsed: 63.4325 learning rate: 0.001402, scenario: 0, slope: -8.00907380965196, fluctuations: 0.0\n",
      "step: 50770 loss: 2720.841724 time elapsed: 63.4443 learning rate: 0.001402, scenario: 0, slope: -7.793586941822253, fluctuations: 0.0\n",
      "step: 50780 loss: 2540.003880 time elapsed: 63.4567 learning rate: 0.001402, scenario: 0, slope: -8.249084266693, fluctuations: 0.0\n",
      "step: 50790 loss: 2468.955181 time elapsed: 63.4707 learning rate: 0.001402, scenario: 0, slope: -8.770127596039568, fluctuations: 0.0\n",
      "step: 50800 loss: 2400.429209 time elapsed: 63.4841 learning rate: 0.001402, scenario: 0, slope: -9.096827493327238, fluctuations: 0.0\n",
      "step: 50810 loss: 2346.291364 time elapsed: 63.4981 learning rate: 0.001402, scenario: 0, slope: -9.268239544803494, fluctuations: 0.0\n",
      "step: 50820 loss: 2294.552924 time elapsed: 63.5120 learning rate: 0.001402, scenario: 0, slope: -9.184599963165901, fluctuations: 0.0\n",
      "step: 50830 loss: 2246.501531 time elapsed: 63.5260 learning rate: 0.001402, scenario: 0, slope: -8.901318942700268, fluctuations: 0.0\n",
      "step: 50840 loss: 2200.622514 time elapsed: 63.5400 learning rate: 0.001402, scenario: 0, slope: -8.373253632424468, fluctuations: 0.0\n",
      "step: 50850 loss: 2156.844494 time elapsed: 63.5522 learning rate: 0.001402, scenario: 0, slope: -7.589487751049089, fluctuations: 0.0\n",
      "step: 50860 loss: 2115.012059 time elapsed: 63.5643 learning rate: 0.001402, scenario: 0, slope: -6.555855207325233, fluctuations: 0.0\n",
      "step: 50870 loss: 2075.038963 time elapsed: 63.5771 learning rate: 0.001402, scenario: 0, slope: -5.404986657157363, fluctuations: 0.0\n",
      "step: 50880 loss: 2036.763407 time elapsed: 63.5898 learning rate: 0.001402, scenario: 0, slope: -4.857443345985522, fluctuations: 0.0\n",
      "step: 50890 loss: 2000.318294 time elapsed: 63.6018 learning rate: 0.001402, scenario: 0, slope: -4.552515989254829, fluctuations: 0.0\n",
      "step: 50900 loss: 1965.650028 time elapsed: 63.6138 learning rate: 0.001402, scenario: 0, slope: -4.3291629487120735, fluctuations: 0.0\n",
      "step: 50910 loss: 1932.252476 time elapsed: 63.6269 learning rate: 0.001402, scenario: 0, slope: -4.103059268509066, fluctuations: 0.0\n",
      "step: 50920 loss: 1887.601360 time elapsed: 63.6389 learning rate: 0.001402, scenario: 0, slope: -3.944804507344828, fluctuations: 0.0\n",
      "step: 50930 loss: 1844.238872 time elapsed: 63.6510 learning rate: 0.001402, scenario: 0, slope: -3.8802835026967415, fluctuations: 0.0\n",
      "step: 50940 loss: 1807.404413 time elapsed: 63.6641 learning rate: 0.001402, scenario: 0, slope: -3.8488459844989142, fluctuations: 0.0\n",
      "step: 50950 loss: 1775.218861 time elapsed: 63.6778 learning rate: 0.001402, scenario: 0, slope: -3.816719816453188, fluctuations: 0.0\n",
      "step: 50960 loss: 1745.244543 time elapsed: 63.6913 learning rate: 0.001402, scenario: 0, slope: -3.768891958834916, fluctuations: 0.0\n",
      "step: 50970 loss: 1716.568588 time elapsed: 63.7049 learning rate: 0.001402, scenario: 0, slope: -3.697937516352543, fluctuations: 0.0\n",
      "step: 50980 loss: 1688.701671 time elapsed: 63.7184 learning rate: 0.001402, scenario: 0, slope: -3.600909988259641, fluctuations: 0.0\n",
      "step: 50990 loss: 1661.417812 time elapsed: 63.7321 learning rate: 0.001402, scenario: 0, slope: -3.4755563619454084, fluctuations: 0.0\n",
      "step: 51000 loss: 1634.591919 time elapsed: 63.7444 learning rate: 0.001402, scenario: 0, slope: -3.3359101847667163, fluctuations: 0.0\n",
      "step: 51010 loss: 1608.161080 time elapsed: 63.7568 learning rate: 0.001402, scenario: 0, slope: -3.127899735231424, fluctuations: 0.0\n",
      "step: 51020 loss: 1582.089715 time elapsed: 63.7683 learning rate: 0.001402, scenario: 0, slope: -2.93186718441667, fluctuations: 0.0\n",
      "step: 51030 loss: 1556.354762 time elapsed: 63.7800 learning rate: 0.001402, scenario: 0, slope: -2.8103449769604794, fluctuations: 0.0\n",
      "step: 51040 loss: 1530.940787 time elapsed: 63.7917 learning rate: 0.001402, scenario: 0, slope: -2.7320609854091353, fluctuations: 0.0\n",
      "step: 51050 loss: 1505.838885 time elapsed: 63.8032 learning rate: 0.001402, scenario: 0, slope: -2.6761521411799434, fluctuations: 0.0\n",
      "step: 51060 loss: 1481.043455 time elapsed: 63.8151 learning rate: 0.001402, scenario: 0, slope: -2.631880020716108, fluctuations: 0.0\n",
      "step: 51070 loss: 1456.967266 time elapsed: 63.8270 learning rate: 0.001402, scenario: 0, slope: -2.5928925167021157, fluctuations: 0.0\n",
      "step: 51080 loss: 1432.057630 time elapsed: 63.8387 learning rate: 0.001402, scenario: 0, slope: -2.558321373691565, fluctuations: 0.0\n",
      "step: 51090 loss: 1404.465220 time elapsed: 63.8502 learning rate: 0.001402, scenario: 0, slope: -2.5407959753265192, fluctuations: 0.0\n",
      "step: 51100 loss: 1380.017956 time elapsed: 63.8621 learning rate: 0.001402, scenario: 0, slope: -2.531343886073555, fluctuations: 0.0\n",
      "step: 51110 loss: 1356.184760 time elapsed: 63.8764 learning rate: 0.001402, scenario: 0, slope: -2.5192126416428215, fluctuations: 0.0\n",
      "step: 51120 loss: 1333.129750 time elapsed: 63.8896 learning rate: 0.001402, scenario: 0, slope: -2.5041334076452286, fluctuations: 0.0\n",
      "step: 51130 loss: 1311.412858 time elapsed: 63.9031 learning rate: 0.001402, scenario: 0, slope: -2.479102516429927, fluctuations: 0.0\n",
      "step: 51140 loss: 1290.406731 time elapsed: 63.9170 learning rate: 0.001402, scenario: 0, slope: -2.441746153179133, fluctuations: 0.0\n",
      "step: 51150 loss: 1269.980456 time elapsed: 63.9303 learning rate: 0.001402, scenario: 0, slope: -2.39150821575527, fluctuations: 0.0\n",
      "step: 51160 loss: 1249.986358 time elapsed: 63.9436 learning rate: 0.001402, scenario: 0, slope: -2.3292157919323535, fluctuations: 0.0\n",
      "step: 51170 loss: 1230.302283 time elapsed: 63.9563 learning rate: 0.001402, scenario: 0, slope: -2.2556903546220397, fluctuations: 0.0\n",
      "step: 51180 loss: 1210.848015 time elapsed: 63.9682 learning rate: 0.001402, scenario: 0, slope: -2.1748750777185877, fluctuations: 0.0\n",
      "step: 51190 loss: 1191.585136 time elapsed: 63.9800 learning rate: 0.001402, scenario: 0, slope: -2.106508738826965, fluctuations: 0.0\n",
      "step: 51200 loss: 1172.510690 time elapsed: 63.9918 learning rate: 0.001402, scenario: 0, slope: -2.0557211695212785, fluctuations: 0.0\n",
      "step: 51210 loss: 1152.478093 time elapsed: 64.0051 learning rate: 0.001402, scenario: 0, slope: -2.0062199854923337, fluctuations: 0.0\n",
      "step: 51220 loss: 1101.575391 time elapsed: 64.0168 learning rate: 0.001402, scenario: 0, slope: -2.0745789203478138, fluctuations: 0.0\n",
      "step: 51230 loss: 1073.785469 time elapsed: 64.0283 learning rate: 0.001402, scenario: 0, slope: -2.2307516336832083, fluctuations: 0.0\n",
      "step: 51240 loss: 1043.450306 time elapsed: 64.0404 learning rate: 0.001402, scenario: 0, slope: -2.4041236402568367, fluctuations: 0.0\n",
      "step: 51250 loss: 990.631383 time elapsed: 64.0521 learning rate: 0.001402, scenario: 0, slope: -2.6455541523604267, fluctuations: 0.0\n",
      "step: 51260 loss: 940.072394 time elapsed: 64.0636 learning rate: 0.001402, scenario: 0, slope: -2.9953380357515016, fluctuations: 0.0\n",
      "step: 51270 loss: 902.155587 time elapsed: 64.0756 learning rate: 0.001402, scenario: 0, slope: -3.3637830663463455, fluctuations: 0.0\n",
      "step: 51280 loss: 872.085742 time elapsed: 64.0890 learning rate: 0.001402, scenario: 0, slope: -3.6505952263038006, fluctuations: 0.0\n",
      "step: 51290 loss: 847.444751 time elapsed: 64.1031 learning rate: 0.001402, scenario: 0, slope: -3.8118437036850423, fluctuations: 0.0\n",
      "step: 51300 loss: 827.217409 time elapsed: 64.1165 learning rate: 0.001402, scenario: 0, slope: -3.8140632102508523, fluctuations: 0.0\n",
      "step: 51310 loss: 810.230680 time elapsed: 64.1308 learning rate: 0.001402, scenario: 0, slope: -3.609570750569346, fluctuations: 0.0\n",
      "step: 51320 loss: 794.504890 time elapsed: 64.1437 learning rate: 0.001402, scenario: 0, slope: -3.329542967679102, fluctuations: 0.0\n",
      "step: 51330 loss: 779.534933 time elapsed: 64.1566 learning rate: 0.001402, scenario: 0, slope: -2.998589469905235, fluctuations: 0.0\n",
      "step: 51340 loss: 765.237815 time elapsed: 64.1690 learning rate: 0.001402, scenario: 0, slope: -2.5783851688599784, fluctuations: 0.0\n",
      "step: 51350 loss: 751.699571 time elapsed: 64.1809 learning rate: 0.001402, scenario: 0, slope: -2.156124976457165, fluctuations: 0.0\n",
      "step: 51360 loss: 738.971046 time elapsed: 64.1927 learning rate: 0.001402, scenario: 0, slope: -1.8420464365701092, fluctuations: 0.0\n",
      "step: 51370 loss: 727.589013 time elapsed: 64.2048 learning rate: 0.001402, scenario: 0, slope: -1.6366288970895997, fluctuations: 0.0\n",
      "step: 51380 loss: 716.303217 time elapsed: 64.2169 learning rate: 0.001402, scenario: 0, slope: -1.4891089580005032, fluctuations: 0.0\n",
      "step: 51390 loss: 705.924887 time elapsed: 64.2288 learning rate: 0.001402, scenario: 0, slope: -1.384481952657091, fluctuations: 0.0\n",
      "step: 51400 loss: 696.461539 time elapsed: 64.2404 learning rate: 0.001402, scenario: 0, slope: -1.307380121026069, fluctuations: 0.0\n",
      "step: 51410 loss: 687.659039 time elapsed: 64.2529 learning rate: 0.001402, scenario: 0, slope: -1.2220001291574234, fluctuations: 0.0\n",
      "step: 51420 loss: 679.390831 time elapsed: 64.2645 learning rate: 0.001402, scenario: 0, slope: -1.146376347269793, fluctuations: 0.0\n",
      "step: 51430 loss: 671.613022 time elapsed: 64.2806 learning rate: 0.001402, scenario: 0, slope: -1.0727022401517994, fluctuations: 0.0\n",
      "step: 51440 loss: 664.231144 time elapsed: 64.2970 learning rate: 0.001402, scenario: 0, slope: -1.0028672509483785, fluctuations: 0.0\n",
      "step: 51450 loss: 657.187143 time elapsed: 64.3113 learning rate: 0.001402, scenario: 0, slope: -0.9387808168586997, fluctuations: 0.0\n",
      "step: 51460 loss: 650.438006 time elapsed: 64.3251 learning rate: 0.001402, scenario: 0, slope: -0.8740740388215347, fluctuations: 0.0\n",
      "step: 51470 loss: 645.539123 time elapsed: 64.3385 learning rate: 0.001402, scenario: 0, slope: -0.8158063669674459, fluctuations: 0.0\n",
      "step: 51480 loss: 637.654868 time elapsed: 64.3523 learning rate: 0.001402, scenario: 0, slope: -0.7665380408191439, fluctuations: 0.0\n",
      "step: 51490 loss: 631.614416 time elapsed: 64.3664 learning rate: 0.001402, scenario: 0, slope: -0.7276128472793224, fluctuations: 0.0\n",
      "step: 51500 loss: 625.623671 time elapsed: 64.3791 learning rate: 0.001402, scenario: 0, slope: -0.698475374489082, fluctuations: 0.0\n",
      "step: 51510 loss: 991.454390 time elapsed: 64.3921 learning rate: 0.003636, scenario: 1, slope: -0.27353135247318794, fluctuations: 0.01\n",
      "step: 51520 loss: 671.359656 time elapsed: 64.4045 learning rate: 0.002008, scenario: -1, slope: 0.2455900190091614, fluctuations: 0.04\n",
      "step: 51530 loss: 618.650479 time elapsed: 64.4162 learning rate: 0.000700, scenario: -1, slope: 0.13363421634945036, fluctuations: 0.06\n",
      "step: 51540 loss: 601.998221 time elapsed: 64.4276 learning rate: 0.000428, scenario: 1, slope: -0.0600758746947382, fluctuations: 0.08\n",
      "step: 51550 loss: 599.843899 time elapsed: 64.4394 learning rate: 0.001110, scenario: 1, slope: -0.25316762101728746, fluctuations: 0.1\n",
      "step: 51560 loss: 593.406950 time elapsed: 64.4510 learning rate: 0.002878, scenario: 1, slope: -0.4559222726285786, fluctuations: 0.12\n",
      "step: 51570 loss: 582.478531 time elapsed: 64.4630 learning rate: 0.005609, scenario: 0, slope: -0.70157421694944, fluctuations: 0.13\n",
      "step: 51580 loss: 1230.196359 time elapsed: 64.4749 learning rate: 0.004250, scenario: -1, slope: 1.1600373031208204, fluctuations: 0.14\n",
      "step: 51590 loss: 668.704229 time elapsed: 64.4865 learning rate: 0.001482, scenario: -1, slope: 0.5077789763724304, fluctuations: 0.18\n",
      "step: 51600 loss: 573.275927 time elapsed: 64.4980 learning rate: 0.000574, scenario: -1, slope: -0.1588162304496384, fluctuations: 0.2\n",
      "step: 51610 loss: 555.528905 time elapsed: 64.5120 learning rate: 0.000247, scenario: -1, slope: -0.5434656392757548, fluctuations: 0.21\n",
      "step: 51620 loss: 554.176389 time elapsed: 64.5260 learning rate: 0.000151, scenario: 1, slope: -0.4061575923985525, fluctuations: 0.19\n",
      "step: 51630 loss: 552.925286 time elapsed: 64.5399 learning rate: 0.000243, scenario: 0, slope: -0.771384969411685, fluctuations: 0.17\n",
      "step: 51640 loss: 551.395959 time elapsed: 64.5535 learning rate: 0.000243, scenario: 0, slope: -1.2035382774556578, fluctuations: 0.16\n",
      "step: 51650 loss: 550.073209 time elapsed: 64.5667 learning rate: 0.000243, scenario: 0, slope: -1.5615665907403962, fluctuations: 0.14\n",
      "step: 51660 loss: 548.849864 time elapsed: 64.5803 learning rate: 0.000243, scenario: 0, slope: -1.8911147213666903, fluctuations: 0.12\n",
      "step: 51670 loss: 547.695563 time elapsed: 64.5936 learning rate: 0.000243, scenario: 0, slope: -2.327016396258823, fluctuations: 0.11\n",
      "step: 51680 loss: 546.562329 time elapsed: 64.6054 learning rate: 0.000356, scenario: 1, slope: -0.5005544894799452, fluctuations: 0.09\n",
      "step: 51690 loss: 544.362754 time elapsed: 64.6175 learning rate: 0.000923, scenario: 1, slope: -0.2626499177662629, fluctuations: 0.05\n",
      "step: 51700 loss: 539.222340 time elapsed: 64.6296 learning rate: 0.002177, scenario: 1, slope: -0.1770809783023211, fluctuations: 0.03\n",
      "step: 51710 loss: 528.815567 time elapsed: 64.6419 learning rate: 0.005647, scenario: 1, slope: -0.19733093931712947, fluctuations: 0.02\n",
      "step: 51720 loss: 2237.272320 time elapsed: 64.6535 learning rate: 0.012711, scenario: -1, slope: 0.7748360812207342, fluctuations: 0.01\n",
      "step: 51730 loss: 3442.213096 time elapsed: 64.6652 learning rate: 0.004432, scenario: -1, slope: 24.616058183504276, fluctuations: 0.04\n",
      "step: 51740 loss: 1626.404839 time elapsed: 64.6776 learning rate: 0.001545, scenario: -1, slope: 28.171498941120664, fluctuations: 0.05\n",
      "step: 51750 loss: 1040.281991 time elapsed: 64.6897 learning rate: 0.000539, scenario: -1, slope: 23.784346568309946, fluctuations: 0.05\n",
      "step: 51760 loss: 956.645934 time elapsed: 64.7016 learning rate: 0.000188, scenario: -1, slope: 17.52759517554925, fluctuations: 0.05\n",
      "step: 51770 loss: 916.447076 time elapsed: 64.7131 learning rate: 0.000066, scenario: -1, slope: 10.791138012057932, fluctuations: 0.05\n",
      "step: 51780 loss: 908.575333 time elapsed: 64.7268 learning rate: 0.000023, scenario: -1, slope: 3.6937869058971153, fluctuations: 0.05\n",
      "step: 51790 loss: 905.860090 time elapsed: 64.7410 learning rate: 0.000016, scenario: 0, slope: -3.9465083331491173, fluctuations: 0.05\n",
      "step: 51800 loss: 903.650533 time elapsed: 64.7547 learning rate: 0.000016, scenario: 0, slope: -11.547832296114047, fluctuations: 0.05\n",
      "step: 51810 loss: 901.611893 time elapsed: 64.7686 learning rate: 0.000016, scenario: 0, slope: -22.255468657815335, fluctuations: 0.05\n",
      "step: 51820 loss: 899.724380 time elapsed: 64.7823 learning rate: 0.000016, scenario: 0, slope: -32.72684427874063, fluctuations: 0.05\n",
      "step: 51830 loss: 897.953782 time elapsed: 64.7953 learning rate: 0.000016, scenario: 0, slope: -8.906151820125887, fluctuations: 0.01\n",
      "step: 51840 loss: 896.269080 time elapsed: 64.8074 learning rate: 0.000016, scenario: 0, slope: -2.600550961786806, fluctuations: 0.0\n",
      "step: 51850 loss: 894.544403 time elapsed: 64.8194 learning rate: 0.000025, scenario: 1, slope: -0.7379584274255696, fluctuations: 0.0\n",
      "step: 51860 loss: 890.913191 time elapsed: 64.8313 learning rate: 0.000066, scenario: 1, slope: -0.3090329293232973, fluctuations: 0.0\n",
      "step: 51870 loss: 882.061998 time elapsed: 64.8428 learning rate: 0.000171, scenario: 1, slope: -0.23458588879171233, fluctuations: 0.0\n",
      "step: 51880 loss: 861.111071 time elapsed: 64.8550 learning rate: 0.000445, scenario: 1, slope: -0.30852872766125605, fluctuations: 0.0\n",
      "step: 51890 loss: 815.551196 time elapsed: 64.8665 learning rate: 0.001153, scenario: 1, slope: -0.5471287361879861, fluctuations: 0.0\n",
      "step: 51900 loss: 745.076628 time elapsed: 64.8779 learning rate: 0.002247, scenario: 0, slope: -0.9891759614639452, fluctuations: 0.0\n",
      "step: 51910 loss: 688.761933 time elapsed: 64.8905 learning rate: 0.002247, scenario: 0, slope: -1.7673991884069475, fluctuations: 0.0\n",
      "step: 51920 loss: 645.941042 time elapsed: 64.9027 learning rate: 0.002247, scenario: 0, slope: -2.524684278632364, fluctuations: 0.0\n",
      "step: 51930 loss: 613.665397 time elapsed: 64.9146 learning rate: 0.002247, scenario: 0, slope: -3.2117150005610626, fluctuations: 0.0\n",
      "step: 51940 loss: 590.861626 time elapsed: 64.9266 learning rate: 0.002247, scenario: 0, slope: -3.7280666077040503, fluctuations: 0.0\n",
      "step: 51950 loss: 573.718096 time elapsed: 64.9397 learning rate: 0.002247, scenario: 0, slope: -4.0039084629056605, fluctuations: 0.0\n",
      "step: 51960 loss: 560.089056 time elapsed: 64.9534 learning rate: 0.002247, scenario: 0, slope: -4.000168361174691, fluctuations: 0.0\n",
      "step: 51970 loss: 548.616400 time elapsed: 64.9665 learning rate: 0.002247, scenario: 0, slope: -3.710733237404995, fluctuations: 0.0\n",
      "step: 51980 loss: 538.531159 time elapsed: 64.9799 learning rate: 0.002247, scenario: 0, slope: -3.1748485430525037, fluctuations: 0.0\n",
      "step: 51990 loss: 529.393517 time elapsed: 64.9933 learning rate: 0.002247, scenario: 0, slope: -2.513057429272391, fluctuations: 0.0\n",
      "step: 52000 loss: 520.961845 time elapsed: 65.0068 learning rate: 0.002247, scenario: 0, slope: -1.9833389283212732, fluctuations: 0.0\n",
      "step: 52010 loss: 513.096436 time elapsed: 65.0195 learning rate: 0.002247, scenario: 0, slope: -1.5248924560523738, fluctuations: 0.0\n",
      "step: 52020 loss: 505.704693 time elapsed: 65.0313 learning rate: 0.002247, scenario: 0, slope: -1.239041860780489, fluctuations: 0.0\n",
      "step: 52030 loss: 498.719385 time elapsed: 65.0430 learning rate: 0.002247, scenario: 0, slope: -1.052877097247137, fluctuations: 0.0\n",
      "step: 52040 loss: 492.087125 time elapsed: 65.0548 learning rate: 0.002247, scenario: 0, slope: -0.9305066395414182, fluctuations: 0.0\n",
      "step: 52050 loss: 485.762722 time elapsed: 65.0666 learning rate: 0.002247, scenario: 0, slope: -0.8451932225811116, fluctuations: 0.0\n",
      "step: 52060 loss: 479.706745 time elapsed: 65.0782 learning rate: 0.002247, scenario: 0, slope: -0.7819977243210963, fluctuations: 0.0\n",
      "step: 52070 loss: 473.884437 time elapsed: 65.0897 learning rate: 0.002247, scenario: 0, slope: -0.7322552796188254, fluctuations: 0.0\n",
      "step: 52080 loss: 468.265294 time elapsed: 65.1017 learning rate: 0.002247, scenario: 0, slope: -0.691246668192532, fluctuations: 0.0\n",
      "step: 52090 loss: 462.822903 time elapsed: 65.1134 learning rate: 0.002247, scenario: 0, slope: -0.6564294388245872, fluctuations: 0.0\n",
      "step: 52100 loss: 457.534821 time elapsed: 65.1250 learning rate: 0.002247, scenario: 0, slope: -0.6292312393475142, fluctuations: 0.0\n",
      "step: 52110 loss: 452.382423 time elapsed: 65.1377 learning rate: 0.002247, scenario: 0, slope: -0.600369019023599, fluctuations: 0.0\n",
      "step: 52120 loss: 447.350666 time elapsed: 65.1506 learning rate: 0.002247, scenario: 0, slope: -0.5776784887378476, fluctuations: 0.0\n",
      "step: 52130 loss: 442.427753 time elapsed: 65.1644 learning rate: 0.002247, scenario: 0, slope: -0.5578719293100954, fluctuations: 0.0\n",
      "step: 52140 loss: 437.604682 time elapsed: 65.1782 learning rate: 0.002247, scenario: 0, slope: -0.5405326489774785, fluctuations: 0.0\n",
      "step: 52150 loss: 432.874687 time elapsed: 65.1916 learning rate: 0.002247, scenario: 0, slope: -0.5252799029143861, fluctuations: 0.0\n",
      "step: 52160 loss: 428.232619 time elapsed: 65.2052 learning rate: 0.002247, scenario: 0, slope: -0.5117632614302061, fluctuations: 0.0\n",
      "step: 52170 loss: 423.674341 time elapsed: 65.2189 learning rate: 0.002247, scenario: 0, slope: -0.4996653784391651, fluctuations: 0.0\n",
      "step: 52180 loss: 419.196219 time elapsed: 65.2309 learning rate: 0.002247, scenario: 0, slope: -0.48870773610082385, fluctuations: 0.0\n",
      "step: 52190 loss: 414.794771 time elapsed: 65.2430 learning rate: 0.002247, scenario: 0, slope: -0.47865593964654357, fluctuations: 0.0\n",
      "step: 52200 loss: 410.466466 time elapsed: 65.2547 learning rate: 0.002247, scenario: 0, slope: -0.47022801293204236, fluctuations: 0.0\n",
      "step: 52210 loss: 406.207649 time elapsed: 65.2666 learning rate: 0.002247, scenario: 0, slope: -0.4605673104598824, fluctuations: 0.0\n",
      "step: 52220 loss: 402.014534 time elapsed: 65.2787 learning rate: 0.002247, scenario: 0, slope: -0.4522924636085656, fluctuations: 0.0\n",
      "step: 52230 loss: 397.883228 time elapsed: 65.2905 learning rate: 0.002247, scenario: 0, slope: -0.44443741951342, fluctuations: 0.0\n",
      "step: 52240 loss: 393.809763 time elapsed: 65.3025 learning rate: 0.002472, scenario: 1, slope: -0.4369700637283329, fluctuations: 0.0\n",
      "step: 52250 loss: 387.509946 time elapsed: 65.3144 learning rate: 0.004817, scenario: 0, slope: -0.4344677120628817, fluctuations: 0.0\n",
      "step: 52260 loss: 379.116169 time elapsed: 65.3262 learning rate: 0.004817, scenario: 0, slope: -0.45275906560406526, fluctuations: 0.0\n",
      "step: 52270 loss: 370.946100 time elapsed: 65.3381 learning rate: 0.004817, scenario: 0, slope: -0.48909845905952654, fluctuations: 0.0\n",
      "step: 52280 loss: 362.984410 time elapsed: 65.3507 learning rate: 0.004817, scenario: 0, slope: -0.5373743528988374, fluctuations: 0.0\n",
      "step: 52290 loss: 355.224418 time elapsed: 65.3648 learning rate: 0.004817, scenario: 0, slope: -0.5917234323785601, fluctuations: 0.0\n",
      "step: 52300 loss: 347.668747 time elapsed: 65.3790 learning rate: 0.004817, scenario: 0, slope: -0.6411224447508715, fluctuations: 0.0\n",
      "step: 52310 loss: 340.297560 time elapsed: 65.3934 learning rate: 0.004817, scenario: 0, slope: -0.6960940702056969, fluctuations: 0.0\n",
      "step: 52320 loss: 333.061018 time elapsed: 65.4067 learning rate: 0.004817, scenario: 0, slope: -0.7355511678595317, fluctuations: 0.0\n",
      "step: 52330 loss: 325.887272 time elapsed: 65.4202 learning rate: 0.004817, scenario: 0, slope: -0.7602021835784573, fluctuations: 0.0\n",
      "step: 52340 loss: 318.665079 time elapsed: 65.4370 learning rate: 0.004817, scenario: 0, slope: -0.7659894627833231, fluctuations: 0.0\n",
      "step: 52350 loss: 311.218018 time elapsed: 65.4496 learning rate: 0.004817, scenario: 0, slope: -0.7546577898930777, fluctuations: 0.0\n",
      "step: 52360 loss: 303.287089 time elapsed: 65.4617 learning rate: 0.004817, scenario: 0, slope: -0.7455338223101493, fluctuations: 0.0\n",
      "step: 52370 loss: 294.661711 time elapsed: 65.4740 learning rate: 0.004817, scenario: 0, slope: -0.7447916585864625, fluctuations: 0.0\n",
      "step: 52380 loss: 285.786016 time elapsed: 65.4858 learning rate: 0.004817, scenario: 0, slope: -0.7538182405081999, fluctuations: 0.0\n",
      "step: 52390 loss: 277.733417 time elapsed: 65.4980 learning rate: 0.004817, scenario: 0, slope: -0.7682187543828883, fluctuations: 0.0\n",
      "step: 52400 loss: 270.404831 time elapsed: 65.5097 learning rate: 0.004817, scenario: 0, slope: -0.7801267304060894, fluctuations: 0.0\n",
      "step: 52410 loss: 263.536842 time elapsed: 65.5222 learning rate: 0.004817, scenario: 0, slope: -0.7887175136742516, fluctuations: 0.0\n",
      "step: 52420 loss: 257.137499 time elapsed: 65.5339 learning rate: 0.004817, scenario: 0, slope: -0.7875934539615337, fluctuations: 0.0\n",
      "step: 52430 loss: 251.216798 time elapsed: 65.5462 learning rate: 0.004817, scenario: 0, slope: -0.7754368554555117, fluctuations: 0.0\n",
      "step: 52440 loss: 245.712128 time elapsed: 65.5583 learning rate: 0.004817, scenario: 0, slope: -0.7510506514986632, fluctuations: 0.0\n",
      "step: 52450 loss: 240.518651 time elapsed: 65.5711 learning rate: 0.004817, scenario: 0, slope: -0.7152050737423994, fluctuations: 0.0\n",
      "step: 52460 loss: 235.517036 time elapsed: 65.5847 learning rate: 0.004817, scenario: 0, slope: -0.6713358014028875, fluctuations: 0.0\n",
      "step: 52470 loss: 230.454849 time elapsed: 65.5982 learning rate: 0.004817, scenario: 0, slope: -0.6266178389857754, fluctuations: 0.0\n",
      "step: 52480 loss: 225.856871 time elapsed: 65.6118 learning rate: 0.004817, scenario: 0, slope: -0.586913719337845, fluctuations: 0.0\n",
      "step: 52490 loss: 221.462064 time elapsed: 65.6255 learning rate: 0.004817, scenario: 0, slope: -0.5532441033211429, fluctuations: 0.0\n",
      "step: 52500 loss: 217.250459 time elapsed: 65.6389 learning rate: 0.004817, scenario: 0, slope: -0.5264617258494955, fluctuations: 0.0\n",
      "step: 52510 loss: 213.154245 time elapsed: 65.6517 learning rate: 0.004817, scenario: 0, slope: -0.49777279197336, fluctuations: 0.0\n",
      "step: 52520 loss: 209.211862 time elapsed: 65.6639 learning rate: 0.004817, scenario: 0, slope: -0.4753484031095309, fluctuations: 0.0\n",
      "step: 52530 loss: 205.392155 time elapsed: 65.6758 learning rate: 0.004817, scenario: 0, slope: -0.4555289613528149, fluctuations: 0.0\n",
      "step: 52540 loss: 201.683507 time elapsed: 65.6876 learning rate: 0.004817, scenario: 0, slope: -0.43744674639489306, fluctuations: 0.0\n",
      "step: 52550 loss: 198.074411 time elapsed: 65.6994 learning rate: 0.004817, scenario: 0, slope: -0.4204704520116439, fluctuations: 0.0\n",
      "step: 52560 loss: 194.556237 time elapsed: 65.7110 learning rate: 0.004817, scenario: 0, slope: -0.4044473919312986, fluctuations: 0.0\n",
      "step: 52570 loss: 191.120683 time elapsed: 65.7227 learning rate: 0.004817, scenario: 0, slope: -0.3906341071636906, fluctuations: 0.0\n",
      "step: 52580 loss: 187.761962 time elapsed: 65.7344 learning rate: 0.004817, scenario: 0, slope: -0.3786602873687214, fluctuations: 0.0\n",
      "step: 52590 loss: 184.481046 time elapsed: 65.7461 learning rate: 0.004817, scenario: 0, slope: -0.36778410301516906, fluctuations: 0.0\n",
      "step: 52600 loss: 181.227674 time elapsed: 65.7579 learning rate: 0.004817, scenario: 0, slope: -0.35892173621748236, fluctuations: 0.0\n",
      "step: 52610 loss: 178.026366 time elapsed: 65.7702 learning rate: 0.004817, scenario: 0, slope: -0.34920955445694873, fluctuations: 0.0\n",
      "step: 52620 loss: 174.861402 time elapsed: 65.7832 learning rate: 0.004817, scenario: 0, slope: -0.3415253037523026, fluctuations: 0.0\n",
      "step: 52630 loss: 171.742469 time elapsed: 65.7972 learning rate: 0.004817, scenario: 0, slope: -0.334731311315225, fluctuations: 0.0\n",
      "step: 52640 loss: 168.740437 time elapsed: 65.8109 learning rate: 0.004817, scenario: 0, slope: -0.3285115691713215, fluctuations: 0.0\n",
      "step: 52650 loss: 165.900375 time elapsed: 65.8247 learning rate: 0.004817, scenario: 0, slope: -0.3221888213315314, fluctuations: 0.0\n",
      "step: 52660 loss: 163.161871 time elapsed: 65.8386 learning rate: 0.004817, scenario: 0, slope: -0.31534693856485724, fluctuations: 0.0\n",
      "step: 52670 loss: 160.464936 time elapsed: 65.8518 learning rate: 0.004817, scenario: 0, slope: -0.3079891045574939, fluctuations: 0.0\n",
      "step: 52680 loss: 157.859011 time elapsed: 65.8639 learning rate: 0.004817, scenario: 0, slope: -0.3001104763184648, fluctuations: 0.0\n",
      "step: 52690 loss: 155.316945 time elapsed: 65.8758 learning rate: 0.004817, scenario: 0, slope: -0.2916978659113526, fluctuations: 0.0\n",
      "step: 52700 loss: 152.838047 time elapsed: 65.8876 learning rate: 0.004817, scenario: 0, slope: -0.28389934094006336, fluctuations: 0.0\n",
      "step: 52710 loss: 150.419406 time elapsed: 65.8996 learning rate: 0.004817, scenario: 0, slope: -0.27431998709662647, fluctuations: 0.0\n",
      "step: 52720 loss: 148.058103 time elapsed: 65.9117 learning rate: 0.004817, scenario: 0, slope: -0.26593978734263074, fluctuations: 0.0\n",
      "step: 52730 loss: 145.752765 time elapsed: 65.9232 learning rate: 0.004817, scenario: 0, slope: -0.25828134534614344, fluctuations: 0.0\n",
      "step: 52740 loss: 143.517066 time elapsed: 65.9343 learning rate: 0.004817, scenario: 0, slope: -0.25143444569477064, fluctuations: 0.0\n",
      "step: 52750 loss: 141.306708 time elapsed: 65.9461 learning rate: 0.004817, scenario: 0, slope: -0.24517908816793366, fluctuations: 0.0\n",
      "step: 52760 loss: 139.151873 time elapsed: 65.9578 learning rate: 0.004817, scenario: 0, slope: -0.23919656436177256, fluctuations: 0.0\n",
      "step: 52770 loss: 137.049707 time elapsed: 65.9695 learning rate: 0.004817, scenario: 0, slope: -0.23348179671300243, fluctuations: 0.0\n",
      "step: 52780 loss: 134.995809 time elapsed: 65.9813 learning rate: 0.004817, scenario: 0, slope: -0.22800613915954768, fluctuations: 0.0\n",
      "step: 52790 loss: 132.987807 time elapsed: 65.9941 learning rate: 0.004817, scenario: 0, slope: -0.22273282095311425, fluctuations: 0.0\n",
      "step: 52800 loss: 131.024576 time elapsed: 66.0077 learning rate: 0.004817, scenario: 0, slope: -0.21813698020492403, fluctuations: 0.0\n",
      "step: 52810 loss: 129.104845 time elapsed: 66.0221 learning rate: 0.004817, scenario: 0, slope: -0.21269730345442425, fluctuations: 0.0\n",
      "step: 52820 loss: 127.227243 time elapsed: 66.0360 learning rate: 0.004817, scenario: 0, slope: -0.20791024221219395, fluctuations: 0.0\n",
      "step: 52830 loss: 125.390462 time elapsed: 66.0490 learning rate: 0.004817, scenario: 0, slope: -0.2032659643307534, fluctuations: 0.0\n",
      "step: 52840 loss: 123.615627 time elapsed: 66.0628 learning rate: 0.004817, scenario: 0, slope: -0.19864480956131345, fluctuations: 0.0\n",
      "step: 52850 loss: 121.832854 time elapsed: 66.0749 learning rate: 0.004817, scenario: 0, slope: -0.19418557466026803, fluctuations: 0.0\n",
      "step: 52860 loss: 120.107908 time elapsed: 66.0868 learning rate: 0.004817, scenario: 0, slope: -0.18993179343357133, fluctuations: 0.0\n",
      "step: 52870 loss: 118.408149 time elapsed: 66.0984 learning rate: 0.004817, scenario: 0, slope: -0.18586198953365793, fluctuations: 0.0\n",
      "step: 52880 loss: 116.732501 time elapsed: 66.1103 learning rate: 0.004817, scenario: 0, slope: -0.1820054967889406, fluctuations: 0.0\n",
      "step: 52890 loss: 115.054017 time elapsed: 66.1219 learning rate: 0.004817, scenario: 0, slope: -0.17846655547090234, fluctuations: 0.0\n",
      "step: 52900 loss: 114.139875 time elapsed: 66.1338 learning rate: 0.004817, scenario: 0, slope: -0.17481928783257142, fluctuations: 0.0\n",
      "step: 52910 loss: 112.010832 time elapsed: 66.1465 learning rate: 0.004817, scenario: 0, slope: -0.1704082300146402, fluctuations: 0.03\n",
      "step: 52920 loss: 110.988607 time elapsed: 66.1581 learning rate: 0.004817, scenario: 0, slope: -0.16761203861201113, fluctuations: 0.03\n",
      "step: 52930 loss: 109.881258 time elapsed: 66.1698 learning rate: 0.004817, scenario: 0, slope: -0.16359443838718557, fluctuations: 0.07\n",
      "step: 52940 loss: 107.572147 time elapsed: 66.1815 learning rate: 0.004817, scenario: 0, slope: -0.16047484475480303, fluctuations: 0.1\n",
      "step: 52950 loss: 105.527705 time elapsed: 66.1934 learning rate: 0.004817, scenario: 0, slope: -0.16057228288715913, fluctuations: 0.14\n",
      "step: 52960 loss: 104.857164 time elapsed: 66.2067 learning rate: 0.004817, scenario: 0, slope: -0.160073004809174, fluctuations: 0.17\n",
      "step: 52970 loss: 102.913535 time elapsed: 66.2208 learning rate: 0.004817, scenario: 0, slope: -0.1590312687817706, fluctuations: 0.22\n",
      "step: 52980 loss: 100.965029 time elapsed: 66.2343 learning rate: 0.004817, scenario: 0, slope: -0.1583455140231589, fluctuations: 0.25\n",
      "step: 52990 loss: 99.370430 time elapsed: 66.2479 learning rate: 0.004817, scenario: 0, slope: -0.15873667605546543, fluctuations: 0.26\n",
      "step: 53000 loss: 97.841151 time elapsed: 66.2614 learning rate: 0.004817, scenario: 0, slope: -0.15895909078221462, fluctuations: 0.26\n",
      "step: 53010 loss: 96.528247 time elapsed: 66.2752 learning rate: 0.004817, scenario: 0, slope: -0.15607291528990452, fluctuations: 0.24\n",
      "step: 53020 loss: 95.172819 time elapsed: 66.2878 learning rate: 0.004817, scenario: 0, slope: -0.1547832399658428, fluctuations: 0.23\n",
      "step: 53030 loss: 94.381051 time elapsed: 66.2997 learning rate: 0.004817, scenario: 0, slope: -0.14804264913644746, fluctuations: 0.2\n",
      "step: 53040 loss: 92.914596 time elapsed: 66.3116 learning rate: 0.004817, scenario: 0, slope: -0.1409192378298446, fluctuations: 0.2\n",
      "step: 53050 loss: 91.629637 time elapsed: 66.3235 learning rate: 0.004817, scenario: 0, slope: -0.1380178343841753, fluctuations: 0.18\n",
      "step: 53060 loss: 90.319464 time elapsed: 66.3354 learning rate: 0.004817, scenario: 0, slope: -0.1339996745754245, fluctuations: 0.15\n",
      "step: 53070 loss: 89.242161 time elapsed: 66.3472 learning rate: 0.004817, scenario: 0, slope: -0.12909563966425924, fluctuations: 0.11\n",
      "step: 53080 loss: 88.110479 time elapsed: 66.3593 learning rate: 0.004817, scenario: 0, slope: -0.12545187330763166, fluctuations: 0.08\n",
      "step: 53090 loss: 87.321395 time elapsed: 66.3711 learning rate: 0.004817, scenario: 0, slope: -0.12037464046736202, fluctuations: 0.08\n",
      "step: 53100 loss: 86.083363 time elapsed: 66.3832 learning rate: 0.004817, scenario: 0, slope: -0.11837562816262472, fluctuations: 0.09\n",
      "step: 53110 loss: 84.804687 time elapsed: 66.3956 learning rate: 0.004817, scenario: 0, slope: -0.1165383875408376, fluctuations: 0.1\n",
      "step: 53120 loss: 83.759669 time elapsed: 66.4077 learning rate: 0.004817, scenario: 0, slope: -0.11570171750756668, fluctuations: 0.1\n",
      "step: 53130 loss: 83.242603 time elapsed: 66.4204 learning rate: 0.004817, scenario: 0, slope: -0.11148764595125055, fluctuations: 0.1\n",
      "step: 53140 loss: 81.952894 time elapsed: 66.4346 learning rate: 0.004817, scenario: 0, slope: -0.10878659981016639, fluctuations: 0.08\n",
      "step: 53150 loss: 80.825374 time elapsed: 66.4482 learning rate: 0.004817, scenario: 0, slope: -0.10736033335046813, fluctuations: 0.06\n",
      "step: 53160 loss: 80.022151 time elapsed: 66.4615 learning rate: 0.004817, scenario: 0, slope: -0.10474833351635617, fluctuations: 0.08\n",
      "step: 53170 loss: 78.854324 time elapsed: 66.4745 learning rate: 0.004817, scenario: 0, slope: -0.10344924820375123, fluctuations: 0.09\n",
      "step: 53180 loss: 78.085326 time elapsed: 66.4873 learning rate: 0.004817, scenario: 0, slope: -0.10289733969571627, fluctuations: 0.09\n",
      "step: 53190 loss: 76.918914 time elapsed: 66.4998 learning rate: 0.004817, scenario: 0, slope: -0.10021206759642758, fluctuations: 0.07\n",
      "step: 53200 loss: 76.022921 time elapsed: 66.5120 learning rate: 0.004817, scenario: 0, slope: -0.09908323759973564, fluctuations: 0.05\n",
      "step: 53210 loss: 75.171251 time elapsed: 66.5243 learning rate: 0.004817, scenario: 0, slope: -0.09260059662217686, fluctuations: 0.06\n",
      "step: 53220 loss: 74.907419 time elapsed: 66.5361 learning rate: 0.004817, scenario: 0, slope: -0.09036787195990897, fluctuations: 0.08\n",
      "step: 53230 loss: 73.466659 time elapsed: 66.5479 learning rate: 0.004817, scenario: 0, slope: -0.08997533901353763, fluctuations: 0.09\n",
      "step: 53240 loss: 72.463467 time elapsed: 66.5600 learning rate: 0.004817, scenario: 0, slope: -0.08997873276465977, fluctuations: 0.1\n",
      "step: 53250 loss: 71.604948 time elapsed: 66.5716 learning rate: 0.004817, scenario: 0, slope: -0.0911653044901242, fluctuations: 0.1\n",
      "step: 53260 loss: 70.727362 time elapsed: 66.5833 learning rate: 0.004817, scenario: 0, slope: -0.09086811469109833, fluctuations: 0.08\n",
      "step: 53270 loss: 69.857498 time elapsed: 66.5972 learning rate: 0.004817, scenario: 0, slope: -0.09159352073368143, fluctuations: 0.07\n",
      "step: 53280 loss: 69.000575 time elapsed: 66.6097 learning rate: 0.004817, scenario: 0, slope: -0.09247594756897785, fluctuations: 0.07\n",
      "step: 53290 loss: 68.156713 time elapsed: 66.6232 learning rate: 0.004817, scenario: 0, slope: -0.09355607430808, fluctuations: 0.07\n",
      "step: 53300 loss: 68.266626 time elapsed: 66.6365 learning rate: 0.004817, scenario: 0, slope: -0.094541537158281, fluctuations: 0.07\n",
      "step: 53310 loss: 67.559656 time elapsed: 66.6505 learning rate: 0.004817, scenario: 0, slope: -0.08345040680280469, fluctuations: 0.08\n",
      "step: 53320 loss: 66.128488 time elapsed: 66.6640 learning rate: 0.004817, scenario: 0, slope: -0.08069674256347543, fluctuations: 0.06\n",
      "step: 53330 loss: 64.984674 time elapsed: 66.6771 learning rate: 0.004817, scenario: 0, slope: -0.08011472638436662, fluctuations: 0.06\n",
      "step: 53340 loss: 64.209614 time elapsed: 66.6899 learning rate: 0.004817, scenario: 0, slope: -0.08022365586280529, fluctuations: 0.05\n",
      "step: 53350 loss: 63.433176 time elapsed: 66.7026 learning rate: 0.004817, scenario: 0, slope: -0.08045298290789664, fluctuations: 0.05\n",
      "step: 53360 loss: 62.736621 time elapsed: 66.7150 learning rate: 0.004817, scenario: 0, slope: -0.08064299045172019, fluctuations: 0.05\n",
      "step: 53370 loss: 62.071442 time elapsed: 66.7269 learning rate: 0.004817, scenario: 0, slope: -0.08063715845670473, fluctuations: 0.05\n",
      "step: 53380 loss: 61.558376 time elapsed: 66.7392 learning rate: 0.004817, scenario: 0, slope: -0.08019195118806466, fluctuations: 0.05\n",
      "step: 53390 loss: 62.009655 time elapsed: 66.7513 learning rate: 0.004817, scenario: 0, slope: -0.07546812513973584, fluctuations: 0.06\n",
      "step: 53400 loss: 60.194116 time elapsed: 66.7632 learning rate: 0.004817, scenario: 0, slope: -0.07358364352346543, fluctuations: 0.07\n",
      "step: 53410 loss: 59.758862 time elapsed: 66.7757 learning rate: 0.007052, scenario: 1, slope: -0.06588495219822235, fluctuations: 0.06\n",
      "step: 53420 loss: 2923.685288 time elapsed: 66.7880 learning rate: 0.003896, scenario: -1, slope: 11.366042014082055, fluctuations: 0.06\n",
      "step: 53430 loss: 880.951417 time elapsed: 66.7999 learning rate: 0.001358, scenario: -1, slope: 13.969961570737873, fluctuations: 0.08\n",
      "step: 53440 loss: 252.768981 time elapsed: 66.8123 learning rate: 0.000474, scenario: -1, slope: 11.699231486175385, fluctuations: 0.09\n",
      "step: 53450 loss: 167.015978 time elapsed: 66.8262 learning rate: 0.000165, scenario: -1, slope: 8.473963707133498, fluctuations: 0.1\n",
      "step: 53460 loss: 154.684882 time elapsed: 66.8405 learning rate: 0.000058, scenario: -1, slope: 5.0706833920829775, fluctuations: 0.11\n",
      "step: 53470 loss: 146.718469 time elapsed: 66.8544 learning rate: 0.000020, scenario: -1, slope: 1.3476295027485672, fluctuations: 0.11\n",
      "step: 53480 loss: 145.286717 time elapsed: 66.8678 learning rate: 0.000015, scenario: 0, slope: -2.494299058893658, fluctuations: 0.11\n",
      "step: 53490 loss: 144.417404 time elapsed: 66.8812 learning rate: 0.000015, scenario: 0, slope: -6.305740033797526, fluctuations: 0.1\n",
      "step: 53500 loss: 143.683755 time elapsed: 66.8942 learning rate: 0.000015, scenario: 0, slope: -9.893792291171387, fluctuations: 0.09\n",
      "step: 53510 loss: 143.008328 time elapsed: 66.9067 learning rate: 0.000015, scenario: 0, slope: -14.768407175915131, fluctuations: 0.08\n",
      "step: 53520 loss: 142.367272 time elapsed: 66.9187 learning rate: 0.000015, scenario: 0, slope: -5.266434759925946, fluctuations: 0.06\n",
      "step: 53530 loss: 141.752182 time elapsed: 66.9307 learning rate: 0.000015, scenario: 0, slope: -1.4221228303765783, fluctuations: 0.03\n",
      "step: 53540 loss: 141.158747 time elapsed: 66.9428 learning rate: 0.000015, scenario: 0, slope: -0.49424591796384676, fluctuations: 0.01\n",
      "step: 53550 loss: 140.584015 time elapsed: 66.9548 learning rate: 0.000016, scenario: 1, slope: -0.16202785935404002, fluctuations: 0.01\n",
      "step: 53560 loss: 139.699912 time elapsed: 66.9665 learning rate: 0.000042, scenario: 1, slope: -0.08469738737091197, fluctuations: 0.0\n",
      "step: 53570 loss: 137.548445 time elapsed: 66.9779 learning rate: 0.000108, scenario: 1, slope: -0.07219533878118091, fluctuations: 0.0\n",
      "step: 53580 loss: 132.700951 time elapsed: 66.9897 learning rate: 0.000281, scenario: 1, slope: -0.08864007574328617, fluctuations: 0.0\n",
      "step: 53590 loss: 123.033342 time elapsed: 67.0016 learning rate: 0.000729, scenario: 1, slope: -0.13905693853835538, fluctuations: 0.0\n",
      "step: 53600 loss: 111.418388 time elapsed: 67.0132 learning rate: 0.000729, scenario: 0, slope: -0.22610544350107567, fluctuations: 0.0\n",
      "step: 53610 loss: 104.045697 time elapsed: 67.0277 learning rate: 0.000729, scenario: 0, slope: -0.3558562838365025, fluctuations: 0.0\n",
      "step: 53620 loss: 98.769193 time elapsed: 67.0413 learning rate: 0.000729, scenario: 0, slope: -0.46791986817081566, fluctuations: 0.0\n",
      "step: 53630 loss: 94.658133 time elapsed: 67.0545 learning rate: 0.000729, scenario: 0, slope: -0.5588133085671244, fluctuations: 0.0\n",
      "step: 53640 loss: 91.301418 time elapsed: 67.0681 learning rate: 0.000729, scenario: 0, slope: -0.6187435397387901, fluctuations: 0.0\n",
      "step: 53650 loss: 88.490556 time elapsed: 67.0812 learning rate: 0.000729, scenario: 0, slope: -0.6404976684564194, fluctuations: 0.0\n",
      "step: 53660 loss: 86.084031 time elapsed: 67.0945 learning rate: 0.000729, scenario: 0, slope: -0.6192252490985358, fluctuations: 0.0\n",
      "step: 53670 loss: 83.986094 time elapsed: 67.1064 learning rate: 0.000729, scenario: 0, slope: -0.5560725856027491, fluctuations: 0.0\n",
      "step: 53680 loss: 82.131594 time elapsed: 67.1185 learning rate: 0.000729, scenario: 0, slope: -0.46168156053372944, fluctuations: 0.0\n",
      "step: 53690 loss: 80.474928 time elapsed: 67.1303 learning rate: 0.000729, scenario: 0, slope: -0.36172247673293234, fluctuations: 0.0\n",
      "step: 53700 loss: 78.983129 time elapsed: 67.1422 learning rate: 0.000729, scenario: 0, slope: -0.29664678330653277, fluctuations: 0.0\n",
      "step: 53710 loss: 77.631453 time elapsed: 67.1545 learning rate: 0.000729, scenario: 0, slope: -0.24439911136142425, fluctuations: 0.0\n",
      "step: 53720 loss: 76.400575 time elapsed: 67.1661 learning rate: 0.000729, scenario: 0, slope: -0.21056557688811903, fluctuations: 0.0\n",
      "step: 53730 loss: 75.275161 time elapsed: 67.1782 learning rate: 0.000729, scenario: 0, slope: -0.18454162981845593, fluctuations: 0.0\n",
      "step: 53740 loss: 74.243091 time elapsed: 67.1902 learning rate: 0.000729, scenario: 0, slope: -0.1638372312703844, fluctuations: 0.0\n",
      "step: 53750 loss: 73.294825 time elapsed: 67.2015 learning rate: 0.000729, scenario: 0, slope: -0.14690745764968238, fluctuations: 0.0\n",
      "step: 53760 loss: 72.422804 time elapsed: 67.2135 learning rate: 0.000729, scenario: 0, slope: -0.13271976570995897, fluctuations: 0.0\n",
      "step: 53770 loss: 71.620934 time elapsed: 67.2254 learning rate: 0.000729, scenario: 0, slope: -0.12057925249942889, fluctuations: 0.0\n",
      "step: 53780 loss: 70.884165 time elapsed: 67.2391 learning rate: 0.000729, scenario: 0, slope: -0.11000651433172856, fluctuations: 0.0\n",
      "step: 53790 loss: 70.208098 time elapsed: 67.2531 learning rate: 0.000729, scenario: 0, slope: -0.10065923216531374, fluctuations: 0.0\n",
      "step: 53800 loss: 69.588441 time elapsed: 67.2666 learning rate: 0.000729, scenario: 0, slope: -0.09308548703297258, fluctuations: 0.0\n",
      "step: 53810 loss: 69.020374 time elapsed: 67.2803 learning rate: 0.000729, scenario: 0, slope: -0.08470321818991983, fluctuations: 0.0\n",
      "step: 53820 loss: 68.498176 time elapsed: 67.2942 learning rate: 0.000729, scenario: 0, slope: -0.07778090045192382, fluctuations: 0.0\n",
      "step: 53830 loss: 67.899893 time elapsed: 67.3078 learning rate: 0.001562, scenario: 1, slope: -0.0716126062706094, fluctuations: 0.0\n",
      "step: 53840 loss: 66.590147 time elapsed: 67.3200 learning rate: 0.004051, scenario: 1, slope: -0.06858067819352173, fluctuations: 0.0\n",
      "step: 53850 loss: 63.950680 time elapsed: 67.3318 learning rate: 0.009553, scenario: 0, slope: -0.0740165665226945, fluctuations: 0.0\n",
      "step: 53860 loss: 60.829543 time elapsed: 67.3439 learning rate: 0.009553, scenario: 0, slope: -0.09216797783235384, fluctuations: 0.0\n",
      "step: 53870 loss: 58.446751 time elapsed: 67.3556 learning rate: 0.009553, scenario: 0, slope: -0.11751511879778828, fluctuations: 0.0\n",
      "step: 53880 loss: 56.589942 time elapsed: 67.3676 learning rate: 0.009553, scenario: 0, slope: -0.14406836856052882, fluctuations: 0.0\n",
      "step: 53890 loss: 55.144895 time elapsed: 67.3795 learning rate: 0.009553, scenario: 0, slope: -0.1673986432854435, fluctuations: 0.0\n",
      "step: 53900 loss: 53.974711 time elapsed: 67.3915 learning rate: 0.009553, scenario: 0, slope: -0.1828866212202537, fluctuations: 0.0\n",
      "step: 53910 loss: 52.968908 time elapsed: 67.4040 learning rate: 0.009553, scenario: 0, slope: -0.19222730987209488, fluctuations: 0.0\n",
      "step: 53920 loss: 52.060197 time elapsed: 67.4160 learning rate: 0.009553, scenario: 0, slope: -0.18991876970665525, fluctuations: 0.0\n",
      "step: 53930 loss: 51.219544 time elapsed: 67.4277 learning rate: 0.009553, scenario: 0, slope: -0.17626518427742674, fluctuations: 0.0\n",
      "step: 53940 loss: 50.633723 time elapsed: 67.4399 learning rate: 0.009553, scenario: 0, slope: -0.15295166382904105, fluctuations: 0.0\n",
      "step: 53950 loss: 408.365177 time elapsed: 67.4553 learning rate: 0.006964, scenario: -1, slope: 0.3736014440168232, fluctuations: 0.0\n",
      "step: 53960 loss: 77.637786 time elapsed: 67.4693 learning rate: 0.002428, scenario: -1, slope: 0.28465545172423173, fluctuations: 0.04\n",
      "step: 53970 loss: 55.907205 time elapsed: 67.4826 learning rate: 0.000847, scenario: -1, slope: 0.2404243917041392, fluctuations: 0.07\n",
      "step: 53980 loss: 49.739431 time elapsed: 67.4961 learning rate: 0.000295, scenario: -1, slope: 0.16231037736422335, fluctuations: 0.09\n",
      "step: 53990 loss: 49.710020 time elapsed: 67.5101 learning rate: 0.000103, scenario: -1, slope: 0.06960160330065993, fluctuations: 0.1\n",
      "step: 54000 loss: 49.678644 time elapsed: 67.5232 learning rate: 0.000057, scenario: 1, slope: -0.015793236582922022, fluctuations: 0.1\n",
      "step: 54010 loss: 49.613945 time elapsed: 67.5366 learning rate: 0.000076, scenario: 0, slope: -0.11126422549387736, fluctuations: 0.11\n",
      "step: 54020 loss: 49.603737 time elapsed: 67.5487 learning rate: 0.000076, scenario: 0, slope: -0.20388105076983712, fluctuations: 0.11\n",
      "step: 54030 loss: 49.581615 time elapsed: 67.5607 learning rate: 0.000076, scenario: 0, slope: -0.3070550873987641, fluctuations: 0.11\n",
      "step: 54040 loss: 49.567398 time elapsed: 67.5727 learning rate: 0.000076, scenario: 0, slope: -0.43290397123415103, fluctuations: 0.11\n",
      "step: 54050 loss: 49.552292 time elapsed: 67.5843 learning rate: 0.000076, scenario: 0, slope: -0.16699886384852922, fluctuations: 0.1\n",
      "step: 54060 loss: 49.535334 time elapsed: 67.5964 learning rate: 0.000148, scenario: 1, slope: -0.03285834588989988, fluctuations: 0.06\n",
      "step: 54070 loss: 49.497060 time elapsed: 67.6082 learning rate: 0.000385, scenario: 1, slope: -0.011444330046141626, fluctuations: 0.03\n",
      "step: 54080 loss: 49.405954 time elapsed: 67.6201 learning rate: 0.000998, scenario: 1, slope: -0.003840452695573447, fluctuations: 0.02\n",
      "step: 54090 loss: 49.210225 time elapsed: 67.6318 learning rate: 0.002587, scenario: 1, slope: -0.003127785358424711, fluctuations: 0.01\n",
      "step: 54100 loss: 48.836633 time elapsed: 67.6438 learning rate: 0.006101, scenario: 1, slope: -0.0049358321189829765, fluctuations: 0.0\n",
      "step: 54110 loss: 48.149810 time elapsed: 67.6559 learning rate: 0.015824, scenario: 1, slope: -0.009554230934833945, fluctuations: 0.0\n",
      "step: 54120 loss: 58099.550799 time elapsed: 67.6700 learning rate: 0.008742, scenario: -1, slope: 171.57112238099697, fluctuations: 0.02\n",
      "step: 54130 loss: 13528.473018 time elapsed: 67.6838 learning rate: 0.003048, scenario: -1, slope: 243.25367153579734, fluctuations: 0.03\n",
      "step: 54140 loss: 6916.029361 time elapsed: 67.6973 learning rate: 0.001063, scenario: -1, slope: 227.5224456294905, fluctuations: 0.04\n",
      "step: 54150 loss: 5464.939890 time elapsed: 67.7106 learning rate: 0.000371, scenario: -1, slope: 184.71859168199646, fluctuations: 0.04\n",
      "step: 54160 loss: 5220.408876 time elapsed: 67.7239 learning rate: 0.000129, scenario: -1, slope: 134.93087597441254, fluctuations: 0.04\n",
      "step: 54170 loss: 5109.187495 time elapsed: 67.7369 learning rate: 0.000045, scenario: -1, slope: 79.72593541887423, fluctuations: 0.04\n",
      "step: 54180 loss: 5078.889360 time elapsed: 67.7495 learning rate: 0.000016, scenario: -1, slope: 18.52722443202679, fluctuations: 0.04\n",
      "step: 54190 loss: 5067.264711 time elapsed: 67.7616 learning rate: 0.000013, scenario: 0, slope: -50.31418854663983, fluctuations: 0.04\n",
      "step: 54200 loss: 5056.930750 time elapsed: 67.7731 learning rate: 0.000013, scenario: 0, slope: -120.78891554930571, fluctuations: 0.04\n",
      "step: 54210 loss: 5047.014176 time elapsed: 67.7854 learning rate: 0.000013, scenario: 0, slope: -221.4967025692842, fluctuations: 0.04\n",
      "step: 54220 loss: 5037.460117 time elapsed: 67.7972 learning rate: 0.000013, scenario: 0, slope: -99.45333841312633, fluctuations: 0.02\n",
      "step: 54230 loss: 5028.222178 time elapsed: 67.8088 learning rate: 0.000013, scenario: 0, slope: -26.246472783452106, fluctuations: 0.0\n",
      "step: 54240 loss: 5019.260686 time elapsed: 67.8207 learning rate: 0.000014, scenario: 1, slope: -6.664771435539054, fluctuations: 0.0\n",
      "step: 54250 loss: 5005.442048 time elapsed: 67.8326 learning rate: 0.000036, scenario: 1, slope: -2.5626265526531067, fluctuations: 0.0\n",
      "step: 54260 loss: 4971.511437 time elapsed: 67.8664 learning rate: 0.000094, scenario: 1, slope: -1.4022546410583825, fluctuations: 0.0\n",
      "step: 54270 loss: 4892.356817 time elapsed: 67.8798 learning rate: 0.000244, scenario: 1, slope: -1.417377595933769, fluctuations: 0.0\n",
      "step: 54280 loss: 4715.501823 time elapsed: 67.8942 learning rate: 0.000633, scenario: 1, slope: -2.2540929565697057, fluctuations: 0.0\n",
      "step: 54290 loss: 4335.494670 time elapsed: 67.9081 learning rate: 0.001643, scenario: 1, slope: -4.391028816897518, fluctuations: 0.0\n",
      "step: 54300 loss: 3793.328971 time elapsed: 67.9220 learning rate: 0.001807, scenario: 0, slope: -8.146469546432163, fluctuations: 0.0\n",
      "step: 54310 loss: 3323.550798 time elapsed: 67.9360 learning rate: 0.001807, scenario: 0, slope: -14.42553995131156, fluctuations: 0.0\n",
      "step: 54320 loss: 2963.579641 time elapsed: 67.9503 learning rate: 0.001807, scenario: 0, slope: -20.6009050349182, fluctuations: 0.0\n",
      "step: 54330 loss: 2463.911981 time elapsed: 67.9638 learning rate: 0.001807, scenario: 0, slope: -26.80810557125195, fluctuations: 0.0\n",
      "step: 54340 loss: 2172.479063 time elapsed: 67.9758 learning rate: 0.001807, scenario: 0, slope: -32.59493305770893, fluctuations: 0.0\n",
      "step: 54350 loss: 1948.522067 time elapsed: 67.9892 learning rate: 0.001807, scenario: 0, slope: -36.40024406492474, fluctuations: 0.0\n",
      "step: 54360 loss: 1765.576370 time elapsed: 68.0011 learning rate: 0.001807, scenario: 0, slope: -37.92269380187961, fluctuations: 0.0\n",
      "step: 54370 loss: 1611.719937 time elapsed: 68.0129 learning rate: 0.001807, scenario: 0, slope: -36.92590889024053, fluctuations: 0.0\n",
      "step: 54380 loss: 1489.665117 time elapsed: 68.0268 learning rate: 0.001807, scenario: 0, slope: -33.584493701730786, fluctuations: 0.0\n",
      "step: 54390 loss: 1389.969922 time elapsed: 68.0393 learning rate: 0.001807, scenario: 0, slope: -28.725738554279268, fluctuations: 0.0\n",
      "step: 54400 loss: 1304.369881 time elapsed: 68.0512 learning rate: 0.001807, scenario: 0, slope: -24.383805312968228, fluctuations: 0.0\n",
      "step: 54410 loss: 1228.425067 time elapsed: 68.0639 learning rate: 0.001807, scenario: 0, slope: -19.61572156725309, fluctuations: 0.0\n",
      "step: 54420 loss: 1160.301337 time elapsed: 68.0757 learning rate: 0.001807, scenario: 0, slope: -15.636365776245281, fluctuations: 0.0\n",
      "step: 54430 loss: 1099.021727 time elapsed: 68.0890 learning rate: 0.001807, scenario: 0, slope: -12.412316350478763, fluctuations: 0.0\n",
      "step: 54440 loss: 1043.990238 time elapsed: 68.1029 learning rate: 0.001807, scenario: 0, slope: -10.466507850935656, fluctuations: 0.0\n",
      "step: 54450 loss: 994.523811 time elapsed: 68.1171 learning rate: 0.001807, scenario: 0, slope: -8.86359293594568, fluctuations: 0.0\n",
      "step: 54460 loss: 949.938636 time elapsed: 68.1309 learning rate: 0.001807, scenario: 0, slope: -7.6463921669070665, fluctuations: 0.0\n",
      "step: 54470 loss: 909.632301 time elapsed: 68.1444 learning rate: 0.001807, scenario: 0, slope: -6.7010100773495065, fluctuations: 0.0\n",
      "step: 54480 loss: 873.070800 time elapsed: 68.1582 learning rate: 0.001807, scenario: 0, slope: -5.965382161896848, fluctuations: 0.0\n",
      "step: 54490 loss: 839.774767 time elapsed: 68.1711 learning rate: 0.001807, scenario: 0, slope: -5.353642921781984, fluctuations: 0.0\n",
      "step: 54500 loss: 809.315495 time elapsed: 68.1834 learning rate: 0.001807, scenario: 0, slope: -4.872949692094258, fluctuations: 0.0\n",
      "step: 54510 loss: 781.313095 time elapsed: 68.1960 learning rate: 0.001807, scenario: 0, slope: -4.3553655964053295, fluctuations: 0.0\n",
      "step: 54520 loss: 755.440075 time elapsed: 68.2079 learning rate: 0.001807, scenario: 0, slope: -3.9434662079803204, fluctuations: 0.0\n",
      "step: 54530 loss: 731.440918 time elapsed: 68.2198 learning rate: 0.001807, scenario: 0, slope: -3.582403178702657, fluctuations: 0.0\n",
      "step: 54540 loss: 709.147890 time elapsed: 68.2314 learning rate: 0.001807, scenario: 0, slope: -3.266922494466601, fluctuations: 0.0\n",
      "step: 54550 loss: 688.437580 time elapsed: 68.2429 learning rate: 0.001807, scenario: 0, slope: -2.991017669551478, fluctuations: 0.0\n",
      "step: 54560 loss: 669.158530 time elapsed: 68.2547 learning rate: 0.001807, scenario: 0, slope: -2.7489035801748836, fluctuations: 0.0\n",
      "step: 54570 loss: 651.129077 time elapsed: 68.2668 learning rate: 0.001807, scenario: 0, slope: -2.535545430823212, fluctuations: 0.0\n",
      "step: 54580 loss: 634.176850 time elapsed: 68.2785 learning rate: 0.001807, scenario: 0, slope: -2.3467415525663005, fluctuations: 0.0\n",
      "step: 54590 loss: 618.156588 time elapsed: 68.2907 learning rate: 0.001807, scenario: 0, slope: -2.1790750843902305, fluctuations: 0.0\n",
      "step: 54600 loss: 602.947659 time elapsed: 68.3055 learning rate: 0.001807, scenario: 0, slope: -2.0439982114714934, fluctuations: 0.0\n",
      "step: 54610 loss: 588.446722 time elapsed: 68.3199 learning rate: 0.001807, scenario: 0, slope: -1.8969911467786724, fluctuations: 0.0\n",
      "step: 54620 loss: 574.560577 time elapsed: 68.3333 learning rate: 0.001807, scenario: 0, slope: -1.7790854978208528, fluctuations: 0.0\n",
      "step: 54630 loss: 561.199711 time elapsed: 68.3468 learning rate: 0.001807, scenario: 0, slope: -1.675155970867639, fluctuations: 0.0\n",
      "step: 54640 loss: 548.273111 time elapsed: 68.3603 learning rate: 0.001807, scenario: 0, slope: -1.5843952027668664, fluctuations: 0.0\n",
      "step: 54650 loss: 535.690582 time elapsed: 68.3733 learning rate: 0.001807, scenario: 0, slope: -1.5058380871138382, fluctuations: 0.0\n",
      "step: 54660 loss: 523.399857 time elapsed: 68.3852 learning rate: 0.001807, scenario: 0, slope: -1.4383035149989818, fluctuations: 0.0\n",
      "step: 54670 loss: 511.485065 time elapsed: 68.3973 learning rate: 0.001807, scenario: 0, slope: -1.380213545529628, fluctuations: 0.0\n",
      "step: 54680 loss: 500.166127 time elapsed: 68.4092 learning rate: 0.001807, scenario: 0, slope: -1.329025866880365, fluctuations: 0.0\n",
      "step: 54690 loss: 489.557075 time elapsed: 68.4205 learning rate: 0.001807, scenario: 0, slope: -1.281481033987471, fluctuations: 0.0\n",
      "step: 54700 loss: 479.599813 time elapsed: 68.4318 learning rate: 0.001807, scenario: 0, slope: -1.2394596343406987, fluctuations: 0.0\n",
      "step: 54710 loss: 470.202612 time elapsed: 68.4438 learning rate: 0.001807, scenario: 0, slope: -1.1870524379322132, fluctuations: 0.0\n",
      "step: 54720 loss: 461.295735 time elapsed: 68.4557 learning rate: 0.001807, scenario: 0, slope: -1.1374036051838263, fluctuations: 0.0\n",
      "step: 54730 loss: 452.826973 time elapsed: 68.4676 learning rate: 0.001807, scenario: 0, slope: -1.0856353481831336, fluctuations: 0.0\n",
      "step: 54740 loss: 444.753508 time elapsed: 68.4794 learning rate: 0.001807, scenario: 0, slope: -1.0322824760264049, fluctuations: 0.0\n",
      "step: 54750 loss: 437.037805 time elapsed: 68.4910 learning rate: 0.001807, scenario: 0, slope: -0.978624117786419, fluctuations: 0.0\n",
      "step: 54760 loss: 429.645735 time elapsed: 68.5049 learning rate: 0.001807, scenario: 0, slope: -0.926619212472327, fluctuations: 0.0\n",
      "step: 54770 loss: 422.545969 time elapsed: 68.5184 learning rate: 0.001807, scenario: 0, slope: -0.8783604372441495, fluctuations: 0.0\n",
      "step: 54780 loss: 415.710194 time elapsed: 68.5313 learning rate: 0.001807, scenario: 0, slope: -0.8349640977254558, fluctuations: 0.0\n",
      "step: 54790 loss: 409.113385 time elapsed: 68.5440 learning rate: 0.001807, scenario: 0, slope: -0.7962150339095051, fluctuations: 0.0\n",
      "step: 54800 loss: 402.733690 time elapsed: 68.5572 learning rate: 0.001807, scenario: 0, slope: -0.7647450214305742, fluctuations: 0.0\n",
      "step: 54810 loss: 396.552064 time elapsed: 68.5720 learning rate: 0.001807, scenario: 0, slope: -0.7300371099360833, fluctuations: 0.0\n",
      "step: 54820 loss: 390.551862 time elapsed: 68.5844 learning rate: 0.001807, scenario: 0, slope: -0.7016382068320983, fluctuations: 0.0\n",
      "step: 54830 loss: 384.718482 time elapsed: 68.5984 learning rate: 0.001807, scenario: 0, slope: -0.675901815703274, fluctuations: 0.0\n",
      "step: 54840 loss: 379.039127 time elapsed: 68.6114 learning rate: 0.001807, scenario: 0, slope: -0.6525361506833582, fluctuations: 0.0\n",
      "step: 54850 loss: 373.502651 time elapsed: 68.6236 learning rate: 0.001807, scenario: 0, slope: -0.6312706596951864, fluctuations: 0.0\n",
      "step: 54860 loss: 368.099507 time elapsed: 68.6358 learning rate: 0.001807, scenario: 0, slope: -0.6118532445949186, fluctuations: 0.0\n",
      "step: 54870 loss: 362.821736 time elapsed: 68.6477 learning rate: 0.001807, scenario: 0, slope: -0.5940523273144346, fluctuations: 0.0\n",
      "step: 54880 loss: 357.662996 time elapsed: 68.6596 learning rate: 0.001807, scenario: 0, slope: -0.5776578633035949, fluctuations: 0.0\n",
      "step: 54890 loss: 352.618576 time elapsed: 68.6715 learning rate: 0.001807, scenario: 0, slope: -0.5624786972453187, fluctuations: 0.0\n",
      "step: 54900 loss: 347.685355 time elapsed: 68.6830 learning rate: 0.001807, scenario: 0, slope: -0.5497098726660974, fluctuations: 0.0\n",
      "step: 54910 loss: 342.861685 time elapsed: 68.6954 learning rate: 0.001807, scenario: 0, slope: -0.5350667666470028, fluctuations: 0.0\n",
      "step: 54920 loss: 338.147160 time elapsed: 68.7078 learning rate: 0.001807, scenario: 0, slope: -0.5225046238316229, fluctuations: 0.0\n",
      "step: 54930 loss: 333.542288 time elapsed: 68.7218 learning rate: 0.001807, scenario: 0, slope: -0.5104960661057463, fluctuations: 0.0\n",
      "step: 54940 loss: 329.048066 time elapsed: 68.7353 learning rate: 0.001807, scenario: 0, slope: -0.4988947584691718, fluctuations: 0.0\n",
      "step: 54950 loss: 324.665541 time elapsed: 68.7490 learning rate: 0.001807, scenario: 0, slope: -0.487567853812148, fluctuations: 0.0\n",
      "step: 54960 loss: 320.395387 time elapsed: 68.7625 learning rate: 0.001807, scenario: 0, slope: -0.47640190786811426, fluctuations: 0.0\n",
      "step: 54970 loss: 316.237591 time elapsed: 68.7757 learning rate: 0.001807, scenario: 0, slope: -0.46530878854049523, fluctuations: 0.0\n",
      "step: 54980 loss: 312.191256 time elapsed: 68.7887 learning rate: 0.001807, scenario: 0, slope: -0.45423025520129706, fluctuations: 0.0\n",
      "step: 54990 loss: 308.254562 time elapsed: 68.8011 learning rate: 0.001807, scenario: 0, slope: -0.44314008375441494, fluctuations: 0.0\n",
      "step: 55000 loss: 304.424810 time elapsed: 68.8132 learning rate: 0.001807, scenario: 0, slope: -0.43315240581173714, fluctuations: 0.0\n",
      "step: 55010 loss: 300.698569 time elapsed: 68.8255 learning rate: 0.001807, scenario: 0, slope: -0.4209713636993832, fluctuations: 0.0\n",
      "step: 55020 loss: 297.071889 time elapsed: 68.8371 learning rate: 0.001807, scenario: 0, slope: -0.4099772169962888, fluctuations: 0.0\n",
      "step: 55030 loss: 293.540487 time elapsed: 68.8487 learning rate: 0.001807, scenario: 0, slope: -0.3991253548423791, fluctuations: 0.0\n",
      "step: 55040 loss: 290.099907 time elapsed: 68.8607 learning rate: 0.001807, scenario: 0, slope: -0.38848428098255416, fluctuations: 0.0\n",
      "step: 55050 loss: 286.745658 time elapsed: 68.8727 learning rate: 0.001807, scenario: 0, slope: -0.37811890444789104, fluctuations: 0.0\n",
      "step: 55060 loss: 283.473307 time elapsed: 68.8845 learning rate: 0.001807, scenario: 0, slope: -0.3680849679210252, fluctuations: 0.0\n",
      "step: 55070 loss: 280.278543 time elapsed: 68.8962 learning rate: 0.001807, scenario: 0, slope: -0.3584257617873054, fluctuations: 0.0\n",
      "step: 55080 loss: 277.157205 time elapsed: 68.9074 learning rate: 0.001807, scenario: 0, slope: -0.3491710477372771, fluctuations: 0.0\n",
      "step: 55090 loss: 274.105298 time elapsed: 68.9203 learning rate: 0.001807, scenario: 0, slope: -0.3403377480427489, fluctuations: 0.0\n",
      "step: 55100 loss: 271.118988 time elapsed: 68.9341 learning rate: 0.001807, scenario: 0, slope: -0.3327532104368601, fluctuations: 0.0\n",
      "step: 55110 loss: 268.194591 time elapsed: 68.9482 learning rate: 0.001807, scenario: 0, slope: -0.3239509292055881, fluctuations: 0.0\n",
      "step: 55120 loss: 265.328553 time elapsed: 68.9619 learning rate: 0.001807, scenario: 0, slope: -0.31638686478895417, fluctuations: 0.0\n",
      "step: 55130 loss: 262.517430 time elapsed: 68.9756 learning rate: 0.001807, scenario: 0, slope: -0.30922795441443934, fluctuations: 0.0\n",
      "step: 55140 loss: 259.757859 time elapsed: 68.9885 learning rate: 0.001807, scenario: 0, slope: -0.30246088702774737, fluctuations: 0.0\n",
      "step: 55150 loss: 257.046537 time elapsed: 69.0011 learning rate: 0.001807, scenario: 0, slope: -0.29607218002625274, fluctuations: 0.0\n",
      "step: 55160 loss: 254.380188 time elapsed: 69.0134 learning rate: 0.001807, scenario: 0, slope: -0.29004922383714693, fluctuations: 0.0\n",
      "step: 55170 loss: 251.755537 time elapsed: 69.0256 learning rate: 0.001807, scenario: 0, slope: -0.28438101074768807, fluctuations: 0.0\n",
      "step: 55180 loss: 249.169273 time elapsed: 69.0372 learning rate: 0.001807, scenario: 0, slope: -0.2790586389674478, fluctuations: 0.0\n",
      "step: 55190 loss: 246.618010 time elapsed: 69.0488 learning rate: 0.001807, scenario: 0, slope: -0.27407568831308426, fluctuations: 0.0\n",
      "step: 55200 loss: 243.238479 time elapsed: 69.0605 learning rate: 0.003874, scenario: 1, slope: -0.270803632403769, fluctuations: 0.0\n",
      "step: 55210 loss: 237.946097 time elapsed: 69.0729 learning rate: 0.003874, scenario: 0, slope: -0.279518718051781, fluctuations: 0.0\n",
      "step: 55220 loss: 232.761461 time elapsed: 69.0843 learning rate: 0.003874, scenario: 0, slope: -0.30049656381504664, fluctuations: 0.0\n",
      "step: 55230 loss: 227.633197 time elapsed: 69.0959 learning rate: 0.003874, scenario: 0, slope: -0.33015156500414505, fluctuations: 0.0\n",
      "step: 55240 loss: 222.377110 time elapsed: 69.1079 learning rate: 0.003874, scenario: 0, slope: -0.3654984211612296, fluctuations: 0.0\n",
      "step: 55250 loss: 216.749740 time elapsed: 69.1199 learning rate: 0.003874, scenario: 0, slope: -0.40554525769994176, fluctuations: 0.0\n",
      "step: 55260 loss: 211.931129 time elapsed: 69.1328 learning rate: 0.003874, scenario: 0, slope: -0.44396407538271593, fluctuations: 0.0\n",
      "step: 55270 loss: 207.190978 time elapsed: 69.1463 learning rate: 0.003874, scenario: 0, slope: -0.4758618168187085, fluctuations: 0.0\n",
      "step: 55280 loss: 201.152628 time elapsed: 69.1601 learning rate: 0.003874, scenario: 0, slope: -0.4992318755687248, fluctuations: 0.0\n",
      "step: 55290 loss: 196.745069 time elapsed: 69.1737 learning rate: 0.003874, scenario: 0, slope: -0.515726399131824, fluctuations: 0.02\n",
      "step: 55300 loss: 191.336886 time elapsed: 69.1873 learning rate: 0.003874, scenario: 0, slope: -0.5195604462042465, fluctuations: 0.02\n",
      "step: 55310 loss: 186.754922 time elapsed: 69.2012 learning rate: 0.003874, scenario: 0, slope: -0.5167158161372198, fluctuations: 0.02\n",
      "step: 55320 loss: 182.797276 time elapsed: 69.2132 learning rate: 0.003874, scenario: 0, slope: -0.509225294969865, fluctuations: 0.02\n",
      "step: 55330 loss: 178.999055 time elapsed: 69.2252 learning rate: 0.003874, scenario: 0, slope: -0.49621413445195156, fluctuations: 0.02\n",
      "step: 55340 loss: 175.422838 time elapsed: 69.2372 learning rate: 0.003874, scenario: 0, slope: -0.4780314535514589, fluctuations: 0.02\n",
      "step: 55350 loss: 172.002313 time elapsed: 69.2491 learning rate: 0.003874, scenario: 0, slope: -0.4580670430456122, fluctuations: 0.02\n",
      "step: 55360 loss: 168.742541 time elapsed: 69.2611 learning rate: 0.003874, scenario: 0, slope: -0.43483955023782167, fluctuations: 0.02\n",
      "step: 55370 loss: 165.612208 time elapsed: 69.2732 learning rate: 0.003874, scenario: 0, slope: -0.40710594491127605, fluctuations: 0.02\n",
      "step: 55380 loss: 162.609461 time elapsed: 69.2849 learning rate: 0.003874, scenario: 0, slope: -0.37642788533423266, fluctuations: 0.02\n",
      "step: 55390 loss: 159.727516 time elapsed: 69.2966 learning rate: 0.003874, scenario: 0, slope: -0.35644818449909654, fluctuations: 0.0\n",
      "step: 55400 loss: 156.961017 time elapsed: 69.3087 learning rate: 0.003874, scenario: 0, slope: -0.3394196247744085, fluctuations: 0.0\n",
      "step: 55410 loss: 154.303913 time elapsed: 69.3210 learning rate: 0.003874, scenario: 0, slope: -0.3219461758930708, fluctuations: 0.0\n",
      "step: 55420 loss: 151.748910 time elapsed: 69.3329 learning rate: 0.003874, scenario: 0, slope: -0.3079721826290197, fluctuations: 0.0\n",
      "step: 55430 loss: 149.288157 time elapsed: 69.3466 learning rate: 0.003874, scenario: 0, slope: -0.2952166809102604, fluctuations: 0.0\n",
      "step: 55440 loss: 146.913779 time elapsed: 69.3606 learning rate: 0.003874, scenario: 0, slope: -0.28330851271708574, fluctuations: 0.0\n",
      "step: 55450 loss: 144.618444 time elapsed: 69.3742 learning rate: 0.003874, scenario: 0, slope: -0.27219177065972816, fluctuations: 0.0\n",
      "step: 55460 loss: 142.395613 time elapsed: 69.3878 learning rate: 0.003874, scenario: 0, slope: -0.26178999194123426, fluctuations: 0.0\n",
      "step: 55470 loss: 140.239588 time elapsed: 69.4014 learning rate: 0.003874, scenario: 0, slope: -0.25206854860266487, fluctuations: 0.0\n",
      "step: 55480 loss: 138.145443 time elapsed: 69.4153 learning rate: 0.003874, scenario: 0, slope: -0.2430316720426515, fluctuations: 0.0\n",
      "step: 55490 loss: 136.108902 time elapsed: 69.4273 learning rate: 0.003874, scenario: 0, slope: -0.23467357749715706, fluctuations: 0.0\n",
      "step: 55500 loss: 134.126215 time elapsed: 69.4386 learning rate: 0.003874, scenario: 0, slope: -0.2277063566972112, fluctuations: 0.0\n",
      "step: 55510 loss: 132.194052 time elapsed: 69.4509 learning rate: 0.003874, scenario: 0, slope: -0.21985301643997676, fluctuations: 0.0\n",
      "step: 55520 loss: 130.309412 time elapsed: 69.4629 learning rate: 0.003874, scenario: 0, slope: -0.21328390819360693, fluctuations: 0.0\n",
      "step: 55530 loss: 128.469564 time elapsed: 69.4747 learning rate: 0.003874, scenario: 0, slope: -0.2071948777749974, fluctuations: 0.0\n",
      "step: 55540 loss: 126.672002 time elapsed: 69.4863 learning rate: 0.003874, scenario: 0, slope: -0.20152902200145828, fluctuations: 0.0\n",
      "step: 55550 loss: 124.914408 time elapsed: 69.4979 learning rate: 0.003874, scenario: 0, slope: -0.19623685818465023, fluctuations: 0.0\n",
      "step: 55560 loss: 123.194626 time elapsed: 69.5093 learning rate: 0.003874, scenario: 0, slope: -0.1912770259245711, fluctuations: 0.0\n",
      "step: 55570 loss: 121.510604 time elapsed: 69.5212 learning rate: 0.003874, scenario: 0, slope: -0.1866156981888749, fluctuations: 0.0\n",
      "step: 55580 loss: 119.860275 time elapsed: 69.5332 learning rate: 0.003874, scenario: 0, slope: -0.1822257706648312, fluctuations: 0.0\n",
      "step: 55590 loss: 118.241241 time elapsed: 69.5448 learning rate: 0.003874, scenario: 0, slope: -0.17808696791230227, fluctuations: 0.0\n",
      "step: 55600 loss: 116.649961 time elapsed: 69.5577 learning rate: 0.003874, scenario: 0, slope: -0.17456770703273763, fluctuations: 0.0\n",
      "step: 55610 loss: 115.079332 time elapsed: 69.5721 learning rate: 0.003874, scenario: 0, slope: -0.17054082914901278, fluctuations: 0.0\n",
      "step: 55620 loss: 113.510169 time elapsed: 69.5857 learning rate: 0.003874, scenario: 0, slope: -0.167209255612037, fluctuations: 0.0\n",
      "step: 55630 loss: 111.872680 time elapsed: 69.5988 learning rate: 0.003874, scenario: 0, slope: -0.1644604659706378, fluctuations: 0.0\n",
      "step: 55640 loss: 109.998599 time elapsed: 69.6122 learning rate: 0.003874, scenario: 0, slope: -0.1632586282486838, fluctuations: 0.0\n",
      "step: 55650 loss: 108.465298 time elapsed: 69.6263 learning rate: 0.003874, scenario: 0, slope: -0.16303920358849125, fluctuations: 0.0\n",
      "step: 55660 loss: 107.004990 time elapsed: 69.6386 learning rate: 0.003874, scenario: 0, slope: -0.16241952834743553, fluctuations: 0.0\n",
      "step: 55670 loss: 105.624079 time elapsed: 69.6511 learning rate: 0.003874, scenario: 0, slope: -0.1611100585209982, fluctuations: 0.0\n",
      "step: 55680 loss: 104.293683 time elapsed: 69.6635 learning rate: 0.003874, scenario: 0, slope: -0.15886354441831788, fluctuations: 0.0\n",
      "step: 55690 loss: 103.004066 time elapsed: 69.6751 learning rate: 0.003874, scenario: 0, slope: -0.15560149373717835, fluctuations: 0.0\n",
      "step: 55700 loss: 101.748070 time elapsed: 69.6865 learning rate: 0.003874, scenario: 0, slope: -0.1517910197695087, fluctuations: 0.0\n",
      "step: 55710 loss: 100.517369 time elapsed: 69.6989 learning rate: 0.003874, scenario: 0, slope: -0.1460958305584632, fluctuations: 0.0\n",
      "step: 55720 loss: 99.300920 time elapsed: 69.7108 learning rate: 0.003874, scenario: 0, slope: -0.14015458381685397, fluctuations: 0.0\n",
      "step: 55730 loss: 98.079981 time elapsed: 69.7225 learning rate: 0.003874, scenario: 0, slope: -0.13406869577999034, fluctuations: 0.0\n",
      "step: 55740 loss: 96.811594 time elapsed: 69.7344 learning rate: 0.003874, scenario: 0, slope: -0.12953183876791474, fluctuations: 0.0\n",
      "step: 55750 loss: 95.364469 time elapsed: 69.7464 learning rate: 0.003874, scenario: 0, slope: -0.12747278603676981, fluctuations: 0.0\n",
      "step: 55760 loss: 93.275858 time elapsed: 69.7586 learning rate: 0.003874, scenario: 0, slope: -0.12688934689259188, fluctuations: 0.02\n",
      "step: 55770 loss: 91.135984 time elapsed: 69.7719 learning rate: 0.003874, scenario: 0, slope: -0.13272287822809203, fluctuations: 0.05\n",
      "step: 55780 loss: 88.992377 time elapsed: 69.7859 learning rate: 0.003874, scenario: 0, slope: -0.14325091436640774, fluctuations: 0.08\n",
      "step: 55790 loss: 87.150065 time elapsed: 69.7988 learning rate: 0.003874, scenario: 0, slope: -0.15699893126429107, fluctuations: 0.08\n",
      "step: 55800 loss: 85.672107 time elapsed: 69.8124 learning rate: 0.003874, scenario: 0, slope: -0.1667687145358165, fluctuations: 0.08\n",
      "step: 55810 loss: 84.397481 time elapsed: 69.8268 learning rate: 0.003874, scenario: 0, slope: -0.1742899462854029, fluctuations: 0.08\n",
      "step: 55820 loss: 83.232474 time elapsed: 69.8403 learning rate: 0.003874, scenario: 0, slope: -0.17634598841140325, fluctuations: 0.08\n",
      "step: 55830 loss: 82.522172 time elapsed: 69.8529 learning rate: 0.003874, scenario: 0, slope: -0.17258975446725824, fluctuations: 0.09\n",
      "step: 55840 loss: 81.218008 time elapsed: 69.8649 learning rate: 0.003874, scenario: 0, slope: -0.16377224949634558, fluctuations: 0.1\n",
      "step: 55850 loss: 80.162600 time elapsed: 69.8771 learning rate: 0.003874, scenario: 0, slope: -0.14836587101527576, fluctuations: 0.1\n",
      "step: 55860 loss: 79.238756 time elapsed: 69.8893 learning rate: 0.003874, scenario: 0, slope: -0.13167442964961826, fluctuations: 0.07\n",
      "step: 55870 loss: 78.288315 time elapsed: 69.9011 learning rate: 0.003874, scenario: 0, slope: -0.11910974586981345, fluctuations: 0.04\n",
      "step: 55880 loss: 77.381712 time elapsed: 69.9130 learning rate: 0.003874, scenario: 0, slope: -0.11029542795592498, fluctuations: 0.02\n",
      "step: 55890 loss: 76.495372 time elapsed: 69.9252 learning rate: 0.003874, scenario: 0, slope: -0.10364700406594611, fluctuations: 0.02\n",
      "step: 55900 loss: 75.626741 time elapsed: 69.9371 learning rate: 0.003874, scenario: 0, slope: -0.09953222522225515, fluctuations: 0.02\n",
      "step: 55910 loss: 74.781146 time elapsed: 69.9496 learning rate: 0.003874, scenario: 0, slope: -0.0960344282746735, fluctuations: 0.02\n",
      "step: 55920 loss: 74.307006 time elapsed: 69.9616 learning rate: 0.003874, scenario: 0, slope: -0.09305038143976213, fluctuations: 0.02\n",
      "step: 55930 loss: 73.297941 time elapsed: 69.9731 learning rate: 0.003874, scenario: 0, slope: -0.08908611024527258, fluctuations: 0.01\n",
      "step: 55940 loss: 72.321203 time elapsed: 69.9873 learning rate: 0.003874, scenario: 0, slope: -0.08698132512944559, fluctuations: 0.0\n",
      "step: 55950 loss: 71.500745 time elapsed: 70.0007 learning rate: 0.003874, scenario: 0, slope: -0.08535089114577352, fluctuations: 0.0\n",
      "step: 55960 loss: 70.724388 time elapsed: 70.0142 learning rate: 0.003874, scenario: 0, slope: -0.08408230381043134, fluctuations: 0.0\n",
      "step: 55970 loss: 69.960956 time elapsed: 70.0281 learning rate: 0.003874, scenario: 0, slope: -0.08294613285006366, fluctuations: 0.0\n",
      "step: 55980 loss: 69.219741 time elapsed: 70.0417 learning rate: 0.003874, scenario: 0, slope: -0.0818645803502098, fluctuations: 0.0\n",
      "step: 55990 loss: 68.490792 time elapsed: 70.0554 learning rate: 0.003874, scenario: 0, slope: -0.08078410151163563, fluctuations: 0.0\n",
      "step: 56000 loss: 67.879616 time elapsed: 70.0673 learning rate: 0.003874, scenario: 0, slope: -0.07970078036915952, fluctuations: 0.0\n",
      "step: 56010 loss: 67.107196 time elapsed: 70.0799 learning rate: 0.003874, scenario: 0, slope: -0.07689654270724917, fluctuations: 0.01\n",
      "step: 56020 loss: 66.445122 time elapsed: 70.0921 learning rate: 0.003874, scenario: 0, slope: -0.07488806000970863, fluctuations: 0.02\n",
      "step: 56030 loss: 142.687517 time elapsed: 70.1039 learning rate: 0.008305, scenario: 1, slope: -0.0219555121508874, fluctuations: 0.02\n",
      "step: 56040 loss: 274.586014 time elapsed: 70.1156 learning rate: 0.003071, scenario: -1, slope: 5.719700086723267, fluctuations: 0.05\n",
      "step: 56050 loss: 270.965434 time elapsed: 70.1275 learning rate: 0.001071, scenario: -1, slope: 6.304696505765531, fluctuations: 0.08\n",
      "step: 56060 loss: 157.261653 time elapsed: 70.1393 learning rate: 0.000373, scenario: -1, slope: 5.036958849832354, fluctuations: 0.09\n",
      "step: 56070 loss: 125.091586 time elapsed: 70.1509 learning rate: 0.000130, scenario: -1, slope: 3.522179182914661, fluctuations: 0.1\n",
      "step: 56080 loss: 121.643190 time elapsed: 70.1625 learning rate: 0.000045, scenario: -1, slope: 1.959483108620965, fluctuations: 0.11\n",
      "step: 56090 loss: 119.366069 time elapsed: 70.1742 learning rate: 0.000016, scenario: -1, slope: 0.25377255662767767, fluctuations: 0.11\n",
      "step: 56100 loss: 118.918406 time elapsed: 70.1858 learning rate: 0.000014, scenario: 0, slope: -1.3453002947287742, fluctuations: 0.11\n",
      "step: 56110 loss: 118.613253 time elapsed: 70.1999 learning rate: 0.000014, scenario: 0, slope: -3.293133482692557, fluctuations: 0.1\n",
      "step: 56120 loss: 118.341229 time elapsed: 70.2141 learning rate: 0.000014, scenario: 0, slope: -5.180867013929458, fluctuations: 0.09\n",
      "step: 56130 loss: 118.080498 time elapsed: 70.2275 learning rate: 0.000014, scenario: 0, slope: -7.70369862092047, fluctuations: 0.09\n",
      "step: 56140 loss: 117.826870 time elapsed: 70.2412 learning rate: 0.000014, scenario: 0, slope: -2.097566136639667, fluctuations: 0.05\n",
      "step: 56150 loss: 117.578504 time elapsed: 70.2548 learning rate: 0.000014, scenario: 0, slope: -0.38485230484932936, fluctuations: 0.03\n",
      "step: 56160 loss: 117.333866 time elapsed: 70.2680 learning rate: 0.000017, scenario: 1, slope: -0.14762402481154388, fluctuations: 0.01\n",
      "step: 56170 loss: 116.911183 time elapsed: 70.2803 learning rate: 0.000045, scenario: 1, slope: -0.05326259344060226, fluctuations: 0.01\n",
      "step: 56180 loss: 115.845072 time elapsed: 70.2925 learning rate: 0.000116, scenario: 1, slope: -0.033882450260708026, fluctuations: 0.0\n",
      "step: 56190 loss: 113.264036 time elapsed: 70.3043 learning rate: 0.000301, scenario: 1, slope: -0.04018831186829222, fluctuations: 0.0\n",
      "step: 56200 loss: 107.585320 time elapsed: 70.3163 learning rate: 0.000709, scenario: 1, slope: -0.06478376082052029, fluctuations: 0.0\n",
      "step: 56210 loss: 98.120619 time elapsed: 70.3288 learning rate: 0.001520, scenario: 0, slope: -0.13206508975989398, fluctuations: 0.0\n",
      "step: 56220 loss: 89.838003 time elapsed: 70.3405 learning rate: 0.001520, scenario: 0, slope: -0.2290429035439074, fluctuations: 0.0\n",
      "step: 56230 loss: 84.592958 time elapsed: 70.3521 learning rate: 0.001520, scenario: 0, slope: -0.3317053085762253, fluctuations: 0.0\n",
      "step: 56240 loss: 80.914407 time elapsed: 70.3638 learning rate: 0.001520, scenario: 0, slope: -0.42160561013117165, fluctuations: 0.0\n",
      "step: 56250 loss: 78.341305 time elapsed: 70.3756 learning rate: 0.001520, scenario: 0, slope: -0.4868892226090963, fluctuations: 0.0\n",
      "step: 56260 loss: 76.520355 time elapsed: 70.3875 learning rate: 0.001520, scenario: 0, slope: -0.519325957522119, fluctuations: 0.0\n",
      "step: 56270 loss: 75.195906 time elapsed: 70.3995 learning rate: 0.001520, scenario: 0, slope: -0.5138184157213117, fluctuations: 0.0\n",
      "step: 56280 loss: 74.186224 time elapsed: 70.4137 learning rate: 0.001520, scenario: 0, slope: -0.46952602190744425, fluctuations: 0.0\n",
      "step: 56290 loss: 73.366695 time elapsed: 70.4275 learning rate: 0.001520, scenario: 0, slope: -0.3916329325910629, fluctuations: 0.0\n",
      "step: 56300 loss: 72.282992 time elapsed: 70.4406 learning rate: 0.001520, scenario: 0, slope: -0.3056133089925216, fluctuations: 0.0\n",
      "step: 56310 loss: 71.071543 time elapsed: 70.4547 learning rate: 0.001520, scenario: 0, slope: -0.21368808592287342, fluctuations: 0.0\n",
      "step: 56320 loss: 70.334606 time elapsed: 70.4681 learning rate: 0.001520, scenario: 0, slope: -0.16154187977375717, fluctuations: 0.0\n",
      "step: 56330 loss: 69.692565 time elapsed: 70.4816 learning rate: 0.001520, scenario: 0, slope: -0.12860125318002394, fluctuations: 0.0\n",
      "step: 56340 loss: 69.104463 time elapsed: 70.4938 learning rate: 0.001520, scenario: 0, slope: -0.1074973462080489, fluctuations: 0.0\n",
      "step: 56350 loss: 68.543411 time elapsed: 70.5059 learning rate: 0.001520, scenario: 0, slope: -0.09392193078628049, fluctuations: 0.0\n",
      "step: 56360 loss: 67.997609 time elapsed: 70.5178 learning rate: 0.001520, scenario: 0, slope: -0.084732549313031, fluctuations: 0.0\n",
      "step: 56370 loss: 67.462879 time elapsed: 70.5294 learning rate: 0.001520, scenario: 0, slope: -0.077747496409384, fluctuations: 0.0\n",
      "step: 56380 loss: 66.907141 time elapsed: 70.5410 learning rate: 0.002448, scenario: 1, slope: -0.07152349359005729, fluctuations: 0.0\n",
      "step: 56390 loss: 67.527121 time elapsed: 70.5526 learning rate: 0.006350, scenario: 1, slope: -0.06436789640705287, fluctuations: 0.01\n",
      "step: 56400 loss: 240.985913 time elapsed: 70.5644 learning rate: 0.014974, scenario: 1, slope: -0.03828875468974719, fluctuations: 0.04\n",
      "step: 56410 loss: 12798.200183 time elapsed: 70.5770 learning rate: 0.005538, scenario: -1, slope: 51.07426913091327, fluctuations: 0.07\n",
      "step: 56420 loss: 4103.323367 time elapsed: 70.5888 learning rate: 0.001931, scenario: -1, slope: 60.24985522782869, fluctuations: 0.09\n",
      "step: 56430 loss: 2018.394317 time elapsed: 70.6003 learning rate: 0.000673, scenario: -1, slope: 55.982842473186004, fluctuations: 0.11\n",
      "step: 56440 loss: 1618.382681 time elapsed: 70.6139 learning rate: 0.000235, scenario: -1, slope: 45.90675687077328, fluctuations: 0.12\n",
      "step: 56450 loss: 1545.729652 time elapsed: 70.6285 learning rate: 0.000082, scenario: -1, slope: 33.0032901046125, fluctuations: 0.12\n",
      "step: 56460 loss: 1516.465181 time elapsed: 70.6425 learning rate: 0.000029, scenario: -1, slope: 19.1784485686663, fluctuations: 0.12\n",
      "step: 56470 loss: 1508.839210 time elapsed: 70.6561 learning rate: 0.000010, scenario: -1, slope: 3.3079726220051215, fluctuations: 0.12\n",
      "step: 56480 loss: 1505.898814 time elapsed: 70.6692 learning rate: 0.000009, scenario: 0, slope: -16.461367764347415, fluctuations: 0.12\n",
      "step: 56490 loss: 1503.265547 time elapsed: 70.6819 learning rate: 0.000009, scenario: 0, slope: -36.090339118862275, fluctuations: 0.1\n",
      "step: 56500 loss: 1500.771342 time elapsed: 70.6954 learning rate: 0.000009, scenario: 0, slope: -53.96472229775867, fluctuations: 0.08\n",
      "step: 56510 loss: 1498.381680 time elapsed: 70.7080 learning rate: 0.000009, scenario: 0, slope: -21.398665800326746, fluctuations: 0.04\n",
      "step: 56520 loss: 1496.074933 time elapsed: 70.7201 learning rate: 0.000009, scenario: 0, slope: -5.532405692734367, fluctuations: 0.02\n",
      "step: 56530 loss: 1493.813675 time elapsed: 70.7322 learning rate: 0.000012, scenario: 1, slope: -1.3755699078929222, fluctuations: 0.01\n",
      "step: 56540 loss: 1489.637234 time elapsed: 70.7444 learning rate: 0.000031, scenario: 1, slope: -0.6723765015355863, fluctuations: 0.0\n",
      "step: 56550 loss: 1479.400549 time elapsed: 70.7564 learning rate: 0.000080, scenario: 1, slope: -0.3653441848487275, fluctuations: 0.0\n",
      "step: 56560 loss: 1455.185950 time elapsed: 70.7682 learning rate: 0.000208, scenario: 1, slope: -0.38842314496725655, fluctuations: 0.0\n",
      "step: 56570 loss: 1398.723267 time elapsed: 70.7798 learning rate: 0.000540, scenario: 1, slope: -0.6593636192007623, fluctuations: 0.0\n",
      "step: 56580 loss: 1276.257426 time elapsed: 70.7919 learning rate: 0.001399, scenario: 1, slope: -1.348672747710168, fluctuations: 0.0\n",
      "step: 56590 loss: 1050.973470 time elapsed: 70.8032 learning rate: 0.001539, scenario: 0, slope: -2.80206188901783, fluctuations: 0.0\n",
      "step: 56600 loss: 900.164555 time elapsed: 70.8146 learning rate: 0.001539, scenario: 0, slope: -4.729673700330792, fluctuations: 0.0\n",
      "step: 56610 loss: 792.080769 time elapsed: 70.8276 learning rate: 0.001539, scenario: 0, slope: -7.142145683804851, fluctuations: 0.0\n",
      "step: 56620 loss: 711.376215 time elapsed: 70.8403 learning rate: 0.001539, scenario: 0, slope: -9.042007036333182, fluctuations: 0.0\n",
      "step: 56630 loss: 651.158179 time elapsed: 70.8533 learning rate: 0.001539, scenario: 0, slope: -10.423119374060354, fluctuations: 0.0\n",
      "step: 56640 loss: 602.820683 time elapsed: 70.8663 learning rate: 0.001539, scenario: 0, slope: -11.131074194535268, fluctuations: 0.0\n",
      "step: 56650 loss: 562.639530 time elapsed: 70.8798 learning rate: 0.001539, scenario: 0, slope: -11.076619480183808, fluctuations: 0.0\n",
      "step: 56660 loss: 527.887307 time elapsed: 70.8928 learning rate: 0.001539, scenario: 0, slope: -10.246766095817442, fluctuations: 0.0\n",
      "step: 56670 loss: 496.923670 time elapsed: 70.9067 learning rate: 0.001539, scenario: 0, slope: -8.742066071574756, fluctuations: 0.0\n",
      "step: 56680 loss: 468.523097 time elapsed: 70.9196 learning rate: 0.001539, scenario: 0, slope: -6.878535228263188, fluctuations: 0.0\n",
      "step: 56690 loss: 441.392123 time elapsed: 70.9346 learning rate: 0.001539, scenario: 0, slope: -5.272743328393769, fluctuations: 0.0\n",
      "step: 56700 loss: 417.736768 time elapsed: 70.9460 learning rate: 0.001539, scenario: 0, slope: -4.377904381322792, fluctuations: 0.0\n",
      "step: 56710 loss: 397.053281 time elapsed: 70.9582 learning rate: 0.001539, scenario: 0, slope: -3.6275421025456627, fluctuations: 0.0\n",
      "step: 56720 loss: 379.832240 time elapsed: 70.9698 learning rate: 0.001539, scenario: 0, slope: -3.1476853926508817, fluctuations: 0.0\n",
      "step: 56730 loss: 366.155497 time elapsed: 70.9817 learning rate: 0.001539, scenario: 0, slope: -2.773369815475838, fluctuations: 0.0\n",
      "step: 56740 loss: 355.214998 time elapsed: 70.9934 learning rate: 0.001539, scenario: 0, slope: -2.4530653631773656, fluctuations: 0.0\n",
      "step: 56750 loss: 345.849834 time elapsed: 71.0050 learning rate: 0.001539, scenario: 0, slope: -2.157163951635129, fluctuations: 0.0\n",
      "step: 56760 loss: 337.255358 time elapsed: 71.0164 learning rate: 0.001539, scenario: 0, slope: -1.8779514946899785, fluctuations: 0.0\n",
      "step: 56770 loss: 329.008805 time elapsed: 71.0279 learning rate: 0.001539, scenario: 0, slope: -1.6167545167681825, fluctuations: 0.0\n",
      "step: 56780 loss: 321.033062 time elapsed: 71.0403 learning rate: 0.001539, scenario: 0, slope: -1.380622342730546, fluctuations: 0.0\n",
      "step: 56790 loss: 313.100884 time elapsed: 71.0546 learning rate: 0.001539, scenario: 0, slope: -1.1817528507882695, fluctuations: 0.0\n",
      "step: 56800 loss: 304.819972 time elapsed: 71.0678 learning rate: 0.001539, scenario: 0, slope: -1.043638126402967, fluctuations: 0.0\n",
      "step: 56810 loss: 295.441714 time elapsed: 71.0822 learning rate: 0.001539, scenario: 0, slope: -0.9295224491633602, fluctuations: 0.0\n",
      "step: 56820 loss: 285.054208 time elapsed: 71.0957 learning rate: 0.001539, scenario: 0, slope: -0.8798090628125883, fluctuations: 0.0\n",
      "step: 56830 loss: 276.393153 time elapsed: 71.1093 learning rate: 0.001539, scenario: 0, slope: -0.8645131882445993, fluctuations: 0.0\n",
      "step: 56840 loss: 267.545203 time elapsed: 71.1231 learning rate: 0.001539, scenario: 0, slope: -0.8665565665200542, fluctuations: 0.0\n",
      "step: 56850 loss: 258.762359 time elapsed: 71.1354 learning rate: 0.001539, scenario: 0, slope: -0.875628690552837, fluctuations: 0.0\n",
      "step: 56860 loss: 249.798549 time elapsed: 71.1473 learning rate: 0.001539, scenario: 0, slope: -0.8868926102931298, fluctuations: 0.0\n",
      "step: 56870 loss: 241.239410 time elapsed: 71.1594 learning rate: 0.001539, scenario: 0, slope: -0.8963554310067487, fluctuations: 0.0\n",
      "step: 56880 loss: 233.953693 time elapsed: 71.1711 learning rate: 0.001539, scenario: 0, slope: -0.8966252274832295, fluctuations: 0.0\n",
      "step: 56890 loss: 227.633786 time elapsed: 71.1829 learning rate: 0.001539, scenario: 0, slope: -0.8805427340721703, fluctuations: 0.0\n",
      "step: 56900 loss: 221.844330 time elapsed: 71.1945 learning rate: 0.001539, scenario: 0, slope: -0.8505897775935255, fluctuations: 0.0\n",
      "step: 56910 loss: 216.449775 time elapsed: 71.2070 learning rate: 0.001539, scenario: 0, slope: -0.7995894649516142, fluctuations: 0.0\n",
      "step: 56920 loss: 211.398627 time elapsed: 71.2189 learning rate: 0.001539, scenario: 0, slope: -0.7500553244923648, fluctuations: 0.0\n",
      "step: 56930 loss: 206.590399 time elapsed: 71.2304 learning rate: 0.001539, scenario: 0, slope: -0.6980862298532073, fluctuations: 0.0\n",
      "step: 56940 loss: 201.941521 time elapsed: 71.2424 learning rate: 0.001539, scenario: 0, slope: -0.6437504212482522, fluctuations: 0.0\n",
      "step: 56950 loss: 197.397564 time elapsed: 71.2542 learning rate: 0.001539, scenario: 0, slope: -0.5908777920861578, fluctuations: 0.0\n",
      "step: 56960 loss: 192.900281 time elapsed: 71.2677 learning rate: 0.001539, scenario: 0, slope: -0.545167753159039, fluctuations: 0.0\n",
      "step: 56970 loss: 188.376170 time elapsed: 71.2816 learning rate: 0.001539, scenario: 0, slope: -0.5113333884961297, fluctuations: 0.0\n",
      "step: 56980 loss: 183.742168 time elapsed: 71.2955 learning rate: 0.001539, scenario: 0, slope: -0.48863239929352725, fluctuations: 0.0\n",
      "step: 56990 loss: 179.056592 time elapsed: 71.3086 learning rate: 0.001539, scenario: 0, slope: -0.4742314923368702, fluctuations: 0.0\n",
      "step: 57000 loss: 174.398466 time elapsed: 71.3221 learning rate: 0.001539, scenario: 0, slope: -0.466214312194032, fluctuations: 0.0\n",
      "step: 57010 loss: 169.852130 time elapsed: 71.3368 learning rate: 0.001539, scenario: 0, slope: -0.4612263362290466, fluctuations: 0.0\n",
      "step: 57020 loss: 165.405863 time elapsed: 71.3490 learning rate: 0.001539, scenario: 0, slope: -0.45900664938751856, fluctuations: 0.0\n",
      "step: 57030 loss: 161.077891 time elapsed: 71.3614 learning rate: 0.001539, scenario: 0, slope: -0.4573767054973757, fluctuations: 0.0\n",
      "step: 57040 loss: 156.900349 time elapsed: 71.3735 learning rate: 0.001539, scenario: 0, slope: -0.45499911333201853, fluctuations: 0.0\n",
      "step: 57050 loss: 152.808357 time elapsed: 71.3855 learning rate: 0.001539, scenario: 0, slope: -0.4508477658110826, fluctuations: 0.0\n",
      "step: 57060 loss: 148.873069 time elapsed: 71.3973 learning rate: 0.001539, scenario: 0, slope: -0.44438739748644074, fluctuations: 0.0\n",
      "step: 57070 loss: 145.003267 time elapsed: 71.4094 learning rate: 0.001539, scenario: 0, slope: -0.4355911639580063, fluctuations: 0.0\n",
      "step: 57080 loss: 141.132381 time elapsed: 71.4214 learning rate: 0.001539, scenario: 0, slope: -0.4254680304522303, fluctuations: 0.0\n",
      "step: 57090 loss: 137.355252 time elapsed: 71.4332 learning rate: 0.001539, scenario: 0, slope: -0.4153059935047332, fluctuations: 0.0\n",
      "step: 57100 loss: 134.018053 time elapsed: 71.4447 learning rate: 0.001539, scenario: 0, slope: -0.40563270346556685, fluctuations: 0.0\n",
      "step: 57110 loss: 130.952664 time elapsed: 71.4564 learning rate: 0.001539, scenario: 0, slope: -0.391792460933528, fluctuations: 0.0\n",
      "step: 57120 loss: 128.109949 time elapsed: 71.4682 learning rate: 0.001539, scenario: 0, slope: -0.37720324229767876, fluctuations: 0.0\n",
      "step: 57130 loss: 125.453790 time elapsed: 71.4813 learning rate: 0.001539, scenario: 0, slope: -0.36061481735267453, fluctuations: 0.0\n",
      "step: 57140 loss: 122.997281 time elapsed: 71.4953 learning rate: 0.001539, scenario: 0, slope: -0.3423505892863212, fluctuations: 0.0\n",
      "step: 57150 loss: 120.597611 time elapsed: 71.5088 learning rate: 0.001539, scenario: 0, slope: -0.32274463385960495, fluctuations: 0.0\n",
      "step: 57160 loss: 118.348623 time elapsed: 71.5222 learning rate: 0.001539, scenario: 0, slope: -0.30245227554243787, fluctuations: 0.0\n",
      "step: 57170 loss: 116.235413 time elapsed: 71.5359 learning rate: 0.001539, scenario: 0, slope: -0.28213297534889015, fluctuations: 0.0\n",
      "step: 57180 loss: 114.186475 time elapsed: 71.5494 learning rate: 0.001539, scenario: 0, slope: -0.26309763914172085, fluctuations: 0.0\n",
      "step: 57190 loss: 112.320414 time elapsed: 71.5612 learning rate: 0.001539, scenario: 0, slope: -0.24672747708168485, fluctuations: 0.0\n",
      "step: 57200 loss: 110.375875 time elapsed: 71.5726 learning rate: 0.001539, scenario: 0, slope: -0.23408804284543325, fluctuations: 0.0\n",
      "step: 57210 loss: 108.586851 time elapsed: 71.5849 learning rate: 0.001539, scenario: 0, slope: -0.22070620360797863, fluctuations: 0.0\n",
      "step: 57220 loss: 106.847753 time elapsed: 71.5967 learning rate: 0.001539, scenario: 0, slope: -0.2101385697420893, fluctuations: 0.0\n",
      "step: 57230 loss: 105.159595 time elapsed: 71.6084 learning rate: 0.001539, scenario: 0, slope: -0.20072275711128393, fluctuations: 0.0\n",
      "step: 57240 loss: 103.519769 time elapsed: 71.6204 learning rate: 0.001539, scenario: 0, slope: -0.19238638848840695, fluctuations: 0.0\n",
      "step: 57250 loss: 101.915935 time elapsed: 71.6323 learning rate: 0.001539, scenario: 0, slope: -0.18499325929257307, fluctuations: 0.0\n",
      "step: 57260 loss: 100.332206 time elapsed: 71.6441 learning rate: 0.001539, scenario: 0, slope: -0.17857447305621463, fluctuations: 0.0\n",
      "step: 57270 loss: 98.729499 time elapsed: 71.6558 learning rate: 0.001539, scenario: 0, slope: -0.17313776468208122, fluctuations: 0.0\n",
      "step: 57280 loss: 97.048994 time elapsed: 71.6674 learning rate: 0.001539, scenario: 0, slope: -0.16887589140147125, fluctuations: 0.0\n",
      "step: 57290 loss: 94.924759 time elapsed: 71.6790 learning rate: 0.001539, scenario: 0, slope: -0.16749300611698378, fluctuations: 0.0\n",
      "step: 57300 loss: 92.466170 time elapsed: 71.6920 learning rate: 0.001539, scenario: 0, slope: -0.1696885698915141, fluctuations: 0.0\n",
      "step: 57310 loss: 90.096695 time elapsed: 71.7065 learning rate: 0.001539, scenario: 0, slope: -0.17640416325487301, fluctuations: 0.01\n",
      "step: 57320 loss: 87.298368 time elapsed: 71.7202 learning rate: 0.001539, scenario: 0, slope: -0.18742573516508054, fluctuations: 0.02\n",
      "step: 57330 loss: 84.784706 time elapsed: 71.7335 learning rate: 0.001539, scenario: 0, slope: -0.20151705537551676, fluctuations: 0.02\n",
      "step: 57340 loss: 82.504850 time elapsed: 71.7471 learning rate: 0.001539, scenario: 0, slope: -0.2151549170098756, fluctuations: 0.02\n",
      "step: 57350 loss: 80.482316 time elapsed: 71.7617 learning rate: 0.001539, scenario: 0, slope: -0.2258988814366955, fluctuations: 0.02\n",
      "step: 57360 loss: 78.702314 time elapsed: 71.7746 learning rate: 0.001539, scenario: 0, slope: -0.23171214808447918, fluctuations: 0.02\n",
      "step: 57370 loss: 77.073469 time elapsed: 71.7873 learning rate: 0.001539, scenario: 0, slope: -0.23137582524311817, fluctuations: 0.02\n",
      "step: 57380 loss: 75.645440 time elapsed: 71.7999 learning rate: 0.001539, scenario: 0, slope: -0.22366650019090153, fluctuations: 0.02\n",
      "step: 57390 loss: 74.143778 time elapsed: 71.8117 learning rate: 0.001539, scenario: 0, slope: -0.21101141616917968, fluctuations: 0.02\n",
      "step: 57400 loss: 72.739603 time elapsed: 71.8233 learning rate: 0.001539, scenario: 0, slope: -0.19705193673653684, fluctuations: 0.02\n",
      "step: 57410 loss: 71.388748 time elapsed: 71.8363 learning rate: 0.001539, scenario: 0, slope: -0.18052577778185513, fluctuations: 0.0\n",
      "step: 57420 loss: 70.100944 time elapsed: 71.8482 learning rate: 0.001539, scenario: 0, slope: -0.1657482657852491, fluctuations: 0.0\n",
      "step: 57430 loss: 69.025712 time elapsed: 71.8604 learning rate: 0.001539, scenario: 0, slope: -0.15381296639382763, fluctuations: 0.0\n",
      "step: 57440 loss: 67.528044 time elapsed: 71.8725 learning rate: 0.001539, scenario: 0, slope: -0.1447394725134192, fluctuations: 0.0\n",
      "step: 57450 loss: 66.231597 time elapsed: 71.8843 learning rate: 0.001539, scenario: 0, slope: -0.13871855181642997, fluctuations: 0.0\n",
      "step: 57460 loss: 64.913379 time elapsed: 71.8967 learning rate: 0.001539, scenario: 0, slope: -0.13493731918251053, fluctuations: 0.0\n",
      "step: 57470 loss: 63.542897 time elapsed: 71.9102 learning rate: 0.001539, scenario: 0, slope: -0.13296185307412262, fluctuations: 0.0\n",
      "step: 57480 loss: 62.284140 time elapsed: 71.9243 learning rate: 0.001539, scenario: 0, slope: -0.13135158615930273, fluctuations: 0.0\n",
      "step: 57490 loss: 60.893733 time elapsed: 71.9378 learning rate: 0.001539, scenario: 0, slope: -0.13124765334567262, fluctuations: 0.0\n",
      "step: 57500 loss: 59.580448 time elapsed: 71.9510 learning rate: 0.001539, scenario: 0, slope: -0.13179157384167198, fluctuations: 0.0\n",
      "step: 57510 loss: 58.237300 time elapsed: 71.9646 learning rate: 0.001539, scenario: 0, slope: -0.13250534212295692, fluctuations: 0.0\n",
      "step: 57520 loss: 57.019212 time elapsed: 71.9770 learning rate: 0.001539, scenario: 0, slope: -0.13303855889484595, fluctuations: 0.0\n",
      "step: 57530 loss: 55.846762 time elapsed: 71.9890 learning rate: 0.001539, scenario: 0, slope: -0.13196594670639403, fluctuations: 0.0\n",
      "step: 57540 loss: 54.392519 time elapsed: 72.0008 learning rate: 0.001539, scenario: 0, slope: -0.13119778597198517, fluctuations: 0.0\n",
      "step: 57550 loss: 53.138123 time elapsed: 72.0127 learning rate: 0.001539, scenario: 0, slope: -0.13055113518767664, fluctuations: 0.0\n",
      "step: 57560 loss: 52.069850 time elapsed: 72.0243 learning rate: 0.001539, scenario: 0, slope: -0.12922372541657262, fluctuations: 0.0\n",
      "step: 57570 loss: 51.047577 time elapsed: 72.0365 learning rate: 0.001539, scenario: 0, slope: -0.12695897225059286, fluctuations: 0.0\n",
      "step: 57580 loss: 50.094241 time elapsed: 72.0484 learning rate: 0.001539, scenario: 0, slope: -0.12333790429530037, fluctuations: 0.0\n",
      "step: 57590 loss: 49.195947 time elapsed: 72.0604 learning rate: 0.001539, scenario: 0, slope: -0.1190238058423576, fluctuations: 0.0\n",
      "step: 57600 loss: 48.344760 time elapsed: 72.0724 learning rate: 0.001539, scenario: 0, slope: -0.11451754191883623, fluctuations: 0.0\n",
      "step: 57610 loss: 47.536127 time elapsed: 72.0849 learning rate: 0.001539, scenario: 0, slope: -0.10822500833813595, fluctuations: 0.0\n",
      "step: 57620 loss: 46.774518 time elapsed: 72.0968 learning rate: 0.001539, scenario: 0, slope: -0.10216469552727925, fluctuations: 0.0\n",
      "step: 57630 loss: 46.057025 time elapsed: 72.1100 learning rate: 0.001539, scenario: 0, slope: -0.09478130743555692, fluctuations: 0.0\n",
      "step: 57640 loss: 45.381741 time elapsed: 72.1239 learning rate: 0.001539, scenario: 0, slope: -0.0885441997470687, fluctuations: 0.0\n",
      "step: 57650 loss: 44.745723 time elapsed: 72.1372 learning rate: 0.001539, scenario: 0, slope: -0.08326561031981793, fluctuations: 0.0\n",
      "step: 57660 loss: 44.220529 time elapsed: 72.1506 learning rate: 0.001539, scenario: 0, slope: -0.07845721777749051, fluctuations: 0.0\n",
      "step: 57670 loss: 43.639344 time elapsed: 72.1640 learning rate: 0.001539, scenario: 0, slope: -0.07342855103215075, fluctuations: 0.0\n",
      "step: 57680 loss: 43.052473 time elapsed: 72.1776 learning rate: 0.001539, scenario: 0, slope: -0.06920089237142794, fluctuations: 0.0\n",
      "step: 57690 loss: 42.550790 time elapsed: 72.1922 learning rate: 0.001539, scenario: 0, slope: -0.06539709578440155, fluctuations: 0.0\n",
      "step: 57700 loss: 42.054386 time elapsed: 72.2054 learning rate: 0.001539, scenario: 0, slope: -0.06219657426636423, fluctuations: 0.0\n",
      "step: 57710 loss: 41.590734 time elapsed: 72.2183 learning rate: 0.001539, scenario: 0, slope: -0.0586255118855636, fluctuations: 0.0\n",
      "step: 57720 loss: 41.146828 time elapsed: 72.2306 learning rate: 0.001539, scenario: 0, slope: -0.055689609659201694, fluctuations: 0.0\n",
      "step: 57730 loss: 40.721251 time elapsed: 72.2440 learning rate: 0.001539, scenario: 0, slope: -0.05304043812572929, fluctuations: 0.0\n",
      "step: 57740 loss: 40.313218 time elapsed: 72.2561 learning rate: 0.001539, scenario: 0, slope: -0.05066252697104817, fluctuations: 0.0\n",
      "step: 57750 loss: 39.919128 time elapsed: 72.2680 learning rate: 0.001539, scenario: 0, slope: -0.048536137821350016, fluctuations: 0.0\n",
      "step: 57760 loss: 39.540134 time elapsed: 72.2797 learning rate: 0.001539, scenario: 0, slope: -0.04652731545897724, fluctuations: 0.0\n",
      "step: 57770 loss: 39.175283 time elapsed: 72.2916 learning rate: 0.001539, scenario: 0, slope: -0.04397513845013384, fluctuations: 0.0\n",
      "step: 57780 loss: 70.483550 time elapsed: 72.3031 learning rate: 0.003300, scenario: 1, slope: -0.021802832913206952, fluctuations: 0.0\n",
      "step: 57790 loss: 59.159800 time elapsed: 72.3154 learning rate: 0.001220, scenario: -1, slope: 0.10759442522847935, fluctuations: 0.03\n",
      "step: 57800 loss: 40.949868 time elapsed: 72.3295 learning rate: 0.000473, scenario: -1, slope: 0.10719644196834618, fluctuations: 0.06\n",
      "step: 57810 loss: 39.695948 time elapsed: 72.3439 learning rate: 0.000165, scenario: -1, slope: 0.07666133293394005, fluctuations: 0.07\n",
      "step: 57820 loss: 39.094063 time elapsed: 72.3575 learning rate: 0.000057, scenario: -1, slope: 0.04094379765681402, fluctuations: 0.07\n",
      "step: 57830 loss: 38.938625 time elapsed: 72.3712 learning rate: 0.000020, scenario: -1, slope: 0.007312591511720613, fluctuations: 0.07\n",
      "step: 57840 loss: 38.883561 time elapsed: 72.3845 learning rate: 0.000033, scenario: 1, slope: -0.025575921958425833, fluctuations: 0.07\n",
      "step: 57850 loss: 38.848326 time elapsed: 72.3975 learning rate: 0.000049, scenario: 0, slope: -0.05960569664031653, fluctuations: 0.07\n",
      "step: 57860 loss: 38.794100 time elapsed: 72.4099 learning rate: 0.000049, scenario: 0, slope: -0.09722744377935807, fluctuations: 0.07\n",
      "step: 57870 loss: 38.752570 time elapsed: 72.4221 learning rate: 0.000049, scenario: 0, slope: -0.14139346589766907, fluctuations: 0.07\n",
      "step: 57880 loss: 38.711492 time elapsed: 72.4341 learning rate: 0.000049, scenario: 0, slope: -0.26204499429523587, fluctuations: 0.06\n",
      "step: 57890 loss: 38.673117 time elapsed: 72.4460 learning rate: 0.000065, scenario: 1, slope: -0.036572188684623516, fluctuations: 0.03\n",
      "step: 57900 loss: 38.604027 time elapsed: 72.4574 learning rate: 0.000153, scenario: 1, slope: -0.013172816409612279, fluctuations: 0.01\n",
      "step: 57910 loss: 38.457822 time elapsed: 72.4692 learning rate: 0.000398, scenario: 1, slope: -0.0064684557802020285, fluctuations: 0.0\n",
      "step: 57920 loss: 38.165825 time elapsed: 72.4806 learning rate: 0.001032, scenario: 1, slope: -0.006323712925494597, fluctuations: 0.0\n",
      "step: 57930 loss: 37.713514 time elapsed: 72.4928 learning rate: 0.002677, scenario: 1, slope: -0.008837222090549393, fluctuations: 0.0\n",
      "step: 57940 loss: 36.995315 time elapsed: 72.5047 learning rate: 0.006944, scenario: 1, slope: -0.013808179871368056, fluctuations: 0.0\n",
      "step: 57950 loss: 17976.047818 time elapsed: 72.5166 learning rate: 0.005730, scenario: -1, slope: 32.940422305377815, fluctuations: 0.01\n",
      "step: 57960 loss: 1613.125353 time elapsed: 72.5306 learning rate: 0.001998, scenario: -1, slope: 43.15819616222547, fluctuations: 0.04\n",
      "step: 57970 loss: 1055.826634 time elapsed: 72.5446 learning rate: 0.000697, scenario: -1, slope: 41.20566583937515, fluctuations: 0.06\n",
      "step: 57980 loss: 518.943319 time elapsed: 72.5584 learning rate: 0.000243, scenario: -1, slope: 31.235037742457866, fluctuations: 0.07\n",
      "step: 57990 loss: 344.349712 time elapsed: 72.5720 learning rate: 0.000085, scenario: -1, slope: 19.235443846598788, fluctuations: 0.07\n",
      "step: 58000 loss: 333.853004 time elapsed: 72.5856 learning rate: 0.000033, scenario: -1, slope: 8.762917233475614, fluctuations: 0.07\n",
      "step: 58010 loss: 329.691201 time elapsed: 72.5995 learning rate: 0.000017, scenario: 0, slope: -3.9372138334634745, fluctuations: 0.07\n",
      "step: 58020 loss: 326.463328 time elapsed: 72.6121 learning rate: 0.000017, scenario: 0, slope: -16.039030942432912, fluctuations: 0.07\n",
      "step: 58030 loss: 323.672226 time elapsed: 72.6251 learning rate: 0.000017, scenario: 0, slope: -29.393413324620308, fluctuations: 0.07\n",
      "step: 58040 loss: 321.327979 time elapsed: 72.6377 learning rate: 0.000017, scenario: 0, slope: -44.9156744548827, fluctuations: 0.07\n",
      "step: 58050 loss: 319.284006 time elapsed: 72.6498 learning rate: 0.000017, scenario: 0, slope: -30.11693817213413, fluctuations: 0.05\n",
      "step: 58060 loss: 317.416789 time elapsed: 72.6618 learning rate: 0.000017, scenario: 0, slope: -6.84105630278173, fluctuations: 0.03\n",
      "step: 58070 loss: 315.653476 time elapsed: 72.6736 learning rate: 0.000017, scenario: 0, slope: -1.5108191683664838, fluctuations: 0.01\n",
      "step: 58080 loss: 313.956575 time elapsed: 72.6855 learning rate: 0.000017, scenario: 0, slope: -0.5938974212835062, fluctuations: 0.0\n",
      "step: 58090 loss: 311.902322 time elapsed: 72.6973 learning rate: 0.000037, scenario: 1, slope: -0.25056300971816586, fluctuations: 0.0\n",
      "step: 58100 loss: 307.003852 time elapsed: 72.7090 learning rate: 0.000088, scenario: 1, slope: -0.22595582781928317, fluctuations: 0.0\n",
      "step: 58110 loss: 296.283503 time elapsed: 72.7215 learning rate: 0.000229, scenario: 1, slope: -0.24769891071783307, fluctuations: 0.0\n",
      "step: 58120 loss: 272.477440 time elapsed: 72.7340 learning rate: 0.000490, scenario: 0, slope: -0.35135353398784896, fluctuations: 0.0\n",
      "step: 58130 loss: 244.019693 time elapsed: 72.7479 learning rate: 0.000490, scenario: 0, slope: -0.5769348918848536, fluctuations: 0.0\n",
      "step: 58140 loss: 221.880813 time elapsed: 72.7618 learning rate: 0.000490, scenario: 0, slope: -0.8704242072666356, fluctuations: 0.0\n",
      "step: 58150 loss: 204.171562 time elapsed: 72.7750 learning rate: 0.000490, scenario: 0, slope: -1.1742435070828154, fluctuations: 0.0\n",
      "step: 58160 loss: 189.384258 time elapsed: 72.7885 learning rate: 0.000490, scenario: 0, slope: -1.4470145858460077, fluctuations: 0.0\n",
      "step: 58170 loss: 176.606383 time elapsed: 72.8016 learning rate: 0.000490, scenario: 0, slope: -1.6582101773075348, fluctuations: 0.0\n",
      "step: 58180 loss: 165.331229 time elapsed: 72.8148 learning rate: 0.000490, scenario: 0, slope: -1.7839521802413605, fluctuations: 0.0\n",
      "step: 58190 loss: 155.331668 time elapsed: 72.8272 learning rate: 0.000490, scenario: 0, slope: -1.8049669034404134, fluctuations: 0.0\n",
      "step: 58200 loss: 146.567903 time elapsed: 72.8394 learning rate: 0.000490, scenario: 0, slope: -1.7281643132718958, fluctuations: 0.0\n",
      "step: 58210 loss: 139.090895 time elapsed: 72.8519 learning rate: 0.000490, scenario: 0, slope: -1.5266595991174414, fluctuations: 0.0\n",
      "step: 58220 loss: 132.944075 time elapsed: 72.8638 learning rate: 0.000490, scenario: 0, slope: -1.2957243409341963, fluctuations: 0.0\n",
      "step: 58230 loss: 128.065256 time elapsed: 72.8754 learning rate: 0.000490, scenario: 0, slope: -1.1024700199164146, fluctuations: 0.0\n",
      "step: 58240 loss: 124.238643 time elapsed: 72.8873 learning rate: 0.000490, scenario: 0, slope: -0.9454707864492247, fluctuations: 0.0\n",
      "step: 58250 loss: 121.162187 time elapsed: 72.8991 learning rate: 0.000490, scenario: 0, slope: -0.8095482816051629, fluctuations: 0.0\n",
      "step: 58260 loss: 118.580269 time elapsed: 72.9109 learning rate: 0.000490, scenario: 0, slope: -0.6873296329823565, fluctuations: 0.0\n",
      "step: 58270 loss: 116.343170 time elapsed: 72.9225 learning rate: 0.000490, scenario: 0, slope: -0.5767184562134511, fluctuations: 0.0\n",
      "step: 58280 loss: 114.374562 time elapsed: 72.9343 learning rate: 0.000490, scenario: 0, slope: -0.47830535300863103, fluctuations: 0.0\n",
      "step: 58290 loss: 112.627620 time elapsed: 72.9479 learning rate: 0.000490, scenario: 0, slope: -0.39361020746338193, fluctuations: 0.0\n",
      "step: 58300 loss: 111.066031 time elapsed: 72.9611 learning rate: 0.000490, scenario: 0, slope: -0.33011071917623586, fluctuations: 0.0\n",
      "step: 58310 loss: 109.659681 time elapsed: 72.9748 learning rate: 0.000490, scenario: 0, slope: -0.26891110023786363, fluctuations: 0.0\n",
      "step: 58320 loss: 108.383674 time elapsed: 72.9882 learning rate: 0.000490, scenario: 0, slope: -0.22737111256111475, fluctuations: 0.0\n",
      "step: 58330 loss: 107.217530 time elapsed: 73.0015 learning rate: 0.000490, scenario: 0, slope: -0.1964095015471568, fluctuations: 0.0\n",
      "step: 58340 loss: 106.144388 time elapsed: 73.0153 learning rate: 0.000490, scenario: 0, slope: -0.17286883261031888, fluctuations: 0.0\n",
      "step: 58350 loss: 105.150330 time elapsed: 73.0280 learning rate: 0.000490, scenario: 0, slope: -0.15419791564505633, fluctuations: 0.0\n",
      "step: 58360 loss: 104.223840 time elapsed: 73.0401 learning rate: 0.000490, scenario: 0, slope: -0.13884408752968508, fluctuations: 0.0\n",
      "step: 58370 loss: 103.355367 time elapsed: 73.0521 learning rate: 0.000490, scenario: 0, slope: -0.1259898811365848, fluctuations: 0.0\n",
      "step: 58380 loss: 102.536968 time elapsed: 73.0640 learning rate: 0.000490, scenario: 0, slope: -0.11515463702278227, fluctuations: 0.0\n",
      "step: 58390 loss: 101.502867 time elapsed: 73.0755 learning rate: 0.001156, scenario: 1, slope: -0.10641390723296386, fluctuations: 0.0\n",
      "step: 58400 loss: 99.116048 time elapsed: 73.0871 learning rate: 0.002725, scenario: 1, slope: -0.10381890956979585, fluctuations: 0.0\n",
      "step: 58410 loss: 94.602012 time elapsed: 73.0994 learning rate: 0.004827, scenario: 0, slope: -0.1171684303296327, fluctuations: 0.0\n",
      "step: 58420 loss: 90.090233 time elapsed: 73.1109 learning rate: 0.004827, scenario: 0, slope: -0.14867850103513017, fluctuations: 0.0\n",
      "step: 58430 loss: 86.356232 time elapsed: 73.1229 learning rate: 0.004827, scenario: 0, slope: -0.18971670969033735, fluctuations: 0.0\n",
      "step: 58440 loss: 83.124436 time elapsed: 73.1346 learning rate: 0.004827, scenario: 0, slope: -0.23296299504279247, fluctuations: 0.0\n",
      "step: 58450 loss: 80.251260 time elapsed: 73.1464 learning rate: 0.004827, scenario: 0, slope: -0.27281309428653727, fluctuations: 0.0\n",
      "step: 58460 loss: 77.634627 time elapsed: 73.1595 learning rate: 0.004827, scenario: 0, slope: -0.30478171599885595, fluctuations: 0.0\n",
      "step: 58470 loss: 75.204367 time elapsed: 73.1728 learning rate: 0.004827, scenario: 0, slope: -0.3251805184487871, fluctuations: 0.0\n",
      "step: 58480 loss: 72.941751 time elapsed: 73.1862 learning rate: 0.004827, scenario: 0, slope: -0.3307937128458254, fluctuations: 0.0\n",
      "step: 58490 loss: 70.835605 time elapsed: 73.1996 learning rate: 0.004827, scenario: 0, slope: -0.3190874776266598, fluctuations: 0.0\n",
      "step: 58500 loss: 68.863040 time elapsed: 73.2125 learning rate: 0.004827, scenario: 0, slope: -0.2959875177457745, fluctuations: 0.0\n",
      "step: 58510 loss: 67.008200 time elapsed: 73.2265 learning rate: 0.004827, scenario: 0, slope: -0.2635305432986752, fluctuations: 0.0\n",
      "step: 58520 loss: 65.249141 time elapsed: 73.2388 learning rate: 0.004827, scenario: 0, slope: -0.2405818656246412, fluctuations: 0.0\n",
      "step: 58530 loss: 63.554230 time elapsed: 73.2512 learning rate: 0.004827, scenario: 0, slope: -0.2225172792350707, fluctuations: 0.0\n",
      "step: 58540 loss: 61.878411 time elapsed: 73.2633 learning rate: 0.004827, scenario: 0, slope: -0.20794401136598223, fluctuations: 0.0\n",
      "step: 58550 loss: 60.037101 time elapsed: 73.2754 learning rate: 0.004827, scenario: 0, slope: -0.1964272383879755, fluctuations: 0.0\n",
      "step: 58560 loss: 57.595415 time elapsed: 73.2873 learning rate: 0.004827, scenario: 0, slope: -0.1898861765392998, fluctuations: 0.0\n",
      "step: 58570 loss: 55.839101 time elapsed: 73.2992 learning rate: 0.004827, scenario: 0, slope: -0.18714979084846775, fluctuations: 0.0\n",
      "step: 58580 loss: 54.428368 time elapsed: 73.3109 learning rate: 0.004827, scenario: 0, slope: -0.1847120959065375, fluctuations: 0.0\n",
      "step: 58590 loss: 53.075224 time elapsed: 73.3225 learning rate: 0.004827, scenario: 0, slope: -0.18147205855986467, fluctuations: 0.0\n",
      "step: 58600 loss: 51.826224 time elapsed: 73.3343 learning rate: 0.004827, scenario: 0, slope: -0.17742702974044208, fluctuations: 0.0\n",
      "step: 58610 loss: 50.644200 time elapsed: 73.3471 learning rate: 0.004827, scenario: 0, slope: -0.17062303026926484, fluctuations: 0.0\n",
      "step: 58620 loss: 49.525633 time elapsed: 73.3596 learning rate: 0.004827, scenario: 0, slope: -0.1624306207374689, fluctuations: 0.0\n",
      "step: 58630 loss: 48.469957 time elapsed: 73.3735 learning rate: 0.004827, scenario: 0, slope: -0.15223232591713481, fluctuations: 0.0\n",
      "step: 58640 loss: 47.476199 time elapsed: 73.3874 learning rate: 0.004827, scenario: 0, slope: -0.1402565161369403, fluctuations: 0.0\n",
      "step: 58650 loss: 46.540471 time elapsed: 73.4008 learning rate: 0.004827, scenario: 0, slope: -0.12722145419545866, fluctuations: 0.0\n",
      "step: 58660 loss: 45.655763 time elapsed: 73.4144 learning rate: 0.004827, scenario: 0, slope: -0.11653074854166079, fluctuations: 0.0\n",
      "step: 58670 loss: 44.812387 time elapsed: 73.4282 learning rate: 0.004827, scenario: 0, slope: -0.1092234822853331, fluctuations: 0.0\n",
      "step: 58680 loss: 43.997005 time elapsed: 73.4421 learning rate: 0.004827, scenario: 0, slope: -0.10302793880945796, fluctuations: 0.0\n",
      "step: 58690 loss: 43.189350 time elapsed: 73.4545 learning rate: 0.004827, scenario: 0, slope: -0.097543639464377, fluctuations: 0.0\n",
      "step: 58700 loss: 42.366897 time elapsed: 73.4661 learning rate: 0.004827, scenario: 0, slope: -0.09327572263433494, fluctuations: 0.0\n",
      "step: 58710 loss: 41.574039 time elapsed: 73.4786 learning rate: 0.004827, scenario: 0, slope: -0.08897811841835956, fluctuations: 0.0\n",
      "step: 58720 loss: 40.890194 time elapsed: 73.4908 learning rate: 0.004827, scenario: 0, slope: -0.0855322312612552, fluctuations: 0.0\n",
      "step: 58730 loss: 40.278202 time elapsed: 73.5027 learning rate: 0.004827, scenario: 0, slope: -0.08210855477731865, fluctuations: 0.0\n",
      "step: 58740 loss: 39.707175 time elapsed: 73.5147 learning rate: 0.004827, scenario: 0, slope: -0.07852936395248533, fluctuations: 0.0\n",
      "step: 58750 loss: 39.166493 time elapsed: 73.5264 learning rate: 0.004827, scenario: 0, slope: -0.07474529130445448, fluctuations: 0.0\n",
      "step: 58760 loss: 38.817762 time elapsed: 73.5381 learning rate: 0.004827, scenario: 0, slope: -0.06965543984381656, fluctuations: 0.02\n",
      "step: 58770 loss: 38.337531 time elapsed: 73.5498 learning rate: 0.004827, scenario: 0, slope: -0.06548202920532002, fluctuations: 0.06\n",
      "step: 58780 loss: 37.788384 time elapsed: 73.5619 learning rate: 0.004827, scenario: 0, slope: -0.06102300505865186, fluctuations: 0.09\n",
      "step: 58790 loss: 37.259384 time elapsed: 73.5739 learning rate: 0.004827, scenario: 0, slope: -0.056779673262967276, fluctuations: 0.11\n",
      "step: 58800 loss: 36.802592 time elapsed: 73.5874 learning rate: 0.004827, scenario: 0, slope: -0.05351832238961967, fluctuations: 0.11\n",
      "step: 58810 loss: 36.372869 time elapsed: 73.6019 learning rate: 0.004827, scenario: 0, slope: -0.050771315662374306, fluctuations: 0.11\n",
      "step: 58820 loss: 35.957273 time elapsed: 73.6154 learning rate: 0.004827, scenario: 0, slope: -0.04906985146828975, fluctuations: 0.11\n",
      "step: 58830 loss: 35.555100 time elapsed: 73.6286 learning rate: 0.004827, scenario: 0, slope: -0.04774503571352399, fluctuations: 0.11\n",
      "step: 58840 loss: 35.167359 time elapsed: 73.6417 learning rate: 0.004827, scenario: 0, slope: -0.046619428222056086, fluctuations: 0.11\n",
      "step: 58850 loss: 34.791843 time elapsed: 73.6555 learning rate: 0.004827, scenario: 0, slope: -0.0456397470478107, fluctuations: 0.11\n",
      "step: 58860 loss: 34.427963 time elapsed: 73.6680 learning rate: 0.004827, scenario: 0, slope: -0.04326423865043819, fluctuations: 0.08\n",
      "step: 58870 loss: 34.075036 time elapsed: 73.6801 learning rate: 0.004827, scenario: 0, slope: -0.04103392789704532, fluctuations: 0.04\n",
      "step: 58880 loss: 33.732446 time elapsed: 73.6924 learning rate: 0.004827, scenario: 0, slope: -0.03961469413708696, fluctuations: 0.01\n",
      "step: 58890 loss: 33.399570 time elapsed: 73.7044 learning rate: 0.004827, scenario: 0, slope: -0.03833055224721716, fluctuations: 0.0\n",
      "step: 58900 loss: 33.075774 time elapsed: 73.7163 learning rate: 0.004827, scenario: 0, slope: -0.037208770779611926, fluctuations: 0.0\n",
      "step: 58910 loss: 32.745330 time elapsed: 73.7286 learning rate: 0.008552, scenario: 1, slope: -0.035971180303217894, fluctuations: 0.0\n",
      "step: 58920 loss: 151.789940 time elapsed: 73.7402 learning rate: 0.004724, scenario: -1, slope: 0.44424567021018196, fluctuations: 0.02\n",
      "step: 58930 loss: 46.875336 time elapsed: 73.7519 learning rate: 0.001647, scenario: -1, slope: 0.46024110548972397, fluctuations: 0.06\n",
      "step: 58940 loss: 38.132114 time elapsed: 73.7637 learning rate: 0.000574, scenario: -1, slope: 0.39280409580634046, fluctuations: 0.08\n",
      "step: 58950 loss: 36.339620 time elapsed: 73.7758 learning rate: 0.000200, scenario: -1, slope: 0.27376046151502137, fluctuations: 0.09\n",
      "step: 58960 loss: 36.084018 time elapsed: 73.7878 learning rate: 0.000070, scenario: -1, slope: 0.15267810182859948, fluctuations: 0.1\n",
      "step: 58970 loss: 35.897323 time elapsed: 73.8015 learning rate: 0.000024, scenario: -1, slope: 0.025604551339575725, fluctuations: 0.1\n",
      "step: 58980 loss: 35.863836 time elapsed: 73.8151 learning rate: 0.000023, scenario: 0, slope: -0.09991420471399887, fluctuations: 0.1\n",
      "step: 58990 loss: 35.834225 time elapsed: 73.8287 learning rate: 0.000023, scenario: 0, slope: -0.2344867191068671, fluctuations: 0.1\n",
      "step: 59000 loss: 35.806491 time elapsed: 73.8427 learning rate: 0.000023, scenario: 0, slope: -0.37364151611988766, fluctuations: 0.1\n",
      "step: 59010 loss: 35.780440 time elapsed: 73.8570 learning rate: 0.000023, scenario: 0, slope: -0.5869014799698764, fluctuations: 0.1\n",
      "step: 59020 loss: 35.755213 time elapsed: 73.8714 learning rate: 0.000023, scenario: 0, slope: -0.15702243195309915, fluctuations: 0.07\n",
      "step: 59030 loss: 35.730371 time elapsed: 73.8839 learning rate: 0.000028, scenario: 1, slope: -0.040615792766219655, fluctuations: 0.04\n",
      "step: 59040 loss: 35.687479 time elapsed: 73.8964 learning rate: 0.000072, scenario: 1, slope: -0.011467813963241685, fluctuations: 0.02\n",
      "step: 59050 loss: 35.579515 time elapsed: 73.9084 learning rate: 0.000186, scenario: 1, slope: -0.00490097145236932, fluctuations: 0.01\n",
      "step: 59060 loss: 35.319583 time elapsed: 73.9197 learning rate: 0.000483, scenario: 1, slope: -0.004326757028532675, fluctuations: 0.0\n",
      "step: 59070 loss: 34.759464 time elapsed: 73.9320 learning rate: 0.001254, scenario: 1, slope: -0.006986462926324912, fluctuations: 0.0\n",
      "step: 59080 loss: 33.842848 time elapsed: 73.9439 learning rate: 0.003253, scenario: 1, slope: -0.01322192118320791, fluctuations: 0.0\n",
      "step: 59090 loss: 33.011566 time elapsed: 73.9557 learning rate: 0.008436, scenario: 1, slope: -0.02264619123744706, fluctuations: 0.0\n",
      "step: 59100 loss: 32.855823 time elapsed: 73.9677 learning rate: 0.019892, scenario: 1, slope: -0.0328204956935308, fluctuations: 0.0\n",
      "step: 59110 loss: 75900.034544 time elapsed: 73.9800 learning rate: 0.008991, scenario: -1, slope: 219.58792142098434, fluctuations: 0.02\n",
      "step: 59120 loss: 17144.155340 time elapsed: 73.9922 learning rate: 0.003135, scenario: -1, slope: 295.9571235480161, fluctuations: 0.04\n",
      "step: 59130 loss: 12111.878193 time elapsed: 74.0044 learning rate: 0.001093, scenario: -1, slope: 299.4284947346963, fluctuations: 0.05\n",
      "step: 59140 loss: 9186.179947 time elapsed: 74.0182 learning rate: 0.000381, scenario: -1, slope: 257.6759203602653, fluctuations: 0.05\n",
      "step: 59150 loss: 8673.021719 time elapsed: 74.0316 learning rate: 0.000133, scenario: -1, slope: 201.32525544433983, fluctuations: 0.05\n",
      "step: 59160 loss: 8517.592868 time elapsed: 74.0462 learning rate: 0.000046, scenario: -1, slope: 135.90428374857842, fluctuations: 0.05\n",
      "step: 59170 loss: 8465.311655 time elapsed: 74.0605 learning rate: 0.000016, scenario: -1, slope: 60.583678797213196, fluctuations: 0.05\n",
      "step: 59180 loss: 8446.840634 time elapsed: 74.0746 learning rate: 0.000008, scenario: 0, slope: -27.272456148358653, fluctuations: 0.05\n",
      "step: 59190 loss: 8434.814426 time elapsed: 74.0882 learning rate: 0.000008, scenario: 0, slope: -131.72741692004394, fluctuations: 0.05\n",
      "step: 59200 loss: 8422.910738 time elapsed: 74.1003 learning rate: 0.000008, scenario: 0, slope: -244.82645453224038, fluctuations: 0.05\n",
      "step: 59210 loss: 8411.124316 time elapsed: 74.1130 learning rate: 0.000008, scenario: 0, slope: -141.16100899422844, fluctuations: 0.02\n",
      "step: 59220 loss: 8399.440557 time elapsed: 74.1252 learning rate: 0.000008, scenario: 0, slope: -39.398429974542786, fluctuations: 0.01\n",
      "step: 59230 loss: 8387.845546 time elapsed: 74.1370 learning rate: 0.000008, scenario: 0, slope: -12.032170451198304, fluctuations: 0.0\n",
      "step: 59240 loss: 8371.074516 time elapsed: 74.1486 learning rate: 0.000020, scenario: 1, slope: -3.728548970454722, fluctuations: 0.0\n",
      "step: 59250 loss: 8328.458195 time elapsed: 74.1603 learning rate: 0.000052, scenario: 1, slope: -1.9213415866439485, fluctuations: 0.0\n",
      "step: 59260 loss: 8221.618502 time elapsed: 74.1720 learning rate: 0.000135, scenario: 1, slope: -1.829873343825205, fluctuations: 0.0\n",
      "step: 59270 loss: 7965.454355 time elapsed: 74.1836 learning rate: 0.000350, scenario: 1, slope: -2.9576245277704247, fluctuations: 0.0\n",
      "step: 59280 loss: 7401.594334 time elapsed: 74.1954 learning rate: 0.000907, scenario: 1, slope: -6.069482207676335, fluctuations: 0.0\n",
      "step: 59290 loss: 6441.901110 time elapsed: 74.2076 learning rate: 0.001328, scenario: 0, slope: -12.740249034475212, fluctuations: 0.0\n",
      "step: 59300 loss: 5677.629590 time elapsed: 74.2211 learning rate: 0.001328, scenario: 0, slope: -21.335887004314387, fluctuations: 0.0\n",
      "step: 59310 loss: 5074.777725 time elapsed: 74.2354 learning rate: 0.001328, scenario: 0, slope: -32.74230153999341, fluctuations: 0.0\n",
      "step: 59320 loss: 4633.688299 time elapsed: 74.2487 learning rate: 0.001328, scenario: 0, slope: -42.240720952283404, fluctuations: 0.0\n",
      "step: 59330 loss: 4273.107260 time elapsed: 74.2617 learning rate: 0.001328, scenario: 0, slope: -49.64821309582549, fluctuations: 0.0\n",
      "step: 59340 loss: 3983.237287 time elapsed: 74.2753 learning rate: 0.001328, scenario: 0, slope: -54.10761511510938, fluctuations: 0.0\n",
      "step: 59350 loss: 3743.786774 time elapsed: 74.2886 learning rate: 0.001328, scenario: 0, slope: -55.029486310853024, fluctuations: 0.0\n",
      "step: 59360 loss: 3538.713240 time elapsed: 74.3011 learning rate: 0.001328, scenario: 0, slope: -52.20713594980779, fluctuations: 0.0\n",
      "step: 59370 loss: 3356.393790 time elapsed: 74.3132 learning rate: 0.001328, scenario: 0, slope: -46.00606061444689, fluctuations: 0.0\n",
      "step: 59380 loss: 3180.476072 time elapsed: 74.3253 learning rate: 0.001328, scenario: 0, slope: -37.83210311106457, fluctuations: 0.0\n",
      "step: 59390 loss: 2790.962301 time elapsed: 74.3371 learning rate: 0.001328, scenario: 0, slope: -30.930389952325516, fluctuations: 0.0\n",
      "step: 59400 loss: 2561.461582 time elapsed: 74.3491 learning rate: 0.001328, scenario: 0, slope: -27.58310420186817, fluctuations: 0.01\n",
      "step: 59410 loss: 2418.891386 time elapsed: 74.3615 learning rate: 0.001328, scenario: 0, slope: -24.83607556886265, fluctuations: 0.01\n",
      "step: 59420 loss: 2327.737007 time elapsed: 74.3735 learning rate: 0.001328, scenario: 0, slope: -22.96711164240178, fluctuations: 0.01\n",
      "step: 59430 loss: 2256.419122 time elapsed: 74.3854 learning rate: 0.001328, scenario: 0, slope: -21.228298993542456, fluctuations: 0.01\n",
      "step: 59440 loss: 2195.674253 time elapsed: 74.3974 learning rate: 0.001328, scenario: 0, slope: -19.40659314993979, fluctuations: 0.01\n",
      "step: 59450 loss: 2143.006157 time elapsed: 74.4096 learning rate: 0.001328, scenario: 0, slope: -17.35007713845923, fluctuations: 0.01\n",
      "step: 59460 loss: 2095.583508 time elapsed: 74.4238 learning rate: 0.001328, scenario: 0, slope: -14.992492868824844, fluctuations: 0.01\n",
      "step: 59470 loss: 2051.837131 time elapsed: 74.4370 learning rate: 0.001328, scenario: 0, slope: -12.314459718228171, fluctuations: 0.01\n",
      "step: 59480 loss: 2010.728162 time elapsed: 74.4508 learning rate: 0.001328, scenario: 0, slope: -9.353791591342862, fluctuations: 0.01\n",
      "step: 59490 loss: 1971.503686 time elapsed: 74.4640 learning rate: 0.001328, scenario: 0, slope: -6.650821671090352, fluctuations: 0.01\n",
      "step: 59500 loss: 1933.684320 time elapsed: 74.4767 learning rate: 0.001328, scenario: 0, slope: -5.694040128934969, fluctuations: 0.0\n",
      "step: 59510 loss: 1896.914789 time elapsed: 74.4907 learning rate: 0.001328, scenario: 0, slope: -4.911440324616636, fluctuations: 0.0\n",
      "step: 59520 loss: 1860.926172 time elapsed: 74.5034 learning rate: 0.001328, scenario: 0, slope: -4.464587282214959, fluctuations: 0.0\n",
      "step: 59530 loss: 1825.476567 time elapsed: 74.5156 learning rate: 0.001328, scenario: 0, slope: -4.164300540416707, fluctuations: 0.0\n",
      "step: 59540 loss: 1790.281681 time elapsed: 74.5271 learning rate: 0.001328, scenario: 0, slope: -3.9548122729738253, fluctuations: 0.0\n",
      "step: 59550 loss: 1754.887438 time elapsed: 74.5388 learning rate: 0.001328, scenario: 0, slope: -3.806002573902374, fluctuations: 0.0\n",
      "step: 59560 loss: 1718.480352 time elapsed: 74.5505 learning rate: 0.001328, scenario: 0, slope: -3.7050361014333477, fluctuations: 0.0\n",
      "step: 59570 loss: 1680.606484 time elapsed: 74.5624 learning rate: 0.001328, scenario: 0, slope: -3.646990885429329, fluctuations: 0.0\n",
      "step: 59580 loss: 1645.608249 time elapsed: 74.5742 learning rate: 0.001328, scenario: 0, slope: -3.618213438602172, fluctuations: 0.0\n",
      "step: 59590 loss: 1613.175618 time elapsed: 74.5863 learning rate: 0.001328, scenario: 0, slope: -3.5901295732604748, fluctuations: 0.0\n",
      "step: 59600 loss: 1579.188051 time elapsed: 74.5979 learning rate: 0.001328, scenario: 0, slope: -3.563067807796691, fluctuations: 0.0\n",
      "step: 59610 loss: 1525.651313 time elapsed: 74.6102 learning rate: 0.001328, scenario: 0, slope: -3.5807631454087305, fluctuations: 0.0\n",
      "step: 59620 loss: 1476.841601 time elapsed: 74.6240 learning rate: 0.001328, scenario: 0, slope: -3.717330120167973, fluctuations: 0.0\n",
      "step: 59630 loss: 1441.867380 time elapsed: 74.6377 learning rate: 0.001328, scenario: 0, slope: -3.8233829117563096, fluctuations: 0.0\n",
      "step: 59640 loss: 1402.253669 time elapsed: 74.6511 learning rate: 0.001328, scenario: 0, slope: -3.915968608114247, fluctuations: 0.0\n",
      "step: 59650 loss: 1370.729700 time elapsed: 74.6644 learning rate: 0.001328, scenario: 0, slope: -3.964637106758834, fluctuations: 0.0\n",
      "step: 59660 loss: 1338.883316 time elapsed: 74.6778 learning rate: 0.001328, scenario: 0, slope: -3.9603030294449226, fluctuations: 0.0\n",
      "step: 59670 loss: 1307.583611 time elapsed: 74.6916 learning rate: 0.001328, scenario: 0, slope: -3.9128240760455157, fluctuations: 0.0\n",
      "step: 59680 loss: 1276.130216 time elapsed: 74.7052 learning rate: 0.001328, scenario: 0, slope: -3.826756928960712, fluctuations: 0.0\n",
      "step: 59690 loss: 1244.047374 time elapsed: 74.7177 learning rate: 0.001328, scenario: 0, slope: -3.686169764475458, fluctuations: 0.0\n",
      "step: 59700 loss: 1211.419975 time elapsed: 74.7294 learning rate: 0.001328, scenario: 0, slope: -3.515596255067619, fluctuations: 0.0\n",
      "step: 59710 loss: 1178.408923 time elapsed: 74.7415 learning rate: 0.001328, scenario: 0, slope: -3.316179306262057, fluctuations: 0.0\n",
      "step: 59720 loss: 1145.352557 time elapsed: 74.7535 learning rate: 0.001328, scenario: 0, slope: -3.265274755957611, fluctuations: 0.0\n",
      "step: 59730 loss: 1112.695999 time elapsed: 74.7655 learning rate: 0.001328, scenario: 0, slope: -3.227829487314887, fluctuations: 0.0\n",
      "step: 59740 loss: 1080.885194 time elapsed: 74.7778 learning rate: 0.001328, scenario: 0, slope: -3.2248727402127133, fluctuations: 0.0\n",
      "step: 59750 loss: 1050.299889 time elapsed: 74.7897 learning rate: 0.001328, scenario: 0, slope: -3.2302268607881683, fluctuations: 0.0\n",
      "step: 59760 loss: 1021.209648 time elapsed: 74.8016 learning rate: 0.001328, scenario: 0, slope: -3.223069777133232, fluctuations: 0.0\n",
      "step: 59770 loss: 993.774877 time elapsed: 74.8135 learning rate: 0.001328, scenario: 0, slope: -3.192285270054293, fluctuations: 0.0\n",
      "step: 59780 loss: 968.065726 time elapsed: 74.8252 learning rate: 0.001328, scenario: 0, slope: -3.1322264115365686, fluctuations: 0.0\n",
      "step: 59790 loss: 944.084481 time elapsed: 74.8392 learning rate: 0.001328, scenario: 0, slope: -3.042604549678287, fluctuations: 0.0\n",
      "step: 59800 loss: 921.783440 time elapsed: 74.8531 learning rate: 0.001328, scenario: 0, slope: -2.939094303356217, fluctuations: 0.0\n",
      "step: 59810 loss: 901.077993 time elapsed: 74.8675 learning rate: 0.001328, scenario: 0, slope: -2.789207647665264, fluctuations: 0.0\n",
      "step: 59820 loss: 881.855706 time elapsed: 74.8806 learning rate: 0.001328, scenario: 0, slope: -2.637836486377709, fluctuations: 0.0\n",
      "step: 59830 loss: 863.982364 time elapsed: 74.8944 learning rate: 0.001328, scenario: 0, slope: -2.4792835318035293, fluctuations: 0.0\n",
      "step: 59840 loss: 847.309387 time elapsed: 74.9087 learning rate: 0.001328, scenario: 0, slope: -2.3197733030313614, fluctuations: 0.0\n",
      "step: 59850 loss: 831.687366 time elapsed: 74.9210 learning rate: 0.001328, scenario: 0, slope: -2.164339852646035, fluctuations: 0.0\n",
      "step: 59860 loss: 816.980980 time elapsed: 74.9329 learning rate: 0.001328, scenario: 0, slope: -2.016726508166432, fluctuations: 0.0\n",
      "step: 59870 loss: 803.075248 time elapsed: 74.9446 learning rate: 0.001328, scenario: 0, slope: -1.8794680049621584, fluctuations: 0.0\n",
      "step: 59880 loss: 789.873894 time elapsed: 74.9567 learning rate: 0.001328, scenario: 0, slope: -1.7540448588338227, fluctuations: 0.0\n",
      "step: 59890 loss: 777.296854 time elapsed: 74.9686 learning rate: 0.001328, scenario: 0, slope: -1.6410496349341774, fluctuations: 0.0\n",
      "step: 59900 loss: 765.278512 time elapsed: 74.9803 learning rate: 0.001328, scenario: 0, slope: -1.549878722457565, fluctuations: 0.0\n",
      "step: 59910 loss: 753.765496 time elapsed: 74.9927 learning rate: 0.001328, scenario: 0, slope: -1.4512178207507, fluctuations: 0.0\n",
      "step: 59920 loss: 742.713825 time elapsed: 75.0045 learning rate: 0.001328, scenario: 0, slope: -1.3725454471661056, fluctuations: 0.0\n",
      "step: 59930 loss: 732.085718 time elapsed: 75.0162 learning rate: 0.001328, scenario: 0, slope: -1.3029770928245925, fluctuations: 0.0\n",
      "step: 59940 loss: 721.846812 time elapsed: 75.0280 learning rate: 0.001328, scenario: 0, slope: -1.2411319809081602, fluctuations: 0.0\n",
      "step: 59950 loss: 711.964711 time elapsed: 75.0397 learning rate: 0.001328, scenario: 0, slope: -1.1857648817727349, fluctuations: 0.0\n",
      "step: 59960 loss: 702.409045 time elapsed: 75.0539 learning rate: 0.001328, scenario: 0, slope: -1.1358468572962492, fluctuations: 0.0\n",
      "step: 59970 loss: 693.152291 time elapsed: 75.0676 learning rate: 0.001328, scenario: 0, slope: -1.0905644151208453, fluctuations: 0.0\n",
      "step: 59980 loss: 684.170351 time elapsed: 75.0814 learning rate: 0.001328, scenario: 0, slope: -1.0492858474155293, fluctuations: 0.0\n",
      "step: 59990 loss: 675.442480 time elapsed: 75.0952 learning rate: 0.001328, scenario: 0, slope: -1.0115231587961333, fluctuations: 0.0\n",
      "step: 60000 loss: 666.950718 time elapsed: 75.1085 learning rate: 0.001328, scenario: 0, slope: -0.9802242952876284, fluctuations: 0.0\n",
      "step: 60010 loss: 658.679243 time elapsed: 75.1225 learning rate: 0.001328, scenario: 0, slope: -0.945078161775781, fluctuations: 0.0\n",
      "step: 60020 loss: 650.613876 time elapsed: 75.1353 learning rate: 0.001328, scenario: 0, slope: -0.915798678033813, fluctuations: 0.0\n",
      "step: 60030 loss: 642.741772 time elapsed: 75.1478 learning rate: 0.001328, scenario: 0, slope: -0.8887911406780705, fluctuations: 0.0\n",
      "step: 60040 loss: 635.051224 time elapsed: 75.1599 learning rate: 0.001328, scenario: 0, slope: -0.8638046817895054, fluctuations: 0.0\n",
      "step: 60050 loss: 627.531517 time elapsed: 75.1719 learning rate: 0.001328, scenario: 0, slope: -0.8406067729539669, fluctuations: 0.0\n",
      "step: 60060 loss: 620.172819 time elapsed: 75.1833 learning rate: 0.001328, scenario: 0, slope: -0.8189922192820861, fluctuations: 0.0\n",
      "step: 60070 loss: 612.966075 time elapsed: 75.1952 learning rate: 0.001328, scenario: 0, slope: -0.7987879107648084, fluctuations: 0.0\n",
      "step: 60080 loss: 605.902926 time elapsed: 75.2069 learning rate: 0.001328, scenario: 0, slope: -0.7798510919940963, fluctuations: 0.0\n",
      "step: 60090 loss: 598.975632 time elapsed: 75.2189 learning rate: 0.001328, scenario: 0, slope: -0.7620635292079401, fluctuations: 0.0\n",
      "step: 60100 loss: 592.177003 time elapsed: 75.2304 learning rate: 0.001328, scenario: 0, slope: -0.7469543417885125, fluctuations: 0.0\n",
      "step: 60110 loss: 585.500336 time elapsed: 75.2428 learning rate: 0.001328, scenario: 0, slope: -0.7295493293322768, fluctuations: 0.0\n",
      "step: 60120 loss: 578.939366 time elapsed: 75.2550 learning rate: 0.001328, scenario: 0, slope: -0.7146598492135865, fluctuations: 0.0\n",
      "step: 60130 loss: 572.488219 time elapsed: 75.2687 learning rate: 0.001328, scenario: 0, slope: -0.7005892214688894, fluctuations: 0.0\n",
      "step: 60140 loss: 566.141373 time elapsed: 75.2822 learning rate: 0.001328, scenario: 0, slope: -0.687277430778988, fluctuations: 0.0\n",
      "step: 60150 loss: 559.893634 time elapsed: 75.2958 learning rate: 0.001328, scenario: 0, slope: -0.6746709823974271, fluctuations: 0.0\n",
      "step: 60160 loss: 553.740103 time elapsed: 75.3097 learning rate: 0.001328, scenario: 0, slope: -0.6627220601362894, fluctuations: 0.0\n",
      "step: 60170 loss: 547.676163 time elapsed: 75.3232 learning rate: 0.001328, scenario: 0, slope: -0.6513877550060335, fluctuations: 0.0\n",
      "step: 60180 loss: 541.697460 time elapsed: 75.3374 learning rate: 0.001328, scenario: 0, slope: -0.640629350240697, fluctuations: 0.0\n",
      "step: 60190 loss: 535.799885 time elapsed: 75.3499 learning rate: 0.001328, scenario: 0, slope: -0.6304116689372641, fluctuations: 0.0\n",
      "step: 60200 loss: 529.979566 time elapsed: 75.3617 learning rate: 0.001328, scenario: 0, slope: -0.6216513994867195, fluctuations: 0.0\n",
      "step: 60210 loss: 524.232846 time elapsed: 75.3743 learning rate: 0.001328, scenario: 0, slope: -0.6114721309109624, fluctuations: 0.0\n",
      "step: 60220 loss: 518.556265 time elapsed: 75.3861 learning rate: 0.001328, scenario: 0, slope: -0.6026929831618754, fluctuations: 0.0\n",
      "step: 60230 loss: 512.946535 time elapsed: 75.3980 learning rate: 0.001328, scenario: 0, slope: -0.5943394149018778, fluctuations: 0.0\n",
      "step: 60240 loss: 507.400494 time elapsed: 75.4099 learning rate: 0.001328, scenario: 0, slope: -0.5863876851526877, fluctuations: 0.0\n",
      "step: 60250 loss: 501.915045 time elapsed: 75.4217 learning rate: 0.001328, scenario: 0, slope: -0.5788161773804185, fluctuations: 0.0\n",
      "step: 60260 loss: 496.487022 time elapsed: 75.4339 learning rate: 0.001328, scenario: 0, slope: -0.571606043600595, fluctuations: 0.0\n",
      "step: 60270 loss: 491.112927 time elapsed: 75.4493 learning rate: 0.001328, scenario: 0, slope: -0.5647426816411294, fluctuations: 0.0\n",
      "step: 60280 loss: 485.788340 time elapsed: 75.4620 learning rate: 0.001328, scenario: 0, slope: -0.5582190513943239, fluctuations: 0.0\n",
      "step: 60290 loss: 480.506746 time elapsed: 75.4751 learning rate: 0.001328, scenario: 0, slope: -0.5520429512323355, fluctuations: 0.0\n",
      "step: 60300 loss: 475.259919 time elapsed: 75.4889 learning rate: 0.001328, scenario: 0, slope: -0.5468081545670914, fluctuations: 0.0\n",
      "step: 60310 loss: 470.064482 time elapsed: 75.5032 learning rate: 0.001328, scenario: 0, slope: -0.5408310366632292, fluctuations: 0.0\n",
      "step: 60320 loss: 464.940378 time elapsed: 75.5169 learning rate: 0.001328, scenario: 0, slope: -0.535609615187943, fluctuations: 0.0\n",
      "step: 60330 loss: 459.861616 time elapsed: 75.5307 learning rate: 0.001328, scenario: 0, slope: -0.5305207382688611, fluctuations: 0.0\n",
      "step: 60340 loss: 454.828360 time elapsed: 75.5439 learning rate: 0.001328, scenario: 0, slope: -0.525521371866411, fluctuations: 0.0\n",
      "step: 60350 loss: 449.837543 time elapsed: 75.5579 learning rate: 0.001328, scenario: 0, slope: -0.5205984050457897, fluctuations: 0.0\n",
      "step: 60360 loss: 444.887080 time elapsed: 75.5700 learning rate: 0.001328, scenario: 0, slope: -0.5157425211613321, fluctuations: 0.0\n",
      "step: 60370 loss: 439.975757 time elapsed: 75.5821 learning rate: 0.001328, scenario: 0, slope: -0.510954380623058, fluctuations: 0.0\n",
      "step: 60380 loss: 435.102194 time elapsed: 75.5940 learning rate: 0.001328, scenario: 0, slope: -0.5062492147577649, fluctuations: 0.0\n",
      "step: 60390 loss: 430.265119 time elapsed: 75.6056 learning rate: 0.001328, scenario: 0, slope: -0.5016688644297628, fluctuations: 0.0\n",
      "step: 60400 loss: 425.463311 time elapsed: 75.6173 learning rate: 0.001328, scenario: 0, slope: -0.4977260613693333, fluctuations: 0.0\n",
      "step: 60410 loss: 420.695548 time elapsed: 75.6296 learning rate: 0.001328, scenario: 0, slope: -0.4932392635293451, fluctuations: 0.0\n",
      "step: 60420 loss: 415.960599 time elapsed: 75.6414 learning rate: 0.001328, scenario: 0, slope: -0.4893650325454656, fluctuations: 0.0\n",
      "step: 60430 loss: 411.257206 time elapsed: 75.6532 learning rate: 0.001328, scenario: 0, slope: -0.48564571755241687, fluctuations: 0.0\n",
      "step: 60440 loss: 406.584054 time elapsed: 75.6649 learning rate: 0.001328, scenario: 0, slope: -0.4820570904750874, fluctuations: 0.0\n",
      "step: 60450 loss: 401.939740 time elapsed: 75.6771 learning rate: 0.001328, scenario: 0, slope: -0.4785977869462427, fluctuations: 0.0\n",
      "step: 60460 loss: 397.322730 time elapsed: 75.6908 learning rate: 0.001328, scenario: 0, slope: -0.4752663402989632, fluctuations: 0.0\n",
      "step: 60470 loss: 392.731305 time elapsed: 75.7044 learning rate: 0.001328, scenario: 0, slope: -0.47206532650318206, fluctuations: 0.0\n",
      "step: 60480 loss: 388.163486 time elapsed: 75.7174 learning rate: 0.001328, scenario: 0, slope: -0.46900065939251767, fluctuations: 0.0\n",
      "step: 60490 loss: 383.616930 time elapsed: 75.7308 learning rate: 0.001328, scenario: 0, slope: -0.46608174242219236, fluctuations: 0.0\n",
      "step: 60500 loss: 379.088782 time elapsed: 75.7442 learning rate: 0.001328, scenario: 0, slope: -0.46359088727819264, fluctuations: 0.0\n",
      "step: 60510 loss: 374.575454 time elapsed: 75.7581 learning rate: 0.001328, scenario: 0, slope: -0.46074401841868823, fluctuations: 0.0\n",
      "step: 60520 loss: 370.072276 time elapsed: 75.7700 learning rate: 0.001328, scenario: 0, slope: -0.45837504693539805, fluctuations: 0.0\n",
      "step: 60530 loss: 365.572942 time elapsed: 75.7822 learning rate: 0.001328, scenario: 0, slope: -0.4562580631720891, fluctuations: 0.0\n",
      "step: 60540 loss: 361.068571 time elapsed: 75.7944 learning rate: 0.001328, scenario: 0, slope: -0.4544549199001932, fluctuations: 0.0\n",
      "step: 60550 loss: 356.546020 time elapsed: 75.8062 learning rate: 0.001328, scenario: 0, slope: -0.4530585498299596, fluctuations: 0.0\n",
      "step: 60560 loss: 351.984680 time elapsed: 75.8179 learning rate: 0.001328, scenario: 0, slope: -0.4522139775961017, fluctuations: 0.0\n",
      "step: 60570 loss: 347.349843 time elapsed: 75.8299 learning rate: 0.001328, scenario: 0, slope: -0.4521593294963099, fluctuations: 0.0\n",
      "step: 60580 loss: 342.577686 time elapsed: 75.8414 learning rate: 0.001328, scenario: 0, slope: -0.453312706438383, fluctuations: 0.0\n",
      "step: 60590 loss: 337.537322 time elapsed: 75.8532 learning rate: 0.001328, scenario: 0, slope: -0.45647535033545134, fluctuations: 0.0\n",
      "step: 60600 loss: 331.924438 time elapsed: 75.8652 learning rate: 0.001328, scenario: 0, slope: -0.4624191613799796, fluctuations: 0.0\n",
      "step: 60610 loss: 324.983849 time elapsed: 75.8779 learning rate: 0.001328, scenario: 0, slope: -0.47804892576807173, fluctuations: 0.0\n",
      "step: 60620 loss: 315.985772 time elapsed: 75.8923 learning rate: 0.001328, scenario: 0, slope: -0.5085926083593826, fluctuations: 0.0\n",
      "step: 60630 loss: 308.360095 time elapsed: 75.9062 learning rate: 0.001328, scenario: 0, slope: -0.5521571017915106, fluctuations: 0.0\n",
      "step: 60640 loss: 301.909755 time elapsed: 75.9198 learning rate: 0.001328, scenario: 0, slope: -0.5956875543352766, fluctuations: 0.0\n",
      "step: 60650 loss: 296.455978 time elapsed: 75.9331 learning rate: 0.001328, scenario: 0, slope: -0.6304752720127188, fluctuations: 0.0\n",
      "step: 60660 loss: 291.594954 time elapsed: 75.9469 learning rate: 0.001328, scenario: 0, slope: -0.6509967940395925, fluctuations: 0.0\n",
      "step: 60670 loss: 287.083941 time elapsed: 75.9613 learning rate: 0.001328, scenario: 0, slope: -0.6546471589234372, fluctuations: 0.0\n",
      "step: 60680 loss: 282.832844 time elapsed: 75.9742 learning rate: 0.001328, scenario: 0, slope: -0.640410286479355, fluctuations: 0.0\n",
      "step: 60690 loss: 278.763150 time elapsed: 75.9870 learning rate: 0.001328, scenario: 0, slope: -0.6088707611006935, fluctuations: 0.0\n",
      "step: 60700 loss: 274.827541 time elapsed: 75.9991 learning rate: 0.001328, scenario: 0, slope: -0.567919158788987, fluctuations: 0.0\n",
      "step: 60710 loss: 270.994844 time elapsed: 76.0115 learning rate: 0.001328, scenario: 0, slope: -0.5092872742338567, fluctuations: 0.0\n",
      "step: 60720 loss: 267.246919 time elapsed: 76.0234 learning rate: 0.001328, scenario: 0, slope: -0.4629733497455387, fluctuations: 0.0\n",
      "step: 60730 loss: 263.572370 time elapsed: 76.0355 learning rate: 0.001328, scenario: 0, slope: -0.43141612693892667, fluctuations: 0.0\n",
      "step: 60740 loss: 259.963761 time elapsed: 76.0479 learning rate: 0.001328, scenario: 0, slope: -0.409723483039586, fluctuations: 0.0\n",
      "step: 60750 loss: 256.415992 time elapsed: 76.0599 learning rate: 0.001328, scenario: 0, slope: -0.3943522789930699, fluctuations: 0.0\n",
      "step: 60760 loss: 252.925604 time elapsed: 76.0716 learning rate: 0.001328, scenario: 0, slope: -0.382637578211858, fluctuations: 0.0\n",
      "step: 60770 loss: 249.490389 time elapsed: 76.0832 learning rate: 0.001328, scenario: 0, slope: -0.37329573804429067, fluctuations: 0.0\n",
      "step: 60780 loss: 246.109094 time elapsed: 76.0949 learning rate: 0.001328, scenario: 0, slope: -0.365467465437513, fluctuations: 0.0\n",
      "step: 60790 loss: 242.781239 time elapsed: 76.1088 learning rate: 0.001328, scenario: 0, slope: -0.3585957500667485, fluctuations: 0.0\n",
      "step: 60800 loss: 239.506980 time elapsed: 76.1225 learning rate: 0.001328, scenario: 0, slope: -0.35293400701724686, fluctuations: 0.0\n",
      "step: 60810 loss: 236.287008 time elapsed: 76.1366 learning rate: 0.001328, scenario: 0, slope: -0.34643633617106384, fluctuations: 0.0\n",
      "step: 60820 loss: 233.122453 time elapsed: 76.1498 learning rate: 0.001328, scenario: 0, slope: -0.34077603904860065, fluctuations: 0.0\n",
      "step: 60830 loss: 230.014794 time elapsed: 76.1633 learning rate: 0.001328, scenario: 0, slope: -0.33524057123494744, fluctuations: 0.0\n",
      "step: 60840 loss: 226.965756 time elapsed: 76.1772 learning rate: 0.001328, scenario: 0, slope: -0.3297491162465461, fluctuations: 0.0\n",
      "step: 60850 loss: 223.977210 time elapsed: 76.1894 learning rate: 0.001328, scenario: 0, slope: -0.3242379169128337, fluctuations: 0.0\n",
      "step: 60860 loss: 221.051710 time elapsed: 76.2014 learning rate: 0.001328, scenario: 0, slope: -0.3186560896413432, fluctuations: 0.0\n",
      "step: 60870 loss: 218.190567 time elapsed: 76.2136 learning rate: 0.001328, scenario: 0, slope: -0.31288884240338116, fluctuations: 0.0\n",
      "step: 60880 loss: 215.398597 time elapsed: 76.2257 learning rate: 0.001328, scenario: 0, slope: -0.3070390722238544, fluctuations: 0.0\n",
      "step: 60890 loss: 212.671387 time elapsed: 76.2376 learning rate: 0.001328, scenario: 0, slope: -0.3010482215292904, fluctuations: 0.0\n",
      "step: 60900 loss: 210.010824 time elapsed: 76.2491 learning rate: 0.001328, scenario: 0, slope: -0.295520794088074, fluctuations: 0.0\n",
      "step: 60910 loss: 207.419405 time elapsed: 76.2615 learning rate: 0.001328, scenario: 0, slope: -0.2885911236894355, fluctuations: 0.0\n",
      "step: 60920 loss: 204.896118 time elapsed: 76.2733 learning rate: 0.001328, scenario: 0, slope: -0.2821421857488827, fluctuations: 0.0\n",
      "step: 60930 loss: 202.440352 time elapsed: 76.2851 learning rate: 0.001328, scenario: 0, slope: -0.2755779374570932, fluctuations: 0.0\n",
      "step: 60940 loss: 200.050669 time elapsed: 76.2973 learning rate: 0.001328, scenario: 0, slope: -0.2689336875579866, fluctuations: 0.0\n",
      "step: 60950 loss: 197.725423 time elapsed: 76.3113 learning rate: 0.001328, scenario: 0, slope: -0.2622514556474935, fluctuations: 0.0\n",
      "step: 60960 loss: 195.462501 time elapsed: 76.3251 learning rate: 0.001328, scenario: 0, slope: -0.25557738060531265, fluctuations: 0.0\n",
      "step: 60970 loss: 193.259505 time elapsed: 76.3389 learning rate: 0.001328, scenario: 0, slope: -0.24886416371212627, fluctuations: 0.0\n",
      "step: 60980 loss: 191.113783 time elapsed: 76.3525 learning rate: 0.001328, scenario: 0, slope: -0.2422870114066866, fluctuations: 0.0\n",
      "step: 60990 loss: 189.022544 time elapsed: 76.3665 learning rate: 0.001328, scenario: 0, slope: -0.2358600264815428, fluctuations: 0.0\n",
      "step: 61000 loss: 186.983128 time elapsed: 76.3797 learning rate: 0.001328, scenario: 0, slope: -0.23022868621998946, fluctuations: 0.0\n",
      "step: 61010 loss: 185.007747 time elapsed: 76.3953 learning rate: 0.001328, scenario: 0, slope: -0.2235144850402621, fluctuations: 0.0\n",
      "step: 61020 loss: 183.053023 time elapsed: 76.4081 learning rate: 0.001328, scenario: 0, slope: -0.2176946601785345, fluctuations: 0.0\n",
      "step: 61030 loss: 181.149700 time elapsed: 76.4203 learning rate: 0.001328, scenario: 0, slope: -0.21214815296425618, fluctuations: 0.0\n",
      "step: 61040 loss: 179.287475 time elapsed: 76.4322 learning rate: 0.001328, scenario: 0, slope: -0.2068765028963613, fluctuations: 0.0\n",
      "step: 61050 loss: 177.464586 time elapsed: 76.4450 learning rate: 0.001328, scenario: 0, slope: -0.20188003214610267, fluctuations: 0.0\n",
      "step: 61060 loss: 175.677754 time elapsed: 76.4574 learning rate: 0.001328, scenario: 0, slope: -0.1971573696784456, fluctuations: 0.0\n",
      "step: 61070 loss: 173.924996 time elapsed: 76.4694 learning rate: 0.001607, scenario: 1, slope: -0.19270382394821509, fluctuations: 0.0\n",
      "step: 61080 loss: 219.901742 time elapsed: 76.4810 learning rate: 0.004169, scenario: 1, slope: -0.14424577607537467, fluctuations: 0.0\n",
      "step: 61090 loss: 197.509888 time elapsed: 76.4931 learning rate: 0.001884, scenario: -1, slope: 0.2532896018988665, fluctuations: 0.03\n",
      "step: 61100 loss: 167.347863 time elapsed: 76.5060 learning rate: 0.000730, scenario: -1, slope: 0.22559077534322142, fluctuations: 0.06\n",
      "step: 61110 loss: 165.481978 time elapsed: 76.5204 learning rate: 0.000255, scenario: -1, slope: 0.1063567274837725, fluctuations: 0.08\n",
      "step: 61120 loss: 165.167240 time elapsed: 76.5337 learning rate: 0.000127, scenario: 1, slope: -0.01339311350755292, fluctuations: 0.09\n",
      "step: 61130 loss: 164.963145 time elapsed: 76.5474 learning rate: 0.000330, scenario: 1, slope: -0.11842592914785145, fluctuations: 0.1\n",
      "step: 61140 loss: 164.247260 time elapsed: 76.5610 learning rate: 0.000585, scenario: 0, slope: -0.21168771084097207, fluctuations: 0.12\n",
      "step: 61150 loss: 163.509492 time elapsed: 76.5746 learning rate: 0.000585, scenario: 0, slope: -0.30961688980408275, fluctuations: 0.12\n",
      "step: 61160 loss: 162.801550 time elapsed: 76.5870 learning rate: 0.000585, scenario: 0, slope: -0.40912862903285524, fluctuations: 0.12\n",
      "step: 61170 loss: 162.118650 time elapsed: 76.5999 learning rate: 0.000585, scenario: 0, slope: -0.5224493679565845, fluctuations: 0.12\n",
      "step: 61180 loss: 161.450396 time elapsed: 76.6123 learning rate: 0.000585, scenario: 0, slope: -0.6083725667150217, fluctuations: 0.12\n",
      "step: 61190 loss: 160.701923 time elapsed: 76.6241 learning rate: 0.001036, scenario: 1, slope: -0.10896367173843699, fluctuations: 0.09\n",
      "step: 61200 loss: 159.013586 time elapsed: 76.6357 learning rate: 0.002442, scenario: 1, slope: -0.0758025070945946, fluctuations: 0.06\n",
      "step: 61210 loss: 155.053081 time elapsed: 76.6484 learning rate: 0.006333, scenario: 1, slope: -0.08393014422602582, fluctuations: 0.04\n",
      "step: 61220 loss: 34195.691467 time elapsed: 76.6598 learning rate: 0.006388, scenario: -1, slope: 30.154812389846892, fluctuations: 0.04\n",
      "step: 61230 loss: 3868.391927 time elapsed: 76.6719 learning rate: 0.002227, scenario: -1, slope: 43.162604522413815, fluctuations: 0.05\n",
      "step: 61240 loss: 2541.727894 time elapsed: 76.6841 learning rate: 0.000777, scenario: -1, slope: 49.15934937775518, fluctuations: 0.06\n",
      "step: 61250 loss: 1581.835199 time elapsed: 76.6961 learning rate: 0.000271, scenario: -1, slope: 43.19313375570792, fluctuations: 0.06\n",
      "step: 61260 loss: 1365.830363 time elapsed: 76.7109 learning rate: 0.000094, scenario: -1, slope: 34.22018045423276, fluctuations: 0.06\n",
      "step: 61270 loss: 1308.966896 time elapsed: 76.7253 learning rate: 0.000033, scenario: -1, slope: 23.849171311960138, fluctuations: 0.06\n",
      "step: 61280 loss: 1291.920754 time elapsed: 76.7393 learning rate: 0.000011, scenario: -1, slope: 12.280122017810314, fluctuations: 0.06\n",
      "step: 61290 loss: 1286.251235 time elapsed: 76.7536 learning rate: 0.000004, scenario: 0, slope: -0.8712182491043292, fluctuations: 0.06\n",
      "step: 61300 loss: 1283.293232 time elapsed: 76.7669 learning rate: 0.000004, scenario: 0, slope: -14.60142041877941, fluctuations: 0.06\n",
      "step: 61310 loss: 1280.387336 time elapsed: 76.7829 learning rate: 0.000004, scenario: 0, slope: -34.91032881557848, fluctuations: 0.06\n",
      "step: 61320 loss: 1277.527124 time elapsed: 76.7985 learning rate: 0.000004, scenario: 0, slope: -41.16966577856447, fluctuations: 0.04\n",
      "step: 61330 loss: 1274.707681 time elapsed: 76.8130 learning rate: 0.000004, scenario: 0, slope: -12.047717075635518, fluctuations: 0.02\n",
      "step: 61340 loss: 1271.924892 time elapsed: 76.8260 learning rate: 0.000004, scenario: 0, slope: -3.771602196290602, fluctuations: 0.0\n",
      "step: 61350 loss: 1269.147856 time elapsed: 76.8380 learning rate: 0.000006, scenario: 1, slope: -1.282974542283211, fluctuations: 0.0\n",
      "step: 61360 loss: 1263.920818 time elapsed: 76.8495 learning rate: 0.000015, scenario: 1, slope: -0.522562520866718, fluctuations: 0.0\n",
      "step: 61370 loss: 1250.712041 time elapsed: 76.8614 learning rate: 0.000040, scenario: 1, slope: -0.3744283373560752, fluctuations: 0.0\n",
      "step: 61380 loss: 1218.215996 time elapsed: 76.8743 learning rate: 0.000103, scenario: 1, slope: -0.4695164428203291, fluctuations: 0.0\n",
      "step: 61390 loss: 1141.516866 time elapsed: 76.8874 learning rate: 0.000268, scenario: 1, slope: -0.8503173619084288, fluctuations: 0.0\n",
      "step: 61400 loss: 986.946388 time elapsed: 76.8990 learning rate: 0.000431, scenario: 0, slope: -1.6544401738263625, fluctuations: 0.0\n",
      "step: 61410 loss: 840.971289 time elapsed: 76.9112 learning rate: 0.000431, scenario: 0, slope: -3.313702327191064, fluctuations: 0.0\n",
      "step: 61420 loss: 727.226256 time elapsed: 76.9252 learning rate: 0.000431, scenario: 0, slope: -5.1004724623749365, fluctuations: 0.0\n",
      "step: 61430 loss: 640.946317 time elapsed: 76.9387 learning rate: 0.000431, scenario: 0, slope: -6.829414633085304, fluctuations: 0.0\n",
      "step: 61440 loss: 575.223691 time elapsed: 76.9521 learning rate: 0.000431, scenario: 0, slope: -8.259030097530834, fluctuations: 0.0\n",
      "step: 61450 loss: 524.463430 time elapsed: 76.9659 learning rate: 0.000431, scenario: 0, slope: -9.209875055505801, fluctuations: 0.0\n",
      "step: 61460 loss: 484.284166 time elapsed: 76.9796 learning rate: 0.000431, scenario: 0, slope: -9.556090760955124, fluctuations: 0.0\n",
      "step: 61470 loss: 451.751059 time elapsed: 76.9932 learning rate: 0.000431, scenario: 0, slope: -9.23549909025086, fluctuations: 0.0\n",
      "step: 61480 loss: 425.042774 time elapsed: 77.0065 learning rate: 0.000431, scenario: 0, slope: -8.268113485014553, fluctuations: 0.0\n",
      "step: 61490 loss: 402.855726 time elapsed: 77.0192 learning rate: 0.000431, scenario: 0, slope: -6.823763879764856, fluctuations: 0.0\n",
      "step: 61500 loss: 384.181781 time elapsed: 77.0316 learning rate: 0.000431, scenario: 0, slope: -5.478404777702785, fluctuations: 0.0\n",
      "step: 61510 loss: 368.230924 time elapsed: 77.0445 learning rate: 0.000431, scenario: 0, slope: -4.200577438376802, fluctuations: 0.0\n",
      "step: 61520 loss: 354.397584 time elapsed: 77.0567 learning rate: 0.000431, scenario: 0, slope: -3.348354783870452, fluctuations: 0.0\n",
      "step: 61530 loss: 342.227038 time elapsed: 77.0688 learning rate: 0.000431, scenario: 0, slope: -2.7171154234226256, fluctuations: 0.0\n",
      "step: 61540 loss: 331.381651 time elapsed: 77.0809 learning rate: 0.000431, scenario: 0, slope: -2.2445234364540636, fluctuations: 0.0\n",
      "step: 61550 loss: 321.612072 time elapsed: 77.0927 learning rate: 0.000431, scenario: 0, slope: -1.8846923999863423, fluctuations: 0.0\n",
      "step: 61560 loss: 312.733859 time elapsed: 77.1040 learning rate: 0.000431, scenario: 0, slope: -1.6060197323199292, fluctuations: 0.0\n",
      "step: 61570 loss: 304.609423 time elapsed: 77.1160 learning rate: 0.000431, scenario: 0, slope: -1.3879226897311758, fluctuations: 0.0\n",
      "step: 61580 loss: 297.134513 time elapsed: 77.1278 learning rate: 0.000431, scenario: 0, slope: -1.21582798810773, fluctuations: 0.0\n",
      "step: 61590 loss: 290.228320 time elapsed: 77.1419 learning rate: 0.000431, scenario: 0, slope: -1.0785820392216172, fluctuations: 0.0\n",
      "step: 61600 loss: 283.826377 time elapsed: 77.1551 learning rate: 0.000431, scenario: 0, slope: -0.9776618052855248, fluctuations: 0.0\n",
      "step: 61610 loss: 277.875513 time elapsed: 77.1691 learning rate: 0.000431, scenario: 0, slope: -0.8761095154723417, fluctuations: 0.0\n",
      "step: 61620 loss: 272.330338 time elapsed: 77.1819 learning rate: 0.000431, scenario: 0, slope: -0.7994359757565528, fluctuations: 0.0\n",
      "step: 61630 loss: 267.150895 time elapsed: 77.1951 learning rate: 0.000431, scenario: 0, slope: -0.7339727769703679, fluctuations: 0.0\n",
      "step: 61640 loss: 262.301169 time elapsed: 77.2088 learning rate: 0.000431, scenario: 0, slope: -0.6771852322167297, fluctuations: 0.0\n",
      "step: 61650 loss: 257.748197 time elapsed: 77.2214 learning rate: 0.000431, scenario: 0, slope: -0.627278185618113, fluctuations: 0.0\n",
      "step: 61660 loss: 253.461544 time elapsed: 77.2336 learning rate: 0.000431, scenario: 0, slope: -0.5829872870290758, fluctuations: 0.0\n",
      "step: 61670 loss: 249.413012 time elapsed: 77.2452 learning rate: 0.000431, scenario: 0, slope: -0.5434209094964115, fluctuations: 0.0\n",
      "step: 61680 loss: 245.576555 time elapsed: 77.2571 learning rate: 0.000431, scenario: 0, slope: -0.5079429592221683, fluctuations: 0.0\n",
      "step: 61690 loss: 241.928350 time elapsed: 77.2690 learning rate: 0.000431, scenario: 0, slope: -0.4760873087497044, fluctuations: 0.0\n",
      "step: 61700 loss: 238.446961 time elapsed: 77.2808 learning rate: 0.000431, scenario: 0, slope: -0.45021691958703763, fluctuations: 0.0\n",
      "step: 61710 loss: 235.113474 time elapsed: 77.2931 learning rate: 0.000431, scenario: 0, slope: -0.42187517970355026, fluctuations: 0.0\n",
      "step: 61720 loss: 231.911485 time elapsed: 77.3050 learning rate: 0.000431, scenario: 0, slope: -0.3989658662539252, fluctuations: 0.0\n",
      "step: 61730 loss: 228.826889 time elapsed: 77.3168 learning rate: 0.000431, scenario: 0, slope: -0.37852536788956914, fluctuations: 0.0\n",
      "step: 61740 loss: 225.847330 time elapsed: 77.3286 learning rate: 0.000431, scenario: 0, slope: -0.36031891411829386, fluctuations: 0.0\n",
      "step: 61750 loss: 222.960443 time elapsed: 77.3424 learning rate: 0.000431, scenario: 0, slope: -0.34412138703016404, fluctuations: 0.0\n",
      "step: 61760 loss: 220.142496 time elapsed: 77.3557 learning rate: 0.000431, scenario: 0, slope: -0.3297508517256688, fluctuations: 0.0\n",
      "step: 61770 loss: 217.274706 time elapsed: 77.3687 learning rate: 0.000431, scenario: 0, slope: -0.31732721520669266, fluctuations: 0.0\n",
      "step: 61780 loss: 214.204141 time elapsed: 77.3820 learning rate: 0.000431, scenario: 0, slope: -0.3080367092313534, fluctuations: 0.0\n",
      "step: 61790 loss: 211.557036 time elapsed: 77.3948 learning rate: 0.000431, scenario: 0, slope: -0.30043546744810673, fluctuations: 0.0\n",
      "step: 61800 loss: 209.003208 time elapsed: 77.4081 learning rate: 0.000431, scenario: 0, slope: -0.2941129019088599, fluctuations: 0.0\n",
      "step: 61810 loss: 206.564785 time elapsed: 77.4216 learning rate: 0.000431, scenario: 0, slope: -0.2864549125267606, fluctuations: 0.0\n",
      "step: 61820 loss: 204.201430 time elapsed: 77.4339 learning rate: 0.000431, scenario: 0, slope: -0.2792116084859254, fluctuations: 0.0\n",
      "step: 61830 loss: 201.898007 time elapsed: 77.4459 learning rate: 0.000431, scenario: 0, slope: -0.27155370285416264, fluctuations: 0.0\n",
      "step: 61840 loss: 199.647612 time elapsed: 77.4575 learning rate: 0.000431, scenario: 0, slope: -0.26341847498681, fluctuations: 0.0\n",
      "step: 61850 loss: 197.446515 time elapsed: 77.4689 learning rate: 0.000431, scenario: 0, slope: -0.2547838352862012, fluctuations: 0.0\n",
      "step: 61860 loss: 195.292054 time elapsed: 77.4809 learning rate: 0.000431, scenario: 0, slope: -0.2456946766677859, fluctuations: 0.0\n",
      "step: 61870 loss: 193.181839 time elapsed: 77.4929 learning rate: 0.000431, scenario: 0, slope: -0.23656979983919935, fluctuations: 0.0\n",
      "step: 61880 loss: 191.113749 time elapsed: 77.5052 learning rate: 0.000431, scenario: 0, slope: -0.2292783998532078, fluctuations: 0.0\n",
      "step: 61890 loss: 189.085878 time elapsed: 77.5174 learning rate: 0.000431, scenario: 0, slope: -0.22337103819558504, fluctuations: 0.0\n",
      "step: 61900 loss: 187.096515 time elapsed: 77.5313 learning rate: 0.000431, scenario: 0, slope: -0.2187086956511124, fluctuations: 0.0\n",
      "step: 61910 loss: 185.144101 time elapsed: 77.5445 learning rate: 0.000431, scenario: 0, slope: -0.2135521867635055, fluctuations: 0.0\n",
      "step: 61920 loss: 183.227219 time elapsed: 77.5585 learning rate: 0.000431, scenario: 0, slope: -0.20917668412584478, fluctuations: 0.0\n",
      "step: 61930 loss: 181.344576 time elapsed: 77.5723 learning rate: 0.000431, scenario: 0, slope: -0.20502091684495796, fluctuations: 0.0\n",
      "step: 61940 loss: 179.495002 time elapsed: 77.5860 learning rate: 0.000431, scenario: 0, slope: -0.2010526295822467, fluctuations: 0.0\n",
      "step: 61950 loss: 177.677432 time elapsed: 77.5994 learning rate: 0.000431, scenario: 0, slope: -0.19725218468074865, fluctuations: 0.0\n",
      "step: 61960 loss: 175.081350 time elapsed: 77.6125 learning rate: 0.001119, scenario: 1, slope: -0.19506806186060693, fluctuations: 0.0\n",
      "step: 61970 loss: 170.651629 time elapsed: 77.6265 learning rate: 0.001119, scenario: 0, slope: -0.20380542073123906, fluctuations: 0.0\n",
      "step: 61980 loss: 166.385614 time elapsed: 77.6391 learning rate: 0.001119, scenario: 0, slope: -0.22406523556436417, fluctuations: 0.0\n",
      "step: 61990 loss: 162.327613 time elapsed: 77.6513 learning rate: 0.001119, scenario: 0, slope: -0.25172480481818327, fluctuations: 0.0\n",
      "step: 62000 loss: 158.470533 time elapsed: 77.6631 learning rate: 0.001119, scenario: 0, slope: -0.2797249619698487, fluctuations: 0.0\n",
      "step: 62010 loss: 154.804704 time elapsed: 77.6751 learning rate: 0.001119, scenario: 0, slope: -0.31394088111161933, fluctuations: 0.0\n",
      "step: 62020 loss: 151.320269 time elapsed: 77.6870 learning rate: 0.001119, scenario: 0, slope: -0.341484495754799, fluctuations: 0.0\n",
      "step: 62030 loss: 148.008066 time elapsed: 77.6990 learning rate: 0.001119, scenario: 0, slope: -0.36238493436902275, fluctuations: 0.0\n",
      "step: 62040 loss: 144.859842 time elapsed: 77.7111 learning rate: 0.001119, scenario: 0, slope: -0.37372651594796447, fluctuations: 0.0\n",
      "step: 62050 loss: 141.867966 time elapsed: 77.7232 learning rate: 0.001119, scenario: 0, slope: -0.37280423819015124, fluctuations: 0.0\n",
      "step: 62060 loss: 139.025034 time elapsed: 77.7346 learning rate: 0.001119, scenario: 0, slope: -0.35870943902579566, fluctuations: 0.0\n",
      "step: 62070 loss: 136.323586 time elapsed: 77.7463 learning rate: 0.001119, scenario: 0, slope: -0.3409895787283663, fluctuations: 0.0\n",
      "step: 62080 loss: 133.755142 time elapsed: 77.7605 learning rate: 0.001119, scenario: 0, slope: -0.3240942958852939, fluctuations: 0.0\n",
      "step: 62090 loss: 131.317446 time elapsed: 77.7743 learning rate: 0.001119, scenario: 0, slope: -0.308027017456094, fluctuations: 0.0\n",
      "step: 62100 loss: 129.056807 time elapsed: 77.7878 learning rate: 0.001119, scenario: 0, slope: -0.29411967970970626, fluctuations: 0.0\n",
      "step: 62110 loss: 126.941592 time elapsed: 77.8016 learning rate: 0.001119, scenario: 0, slope: -0.27749996580295605, fluctuations: 0.0\n",
      "step: 62120 loss: 124.953227 time elapsed: 77.8146 learning rate: 0.001119, scenario: 0, slope: -0.26272592479927287, fluctuations: 0.0\n",
      "step: 62130 loss: 123.078993 time elapsed: 77.8284 learning rate: 0.001119, scenario: 0, slope: -0.2482743357906599, fluctuations: 0.0\n",
      "step: 62140 loss: 121.310602 time elapsed: 77.8407 learning rate: 0.001119, scenario: 0, slope: -0.2342065178694323, fluctuations: 0.0\n",
      "step: 62150 loss: 119.639497 time elapsed: 77.8528 learning rate: 0.001119, scenario: 0, slope: -0.22061007031677735, fluctuations: 0.0\n",
      "step: 62160 loss: 118.058112 time elapsed: 77.8647 learning rate: 0.001119, scenario: 0, slope: -0.20759378127210887, fluctuations: 0.0\n",
      "step: 62170 loss: 116.559276 time elapsed: 77.8766 learning rate: 0.001119, scenario: 0, slope: -0.1952845207384396, fluctuations: 0.0\n",
      "step: 62180 loss: 115.136255 time elapsed: 77.8889 learning rate: 0.001119, scenario: 0, slope: -0.1838275720992898, fluctuations: 0.0\n",
      "step: 62190 loss: 113.782809 time elapsed: 77.9010 learning rate: 0.001119, scenario: 0, slope: -0.17338041782127825, fluctuations: 0.0\n",
      "step: 62200 loss: 112.493188 time elapsed: 77.9129 learning rate: 0.001119, scenario: 0, slope: -0.1648304593784235, fluctuations: 0.0\n",
      "step: 62210 loss: 111.262135 time elapsed: 77.9252 learning rate: 0.001119, scenario: 0, slope: -0.1552142093240769, fluctuations: 0.0\n",
      "step: 62220 loss: 110.084877 time elapsed: 77.9370 learning rate: 0.001119, scenario: 0, slope: -0.14721942249103714, fluctuations: 0.0\n",
      "step: 62230 loss: 108.957092 time elapsed: 77.9491 learning rate: 0.001119, scenario: 0, slope: -0.13986817439700558, fluctuations: 0.0\n",
      "step: 62240 loss: 107.874873 time elapsed: 77.9631 learning rate: 0.001119, scenario: 0, slope: -0.13311374551630556, fluctuations: 0.0\n",
      "step: 62250 loss: 106.834690 time elapsed: 77.9766 learning rate: 0.001119, scenario: 0, slope: -0.1269090049086039, fluctuations: 0.0\n",
      "step: 62260 loss: 105.833347 time elapsed: 77.9903 learning rate: 0.001119, scenario: 0, slope: -0.12120784403140662, fluctuations: 0.0\n",
      "step: 62270 loss: 104.867946 time elapsed: 78.0038 learning rate: 0.001354, scenario: 1, slope: -0.11596617815740133, fluctuations: 0.0\n",
      "step: 62280 loss: 103.249659 time elapsed: 78.0171 learning rate: 0.003512, scenario: 1, slope: -0.11259926096710723, fluctuations: 0.0\n",
      "step: 62290 loss: 99.666400 time elapsed: 78.0309 learning rate: 0.005656, scenario: 0, slope: -0.11892814977244898, fluctuations: 0.0\n",
      "step: 62300 loss: 95.906340 time elapsed: 78.0439 learning rate: 0.005656, scenario: 0, slope: -0.1368747175032957, fluctuations: 0.0\n",
      "step: 62310 loss: 92.633020 time elapsed: 78.0568 learning rate: 0.005656, scenario: 0, slope: -0.16831622445086808, fluctuations: 0.0\n",
      "step: 62320 loss: 89.737531 time elapsed: 78.0691 learning rate: 0.005656, scenario: 0, slope: -0.20034964674409975, fluctuations: 0.0\n",
      "step: 62330 loss: 87.135956 time elapsed: 78.0811 learning rate: 0.005656, scenario: 0, slope: -0.23116182990514636, fluctuations: 0.0\n",
      "step: 62340 loss: 84.767910 time elapsed: 78.0927 learning rate: 0.005656, scenario: 0, slope: -0.2571526780230172, fluctuations: 0.0\n",
      "step: 62350 loss: 82.589134 time elapsed: 78.1044 learning rate: 0.005656, scenario: 0, slope: -0.27527464469751334, fluctuations: 0.0\n",
      "step: 62360 loss: 80.566909 time elapsed: 78.1165 learning rate: 0.005656, scenario: 0, slope: -0.2829018697303944, fluctuations: 0.0\n",
      "step: 62370 loss: 78.677456 time elapsed: 78.1283 learning rate: 0.005656, scenario: 0, slope: -0.2777330397156981, fluctuations: 0.0\n",
      "step: 62380 loss: 76.903362 time elapsed: 78.1400 learning rate: 0.005656, scenario: 0, slope: -0.2593372514294735, fluctuations: 0.0\n",
      "step: 62390 loss: 75.229576 time elapsed: 78.1523 learning rate: 0.005656, scenario: 0, slope: -0.2358597278651969, fluctuations: 0.0\n",
      "step: 62400 loss: 73.634200 time elapsed: 78.1660 learning rate: 0.005656, scenario: 0, slope: -0.21803259624127735, fluctuations: 0.0\n",
      "step: 62410 loss: 72.897244 time elapsed: 78.1804 learning rate: 0.005656, scenario: 0, slope: -0.19777699868337065, fluctuations: 0.02\n",
      "step: 62420 loss: 71.319242 time elapsed: 78.1943 learning rate: 0.005656, scenario: 0, slope: -0.18183258996280527, fluctuations: 0.05\n",
      "step: 62430 loss: 69.486011 time elapsed: 78.2073 learning rate: 0.005656, scenario: 0, slope: -0.16954934580580577, fluctuations: 0.07\n",
      "step: 62440 loss: 68.146751 time elapsed: 78.2201 learning rate: 0.005656, scenario: 0, slope: -0.1599482985533485, fluctuations: 0.07\n",
      "step: 62450 loss: 66.764171 time elapsed: 78.2335 learning rate: 0.005656, scenario: 0, slope: -0.15285783191234698, fluctuations: 0.07\n",
      "step: 62460 loss: 65.515130 time elapsed: 78.2466 learning rate: 0.005656, scenario: 0, slope: -0.14726008979435626, fluctuations: 0.07\n",
      "step: 62470 loss: 64.295190 time elapsed: 78.2585 learning rate: 0.005656, scenario: 0, slope: -0.14273433573845157, fluctuations: 0.07\n",
      "step: 62480 loss: 63.098277 time elapsed: 78.2707 learning rate: 0.005656, scenario: 0, slope: -0.13912538939435093, fluctuations: 0.07\n",
      "step: 62490 loss: 61.989494 time elapsed: 78.2826 learning rate: 0.005656, scenario: 0, slope: -0.13611470683610935, fluctuations: 0.07\n",
      "step: 62500 loss: 60.933425 time elapsed: 78.2941 learning rate: 0.005656, scenario: 0, slope: -0.1337222378185629, fluctuations: 0.07\n",
      "step: 62510 loss: 59.925602 time elapsed: 78.3067 learning rate: 0.005656, scenario: 0, slope: -0.12780407707180533, fluctuations: 0.04\n",
      "step: 62520 loss: 58.959332 time elapsed: 78.3186 learning rate: 0.005656, scenario: 0, slope: -0.11957811156025595, fluctuations: 0.02\n",
      "step: 62530 loss: 58.027906 time elapsed: 78.3304 learning rate: 0.005656, scenario: 0, slope: -0.11423344561536665, fluctuations: 0.0\n",
      "step: 62540 loss: 57.124606 time elapsed: 78.3427 learning rate: 0.005656, scenario: 0, slope: -0.10911195531584951, fluctuations: 0.0\n",
      "step: 62550 loss: 56.240859 time elapsed: 78.3547 learning rate: 0.005656, scenario: 0, slope: -0.10449343092808899, fluctuations: 0.0\n",
      "step: 62560 loss: 55.340878 time elapsed: 78.3682 learning rate: 0.005656, scenario: 0, slope: -0.10027801798976967, fluctuations: 0.0\n",
      "step: 62570 loss: 55.677921 time elapsed: 78.3821 learning rate: 0.005656, scenario: 0, slope: -0.09044394012497532, fluctuations: 0.01\n",
      "step: 62580 loss: 55.491594 time elapsed: 78.3959 learning rate: 0.005656, scenario: 0, slope: -0.08122555436700915, fluctuations: 0.04\n",
      "step: 62590 loss: 53.509989 time elapsed: 78.4096 learning rate: 0.005656, scenario: 0, slope: -0.0777929082972444, fluctuations: 0.08\n",
      "step: 62600 loss: 52.344006 time elapsed: 78.4230 learning rate: 0.005656, scenario: 0, slope: -0.07654766645439531, fluctuations: 0.1\n",
      "step: 62610 loss: 51.524161 time elapsed: 78.4376 learning rate: 0.005656, scenario: 0, slope: -0.07742032684594813, fluctuations: 0.12\n",
      "step: 62620 loss: 50.685278 time elapsed: 78.4502 learning rate: 0.005656, scenario: 0, slope: -0.07921532811914013, fluctuations: 0.12\n",
      "step: 62630 loss: 49.859641 time elapsed: 78.4621 learning rate: 0.005656, scenario: 0, slope: -0.08138070579427276, fluctuations: 0.12\n",
      "step: 62640 loss: 49.040013 time elapsed: 78.4741 learning rate: 0.005656, scenario: 0, slope: -0.08410802870245039, fluctuations: 0.12\n",
      "step: 62650 loss: 48.178282 time elapsed: 78.4858 learning rate: 0.005656, scenario: 0, slope: -0.08781677460079161, fluctuations: 0.12\n",
      "step: 62660 loss: 47.546834 time elapsed: 78.4980 learning rate: 0.005656, scenario: 0, slope: -0.0930030344913611, fluctuations: 0.12\n",
      "step: 62670 loss: 52.349181 time elapsed: 78.5096 learning rate: 0.005656, scenario: 0, slope: -0.05756647274352235, fluctuations: 0.12\n",
      "step: 62680 loss: 2299.288798 time elapsed: 78.5214 learning rate: 0.007748, scenario: -1, slope: 2.53347995666366, fluctuations: 0.1\n",
      "step: 62690 loss: 402.138408 time elapsed: 78.5333 learning rate: 0.002702, scenario: -1, slope: 5.071560087675256, fluctuations: 0.1\n",
      "step: 62700 loss: 125.317323 time elapsed: 78.5447 learning rate: 0.001047, scenario: -1, slope: 4.469160265662969, fluctuations: 0.1\n",
      "step: 62710 loss: 81.165591 time elapsed: 78.5575 learning rate: 0.000365, scenario: -1, slope: 3.1358143535482417, fluctuations: 0.1\n",
      "step: 62720 loss: 70.669916 time elapsed: 78.5711 learning rate: 0.000127, scenario: -1, slope: 1.9738997792445123, fluctuations: 0.11\n",
      "step: 62730 loss: 67.050206 time elapsed: 78.5843 learning rate: 0.000044, scenario: -1, slope: 0.7274193868804069, fluctuations: 0.11\n",
      "step: 62740 loss: 66.843940 time elapsed: 78.5978 learning rate: 0.000024, scenario: 0, slope: -0.4881641978027431, fluctuations: 0.11\n",
      "step: 62750 loss: 66.693355 time elapsed: 78.6113 learning rate: 0.000024, scenario: 0, slope: -1.7809311501697511, fluctuations: 0.11\n",
      "step: 62760 loss: 66.535218 time elapsed: 78.6248 learning rate: 0.000024, scenario: 0, slope: -3.2881555393175597, fluctuations: 0.11\n",
      "step: 62770 loss: 66.415639 time elapsed: 78.6383 learning rate: 0.000024, scenario: 0, slope: -4.5615988273379235, fluctuations: 0.09\n",
      "step: 62780 loss: 66.313015 time elapsed: 78.6510 learning rate: 0.000024, scenario: 0, slope: -4.745212721327034, fluctuations: 0.07\n",
      "step: 62790 loss: 66.212656 time elapsed: 78.6633 learning rate: 0.000024, scenario: 0, slope: -0.6010703908951194, fluctuations: 0.04\n",
      "step: 62800 loss: 66.113438 time elapsed: 78.6750 learning rate: 0.000024, scenario: 0, slope: -0.21731171830646837, fluctuations: 0.01\n",
      "step: 62810 loss: 66.009636 time elapsed: 78.6879 learning rate: 0.000038, scenario: 1, slope: -0.044289595189978284, fluctuations: 0.01\n",
      "step: 62820 loss: 65.786852 time elapsed: 78.6995 learning rate: 0.000098, scenario: 1, slope: -0.018902707357123558, fluctuations: 0.0\n",
      "step: 62830 loss: 65.233741 time elapsed: 78.7120 learning rate: 0.000255, scenario: 1, slope: -0.013480016074551522, fluctuations: 0.0\n",
      "step: 62840 loss: 63.933191 time elapsed: 78.7248 learning rate: 0.000663, scenario: 1, slope: -0.018793448277794302, fluctuations: 0.0\n",
      "step: 62850 loss: 61.172032 time elapsed: 78.7374 learning rate: 0.001719, scenario: 1, slope: -0.03337144210407113, fluctuations: 0.0\n",
      "step: 62860 loss: 56.455485 time elapsed: 78.7495 learning rate: 0.004458, scenario: 1, slope: -0.06501737325005111, fluctuations: 0.0\n",
      "step: 62870 loss: 52.583828 time elapsed: 78.7621 learning rate: 0.004458, scenario: 0, slope: -0.11302737116325672, fluctuations: 0.0\n",
      "step: 62880 loss: 50.300745 time elapsed: 78.7760 learning rate: 0.004458, scenario: 0, slope: -0.16221074315841846, fluctuations: 0.0\n",
      "step: 62890 loss: 48.732202 time elapsed: 78.7899 learning rate: 0.004458, scenario: 0, slope: -0.20324133876576003, fluctuations: 0.0\n",
      "step: 62900 loss: 47.382555 time elapsed: 78.8027 learning rate: 0.004458, scenario: 0, slope: -0.23039944578120947, fluctuations: 0.0\n",
      "step: 62910 loss: 46.171431 time elapsed: 78.8160 learning rate: 0.004458, scenario: 0, slope: -0.24791411096263027, fluctuations: 0.0\n",
      "step: 62920 loss: 44.973005 time elapsed: 78.8294 learning rate: 0.004458, scenario: 0, slope: -0.24741644051424394, fluctuations: 0.0\n",
      "step: 62930 loss: 44.006366 time elapsed: 78.8433 learning rate: 0.004458, scenario: 0, slope: -0.23081217404034995, fluctuations: 0.0\n",
      "step: 62940 loss: 43.223351 time elapsed: 78.8554 learning rate: 0.004458, scenario: 0, slope: -0.19940620173559512, fluctuations: 0.0\n",
      "step: 62950 loss: 42.550065 time elapsed: 78.8673 learning rate: 0.004458, scenario: 0, slope: -0.15993198224278296, fluctuations: 0.0\n",
      "step: 62960 loss: 41.944168 time elapsed: 78.8793 learning rate: 0.004458, scenario: 0, slope: -0.12546105022671283, fluctuations: 0.0\n",
      "step: 62970 loss: 41.389819 time elapsed: 78.8912 learning rate: 0.004458, scenario: 0, slope: -0.10502523927447463, fluctuations: 0.0\n",
      "step: 62980 loss: 40.873359 time elapsed: 78.9031 learning rate: 0.004458, scenario: 0, slope: -0.091578744163045, fluctuations: 0.0\n",
      "step: 62990 loss: 40.385975 time elapsed: 78.9153 learning rate: 0.004458, scenario: 0, slope: -0.08028280138078166, fluctuations: 0.0\n",
      "step: 63000 loss: 39.921426 time elapsed: 78.9270 learning rate: 0.004458, scenario: 0, slope: -0.07160414684299794, fluctuations: 0.0\n",
      "step: 63010 loss: 39.475426 time elapsed: 78.9398 learning rate: 0.004458, scenario: 0, slope: -0.06253282867554914, fluctuations: 0.0\n",
      "step: 63020 loss: 39.044756 time elapsed: 78.9519 learning rate: 0.004458, scenario: 0, slope: -0.05624848905018339, fluctuations: 0.0\n",
      "step: 63030 loss: 38.626744 time elapsed: 78.9641 learning rate: 0.004458, scenario: 0, slope: -0.05197770561520567, fluctuations: 0.0\n",
      "step: 63040 loss: 38.218808 time elapsed: 78.9779 learning rate: 0.004458, scenario: 0, slope: -0.04881375018646649, fluctuations: 0.0\n",
      "step: 63050 loss: 37.817541 time elapsed: 78.9913 learning rate: 0.004458, scenario: 0, slope: -0.046408909531777814, fluctuations: 0.0\n",
      "step: 63060 loss: 37.415609 time elapsed: 79.0046 learning rate: 0.004458, scenario: 0, slope: -0.04453622427795282, fluctuations: 0.0\n",
      "step: 63070 loss: 37.003431 time elapsed: 79.0184 learning rate: 0.004458, scenario: 0, slope: -0.04314748920064454, fluctuations: 0.0\n",
      "step: 63080 loss: 36.606034 time elapsed: 79.0319 learning rate: 0.004458, scenario: 0, slope: -0.042155861188698435, fluctuations: 0.0\n",
      "step: 63090 loss: 37.492561 time elapsed: 79.0456 learning rate: 0.005933, scenario: 1, slope: -0.03931312690010432, fluctuations: 0.02\n",
      "step: 63100 loss: 24318.567493 time elapsed: 79.0589 learning rate: 0.008127, scenario: -1, slope: 5.918392327330732, fluctuations: 0.03\n",
      "step: 63110 loss: 3578.895968 time elapsed: 79.0724 learning rate: 0.002834, scenario: -1, slope: 21.994635936019517, fluctuations: 0.07\n",
      "step: 63120 loss: 914.491145 time elapsed: 79.0847 learning rate: 0.000988, scenario: -1, slope: 23.725193951088777, fluctuations: 0.09\n",
      "step: 63130 loss: 431.610962 time elapsed: 79.0968 learning rate: 0.000345, scenario: -1, slope: 19.51389838168027, fluctuations: 0.1\n",
      "step: 63140 loss: 309.174870 time elapsed: 79.1084 learning rate: 0.000120, scenario: -1, slope: 14.058047811874628, fluctuations: 0.11\n",
      "step: 63150 loss: 284.468071 time elapsed: 79.1203 learning rate: 0.000042, scenario: -1, slope: 7.8297622384665795, fluctuations: 0.11\n",
      "step: 63160 loss: 278.854527 time elapsed: 79.1323 learning rate: 0.000015, scenario: -1, slope: 1.5373309930943009, fluctuations: 0.11\n",
      "step: 63170 loss: 276.512439 time elapsed: 79.1439 learning rate: 0.000012, scenario: 0, slope: -5.3620232111086, fluctuations: 0.11\n",
      "step: 63180 loss: 274.481982 time elapsed: 79.1561 learning rate: 0.000012, scenario: 0, slope: -12.621221865206723, fluctuations: 0.1\n",
      "step: 63190 loss: 272.637891 time elapsed: 79.1687 learning rate: 0.000012, scenario: 0, slope: -19.59978127545438, fluctuations: 0.08\n",
      "step: 63200 loss: 270.944377 time elapsed: 79.1825 learning rate: 0.000012, scenario: 0, slope: -37.88016483728076, fluctuations: 0.07\n",
      "step: 63210 loss: 269.356520 time elapsed: 79.1964 learning rate: 0.000012, scenario: 0, slope: -8.451243453416145, fluctuations: 0.03\n",
      "step: 63220 loss: 267.839510 time elapsed: 79.2096 learning rate: 0.000012, scenario: 0, slope: -2.29144627768009, fluctuations: 0.01\n",
      "step: 63230 loss: 266.370342 time elapsed: 79.2229 learning rate: 0.000012, scenario: 0, slope: -0.6882860614201729, fluctuations: 0.0\n",
      "step: 63240 loss: 264.843682 time elapsed: 79.2365 learning rate: 0.000019, scenario: 1, slope: -0.24008397921478816, fluctuations: 0.0\n",
      "step: 63250 loss: 261.582375 time elapsed: 79.2503 learning rate: 0.000049, scenario: 1, slope: -0.18242736548223254, fluctuations: 0.0\n",
      "step: 63260 loss: 253.523358 time elapsed: 79.2629 learning rate: 0.000128, scenario: 1, slope: -0.19303633403816484, fluctuations: 0.0\n",
      "step: 63270 loss: 234.681965 time elapsed: 79.2752 learning rate: 0.000332, scenario: 1, slope: -0.27176821198736445, fluctuations: 0.0\n",
      "step: 63280 loss: 207.192415 time elapsed: 79.2873 learning rate: 0.000332, scenario: 0, slope: -0.4621849526544508, fluctuations: 0.0\n",
      "step: 63290 loss: 185.719297 time elapsed: 79.2993 learning rate: 0.000332, scenario: 0, slope: -0.7277249144839908, fluctuations: 0.0\n",
      "step: 63300 loss: 168.871652 time elapsed: 79.3112 learning rate: 0.000332, scenario: 0, slope: -0.9837676806449079, fluctuations: 0.0\n",
      "step: 63310 loss: 155.232667 time elapsed: 79.3237 learning rate: 0.000332, scenario: 0, slope: -1.2723910515714045, fluctuations: 0.0\n",
      "step: 63320 loss: 143.800022 time elapsed: 79.3353 learning rate: 0.000332, scenario: 0, slope: -1.4791203497785055, fluctuations: 0.0\n",
      "step: 63330 loss: 133.941685 time elapsed: 79.3470 learning rate: 0.000332, scenario: 0, slope: -1.6086972918626823, fluctuations: 0.0\n",
      "step: 63340 loss: 125.272466 time elapsed: 79.3590 learning rate: 0.000332, scenario: 0, slope: -1.642869589443828, fluctuations: 0.0\n",
      "step: 63350 loss: 117.558536 time elapsed: 79.3709 learning rate: 0.000332, scenario: 0, slope: -1.5718547180778035, fluctuations: 0.0\n",
      "step: 63360 loss: 110.657225 time elapsed: 79.3849 learning rate: 0.000332, scenario: 0, slope: -1.4053576986059464, fluctuations: 0.0\n",
      "step: 63370 loss: 104.477165 time elapsed: 79.3988 learning rate: 0.000332, scenario: 0, slope: -1.1879197301621853, fluctuations: 0.0\n",
      "step: 63380 loss: 98.953738 time elapsed: 79.4122 learning rate: 0.000332, scenario: 0, slope: -1.003351031636114, fluctuations: 0.0\n",
      "step: 63390 loss: 94.034695 time elapsed: 79.4254 learning rate: 0.000332, scenario: 0, slope: -0.8656281448972349, fluctuations: 0.0\n",
      "step: 63400 loss: 89.672210 time elapsed: 79.4385 learning rate: 0.000332, scenario: 0, slope: -0.7684209073889017, fluctuations: 0.0\n",
      "step: 63410 loss: 85.819064 time elapsed: 79.4523 learning rate: 0.000332, scenario: 0, slope: -0.6716302449782815, fluctuations: 0.0\n",
      "step: 63420 loss: 82.427384 time elapsed: 79.4655 learning rate: 0.000332, scenario: 0, slope: -0.597281683642304, fluctuations: 0.0\n",
      "step: 63430 loss: 79.448888 time elapsed: 79.4776 learning rate: 0.000332, scenario: 0, slope: -0.5317374971675578, fluctuations: 0.0\n",
      "step: 63440 loss: 76.835907 time elapsed: 79.4892 learning rate: 0.000332, scenario: 0, slope: -0.47282706318119, fluctuations: 0.0\n",
      "step: 63450 loss: 74.542623 time elapsed: 79.5011 learning rate: 0.000332, scenario: 0, slope: -0.4194475186128449, fluctuations: 0.0\n",
      "step: 63460 loss: 72.526214 time elapsed: 79.5129 learning rate: 0.000332, scenario: 0, slope: -0.37107633327220657, fluctuations: 0.0\n",
      "step: 63470 loss: 70.747677 time elapsed: 79.5251 learning rate: 0.000332, scenario: 0, slope: -0.3274636253494873, fluctuations: 0.0\n",
      "step: 63480 loss: 69.172284 time elapsed: 79.5372 learning rate: 0.000332, scenario: 0, slope: -0.28845039917972437, fluctuations: 0.0\n",
      "step: 63490 loss: 67.769711 time elapsed: 79.5490 learning rate: 0.000332, scenario: 0, slope: -0.25386962771353255, fluctuations: 0.0\n",
      "step: 63500 loss: 66.513893 time elapsed: 79.5602 learning rate: 0.000332, scenario: 0, slope: -0.22635642437246403, fluctuations: 0.0\n",
      "step: 63510 loss: 65.382705 time elapsed: 79.5722 learning rate: 0.000332, scenario: 0, slope: -0.19706364078099672, fluctuations: 0.0\n",
      "step: 63520 loss: 64.357535 time elapsed: 79.5855 learning rate: 0.000332, scenario: 0, slope: -0.17421965428900255, fluctuations: 0.0\n",
      "step: 63530 loss: 63.422830 time elapsed: 79.5993 learning rate: 0.000332, scenario: 0, slope: -0.15459956538049702, fluctuations: 0.0\n",
      "step: 63540 loss: 62.565632 time elapsed: 79.6124 learning rate: 0.000332, scenario: 0, slope: -0.13782106768276856, fluctuations: 0.0\n",
      "step: 63550 loss: 61.775156 time elapsed: 79.6260 learning rate: 0.000332, scenario: 0, slope: -0.12350927765384791, fluctuations: 0.0\n",
      "step: 63560 loss: 61.042405 time elapsed: 79.6395 learning rate: 0.000332, scenario: 0, slope: -0.11131183426997375, fluctuations: 0.0\n",
      "step: 63570 loss: 60.359845 time elapsed: 79.6527 learning rate: 0.000332, scenario: 0, slope: -0.10090877602203027, fluctuations: 0.0\n",
      "step: 63580 loss: 59.721128 time elapsed: 79.6656 learning rate: 0.000332, scenario: 0, slope: -0.09201762669231453, fluctuations: 0.0\n",
      "step: 63590 loss: 59.120857 time elapsed: 79.6774 learning rate: 0.000332, scenario: 0, slope: -0.08439466602843247, fluctuations: 0.0\n",
      "step: 63600 loss: 58.554394 time elapsed: 79.6892 learning rate: 0.000332, scenario: 0, slope: -0.07844700470586374, fluctuations: 0.0\n",
      "step: 63610 loss: 58.017703 time elapsed: 79.7013 learning rate: 0.000332, scenario: 0, slope: -0.07216207261786088, fluctuations: 0.0\n",
      "step: 63620 loss: 57.507208 time elapsed: 79.7133 learning rate: 0.000332, scenario: 0, slope: -0.06723866186567286, fluctuations: 0.0\n",
      "step: 63630 loss: 57.019672 time elapsed: 79.7254 learning rate: 0.000402, scenario: 1, slope: -0.06294798391666477, fluctuations: 0.0\n",
      "step: 63640 loss: 56.208781 time elapsed: 79.7376 learning rate: 0.001043, scenario: 1, slope: -0.059927062091030145, fluctuations: 0.0\n",
      "step: 63650 loss: 54.284293 time elapsed: 79.7493 learning rate: 0.002460, scenario: 0, slope: -0.062156621783365805, fluctuations: 0.0\n",
      "step: 63660 loss: 51.706381 time elapsed: 79.7614 learning rate: 0.002460, scenario: 0, slope: -0.07468812004267722, fluctuations: 0.0\n",
      "step: 63670 loss: 49.846917 time elapsed: 79.7734 learning rate: 0.002460, scenario: 0, slope: -0.09318521610822675, fluctuations: 0.0\n",
      "step: 63680 loss: 48.469215 time elapsed: 79.7851 learning rate: 0.002460, scenario: 0, slope: -0.1124019503553233, fluctuations: 0.0\n",
      "step: 63690 loss: 47.336698 time elapsed: 79.7986 learning rate: 0.002460, scenario: 0, slope: -0.12919298295357146, fluctuations: 0.0\n",
      "step: 63700 loss: 46.359960 time elapsed: 79.8122 learning rate: 0.002460, scenario: 0, slope: -0.14057220921533567, fluctuations: 0.0\n",
      "step: 63710 loss: 45.497787 time elapsed: 79.8259 learning rate: 0.002460, scenario: 0, slope: -0.14806042495098853, fluctuations: 0.0\n",
      "step: 63720 loss: 44.725575 time elapsed: 79.8394 learning rate: 0.002460, scenario: 0, slope: -0.1475162647756364, fluctuations: 0.0\n",
      "step: 63730 loss: 44.024693 time elapsed: 79.8528 learning rate: 0.002460, scenario: 0, slope: -0.1389998951500672, fluctuations: 0.0\n",
      "step: 63740 loss: 43.381580 time elapsed: 79.8665 learning rate: 0.002460, scenario: 0, slope: -0.12255804213336116, fluctuations: 0.0\n",
      "step: 63750 loss: 42.785723 time elapsed: 79.8809 learning rate: 0.002460, scenario: 0, slope: -0.10268311156099914, fluctuations: 0.0\n",
      "step: 63760 loss: 42.228780 time elapsed: 79.8933 learning rate: 0.002460, scenario: 0, slope: -0.08761028019611676, fluctuations: 0.0\n",
      "step: 63770 loss: 41.703904 time elapsed: 79.9062 learning rate: 0.002460, scenario: 0, slope: -0.07740550193246194, fluctuations: 0.0\n",
      "step: 63780 loss: 41.205122 time elapsed: 79.9188 learning rate: 0.002460, scenario: 0, slope: -0.06990741074106466, fluctuations: 0.0\n",
      "step: 63790 loss: 40.722338 time elapsed: 79.9307 learning rate: 0.002460, scenario: 0, slope: -0.06403585004067822, fluctuations: 0.0\n",
      "step: 63800 loss: 40.250239 time elapsed: 79.9426 learning rate: 0.002460, scenario: 0, slope: -0.059803026544382645, fluctuations: 0.0\n",
      "step: 63810 loss: 39.801644 time elapsed: 79.9566 learning rate: 0.002460, scenario: 0, slope: -0.055622505200572596, fluctuations: 0.0\n",
      "step: 63820 loss: 39.367888 time elapsed: 79.9688 learning rate: 0.002460, scenario: 0, slope: -0.052546435951949994, fluctuations: 0.0\n",
      "step: 63830 loss: 38.948304 time elapsed: 79.9806 learning rate: 0.002460, scenario: 0, slope: -0.0500012962401774, fluctuations: 0.0\n",
      "step: 63840 loss: 38.543479 time elapsed: 79.9919 learning rate: 0.002460, scenario: 0, slope: -0.04785464885145934, fluctuations: 0.0\n",
      "step: 63850 loss: 38.155867 time elapsed: 80.0041 learning rate: 0.002460, scenario: 0, slope: -0.04598335933828515, fluctuations: 0.0\n",
      "step: 63860 loss: 37.785791 time elapsed: 80.0176 learning rate: 0.002460, scenario: 0, slope: -0.044276267152989296, fluctuations: 0.0\n",
      "step: 63870 loss: 37.431420 time elapsed: 80.0313 learning rate: 0.002460, scenario: 0, slope: -0.04264683833101618, fluctuations: 0.0\n",
      "step: 63880 loss: 37.090249 time elapsed: 80.0446 learning rate: 0.002977, scenario: 1, slope: -0.04104062899551369, fluctuations: 0.0\n",
      "step: 63890 loss: 36.517418 time elapsed: 80.0582 learning rate: 0.007721, scenario: 1, slope: -0.03995956235141967, fluctuations: 0.0\n",
      "step: 63900 loss: 35.284783 time elapsed: 80.0732 learning rate: 0.011304, scenario: 0, slope: -0.04175446628295162, fluctuations: 0.0\n",
      "step: 63910 loss: 25663.264025 time elapsed: 80.0884 learning rate: 0.009518, scenario: -1, slope: 19.722239048582214, fluctuations: 0.01\n",
      "step: 63920 loss: 5710.343253 time elapsed: 80.1012 learning rate: 0.003319, scenario: -1, slope: 51.91202059953052, fluctuations: 0.03\n",
      "step: 63930 loss: 1024.785084 time elapsed: 80.1134 learning rate: 0.001157, scenario: -1, slope: 48.304661671692706, fluctuations: 0.05\n",
      "step: 63940 loss: 491.010866 time elapsed: 80.1250 learning rate: 0.000403, scenario: -1, slope: 37.70397921262016, fluctuations: 0.06\n",
      "step: 63950 loss: 455.154589 time elapsed: 80.1367 learning rate: 0.000141, scenario: -1, slope: 25.388140457595295, fluctuations: 0.07\n",
      "step: 63960 loss: 399.604326 time elapsed: 80.1483 learning rate: 0.000049, scenario: -1, slope: 11.602737677670865, fluctuations: 0.07\n",
      "step: 63970 loss: 387.838422 time elapsed: 80.1601 learning rate: 0.000021, scenario: 0, slope: -2.0290931790087168, fluctuations: 0.07\n",
      "step: 63980 loss: 382.794734 time elapsed: 80.1721 learning rate: 0.000021, scenario: 0, slope: -16.12282712650078, fluctuations: 0.07\n",
      "step: 63990 loss: 378.563131 time elapsed: 80.1837 learning rate: 0.000021, scenario: 0, slope: -31.4597844719429, fluctuations: 0.07\n",
      "step: 64000 loss: 374.827294 time elapsed: 80.1956 learning rate: 0.000021, scenario: 0, slope: -47.1377330073453, fluctuations: 0.07\n",
      "step: 64010 loss: 371.429966 time elapsed: 80.2082 learning rate: 0.000021, scenario: 0, slope: -43.43302997445044, fluctuations: 0.06\n",
      "step: 64020 loss: 368.283940 time elapsed: 80.2214 learning rate: 0.000021, scenario: 0, slope: -10.132149886007273, fluctuations: 0.03\n",
      "step: 64030 loss: 365.335813 time elapsed: 80.2350 learning rate: 0.000021, scenario: 0, slope: -2.6412423193637267, fluctuations: 0.02\n",
      "step: 64040 loss: 362.549755 time elapsed: 80.2484 learning rate: 0.000021, scenario: 0, slope: -0.949895241067151, fluctuations: 0.01\n",
      "step: 64050 loss: 359.899873 time elapsed: 80.2621 learning rate: 0.000021, scenario: 0, slope: -0.5105920999411915, fluctuations: 0.0\n",
      "step: 64060 loss: 356.940631 time elapsed: 80.2755 learning rate: 0.000041, scenario: 1, slope: -0.3540868726775873, fluctuations: 0.0\n",
      "step: 64070 loss: 350.276435 time elapsed: 80.2891 learning rate: 0.000107, scenario: 1, slope: -0.3293368612064584, fluctuations: 0.0\n",
      "step: 64080 loss: 335.241420 time elapsed: 80.3018 learning rate: 0.000277, scenario: 1, slope: -0.36340888167062, fluctuations: 0.0\n",
      "step: 64090 loss: 310.761899 time elapsed: 80.3138 learning rate: 0.000335, scenario: 0, slope: -0.49266769426464724, fluctuations: 0.0\n",
      "step: 64100 loss: 290.494101 time elapsed: 80.3253 learning rate: 0.000335, scenario: 0, slope: -0.6759817973771245, fluctuations: 0.0\n",
      "step: 64110 loss: 273.910268 time elapsed: 80.3376 learning rate: 0.000335, scenario: 0, slope: -0.9327071073390335, fluctuations: 0.0\n",
      "step: 64120 loss: 260.008486 time elapsed: 80.3492 learning rate: 0.000335, scenario: 0, slope: -1.1591729431957432, fluctuations: 0.0\n",
      "step: 64130 loss: 248.118509 time elapsed: 80.3608 learning rate: 0.000335, scenario: 0, slope: -1.3500007130145146, fluctuations: 0.0\n",
      "step: 64140 loss: 237.772670 time elapsed: 80.3726 learning rate: 0.000335, scenario: 0, slope: -1.4829988200979771, fluctuations: 0.0\n",
      "step: 64150 loss: 228.635835 time elapsed: 80.3844 learning rate: 0.000335, scenario: 0, slope: -1.5401457889873984, fluctuations: 0.0\n",
      "step: 64160 loss: 220.462662 time elapsed: 80.3963 learning rate: 0.000335, scenario: 0, slope: -1.5071304095614684, fluctuations: 0.0\n",
      "step: 64170 loss: 213.070793 time elapsed: 80.4082 learning rate: 0.000335, scenario: 0, slope: -1.3849196105060482, fluctuations: 0.0\n",
      "step: 64180 loss: 206.292938 time elapsed: 80.4216 learning rate: 0.000335, scenario: 0, slope: -1.206183956133367, fluctuations: 0.0\n",
      "step: 64190 loss: 200.016671 time elapsed: 80.4354 learning rate: 0.000335, scenario: 0, slope: -1.0411975326123892, fluctuations: 0.0\n",
      "step: 64200 loss: 194.390857 time elapsed: 80.4494 learning rate: 0.000335, scenario: 0, slope: -0.9254840233890542, fluctuations: 0.0\n",
      "step: 64210 loss: 189.197640 time elapsed: 80.4632 learning rate: 0.000335, scenario: 0, slope: -0.8133530911821224, fluctuations: 0.0\n",
      "step: 64220 loss: 184.365188 time elapsed: 80.4766 learning rate: 0.000335, scenario: 0, slope: -0.7315389389490916, fluctuations: 0.0\n",
      "step: 64230 loss: 179.843789 time elapsed: 80.4897 learning rate: 0.000335, scenario: 0, slope: -0.6637056240857991, fluctuations: 0.0\n",
      "step: 64240 loss: 175.587090 time elapsed: 80.5028 learning rate: 0.000335, scenario: 0, slope: -0.6064963205602307, fluctuations: 0.0\n",
      "step: 64250 loss: 171.553614 time elapsed: 80.5146 learning rate: 0.000335, scenario: 0, slope: -0.5576296489875452, fluctuations: 0.0\n",
      "step: 64260 loss: 167.708779 time elapsed: 80.5266 learning rate: 0.000335, scenario: 0, slope: -0.5155692513449939, fluctuations: 0.0\n",
      "step: 64270 loss: 164.023826 time elapsed: 80.5388 learning rate: 0.000335, scenario: 0, slope: -0.47929024345806076, fluctuations: 0.0\n",
      "step: 64280 loss: 160.474410 time elapsed: 80.5506 learning rate: 0.000335, scenario: 0, slope: -0.44819890712267624, fluctuations: 0.0\n",
      "step: 64290 loss: 157.040057 time elapsed: 80.5625 learning rate: 0.000335, scenario: 0, slope: -0.42226537196830344, fluctuations: 0.0\n",
      "step: 64300 loss: 153.703664 time elapsed: 80.5741 learning rate: 0.000335, scenario: 0, slope: -0.40245732237879944, fluctuations: 0.0\n",
      "step: 64310 loss: 150.450971 time elapsed: 80.5862 learning rate: 0.000335, scenario: 0, slope: -0.3819682739982578, fluctuations: 0.0\n",
      "step: 64320 loss: 147.270112 time elapsed: 80.5977 learning rate: 0.000335, scenario: 0, slope: -0.36632332773989795, fluctuations: 0.0\n",
      "step: 64330 loss: 144.151189 time elapsed: 80.6094 learning rate: 0.000335, scenario: 0, slope: -0.3530437007917903, fluctuations: 0.0\n",
      "step: 64340 loss: 141.085894 time elapsed: 80.6212 learning rate: 0.000335, scenario: 0, slope: -0.3417519519035807, fluctuations: 0.0\n",
      "step: 64350 loss: 138.067251 time elapsed: 80.6351 learning rate: 0.000335, scenario: 0, slope: -0.33212746624220807, fluctuations: 0.0\n",
      "step: 64360 loss: 135.089537 time elapsed: 80.6494 learning rate: 0.000335, scenario: 0, slope: -0.323898074282415, fluctuations: 0.0\n",
      "step: 64370 loss: 132.148385 time elapsed: 80.6628 learning rate: 0.000335, scenario: 0, slope: -0.31683336621689223, fluctuations: 0.0\n",
      "step: 64380 loss: 129.241051 time elapsed: 80.6763 learning rate: 0.000335, scenario: 0, slope: -0.3107351476344041, fluctuations: 0.0\n",
      "step: 64390 loss: 126.366737 time elapsed: 80.6900 learning rate: 0.000335, scenario: 0, slope: -0.3054273112403298, fluctuations: 0.0\n",
      "step: 64400 loss: 123.526887 time elapsed: 80.7036 learning rate: 0.000335, scenario: 0, slope: -0.301189930067024, fluctuations: 0.0\n",
      "step: 64410 loss: 120.725335 time elapsed: 80.7182 learning rate: 0.000335, scenario: 0, slope: -0.29652577383624573, fluctuations: 0.0\n",
      "step: 64420 loss: 117.968267 time elapsed: 80.7314 learning rate: 0.000335, scenario: 0, slope: -0.2925987996181286, fluctuations: 0.0\n",
      "step: 64430 loss: 115.263926 time elapsed: 80.7436 learning rate: 0.000335, scenario: 0, slope: -0.28878296610388043, fluctuations: 0.0\n",
      "step: 64440 loss: 112.622101 time elapsed: 80.7554 learning rate: 0.000335, scenario: 0, slope: -0.2848860356293578, fluctuations: 0.0\n",
      "step: 64450 loss: 110.053411 time elapsed: 80.7673 learning rate: 0.000335, scenario: 0, slope: -0.28071096161448733, fluctuations: 0.0\n",
      "step: 64460 loss: 107.568483 time elapsed: 80.7786 learning rate: 0.000335, scenario: 0, slope: -0.27606694669560394, fluctuations: 0.0\n",
      "step: 64470 loss: 105.177112 time elapsed: 80.7904 learning rate: 0.000335, scenario: 0, slope: -0.2707838395363453, fluctuations: 0.0\n",
      "step: 64480 loss: 102.887520 time elapsed: 80.8023 learning rate: 0.000335, scenario: 0, slope: -0.26472740665153816, fluctuations: 0.0\n",
      "step: 64490 loss: 100.705804 time elapsed: 80.8140 learning rate: 0.000335, scenario: 0, slope: -0.2578127623621293, fluctuations: 0.0\n",
      "step: 64500 loss: 98.635618 time elapsed: 80.8253 learning rate: 0.000335, scenario: 0, slope: -0.25083274587102866, fluctuations: 0.0\n",
      "step: 64510 loss: 96.678117 time elapsed: 80.8383 learning rate: 0.000335, scenario: 0, slope: -0.2413652129201428, fluctuations: 0.0\n",
      "step: 64520 loss: 94.832123 time elapsed: 80.8522 learning rate: 0.000335, scenario: 0, slope: -0.2319617392480495, fluctuations: 0.0\n",
      "step: 64530 loss: 93.094461 time elapsed: 80.8655 learning rate: 0.000335, scenario: 0, slope: -0.2219463160286634, fluctuations: 0.0\n",
      "step: 64540 loss: 91.460385 time elapsed: 80.8784 learning rate: 0.000335, scenario: 0, slope: -0.21149734082538735, fluctuations: 0.0\n",
      "step: 64550 loss: 89.924042 time elapsed: 80.8920 learning rate: 0.000335, scenario: 0, slope: -0.2008118543660564, fluctuations: 0.0\n",
      "step: 64560 loss: 88.478905 time elapsed: 80.9061 learning rate: 0.000335, scenario: 0, slope: -0.19008872052242104, fluctuations: 0.0\n",
      "step: 64570 loss: 87.118146 time elapsed: 80.9196 learning rate: 0.000335, scenario: 0, slope: -0.17951367871539145, fluctuations: 0.0\n",
      "step: 64580 loss: 85.834937 time elapsed: 80.9348 learning rate: 0.000335, scenario: 0, slope: -0.16924784028237708, fluctuations: 0.0\n",
      "step: 64590 loss: 84.622669 time elapsed: 80.9492 learning rate: 0.000335, scenario: 0, slope: -0.15942044226223723, fluctuations: 0.0\n",
      "step: 64600 loss: 83.475100 time elapsed: 80.9628 learning rate: 0.000335, scenario: 0, slope: -0.1510293990285267, fluctuations: 0.0\n",
      "step: 64610 loss: 82.386443 time elapsed: 80.9759 learning rate: 0.000335, scenario: 0, slope: -0.14142455031977144, fluctuations: 0.0\n",
      "step: 64620 loss: 81.351405 time elapsed: 80.9878 learning rate: 0.000335, scenario: 0, slope: -0.13334617105009647, fluctuations: 0.0\n",
      "step: 64630 loss: 80.365200 time elapsed: 80.9995 learning rate: 0.000335, scenario: 0, slope: -0.12589508875643843, fluctuations: 0.0\n",
      "step: 64640 loss: 79.423527 time elapsed: 81.0117 learning rate: 0.000335, scenario: 0, slope: -0.11905597193716419, fluctuations: 0.0\n",
      "step: 64650 loss: 78.522545 time elapsed: 81.0240 learning rate: 0.000335, scenario: 0, slope: -0.11279947704351811, fluctuations: 0.0\n",
      "step: 64660 loss: 77.658836 time elapsed: 81.0380 learning rate: 0.000335, scenario: 0, slope: -0.10708726025884084, fluctuations: 0.0\n",
      "step: 64670 loss: 76.829361 time elapsed: 81.0517 learning rate: 0.000335, scenario: 0, slope: -0.10187609521833313, fluctuations: 0.0\n",
      "step: 64680 loss: 76.031423 time elapsed: 81.0656 learning rate: 0.000335, scenario: 0, slope: -0.09712103326683702, fluctuations: 0.0\n",
      "step: 64690 loss: 75.262628 time elapsed: 81.0799 learning rate: 0.000335, scenario: 0, slope: -0.09277767070193106, fluctuations: 0.0\n",
      "step: 64700 loss: 74.520848 time elapsed: 81.0932 learning rate: 0.000335, scenario: 0, slope: -0.08918558552756341, fluctuations: 0.0\n",
      "step: 64710 loss: 73.804190 time elapsed: 81.1071 learning rate: 0.000335, scenario: 0, slope: -0.08515959557019451, fluctuations: 0.0\n",
      "step: 64720 loss: 73.110968 time elapsed: 81.1204 learning rate: 0.000335, scenario: 0, slope: -0.08180950740605765, fluctuations: 0.0\n",
      "step: 64730 loss: 72.275394 time elapsed: 81.1344 learning rate: 0.000718, scenario: 1, slope: -0.07896546744583874, fluctuations: 0.0\n",
      "step: 64740 loss: 70.315692 time elapsed: 81.1473 learning rate: 0.001539, scenario: 0, slope: -0.08021069678981943, fluctuations: 0.0\n",
      "step: 64750 loss: 67.702659 time elapsed: 81.1595 learning rate: 0.001539, scenario: 0, slope: -0.09079876364345846, fluctuations: 0.0\n",
      "step: 64760 loss: 65.412934 time elapsed: 81.1709 learning rate: 0.001539, scenario: 0, slope: -0.10821339682230141, fluctuations: 0.0\n",
      "step: 64770 loss: 63.401837 time elapsed: 81.1829 learning rate: 0.001539, scenario: 0, slope: -0.12874335469964576, fluctuations: 0.0\n",
      "step: 64780 loss: 61.625335 time elapsed: 81.1950 learning rate: 0.001539, scenario: 0, slope: -0.1492605416316365, fluctuations: 0.0\n",
      "step: 64790 loss: 60.047828 time elapsed: 81.2068 learning rate: 0.001539, scenario: 0, slope: -0.1671309952524181, fluctuations: 0.0\n",
      "step: 64800 loss: 58.640814 time elapsed: 81.2183 learning rate: 0.001539, scenario: 0, slope: -0.1791022291584602, fluctuations: 0.0\n",
      "step: 64810 loss: 57.380790 time elapsed: 81.2306 learning rate: 0.001539, scenario: 0, slope: -0.18635388615159748, fluctuations: 0.0\n",
      "step: 64820 loss: 56.247860 time elapsed: 81.2421 learning rate: 0.001539, scenario: 0, slope: -0.18420742355922484, fluctuations: 0.0\n",
      "step: 64830 loss: 55.224934 time elapsed: 81.2539 learning rate: 0.001539, scenario: 0, slope: -0.17258424649996265, fluctuations: 0.0\n",
      "step: 64840 loss: 54.297218 time elapsed: 81.2677 learning rate: 0.001539, scenario: 0, slope: -0.15462807443765997, fluctuations: 0.0\n",
      "step: 64850 loss: 53.451844 time elapsed: 81.2813 learning rate: 0.001539, scenario: 0, slope: -0.13781971018607275, fluctuations: 0.0\n",
      "step: 64860 loss: 52.677598 time elapsed: 81.2949 learning rate: 0.001539, scenario: 0, slope: -0.12340850382360381, fluctuations: 0.0\n",
      "step: 64870 loss: 51.964725 time elapsed: 81.3089 learning rate: 0.001539, scenario: 0, slope: -0.1110109509351598, fluctuations: 0.0\n",
      "step: 64880 loss: 51.304779 time elapsed: 81.3226 learning rate: 0.001539, scenario: 0, slope: -0.1003142218790426, fluctuations: 0.0\n",
      "step: 64890 loss: 50.690487 time elapsed: 81.3357 learning rate: 0.001539, scenario: 0, slope: -0.09107199566714108, fluctuations: 0.0\n",
      "step: 64900 loss: 50.115594 time elapsed: 81.3485 learning rate: 0.001539, scenario: 0, slope: -0.08383141742607439, fluctuations: 0.0\n",
      "step: 64910 loss: 49.574689 time elapsed: 81.3608 learning rate: 0.001539, scenario: 0, slope: -0.07618198499727534, fluctuations: 0.0\n",
      "step: 64920 loss: 49.063009 time elapsed: 81.3725 learning rate: 0.001539, scenario: 0, slope: -0.07022125164478887, fluctuations: 0.0\n",
      "step: 64930 loss: 48.576224 time elapsed: 81.3844 learning rate: 0.001539, scenario: 0, slope: -0.06507733350837872, fluctuations: 0.0\n",
      "step: 64940 loss: 48.110132 time elapsed: 81.3960 learning rate: 0.001539, scenario: 0, slope: -0.060644188143422796, fluctuations: 0.0\n",
      "step: 64950 loss: 47.660168 time elapsed: 81.4079 learning rate: 0.001539, scenario: 0, slope: -0.056835025176815095, fluctuations: 0.0\n",
      "step: 64960 loss: 47.220387 time elapsed: 81.4198 learning rate: 0.001539, scenario: 0, slope: -0.05358656431113475, fluctuations: 0.0\n",
      "step: 64970 loss: 46.732030 time elapsed: 81.4320 learning rate: 0.002727, scenario: 1, slope: -0.05092666117868119, fluctuations: 0.0\n",
      "step: 64980 loss: 45.503130 time elapsed: 81.4438 learning rate: 0.007073, scenario: 1, slope: -0.05082930599272284, fluctuations: 0.0\n",
      "step: 64990 loss: 43.661779 time elapsed: 81.4557 learning rate: 0.007780, scenario: 0, slope: -0.057483266523792056, fluctuations: 0.0\n",
      "step: 65000 loss: 42.059402 time elapsed: 81.4697 learning rate: 0.007780, scenario: 0, slope: -0.06826683744432274, fluctuations: 0.0\n",
      "step: 65010 loss: 40.682825 time elapsed: 81.4841 learning rate: 0.007780, scenario: 0, slope: -0.08426654707536063, fluctuations: 0.0\n",
      "step: 65020 loss: 39.476222 time elapsed: 81.4978 learning rate: 0.007780, scenario: 0, slope: -0.09899901692495323, fluctuations: 0.0\n",
      "step: 65030 loss: 38.389526 time elapsed: 81.5110 learning rate: 0.007780, scenario: 0, slope: -0.1120083015591599, fluctuations: 0.0\n",
      "step: 65040 loss: 37.391392 time elapsed: 81.5245 learning rate: 0.007780, scenario: 0, slope: -0.12183432055234913, fluctuations: 0.0\n",
      "step: 65050 loss: 36.462647 time elapsed: 81.5380 learning rate: 0.007780, scenario: 0, slope: -0.12726701426739984, fluctuations: 0.0\n",
      "step: 65060 loss: 35.594771 time elapsed: 81.5511 learning rate: 0.007780, scenario: 0, slope: -0.1272732361069704, fluctuations: 0.0\n",
      "step: 65070 loss: 34.795432 time elapsed: 81.5637 learning rate: 0.007780, scenario: 0, slope: -0.12099336746098999, fluctuations: 0.0\n",
      "step: 65080 loss: 60.592823 time elapsed: 81.5756 learning rate: 0.007780, scenario: 0, slope: -0.08313170230536979, fluctuations: 0.0\n",
      "step: 65090 loss: 432.961073 time elapsed: 81.5877 learning rate: 0.003516, scenario: -1, slope: 1.4517024733941764, fluctuations: 0.02\n",
      "step: 65100 loss: 43.545249 time elapsed: 81.5994 learning rate: 0.001362, scenario: -1, slope: 1.3808368520852832, fluctuations: 0.06\n",
      "step: 65110 loss: 37.247107 time elapsed: 81.6116 learning rate: 0.000475, scenario: -1, slope: 1.0741955125699958, fluctuations: 0.08\n",
      "step: 65120 loss: 37.716363 time elapsed: 81.6236 learning rate: 0.000166, scenario: -1, slope: 0.685376037346399, fluctuations: 0.09\n",
      "step: 65130 loss: 36.333394 time elapsed: 81.6354 learning rate: 0.000058, scenario: -1, slope: 0.3060944438811924, fluctuations: 0.1\n",
      "step: 65140 loss: 36.247646 time elapsed: 81.6473 learning rate: 0.000029, scenario: 0, slope: -0.08174571520811534, fluctuations: 0.1\n",
      "step: 65150 loss: 36.167560 time elapsed: 81.6593 learning rate: 0.000029, scenario: 0, slope: -0.4511979387506414, fluctuations: 0.11\n",
      "step: 65160 loss: 36.087176 time elapsed: 81.6726 learning rate: 0.000029, scenario: 0, slope: -0.852220231970795, fluctuations: 0.11\n",
      "step: 65170 loss: 36.050129 time elapsed: 81.6857 learning rate: 0.000029, scenario: 0, slope: -1.307027507278718, fluctuations: 0.11\n",
      "step: 65180 loss: 36.010453 time elapsed: 81.6988 learning rate: 0.000029, scenario: 0, slope: -1.8358895323505453, fluctuations: 0.11\n",
      "step: 65190 loss: 35.971384 time elapsed: 81.7118 learning rate: 0.000029, scenario: 0, slope: -0.3588188729963253, fluctuations: 0.08\n",
      "step: 65200 loss: 35.934956 time elapsed: 81.7253 learning rate: 0.000029, scenario: 0, slope: -0.09596173266559509, fluctuations: 0.05\n",
      "step: 65210 loss: 35.895207 time elapsed: 81.7392 learning rate: 0.000052, scenario: 1, slope: -0.027206359681733728, fluctuations: 0.03\n",
      "step: 65220 loss: 35.806480 time elapsed: 81.7521 learning rate: 0.000134, scenario: 1, slope: -0.010220051211980219, fluctuations: 0.02\n",
      "step: 65230 loss: 35.593915 time elapsed: 81.7641 learning rate: 0.000347, scenario: 1, slope: -0.005264105725048125, fluctuations: 0.01\n",
      "step: 65240 loss: 35.141037 time elapsed: 81.7761 learning rate: 0.000901, scenario: 1, slope: -0.007273848617728087, fluctuations: 0.0\n",
      "step: 65250 loss: 34.399086 time elapsed: 81.7879 learning rate: 0.002338, scenario: 1, slope: -0.011720247386922124, fluctuations: 0.0\n",
      "step: 65260 loss: 33.567074 time elapsed: 81.7998 learning rate: 0.006063, scenario: 1, slope: -0.019421199794639132, fluctuations: 0.0\n",
      "step: 65270 loss: 32.635139 time elapsed: 81.8116 learning rate: 0.015727, scenario: 1, slope: -0.02996294150010596, fluctuations: 0.0\n",
      "step: 65280 loss: 22110.075925 time elapsed: 81.8235 learning rate: 0.007109, scenario: -1, slope: 122.78773398817472, fluctuations: 0.02\n",
      "step: 65290 loss: 6890.931738 time elapsed: 81.8356 learning rate: 0.002479, scenario: -1, slope: 156.58397459845844, fluctuations: 0.03\n",
      "step: 65300 loss: 3977.139956 time elapsed: 81.8474 learning rate: 0.000960, scenario: -1, slope: 143.15637772957214, fluctuations: 0.03\n",
      "step: 65310 loss: 3359.113299 time elapsed: 81.8601 learning rate: 0.000335, scenario: -1, slope: 113.22751080129882, fluctuations: 0.03\n",
      "step: 65320 loss: 3185.894063 time elapsed: 81.8735 learning rate: 0.000117, scenario: -1, slope: 80.7316789744332, fluctuations: 0.03\n",
      "step: 65330 loss: 3121.001925 time elapsed: 81.8868 learning rate: 0.000041, scenario: -1, slope: 44.59279283692616, fluctuations: 0.03\n",
      "step: 65340 loss: 3099.752454 time elapsed: 81.8999 learning rate: 0.000014, scenario: -1, slope: 4.5237193511649005, fluctuations: 0.03\n",
      "step: 65350 loss: 3090.667088 time elapsed: 81.9135 learning rate: 0.000013, scenario: 0, slope: -40.266380801261064, fluctuations: 0.03\n",
      "step: 65360 loss: 3082.129591 time elapsed: 81.9269 learning rate: 0.000013, scenario: 0, slope: -90.95727859937787, fluctuations: 0.03\n",
      "step: 65370 loss: 3073.784504 time elapsed: 81.9405 learning rate: 0.000013, scenario: 0, slope: -149.11695971856992, fluctuations: 0.03\n",
      "step: 65380 loss: 3065.599040 time elapsed: 81.9537 learning rate: 0.000013, scenario: 0, slope: -53.59690521383747, fluctuations: 0.01\n",
      "step: 65390 loss: 3057.549073 time elapsed: 81.9657 learning rate: 0.000013, scenario: 0, slope: -13.373710076621771, fluctuations: 0.0\n",
      "step: 65400 loss: 3049.616358 time elapsed: 81.9776 learning rate: 0.000013, scenario: 0, slope: -4.640475579492419, fluctuations: 0.0\n",
      "step: 65410 loss: 3038.226153 time elapsed: 81.9901 learning rate: 0.000033, scenario: 1, slope: -1.7082286238610846, fluctuations: 0.0\n",
      "step: 65420 loss: 3009.603067 time elapsed: 82.0022 learning rate: 0.000086, scenario: 1, slope: -1.103670752274554, fluctuations: 0.0\n",
      "step: 65430 loss: 2939.334173 time elapsed: 82.0141 learning rate: 0.000223, scenario: 1, slope: -1.1958552124839528, fluctuations: 0.0\n",
      "step: 65440 loss: 2776.305863 time elapsed: 82.0259 learning rate: 0.000578, scenario: 1, slope: -1.9684291127297329, fluctuations: 0.0\n",
      "step: 65450 loss: 2449.545536 time elapsed: 82.0379 learning rate: 0.001024, scenario: 0, slope: -3.8996251663216652, fluctuations: 0.0\n",
      "step: 65460 loss: 2127.420908 time elapsed: 82.0498 learning rate: 0.001024, scenario: 0, slope: -7.153720419647771, fluctuations: 0.0\n",
      "step: 65470 loss: 1862.141770 time elapsed: 82.0617 learning rate: 0.001024, scenario: 0, slope: -11.055254566465548, fluctuations: 0.0\n",
      "step: 65480 loss: 1681.748837 time elapsed: 82.0736 learning rate: 0.001024, scenario: 0, slope: -14.851568755008852, fluctuations: 0.0\n",
      "step: 65490 loss: 1525.034476 time elapsed: 82.0883 learning rate: 0.001024, scenario: 0, slope: -17.99413476050706, fluctuations: 0.0\n",
      "step: 65500 loss: 1335.514973 time elapsed: 82.1016 learning rate: 0.001024, scenario: 0, slope: -20.230440641304302, fluctuations: 0.0\n",
      "step: 65510 loss: 1227.807637 time elapsed: 82.1160 learning rate: 0.001024, scenario: 0, slope: -21.715933407685224, fluctuations: 0.0\n",
      "step: 65520 loss: 1151.374118 time elapsed: 82.1291 learning rate: 0.001024, scenario: 0, slope: -21.4664035029467, fluctuations: 0.0\n",
      "step: 65530 loss: 1093.283525 time elapsed: 82.1425 learning rate: 0.001024, scenario: 0, slope: -19.676323144767654, fluctuations: 0.0\n",
      "step: 65540 loss: 1044.943348 time elapsed: 82.1558 learning rate: 0.001024, scenario: 0, slope: -16.693769557522806, fluctuations: 0.0\n",
      "step: 65550 loss: 1002.657152 time elapsed: 82.1693 learning rate: 0.001024, scenario: 0, slope: -13.434798609390732, fluctuations: 0.0\n",
      "step: 65560 loss: 963.495187 time elapsed: 82.1820 learning rate: 0.001024, scenario: 0, slope: -10.74864793722775, fluctuations: 0.0\n",
      "step: 65570 loss: 832.763020 time elapsed: 82.1939 learning rate: 0.001024, scenario: 0, slope: -8.878260597855094, fluctuations: 0.0\n",
      "step: 65580 loss: 739.730880 time elapsed: 82.2054 learning rate: 0.001024, scenario: 0, slope: -7.854542625705892, fluctuations: 0.0\n",
      "step: 65590 loss: 702.275020 time elapsed: 82.2171 learning rate: 0.001024, scenario: 0, slope: -7.0212226430339095, fluctuations: 0.0\n",
      "step: 65600 loss: 670.972167 time elapsed: 82.2289 learning rate: 0.001024, scenario: 0, slope: -6.5618142259635395, fluctuations: 0.0\n",
      "step: 65610 loss: 638.781399 time elapsed: 82.2411 learning rate: 0.001024, scenario: 0, slope: -6.28147425637306, fluctuations: 0.0\n",
      "step: 65620 loss: 616.373338 time elapsed: 82.2527 learning rate: 0.001024, scenario: 0, slope: -6.008614127969322, fluctuations: 0.0\n",
      "step: 65630 loss: 596.231006 time elapsed: 82.2647 learning rate: 0.001024, scenario: 0, slope: -5.615647875934081, fluctuations: 0.0\n",
      "step: 65640 loss: 578.777174 time elapsed: 82.2765 learning rate: 0.001024, scenario: 0, slope: -5.053887123157717, fluctuations: 0.0\n",
      "step: 65650 loss: 562.950131 time elapsed: 82.2884 learning rate: 0.001024, scenario: 0, slope: -4.301712462375661, fluctuations: 0.0\n",
      "step: 65660 loss: 548.429708 time elapsed: 82.3019 learning rate: 0.001024, scenario: 0, slope: -3.354442665402351, fluctuations: 0.0\n",
      "step: 65670 loss: 534.978978 time elapsed: 82.3156 learning rate: 0.001024, scenario: 0, slope: -2.4630939224679236, fluctuations: 0.0\n",
      "step: 65680 loss: 522.442691 time elapsed: 82.3291 learning rate: 0.001024, scenario: 0, slope: -2.0616691958632196, fluctuations: 0.0\n",
      "step: 65690 loss: 510.713102 time elapsed: 82.3423 learning rate: 0.001024, scenario: 0, slope: -1.812175237024042, fluctuations: 0.0\n",
      "step: 65700 loss: 499.710021 time elapsed: 82.3551 learning rate: 0.001024, scenario: 0, slope: -1.6202214611161887, fluctuations: 0.0\n",
      "step: 65710 loss: 489.363807 time elapsed: 82.3698 learning rate: 0.001024, scenario: 0, slope: -1.4473331662565256, fluctuations: 0.0\n",
      "step: 65720 loss: 479.611731 time elapsed: 82.3831 learning rate: 0.001024, scenario: 0, slope: -1.3296129663983833, fluctuations: 0.0\n",
      "step: 65730 loss: 470.396510 time elapsed: 82.3951 learning rate: 0.001024, scenario: 0, slope: -1.2337999643157116, fluctuations: 0.0\n",
      "step: 65740 loss: 461.665269 time elapsed: 82.4071 learning rate: 0.001024, scenario: 0, slope: -1.1519556434478737, fluctuations: 0.0\n",
      "step: 65750 loss: 453.369127 time elapsed: 82.4191 learning rate: 0.001024, scenario: 0, slope: -1.0801791356138868, fluctuations: 0.0\n",
      "step: 65760 loss: 445.462872 time elapsed: 82.4312 learning rate: 0.001024, scenario: 0, slope: -1.0161399757796201, fluctuations: 0.0\n",
      "step: 65770 loss: 437.904518 time elapsed: 82.4430 learning rate: 0.001024, scenario: 0, slope: -0.9586046571442585, fluctuations: 0.0\n",
      "step: 65780 loss: 430.654737 time elapsed: 82.4551 learning rate: 0.001024, scenario: 0, slope: -0.9068016902028013, fluctuations: 0.0\n",
      "step: 65790 loss: 423.676154 time elapsed: 82.4666 learning rate: 0.001024, scenario: 0, slope: -0.8601985993725143, fluctuations: 0.0\n",
      "step: 65800 loss: 416.932486 time elapsed: 82.4785 learning rate: 0.001024, scenario: 0, slope: -0.82236707436125, fluctuations: 0.0\n",
      "step: 65810 loss: 410.387556 time elapsed: 82.4909 learning rate: 0.001024, scenario: 0, slope: -0.781054570514466, fluctuations: 0.0\n",
      "step: 65820 loss: 404.004225 time elapsed: 82.5028 learning rate: 0.001024, scenario: 0, slope: -0.7479211460918646, fluctuations: 0.0\n",
      "step: 65830 loss: 397.743394 time elapsed: 82.5160 learning rate: 0.001024, scenario: 0, slope: -0.7187959377211396, fluctuations: 0.0\n",
      "step: 65840 loss: 391.563367 time elapsed: 82.5293 learning rate: 0.001024, scenario: 0, slope: -0.6935575009738286, fluctuations: 0.0\n",
      "step: 65850 loss: 385.420209 time elapsed: 82.5427 learning rate: 0.001024, scenario: 0, slope: -0.6721624774599123, fluctuations: 0.0\n",
      "step: 65860 loss: 379.269960 time elapsed: 82.5565 learning rate: 0.001024, scenario: 0, slope: -0.6546358163191842, fluctuations: 0.0\n",
      "step: 65870 loss: 373.073590 time elapsed: 82.5700 learning rate: 0.001024, scenario: 0, slope: -0.6410373972515279, fluctuations: 0.0\n",
      "step: 65880 loss: 366.804387 time elapsed: 82.5831 learning rate: 0.001024, scenario: 0, slope: -0.6313960050020715, fluctuations: 0.0\n",
      "step: 65890 loss: 360.454928 time elapsed: 82.5963 learning rate: 0.001024, scenario: 0, slope: -0.6256140578380617, fluctuations: 0.0\n",
      "step: 65900 loss: 354.038135 time elapsed: 82.6080 learning rate: 0.001024, scenario: 0, slope: -0.6234518747274409, fluctuations: 0.0\n",
      "step: 65910 loss: 347.578564 time elapsed: 82.6203 learning rate: 0.001024, scenario: 0, slope: -0.6240880283184701, fluctuations: 0.0\n",
      "step: 65920 loss: 341.097499 time elapsed: 82.6325 learning rate: 0.001024, scenario: 0, slope: -0.6269672866132059, fluctuations: 0.0\n",
      "step: 65930 loss: 334.602188 time elapsed: 82.6441 learning rate: 0.001024, scenario: 0, slope: -0.6311434104173519, fluctuations: 0.0\n",
      "step: 65940 loss: 328.087071 time elapsed: 82.6561 learning rate: 0.001024, scenario: 0, slope: -0.6358326071216743, fluctuations: 0.0\n",
      "step: 65950 loss: 321.547956 time elapsed: 82.6682 learning rate: 0.001024, scenario: 0, slope: -0.6404299575963276, fluctuations: 0.0\n",
      "step: 65960 loss: 315.009266 time elapsed: 82.6800 learning rate: 0.001024, scenario: 0, slope: -0.6444761807518895, fluctuations: 0.0\n",
      "step: 65970 loss: 308.547309 time elapsed: 82.6916 learning rate: 0.001024, scenario: 0, slope: -0.6474643837185633, fluctuations: 0.0\n",
      "step: 65980 loss: 302.239392 time elapsed: 82.7035 learning rate: 0.001024, scenario: 0, slope: -0.6487340594362427, fluctuations: 0.0\n",
      "step: 65990 loss: 296.135117 time elapsed: 82.7154 learning rate: 0.001024, scenario: 0, slope: -0.6476454475101736, fluctuations: 0.0\n",
      "step: 66000 loss: 290.296143 time elapsed: 82.7285 learning rate: 0.001024, scenario: 0, slope: -0.6441082232962357, fluctuations: 0.0\n",
      "step: 66010 loss: 284.772094 time elapsed: 82.7427 learning rate: 0.001024, scenario: 0, slope: -0.6356956445524815, fluctuations: 0.0\n",
      "step: 66020 loss: 279.585379 time elapsed: 82.7561 learning rate: 0.001024, scenario: 0, slope: -0.6234567088848767, fluctuations: 0.0\n",
      "step: 66030 loss: 274.731324 time elapsed: 82.7698 learning rate: 0.001024, scenario: 0, slope: -0.6064730074034071, fluctuations: 0.0\n",
      "step: 66040 loss: 270.180413 time elapsed: 82.7834 learning rate: 0.001024, scenario: 0, slope: -0.5849217788424157, fluctuations: 0.0\n",
      "step: 66050 loss: 265.597085 time elapsed: 82.7966 learning rate: 0.001024, scenario: 0, slope: -0.5600162827798927, fluctuations: 0.0\n",
      "step: 66060 loss: 260.421969 time elapsed: 82.8119 learning rate: 0.001024, scenario: 0, slope: -0.5396173863107108, fluctuations: 0.01\n",
      "step: 66070 loss: 256.136133 time elapsed: 82.8248 learning rate: 0.001024, scenario: 0, slope: -0.5184327664368914, fluctuations: 0.01\n",
      "step: 66080 loss: 252.278247 time elapsed: 82.8369 learning rate: 0.001024, scenario: 0, slope: -0.49690623718226107, fluctuations: 0.01\n",
      "step: 66090 loss: 248.632748 time elapsed: 82.8489 learning rate: 0.001024, scenario: 0, slope: -0.4752245951219214, fluctuations: 0.01\n",
      "step: 66100 loss: 245.138529 time elapsed: 82.8607 learning rate: 0.001024, scenario: 0, slope: -0.4559682868996286, fluctuations: 0.01\n",
      "step: 66110 loss: 241.764552 time elapsed: 82.8729 learning rate: 0.001024, scenario: 0, slope: -0.4329991498923585, fluctuations: 0.01\n",
      "step: 66120 loss: 238.488149 time elapsed: 82.8849 learning rate: 0.001024, scenario: 0, slope: -0.4125490672556178, fluctuations: 0.01\n",
      "step: 66130 loss: 235.296760 time elapsed: 82.8968 learning rate: 0.001024, scenario: 0, slope: -0.39213696678239895, fluctuations: 0.01\n",
      "step: 66140 loss: 232.181981 time elapsed: 82.9086 learning rate: 0.001024, scenario: 0, slope: -0.3712528493911931, fluctuations: 0.01\n",
      "step: 66150 loss: 229.138446 time elapsed: 82.9206 learning rate: 0.001024, scenario: 0, slope: -0.34978142893788045, fluctuations: 0.01\n",
      "step: 66160 loss: 226.163835 time elapsed: 82.9336 learning rate: 0.001024, scenario: 0, slope: -0.33631238278538617, fluctuations: 0.0\n",
      "step: 66170 loss: 223.258797 time elapsed: 82.9477 learning rate: 0.001024, scenario: 0, slope: -0.32556984026790253, fluctuations: 0.0\n",
      "step: 66180 loss: 220.419702 time elapsed: 82.9617 learning rate: 0.001024, scenario: 0, slope: -0.3165818216923704, fluctuations: 0.0\n",
      "step: 66190 loss: 217.639750 time elapsed: 82.9754 learning rate: 0.001024, scenario: 0, slope: -0.3085615621026375, fluctuations: 0.0\n",
      "step: 66200 loss: 214.917170 time elapsed: 82.9888 learning rate: 0.001024, scenario: 0, slope: -0.30189812262971466, fluctuations: 0.0\n",
      "step: 66210 loss: 212.249765 time elapsed: 83.0030 learning rate: 0.001024, scenario: 0, slope: -0.29426773302999915, fluctuations: 0.0\n",
      "step: 66220 loss: 209.635579 time elapsed: 83.0162 learning rate: 0.001024, scenario: 0, slope: -0.2877111001208151, fluctuations: 0.0\n",
      "step: 66230 loss: 207.073006 time elapsed: 83.0290 learning rate: 0.001024, scenario: 0, slope: -0.281454631038775, fluctuations: 0.0\n",
      "step: 66240 loss: 204.560508 time elapsed: 83.0412 learning rate: 0.001024, scenario: 0, slope: -0.2754662982577473, fluctuations: 0.0\n",
      "step: 66250 loss: 202.096673 time elapsed: 83.0531 learning rate: 0.001024, scenario: 0, slope: -0.2697304137000842, fluctuations: 0.0\n",
      "step: 66260 loss: 199.680148 time elapsed: 83.0650 learning rate: 0.001024, scenario: 0, slope: -0.2642361556392705, fluctuations: 0.0\n",
      "step: 66270 loss: 197.309623 time elapsed: 83.0766 learning rate: 0.001024, scenario: 0, slope: -0.258956901092618, fluctuations: 0.0\n",
      "step: 66280 loss: 194.983815 time elapsed: 83.0889 learning rate: 0.001024, scenario: 0, slope: -0.25385150929840555, fluctuations: 0.0\n",
      "step: 66290 loss: 192.701460 time elapsed: 83.1010 learning rate: 0.001024, scenario: 0, slope: -0.24890025721161407, fluctuations: 0.0\n",
      "step: 66300 loss: 190.461307 time elapsed: 83.1130 learning rate: 0.001024, scenario: 0, slope: -0.24456906195859868, fluctuations: 0.0\n",
      "step: 66310 loss: 188.262117 time elapsed: 83.1255 learning rate: 0.001024, scenario: 0, slope: -0.23942674022588636, fluctuations: 0.0\n",
      "step: 66320 loss: 186.102664 time elapsed: 83.1381 learning rate: 0.001024, scenario: 0, slope: -0.2348912324697819, fluctuations: 0.0\n",
      "step: 66330 loss: 183.981732 time elapsed: 83.1520 learning rate: 0.001024, scenario: 0, slope: -0.23048479176041206, fluctuations: 0.0\n",
      "step: 66340 loss: 181.898124 time elapsed: 83.1660 learning rate: 0.001024, scenario: 0, slope: -0.22620495240088664, fluctuations: 0.0\n",
      "step: 66350 loss: 179.850659 time elapsed: 83.1797 learning rate: 0.001024, scenario: 0, slope: -0.2220498608534161, fluctuations: 0.0\n",
      "step: 66360 loss: 177.838180 time elapsed: 83.1933 learning rate: 0.001024, scenario: 0, slope: -0.21801791514306906, fluctuations: 0.0\n",
      "step: 66370 loss: 175.859555 time elapsed: 83.2071 learning rate: 0.001024, scenario: 0, slope: -0.21410757021797255, fluctuations: 0.0\n",
      "step: 66380 loss: 173.913677 time elapsed: 83.2201 learning rate: 0.001024, scenario: 0, slope: -0.21031718977379493, fluctuations: 0.0\n",
      "step: 66390 loss: 171.999471 time elapsed: 83.2334 learning rate: 0.001024, scenario: 0, slope: -0.20664495207293487, fluctuations: 0.0\n",
      "step: 66400 loss: 170.115894 time elapsed: 83.2459 learning rate: 0.001024, scenario: 0, slope: -0.20343925348942532, fluctuations: 0.0\n",
      "step: 66410 loss: 168.261934 time elapsed: 83.2587 learning rate: 0.001024, scenario: 0, slope: -0.19964641826150425, fluctuations: 0.0\n",
      "step: 66420 loss: 166.436608 time elapsed: 83.2703 learning rate: 0.001024, scenario: 0, slope: -0.19631527034018217, fluctuations: 0.0\n",
      "step: 66430 loss: 164.638965 time elapsed: 83.2823 learning rate: 0.001024, scenario: 0, slope: -0.1930926188732045, fluctuations: 0.0\n",
      "step: 66440 loss: 162.868078 time elapsed: 83.2940 learning rate: 0.001024, scenario: 0, slope: -0.1899756013476246, fluctuations: 0.0\n",
      "step: 66450 loss: 161.123037 time elapsed: 83.3057 learning rate: 0.001024, scenario: 0, slope: -0.186961321642046, fluctuations: 0.0\n",
      "step: 66460 loss: 159.402936 time elapsed: 83.3172 learning rate: 0.001024, scenario: 0, slope: -0.18404698406548192, fluctuations: 0.0\n",
      "step: 66470 loss: 157.706849 time elapsed: 83.3293 learning rate: 0.001024, scenario: 0, slope: -0.18123009891803213, fluctuations: 0.0\n",
      "step: 66480 loss: 156.033779 time elapsed: 83.3414 learning rate: 0.001024, scenario: 0, slope: -0.17850883364805684, fluctuations: 0.0\n",
      "step: 66490 loss: 154.382544 time elapsed: 83.3549 learning rate: 0.001024, scenario: 0, slope: -0.17588270153195756, fluctuations: 0.0\n",
      "step: 66500 loss: 152.751470 time elapsed: 83.3686 learning rate: 0.001024, scenario: 0, slope: -0.17360243553996052, fluctuations: 0.0\n",
      "step: 66510 loss: 151.137358 time elapsed: 83.3819 learning rate: 0.001024, scenario: 0, slope: -0.17093319323454084, fluctuations: 0.0\n",
      "step: 66520 loss: 149.530307 time elapsed: 83.3953 learning rate: 0.001024, scenario: 0, slope: -0.16865566344653138, fluctuations: 0.0\n",
      "step: 66530 loss: 147.855849 time elapsed: 83.4089 learning rate: 0.001024, scenario: 0, slope: -0.16672434421667573, fluctuations: 0.0\n",
      "step: 66540 loss: 144.889643 time elapsed: 83.4221 learning rate: 0.001024, scenario: 0, slope: -0.16826639513114933, fluctuations: 0.01\n",
      "step: 66550 loss: 142.072349 time elapsed: 83.4355 learning rate: 0.001024, scenario: 0, slope: -0.17785273257055945, fluctuations: 0.01\n",
      "step: 66560 loss: 139.491714 time elapsed: 83.4477 learning rate: 0.001024, scenario: 0, slope: -0.19104661362665964, fluctuations: 0.01\n",
      "step: 66570 loss: 137.284338 time elapsed: 83.4597 learning rate: 0.001024, scenario: 0, slope: -0.2045348873399166, fluctuations: 0.01\n",
      "step: 66580 loss: 135.303650 time elapsed: 83.4717 learning rate: 0.001024, scenario: 0, slope: -0.21597192881980168, fluctuations: 0.01\n",
      "step: 66590 loss: 133.488819 time elapsed: 83.4836 learning rate: 0.001024, scenario: 0, slope: -0.22379931256509397, fluctuations: 0.01\n",
      "step: 66600 loss: 131.790417 time elapsed: 83.4953 learning rate: 0.001024, scenario: 0, slope: -0.22684558344835506, fluctuations: 0.01\n",
      "step: 66610 loss: 130.178025 time elapsed: 83.5073 learning rate: 0.001024, scenario: 0, slope: -0.2245490819549855, fluctuations: 0.01\n",
      "step: 66620 loss: 128.631999 time elapsed: 83.5190 learning rate: 0.001024, scenario: 0, slope: -0.21607155661364877, fluctuations: 0.01\n",
      "step: 66630 loss: 127.138762 time elapsed: 83.5307 learning rate: 0.001024, scenario: 0, slope: -0.20115625105569945, fluctuations: 0.01\n",
      "step: 66640 loss: 125.688894 time elapsed: 83.5428 learning rate: 0.001024, scenario: 0, slope: -0.18480348537927835, fluctuations: 0.0\n",
      "step: 66650 loss: 124.275733 time elapsed: 83.5568 learning rate: 0.001024, scenario: 0, slope: -0.1717202140882598, fluctuations: 0.0\n",
      "step: 66660 loss: 122.894286 time elapsed: 83.5712 learning rate: 0.001024, scenario: 0, slope: -0.1619850095360138, fluctuations: 0.0\n",
      "step: 66670 loss: 121.540700 time elapsed: 83.5852 learning rate: 0.001024, scenario: 0, slope: -0.15467268460941538, fluctuations: 0.0\n",
      "step: 66680 loss: 120.211872 time elapsed: 83.5986 learning rate: 0.001024, scenario: 0, slope: -0.14899411225750103, fluctuations: 0.0\n",
      "step: 66690 loss: 118.905179 time elapsed: 83.6122 learning rate: 0.001024, scenario: 0, slope: -0.1444431680573862, fluctuations: 0.0\n",
      "step: 66700 loss: 117.618292 time elapsed: 83.6258 learning rate: 0.001024, scenario: 0, slope: -0.14102176223910612, fluctuations: 0.0\n",
      "step: 66710 loss: 116.349026 time elapsed: 83.6396 learning rate: 0.001024, scenario: 0, slope: -0.13747037634800993, fluctuations: 0.0\n",
      "step: 66720 loss: 115.095206 time elapsed: 83.6535 learning rate: 0.001024, scenario: 0, slope: -0.13469785581512894, fluctuations: 0.0\n",
      "step: 66730 loss: 113.854529 time elapsed: 83.6657 learning rate: 0.001024, scenario: 0, slope: -0.13226834357587477, fluctuations: 0.0\n",
      "step: 66740 loss: 112.624406 time elapsed: 83.6776 learning rate: 0.001024, scenario: 0, slope: -0.13012906587054546, fluctuations: 0.0\n",
      "step: 66750 loss: 111.401756 time elapsed: 83.6905 learning rate: 0.001024, scenario: 0, slope: -0.12825138887396678, fluctuations: 0.0\n",
      "step: 66760 loss: 110.182743 time elapsed: 83.7031 learning rate: 0.001024, scenario: 0, slope: -0.12662661666460634, fluctuations: 0.0\n",
      "step: 66770 loss: 108.962417 time elapsed: 83.7152 learning rate: 0.001024, scenario: 0, slope: -0.12526460627534214, fluctuations: 0.0\n",
      "step: 66780 loss: 107.734271 time elapsed: 83.7270 learning rate: 0.001024, scenario: 0, slope: -0.12419491204170587, fluctuations: 0.0\n",
      "step: 66790 loss: 106.489761 time elapsed: 83.7387 learning rate: 0.001024, scenario: 0, slope: -0.12346993084396753, fluctuations: 0.0\n",
      "step: 66800 loss: 105.218064 time elapsed: 83.7503 learning rate: 0.001024, scenario: 0, slope: -0.12317688517296563, fluctuations: 0.0\n",
      "step: 66810 loss: 103.906801 time elapsed: 83.7632 learning rate: 0.001024, scenario: 0, slope: -0.12339726130453343, fluctuations: 0.0\n",
      "step: 66820 loss: 102.545364 time elapsed: 83.7760 learning rate: 0.001024, scenario: 0, slope: -0.12427694515302551, fluctuations: 0.0\n",
      "step: 66830 loss: 101.133237 time elapsed: 83.7895 learning rate: 0.001024, scenario: 0, slope: -0.1259003311003246, fluctuations: 0.0\n",
      "step: 66840 loss: 99.693536 time elapsed: 83.8031 learning rate: 0.001024, scenario: 0, slope: -0.12823355619812543, fluctuations: 0.0\n",
      "step: 66850 loss: 98.281503 time elapsed: 83.8166 learning rate: 0.001024, scenario: 0, slope: -0.13097478159416537, fluctuations: 0.0\n",
      "step: 66860 loss: 96.964711 time elapsed: 83.8301 learning rate: 0.001024, scenario: 0, slope: -0.13348095239995506, fluctuations: 0.0\n",
      "step: 66870 loss: 95.773619 time elapsed: 83.8431 learning rate: 0.001024, scenario: 0, slope: -0.13493746752791638, fluctuations: 0.0\n",
      "step: 66880 loss: 94.680380 time elapsed: 83.8554 learning rate: 0.001024, scenario: 0, slope: -0.13472754888345956, fluctuations: 0.0\n",
      "step: 66890 loss: 93.639873 time elapsed: 83.8676 learning rate: 0.001024, scenario: 0, slope: -0.13265512670555143, fluctuations: 0.0\n",
      "step: 66900 loss: 92.628479 time elapsed: 83.8790 learning rate: 0.001024, scenario: 0, slope: -0.12932013419761113, fluctuations: 0.0\n",
      "step: 66910 loss: 91.640385 time elapsed: 83.8913 learning rate: 0.001024, scenario: 0, slope: -0.12375483152738287, fluctuations: 0.0\n",
      "step: 66920 loss: 90.673997 time elapsed: 83.9033 learning rate: 0.001024, scenario: 0, slope: -0.11783000064157402, fluctuations: 0.0\n",
      "step: 66930 loss: 89.727939 time elapsed: 83.9150 learning rate: 0.001024, scenario: 0, slope: -0.11176350855265418, fluctuations: 0.0\n",
      "step: 66940 loss: 88.801105 time elapsed: 83.9270 learning rate: 0.001024, scenario: 0, slope: -0.10624085297197888, fluctuations: 0.0\n",
      "step: 66950 loss: 87.892667 time elapsed: 83.9389 learning rate: 0.001024, scenario: 0, slope: -0.10175827520543329, fluctuations: 0.0\n",
      "step: 66960 loss: 87.001975 time elapsed: 83.9507 learning rate: 0.001024, scenario: 0, slope: -0.09840795248215184, fluctuations: 0.0\n",
      "step: 66970 loss: 86.128488 time elapsed: 83.9627 learning rate: 0.001024, scenario: 0, slope: -0.09588570170142947, fluctuations: 0.0\n",
      "step: 66980 loss: 85.060818 time elapsed: 83.9757 learning rate: 0.002196, scenario: 1, slope: -0.09409654207849895, fluctuations: 0.0\n",
      "step: 66990 loss: 587.068184 time elapsed: 83.9897 learning rate: 0.002237, scenario: -1, slope: 0.4205444810029093, fluctuations: 0.01\n",
      "step: 67000 loss: 92.704615 time elapsed: 84.0028 learning rate: 0.000867, scenario: -1, slope: 0.35838412527211994, fluctuations: 0.05\n",
      "step: 67010 loss: 89.835997 time elapsed: 84.0169 learning rate: 0.000302, scenario: -1, slope: 0.36446535181077977, fluctuations: 0.07\n",
      "step: 67020 loss: 84.061933 time elapsed: 84.0303 learning rate: 0.000105, scenario: -1, slope: 0.2578649059938714, fluctuations: 0.08\n",
      "step: 67030 loss: 84.454578 time elapsed: 84.0437 learning rate: 0.000037, scenario: -1, slope: 0.13810572123860343, fluctuations: 0.09\n",
      "step: 67040 loss: 83.574184 time elapsed: 84.0569 learning rate: 0.000013, scenario: -1, slope: 0.012924690982347652, fluctuations: 0.09\n",
      "step: 67050 loss: 83.429684 time elapsed: 84.0690 learning rate: 0.000022, scenario: 0, slope: -0.10549995465531238, fluctuations: 0.09\n",
      "step: 67060 loss: 83.385974 time elapsed: 84.0811 learning rate: 0.000022, scenario: 0, slope: -0.22530982647202377, fluctuations: 0.09\n",
      "step: 67070 loss: 83.319037 time elapsed: 84.0930 learning rate: 0.000022, scenario: 0, slope: -0.3567208668720997, fluctuations: 0.09\n",
      "step: 67080 loss: 83.259380 time elapsed: 84.1054 learning rate: 0.000022, scenario: 0, slope: -0.5133768591744557, fluctuations: 0.09\n",
      "step: 67090 loss: 83.210245 time elapsed: 84.1179 learning rate: 0.000022, scenario: 0, slope: -0.3784101087538913, fluctuations: 0.07\n",
      "step: 67100 loss: 83.161310 time elapsed: 84.1296 learning rate: 0.000022, scenario: 0, slope: -0.12911872293690646, fluctuations: 0.04\n",
      "step: 67110 loss: 83.097249 time elapsed: 84.1416 learning rate: 0.000051, scenario: 1, slope: -0.03170313744016015, fluctuations: 0.02\n",
      "step: 67120 loss: 82.941045 time elapsed: 84.1532 learning rate: 0.000132, scenario: 1, slope: -0.011385822405970601, fluctuations: 0.01\n",
      "step: 67130 loss: 82.567934 time elapsed: 84.1653 learning rate: 0.000342, scenario: 1, slope: -0.009180046319433477, fluctuations: 0.0\n",
      "step: 67140 loss: 81.764509 time elapsed: 84.1768 learning rate: 0.000887, scenario: 1, slope: -0.011250656496463545, fluctuations: 0.0\n",
      "step: 67150 loss: 80.328224 time elapsed: 84.1902 learning rate: 0.002301, scenario: 1, slope: -0.020064503063199696, fluctuations: 0.0\n",
      "step: 67160 loss: 77.885056 time elapsed: 84.2039 learning rate: 0.005967, scenario: 1, slope: -0.03685666768158568, fluctuations: 0.0\n",
      "step: 67170 loss: 12982.215809 time elapsed: 84.2178 learning rate: 0.010989, scenario: -1, slope: 8.498047688175483, fluctuations: 0.0\n",
      "step: 67180 loss: 6094.013185 time elapsed: 84.2313 learning rate: 0.003831, scenario: -1, slope: 80.06014104998857, fluctuations: 0.03\n",
      "step: 67190 loss: 2746.662194 time elapsed: 84.2441 learning rate: 0.001336, scenario: -1, slope: 84.60195206891478, fluctuations: 0.05\n",
      "step: 67200 loss: 1313.920341 time elapsed: 84.2572 learning rate: 0.000518, scenario: -1, slope: 71.66920457320563, fluctuations: 0.06\n",
      "step: 67210 loss: 1100.108801 time elapsed: 84.2713 learning rate: 0.000180, scenario: -1, slope: 48.2281535768898, fluctuations: 0.06\n",
      "step: 67220 loss: 989.950772 time elapsed: 84.2838 learning rate: 0.000063, scenario: -1, slope: 26.944212428735394, fluctuations: 0.06\n",
      "step: 67230 loss: 963.060938 time elapsed: 84.2961 learning rate: 0.000022, scenario: -1, slope: 5.197813343772605, fluctuations: 0.06\n",
      "step: 67240 loss: 954.938585 time elapsed: 84.3082 learning rate: 0.000018, scenario: 0, slope: -17.69331458661278, fluctuations: 0.06\n",
      "step: 67250 loss: 948.324115 time elapsed: 84.3202 learning rate: 0.000018, scenario: 0, slope: -42.78807419513906, fluctuations: 0.06\n",
      "step: 67260 loss: 942.246782 time elapsed: 84.3319 learning rate: 0.000018, scenario: 0, slope: -71.47041968043243, fluctuations: 0.06\n",
      "step: 67270 loss: 936.539766 time elapsed: 84.3437 learning rate: 0.000018, scenario: 0, slope: -129.83374395255402, fluctuations: 0.05\n",
      "step: 67280 loss: 931.118263 time elapsed: 84.3555 learning rate: 0.000018, scenario: 0, slope: -19.441813328506438, fluctuations: 0.03\n",
      "step: 67290 loss: 925.932537 time elapsed: 84.3674 learning rate: 0.000018, scenario: 0, slope: -5.174522819426495, fluctuations: 0.01\n",
      "step: 67300 loss: 920.949795 time elapsed: 84.3791 learning rate: 0.000018, scenario: 0, slope: -1.9674401343494843, fluctuations: 0.0\n",
      "step: 67310 loss: 916.099132 time elapsed: 84.3917 learning rate: 0.000024, scenario: 1, slope: -0.952094316229882, fluctuations: 0.0\n",
      "step: 67320 loss: 907.245298 time elapsed: 84.4054 learning rate: 0.000061, scenario: 1, slope: -0.6177328770714331, fluctuations: 0.0\n",
      "step: 67330 loss: 886.210920 time elapsed: 84.4194 learning rate: 0.000159, scenario: 1, slope: -0.6151092246483278, fluctuations: 0.0\n",
      "step: 67340 loss: 841.392906 time elapsed: 84.4328 learning rate: 0.000413, scenario: 1, slope: -0.7953385058687854, fluctuations: 0.0\n",
      "step: 67350 loss: 767.646677 time elapsed: 84.4459 learning rate: 0.000604, scenario: 0, slope: -1.248644344042165, fluctuations: 0.0\n",
      "step: 67360 loss: 706.661881 time elapsed: 84.4592 learning rate: 0.000604, scenario: 0, slope: -1.9356495761402592, fluctuations: 0.0\n",
      "step: 67370 loss: 661.989625 time elapsed: 84.4725 learning rate: 0.000604, scenario: 0, slope: -2.6842919602575956, fluctuations: 0.0\n",
      "step: 67380 loss: 628.263769 time elapsed: 84.4860 learning rate: 0.000604, scenario: 0, slope: -3.3646289858580554, fluctuations: 0.0\n",
      "step: 67390 loss: 600.923713 time elapsed: 84.4985 learning rate: 0.000604, scenario: 0, slope: -3.8922672145786104, fluctuations: 0.0\n",
      "step: 67400 loss: 577.953605 time elapsed: 84.5105 learning rate: 0.000604, scenario: 0, slope: -4.187664240175696, fluctuations: 0.0\n",
      "step: 67410 loss: 558.133890 time elapsed: 84.5226 learning rate: 0.000604, scenario: 0, slope: -4.268511192997285, fluctuations: 0.0\n",
      "step: 67420 loss: 540.682468 time elapsed: 84.5342 learning rate: 0.000604, scenario: 0, slope: -4.048209977110424, fluctuations: 0.0\n",
      "step: 67430 loss: 525.084061 time elapsed: 84.5459 learning rate: 0.000604, scenario: 0, slope: -3.57632498419813, fluctuations: 0.0\n",
      "step: 67440 loss: 510.964438 time elapsed: 84.5579 learning rate: 0.000604, scenario: 0, slope: -2.9613945589996646, fluctuations: 0.0\n",
      "step: 67450 loss: 498.051496 time elapsed: 84.5698 learning rate: 0.000604, scenario: 0, slope: -2.4111734728723206, fluctuations: 0.0\n",
      "step: 67460 loss: 486.139745 time elapsed: 84.5813 learning rate: 0.000604, scenario: 0, slope: -2.0276113111066043, fluctuations: 0.0\n",
      "step: 67470 loss: 475.072435 time elapsed: 84.5935 learning rate: 0.000604, scenario: 0, slope: -1.7592092409755076, fluctuations: 0.0\n",
      "step: 67480 loss: 464.727964 time elapsed: 84.6052 learning rate: 0.000604, scenario: 0, slope: -1.5599312383485093, fluctuations: 0.0\n",
      "step: 67490 loss: 455.010185 time elapsed: 84.6188 learning rate: 0.000604, scenario: 0, slope: -1.4049094952254282, fluctuations: 0.0\n",
      "step: 67500 loss: 445.841742 time elapsed: 84.6328 learning rate: 0.000604, scenario: 0, slope: -1.29198562015377, fluctuations: 0.0\n",
      "step: 67510 loss: 437.159479 time elapsed: 84.6469 learning rate: 0.000604, scenario: 0, slope: -1.1787172252398077, fluctuations: 0.0\n",
      "step: 67520 loss: 428.911095 time elapsed: 84.6602 learning rate: 0.000604, scenario: 0, slope: -1.0934304529398553, fluctuations: 0.0\n",
      "step: 67530 loss: 421.052610 time elapsed: 84.6738 learning rate: 0.000604, scenario: 0, slope: -1.0209426082287516, fluctuations: 0.0\n",
      "step: 67540 loss: 413.546448 time elapsed: 84.6874 learning rate: 0.000604, scenario: 0, slope: -0.9584565570607629, fluctuations: 0.0\n",
      "step: 67550 loss: 406.360002 time elapsed: 84.7012 learning rate: 0.000604, scenario: 0, slope: -0.9039358221607371, fluctuations: 0.0\n",
      "step: 67560 loss: 399.464592 time elapsed: 84.7134 learning rate: 0.000604, scenario: 0, slope: -0.8558653818781782, fluctuations: 0.0\n",
      "step: 67570 loss: 392.834680 time elapsed: 84.7250 learning rate: 0.000604, scenario: 0, slope: -0.813109849869224, fluctuations: 0.0\n",
      "step: 67580 loss: 386.447315 time elapsed: 84.7371 learning rate: 0.000604, scenario: 0, slope: -0.7748056166351848, fluctuations: 0.0\n",
      "step: 67590 loss: 380.281746 time elapsed: 84.7489 learning rate: 0.000604, scenario: 0, slope: -0.7402874846211387, fluctuations: 0.0\n",
      "step: 67600 loss: 374.319174 time elapsed: 84.7607 learning rate: 0.000604, scenario: 0, slope: -0.712027307204133, fluctuations: 0.0\n",
      "step: 67610 loss: 368.542581 time elapsed: 84.7732 learning rate: 0.000604, scenario: 0, slope: -0.6806437152980274, fluctuations: 0.0\n",
      "step: 67620 loss: 362.936519 time elapsed: 84.7851 learning rate: 0.000604, scenario: 0, slope: -0.6547775957488884, fluctuations: 0.0\n",
      "step: 67630 loss: 357.486328 time elapsed: 84.7968 learning rate: 0.000604, scenario: 0, slope: -0.6311679777165194, fluctuations: 0.0\n",
      "step: 67640 loss: 352.172904 time elapsed: 84.8083 learning rate: 0.000604, scenario: 0, slope: -0.6096026480759145, fluctuations: 0.0\n",
      "step: 67650 loss: 346.904571 time elapsed: 84.8219 learning rate: 0.000604, scenario: 0, slope: -0.590076689167111, fluctuations: 0.0\n",
      "step: 67660 loss: 341.510027 time elapsed: 84.8353 learning rate: 0.000604, scenario: 0, slope: -0.5739651356366117, fluctuations: 0.0\n",
      "step: 67670 loss: 336.436945 time elapsed: 84.8483 learning rate: 0.000604, scenario: 0, slope: -0.5599107681783068, fluctuations: 0.0\n",
      "step: 67680 loss: 331.563413 time elapsed: 84.8621 learning rate: 0.000604, scenario: 0, slope: -0.5467774944733103, fluctuations: 0.0\n",
      "step: 67690 loss: 326.853696 time elapsed: 84.8752 learning rate: 0.000604, scenario: 0, slope: -0.5341044817612124, fluctuations: 0.0\n",
      "step: 67700 loss: 322.270533 time elapsed: 84.8880 learning rate: 0.000604, scenario: 0, slope: -0.5228279927472086, fluctuations: 0.0\n",
      "step: 67710 loss: 317.814590 time elapsed: 84.9020 learning rate: 0.000604, scenario: 0, slope: -0.5088927532351153, fluctuations: 0.0\n",
      "step: 67720 loss: 313.490854 time elapsed: 84.9150 learning rate: 0.000604, scenario: 0, slope: -0.495800133763257, fluctuations: 0.0\n",
      "step: 67730 loss: 309.297722 time elapsed: 84.9273 learning rate: 0.000604, scenario: 0, slope: -0.48207236094588657, fluctuations: 0.0\n",
      "step: 67740 loss: 305.234493 time elapsed: 84.9398 learning rate: 0.000604, scenario: 0, slope: -0.46755852950390003, fluctuations: 0.0\n",
      "step: 67750 loss: 301.299605 time elapsed: 84.9520 learning rate: 0.000604, scenario: 0, slope: -0.45237734045803585, fluctuations: 0.0\n",
      "step: 67760 loss: 297.490216 time elapsed: 84.9637 learning rate: 0.000604, scenario: 0, slope: -0.4384334448926974, fluctuations: 0.0\n",
      "step: 67770 loss: 293.802591 time elapsed: 84.9756 learning rate: 0.000604, scenario: 0, slope: -0.4253553202727107, fluctuations: 0.0\n",
      "step: 67780 loss: 290.232055 time elapsed: 84.9877 learning rate: 0.000604, scenario: 0, slope: -0.41249567747750265, fluctuations: 0.0\n",
      "step: 67790 loss: 286.773229 time elapsed: 84.9993 learning rate: 0.000604, scenario: 0, slope: -0.3998041830508302, fluctuations: 0.0\n",
      "step: 67800 loss: 283.420224 time elapsed: 85.0108 learning rate: 0.000604, scenario: 0, slope: -0.3885814300746479, fluctuations: 0.0\n",
      "step: 67810 loss: 280.166862 time elapsed: 85.0227 learning rate: 0.000604, scenario: 0, slope: -0.3752128383843639, fluctuations: 0.0\n",
      "step: 67820 loss: 277.006868 time elapsed: 85.0366 learning rate: 0.000604, scenario: 0, slope: -0.3634727528219326, fluctuations: 0.0\n",
      "step: 67830 loss: 273.934044 time elapsed: 85.0750 learning rate: 0.000604, scenario: 0, slope: -0.35219683875944224, fluctuations: 0.0\n",
      "step: 67840 loss: 270.942391 time elapsed: 85.0910 learning rate: 0.000604, scenario: 0, slope: -0.34144122681631545, fluctuations: 0.0\n",
      "step: 67850 loss: 268.026207 time elapsed: 85.1067 learning rate: 0.000604, scenario: 0, slope: -0.3312452444678149, fluctuations: 0.0\n",
      "step: 67860 loss: 265.180147 time elapsed: 85.1226 learning rate: 0.000604, scenario: 0, slope: -0.3216318123786359, fluctuations: 0.0\n",
      "step: 67870 loss: 262.399246 time elapsed: 85.1366 learning rate: 0.000604, scenario: 0, slope: -0.31260804853397656, fluctuations: 0.0\n",
      "step: 67880 loss: 259.678939 time elapsed: 85.1486 learning rate: 0.000604, scenario: 0, slope: -0.3041674026251111, fluctuations: 0.0\n",
      "step: 67890 loss: 257.015046 time elapsed: 85.1602 learning rate: 0.000604, scenario: 0, slope: -0.2962922406176038, fluctuations: 0.0\n",
      "step: 67900 loss: 254.403759 time elapsed: 85.1716 learning rate: 0.000604, scenario: 0, slope: -0.2896668095112523, fluctuations: 0.0\n",
      "step: 67910 loss: 251.841618 time elapsed: 85.1842 learning rate: 0.000604, scenario: 0, slope: -0.28212923118280975, fluctuations: 0.0\n",
      "step: 67920 loss: 249.300536 time elapsed: 85.1979 learning rate: 0.000804, scenario: 1, slope: -0.27579007645565345, fluctuations: 0.0\n",
      "step: 67930 loss: 244.563008 time elapsed: 85.2100 learning rate: 0.001724, scenario: 0, slope: -0.2750318199010384, fluctuations: 0.0\n",
      "step: 67940 loss: 237.812685 time elapsed: 85.2218 learning rate: 0.001724, scenario: 0, slope: -0.2941597452203376, fluctuations: 0.0\n",
      "step: 67950 loss: 231.375373 time elapsed: 85.2338 learning rate: 0.001724, scenario: 0, slope: -0.3305019365494275, fluctuations: 0.0\n",
      "step: 67960 loss: 225.229996 time elapsed: 85.2471 learning rate: 0.001724, scenario: 0, slope: -0.3775920426268311, fluctuations: 0.0\n",
      "step: 67970 loss: 219.346904 time elapsed: 85.2609 learning rate: 0.001724, scenario: 0, slope: -0.4294211954007368, fluctuations: 0.0\n",
      "step: 67980 loss: 213.690071 time elapsed: 85.2741 learning rate: 0.001724, scenario: 0, slope: -0.4804354404988013, fluctuations: 0.0\n",
      "step: 67990 loss: 208.189347 time elapsed: 85.2878 learning rate: 0.001724, scenario: 0, slope: -0.5256407572826901, fluctuations: 0.0\n",
      "step: 68000 loss: 202.893598 time elapsed: 85.3012 learning rate: 0.001724, scenario: 0, slope: -0.5574189219200918, fluctuations: 0.0\n",
      "step: 68010 loss: 197.762702 time elapsed: 85.3154 learning rate: 0.001724, scenario: 0, slope: -0.5796044115408617, fluctuations: 0.0\n",
      "step: 68020 loss: 192.956541 time elapsed: 85.3294 learning rate: 0.001724, scenario: 0, slope: -0.5793054845602049, fluctuations: 0.0\n",
      "step: 68030 loss: 187.887072 time elapsed: 85.3424 learning rate: 0.001724, scenario: 0, slope: -0.561588303641632, fluctuations: 0.0\n",
      "step: 68040 loss: 182.937883 time elapsed: 85.3540 learning rate: 0.001724, scenario: 0, slope: -0.5428517958053392, fluctuations: 0.0\n",
      "step: 68050 loss: 178.537346 time elapsed: 85.3658 learning rate: 0.001724, scenario: 0, slope: -0.5261897942635911, fluctuations: 0.0\n",
      "step: 68060 loss: 174.057231 time elapsed: 85.3775 learning rate: 0.001724, scenario: 0, slope: -0.5111589746150024, fluctuations: 0.01\n",
      "step: 68070 loss: 169.503533 time elapsed: 85.3895 learning rate: 0.001724, scenario: 0, slope: -0.49717556257752743, fluctuations: 0.01\n",
      "step: 68080 loss: 165.217630 time elapsed: 85.4013 learning rate: 0.001724, scenario: 0, slope: -0.4838916591891877, fluctuations: 0.01\n",
      "step: 68090 loss: 161.267593 time elapsed: 85.4131 learning rate: 0.001724, scenario: 0, slope: -0.47031340562905233, fluctuations: 0.01\n",
      "step: 68100 loss: 157.539994 time elapsed: 85.4248 learning rate: 0.001724, scenario: 0, slope: -0.45743012100221286, fluctuations: 0.01\n",
      "step: 68110 loss: 153.991466 time elapsed: 85.4371 learning rate: 0.001724, scenario: 0, slope: -0.4402416310437762, fluctuations: 0.01\n",
      "step: 68120 loss: 150.585137 time elapsed: 85.4496 learning rate: 0.001724, scenario: 0, slope: -0.42303848494027263, fluctuations: 0.01\n",
      "step: 68130 loss: 147.290715 time elapsed: 85.4658 learning rate: 0.001724, scenario: 0, slope: -0.40483382719015343, fluctuations: 0.01\n",
      "step: 68140 loss: 144.057382 time elapsed: 85.4799 learning rate: 0.001724, scenario: 0, slope: -0.387053233301661, fluctuations: 0.01\n",
      "step: 68150 loss: 140.884879 time elapsed: 85.4939 learning rate: 0.001724, scenario: 0, slope: -0.3698615313757749, fluctuations: 0.01\n",
      "step: 68160 loss: 137.691888 time elapsed: 85.5076 learning rate: 0.001724, scenario: 0, slope: -0.3552156164206445, fluctuations: 0.0\n",
      "step: 68170 loss: 134.444862 time elapsed: 85.5216 learning rate: 0.001724, scenario: 0, slope: -0.3429979144444725, fluctuations: 0.0\n",
      "step: 68180 loss: 131.073126 time elapsed: 85.5348 learning rate: 0.001724, scenario: 0, slope: -0.33453823537179944, fluctuations: 0.0\n",
      "step: 68190 loss: 127.504643 time elapsed: 85.5477 learning rate: 0.001724, scenario: 0, slope: -0.3301811351550635, fluctuations: 0.0\n",
      "step: 68200 loss: 123.680563 time elapsed: 85.5594 learning rate: 0.001724, scenario: 0, slope: -0.33021869014613886, fluctuations: 0.0\n",
      "step: 68210 loss: 119.590473 time elapsed: 85.5721 learning rate: 0.001724, scenario: 0, slope: -0.3357158772440924, fluctuations: 0.0\n",
      "step: 68220 loss: 115.319044 time elapsed: 85.5844 learning rate: 0.001724, scenario: 0, slope: -0.3456237442085868, fluctuations: 0.0\n",
      "step: 68230 loss: 111.057563 time elapsed: 85.5968 learning rate: 0.001724, scenario: 0, slope: -0.35864669908777946, fluctuations: 0.0\n",
      "step: 68240 loss: 107.043316 time elapsed: 85.6087 learning rate: 0.001724, scenario: 0, slope: -0.372749558851523, fluctuations: 0.0\n",
      "step: 68250 loss: 103.461286 time elapsed: 85.6207 learning rate: 0.001724, scenario: 0, slope: -0.38434350558920294, fluctuations: 0.0\n",
      "step: 68260 loss: 100.360884 time elapsed: 85.6325 learning rate: 0.001724, scenario: 0, slope: -0.3900628827109222, fluctuations: 0.0\n",
      "step: 68270 loss: 97.602803 time elapsed: 85.6440 learning rate: 0.001724, scenario: 0, slope: -0.387680478335805, fluctuations: 0.0\n",
      "step: 68280 loss: 94.916691 time elapsed: 85.6566 learning rate: 0.001724, scenario: 0, slope: -0.37738606016838744, fluctuations: 0.0\n",
      "step: 68290 loss: 93.220020 time elapsed: 85.6703 learning rate: 0.001724, scenario: 0, slope: -0.3621776506610387, fluctuations: 0.04\n",
      "step: 68300 loss: 90.987203 time elapsed: 85.6839 learning rate: 0.001724, scenario: 0, slope: -0.33956073822105143, fluctuations: 0.07\n",
      "step: 68310 loss: 89.486429 time elapsed: 85.6974 learning rate: 0.001724, scenario: 0, slope: -0.3012999674697328, fluctuations: 0.08\n",
      "step: 68320 loss: 88.079596 time elapsed: 85.7107 learning rate: 0.001724, scenario: 0, slope: -0.2662863964000904, fluctuations: 0.08\n",
      "step: 68330 loss: 86.812910 time elapsed: 85.7243 learning rate: 0.001724, scenario: 0, slope: -0.23449488682912606, fluctuations: 0.08\n",
      "step: 68340 loss: 85.656477 time elapsed: 85.7378 learning rate: 0.001724, scenario: 0, slope: -0.2066339331627685, fluctuations: 0.08\n",
      "step: 68350 loss: 84.566282 time elapsed: 85.7506 learning rate: 0.001724, scenario: 0, slope: -0.18250907878154551, fluctuations: 0.08\n",
      "step: 68360 loss: 83.530826 time elapsed: 85.7629 learning rate: 0.001724, scenario: 0, slope: -0.16119791152472418, fluctuations: 0.08\n",
      "step: 68370 loss: 82.541619 time elapsed: 85.7749 learning rate: 0.001724, scenario: 0, slope: -0.14176566595085932, fluctuations: 0.08\n",
      "step: 68380 loss: 81.590245 time elapsed: 85.7869 learning rate: 0.001724, scenario: 0, slope: -0.12628658466968104, fluctuations: 0.07\n",
      "step: 68390 loss: 80.671434 time elapsed: 85.7989 learning rate: 0.001724, scenario: 0, slope: -0.11659730186117906, fluctuations: 0.03\n",
      "step: 68400 loss: 79.781117 time elapsed: 85.8107 learning rate: 0.001724, scenario: 0, slope: -0.11021990092898959, fluctuations: 0.0\n",
      "step: 68410 loss: 78.916133 time elapsed: 85.8230 learning rate: 0.001724, scenario: 0, slope: -0.10330059606181957, fluctuations: 0.0\n",
      "step: 68420 loss: 78.075821 time elapsed: 85.8347 learning rate: 0.001724, scenario: 0, slope: -0.09846899716067652, fluctuations: 0.0\n",
      "step: 68430 loss: 77.272332 time elapsed: 85.8467 learning rate: 0.001724, scenario: 0, slope: -0.09442756454648316, fluctuations: 0.0\n",
      "step: 68440 loss: 76.463739 time elapsed: 85.8586 learning rate: 0.001724, scenario: 0, slope: -0.09103742203766843, fluctuations: 0.0\n",
      "step: 68450 loss: 75.693562 time elapsed: 85.8724 learning rate: 0.001724, scenario: 0, slope: -0.08808104997975441, fluctuations: 0.0\n",
      "step: 68460 loss: 74.939996 time elapsed: 85.8861 learning rate: 0.001724, scenario: 0, slope: -0.08542408132867159, fluctuations: 0.0\n",
      "step: 68470 loss: 74.205010 time elapsed: 85.8995 learning rate: 0.001724, scenario: 0, slope: -0.08298266712922382, fluctuations: 0.0\n",
      "step: 68480 loss: 73.410276 time elapsed: 85.9129 learning rate: 0.003360, scenario: 1, slope: -0.08083985690886553, fluctuations: 0.0\n",
      "step: 68490 loss: 104.862869 time elapsed: 85.9265 learning rate: 0.005062, scenario: -1, slope: 0.039216388706388036, fluctuations: 0.03\n",
      "step: 68500 loss: 79.197784 time elapsed: 85.9398 learning rate: 0.001961, scenario: -1, slope: 0.08785442987353007, fluctuations: 0.06\n",
      "step: 68510 loss: 74.343531 time elapsed: 85.9545 learning rate: 0.000684, scenario: -1, slope: 0.07563527726396105, fluctuations: 0.09\n",
      "step: 68520 loss: 73.351677 time elapsed: 85.9667 learning rate: 0.000238, scenario: -1, slope: 0.04274418040464855, fluctuations: 0.1\n",
      "step: 68530 loss: 73.052333 time elapsed: 85.9785 learning rate: 0.000083, scenario: -1, slope: 0.009750168338254735, fluctuations: 0.11\n",
      "step: 68540 loss: 72.928273 time elapsed: 85.9907 learning rate: 0.000139, scenario: 1, slope: -0.024210834090254988, fluctuations: 0.11\n",
      "step: 68550 loss: 72.699134 time elapsed: 86.0028 learning rate: 0.000359, scenario: 1, slope: -0.05766911406407883, fluctuations: 0.11\n",
      "step: 68560 loss: 72.229616 time elapsed: 86.0146 learning rate: 0.000526, scenario: 0, slope: -0.09578315868938622, fluctuations: 0.11\n",
      "step: 68570 loss: 71.742416 time elapsed: 86.0265 learning rate: 0.000526, scenario: 0, slope: -0.14395893910809304, fluctuations: 0.11\n",
      "step: 68580 loss: 71.291155 time elapsed: 86.0386 learning rate: 0.000526, scenario: 0, slope: -0.20959519369652715, fluctuations: 0.11\n",
      "step: 68590 loss: 70.873070 time elapsed: 86.0504 learning rate: 0.000526, scenario: 0, slope: -0.12171462297192999, fluctuations: 0.07\n",
      "step: 68600 loss: 70.393381 time elapsed: 86.0625 learning rate: 0.001025, scenario: 1, slope: -0.053750065882348576, fluctuations: 0.04\n",
      "step: 68610 loss: 69.438880 time elapsed: 86.0769 learning rate: 0.002659, scenario: 1, slope: -0.041363248648277506, fluctuations: 0.02\n",
      "step: 68620 loss: 67.557233 time elapsed: 86.0912 learning rate: 0.006897, scenario: 1, slope: -0.04826793487142824, fluctuations: 0.0\n",
      "step: 68630 loss: 1447.337961 time elapsed: 86.1048 learning rate: 0.012700, scenario: -1, slope: 0.8303864178972241, fluctuations: 0.0\n",
      "step: 68640 loss: 2562.149618 time elapsed: 86.1204 learning rate: 0.004428, scenario: -1, slope: 18.22966656936578, fluctuations: 0.04\n",
      "step: 68650 loss: 904.042328 time elapsed: 86.1338 learning rate: 0.001544, scenario: -1, slope: 22.227268893172145, fluctuations: 0.06\n",
      "step: 68660 loss: 676.772981 time elapsed: 86.1472 learning rate: 0.000538, scenario: -1, slope: 19.89886217760888, fluctuations: 0.07\n",
      "step: 68670 loss: 552.115307 time elapsed: 86.1607 learning rate: 0.000188, scenario: -1, slope: 15.93091716362873, fluctuations: 0.08\n",
      "step: 68680 loss: 536.845799 time elapsed: 86.1729 learning rate: 0.000065, scenario: -1, slope: 10.928765998432004, fluctuations: 0.08\n",
      "step: 68690 loss: 529.218363 time elapsed: 86.1848 learning rate: 0.000023, scenario: -1, slope: 5.679923763943672, fluctuations: 0.08\n",
      "step: 68700 loss: 526.701298 time elapsed: 86.1965 learning rate: 0.000009, scenario: -1, slope: 0.520145264928606, fluctuations: 0.08\n",
      "step: 68710 loss: 525.468058 time elapsed: 86.2088 learning rate: 0.000009, scenario: 0, slope: -6.747966230809825, fluctuations: 0.08\n",
      "step: 68720 loss: 524.311101 time elapsed: 86.2205 learning rate: 0.000009, scenario: 0, slope: -14.834457445032928, fluctuations: 0.08\n",
      "step: 68730 loss: 523.214993 time elapsed: 86.2323 learning rate: 0.000009, scenario: 0, slope: -24.040984633429407, fluctuations: 0.08\n",
      "step: 68740 loss: 522.166159 time elapsed: 86.2440 learning rate: 0.000009, scenario: 0, slope: -7.763372177858117, fluctuations: 0.03\n",
      "step: 68750 loss: 521.153681 time elapsed: 86.2560 learning rate: 0.000009, scenario: 0, slope: -1.7418560131350729, fluctuations: 0.02\n",
      "step: 68760 loss: 520.169108 time elapsed: 86.2686 learning rate: 0.000011, scenario: 1, slope: -0.60322418846036, fluctuations: 0.0\n",
      "step: 68770 loss: 518.490027 time elapsed: 86.2824 learning rate: 0.000028, scenario: 1, slope: -0.20118367734614678, fluctuations: 0.0\n",
      "step: 68780 loss: 514.313378 time elapsed: 86.2958 learning rate: 0.000072, scenario: 1, slope: -0.14327364860370181, fluctuations: 0.0\n",
      "step: 68790 loss: 504.119733 time elapsed: 86.3093 learning rate: 0.000187, scenario: 1, slope: -0.1630876708190279, fluctuations: 0.0\n",
      "step: 68800 loss: 479.482295 time elapsed: 86.3228 learning rate: 0.000440, scenario: 1, slope: -0.2617154821401136, fluctuations: 0.0\n",
      "step: 68810 loss: 428.267573 time elapsed: 86.3367 learning rate: 0.000944, scenario: 0, slope: -0.5716602763812136, fluctuations: 0.0\n",
      "step: 68820 loss: 367.367094 time elapsed: 86.3498 learning rate: 0.000944, scenario: 0, slope: -1.1156291456525025, fluctuations: 0.0\n",
      "step: 68830 loss: 320.159144 time elapsed: 86.3627 learning rate: 0.000944, scenario: 0, slope: -1.794128877345677, fluctuations: 0.0\n",
      "step: 68840 loss: 282.589181 time elapsed: 86.3751 learning rate: 0.000944, scenario: 0, slope: -2.4836130712705766, fluctuations: 0.0\n",
      "step: 68850 loss: 252.185716 time elapsed: 86.3865 learning rate: 0.000944, scenario: 0, slope: -3.0913411000737754, fluctuations: 0.0\n",
      "step: 68860 loss: 226.973408 time elapsed: 86.3984 learning rate: 0.000944, scenario: 0, slope: -3.545615586560927, fluctuations: 0.0\n",
      "step: 68870 loss: 204.861919 time elapsed: 86.4101 learning rate: 0.000944, scenario: 0, slope: -3.7939825653150634, fluctuations: 0.0\n",
      "step: 68880 loss: 186.565261 time elapsed: 86.4217 learning rate: 0.000944, scenario: 0, slope: -3.8035930424125532, fluctuations: 0.0\n",
      "step: 68890 loss: 171.711140 time elapsed: 86.4334 learning rate: 0.000944, scenario: 0, slope: -3.561342899594528, fluctuations: 0.0\n",
      "step: 68900 loss: 159.787629 time elapsed: 86.4449 learning rate: 0.000944, scenario: 0, slope: -3.1567645150058796, fluctuations: 0.0\n",
      "step: 68910 loss: 150.268421 time elapsed: 86.4572 learning rate: 0.000944, scenario: 0, slope: -2.5587661390807783, fluctuations: 0.0\n",
      "step: 68920 loss: 142.614288 time elapsed: 86.4692 learning rate: 0.000944, scenario: 0, slope: -2.100254962205184, fluctuations: 0.0\n",
      "step: 68930 loss: 136.334346 time elapsed: 86.4823 learning rate: 0.000944, scenario: 0, slope: -1.732709951034929, fluctuations: 0.0\n",
      "step: 68940 loss: 131.051036 time elapsed: 86.4956 learning rate: 0.000944, scenario: 0, slope: -1.4298619530899717, fluctuations: 0.0\n",
      "step: 68950 loss: 126.498994 time elapsed: 86.5089 learning rate: 0.000944, scenario: 0, slope: -1.1762459067215725, fluctuations: 0.0\n",
      "step: 68960 loss: 122.497935 time elapsed: 86.5220 learning rate: 0.000944, scenario: 0, slope: -0.9625972504407538, fluctuations: 0.0\n",
      "step: 68970 loss: 118.921246 time elapsed: 86.5355 learning rate: 0.000944, scenario: 0, slope: -0.786449913087335, fluctuations: 0.0\n",
      "step: 68980 loss: 115.678570 time elapsed: 86.5485 learning rate: 0.000944, scenario: 0, slope: -0.6485205102586634, fluctuations: 0.0\n",
      "step: 68990 loss: 112.704411 time elapsed: 86.5620 learning rate: 0.000944, scenario: 0, slope: -0.5431763122471097, fluctuations: 0.0\n",
      "step: 69000 loss: 109.950765 time elapsed: 86.5741 learning rate: 0.000944, scenario: 0, slope: -0.4707247048704424, fluctuations: 0.0\n",
      "step: 69010 loss: 107.382171 time elapsed: 86.5867 learning rate: 0.000944, scenario: 0, slope: -0.40412611035658647, fluctuations: 0.0\n",
      "step: 69020 loss: 104.972197 time elapsed: 86.5982 learning rate: 0.000944, scenario: 0, slope: -0.35877781342703025, fluctuations: 0.0\n",
      "step: 69030 loss: 102.700929 time elapsed: 86.6100 learning rate: 0.000944, scenario: 0, slope: -0.32357612720029816, fluctuations: 0.0\n",
      "step: 69040 loss: 100.553129 time elapsed: 86.6220 learning rate: 0.000944, scenario: 0, slope: -0.2955879316176078, fluctuations: 0.0\n",
      "step: 69050 loss: 98.516900 time elapsed: 86.6333 learning rate: 0.000944, scenario: 0, slope: -0.27280265827008165, fluctuations: 0.0\n",
      "step: 69060 loss: 96.582707 time elapsed: 86.6454 learning rate: 0.000944, scenario: 0, slope: -0.2538267655247437, fluctuations: 0.0\n",
      "step: 69070 loss: 94.742690 time elapsed: 86.6569 learning rate: 0.000944, scenario: 0, slope: -0.23767729857805406, fluctuations: 0.0\n",
      "step: 69080 loss: 92.990186 time elapsed: 86.6689 learning rate: 0.000944, scenario: 0, slope: -0.22365356630080815, fluctuations: 0.0\n",
      "step: 69090 loss: 91.319409 time elapsed: 86.6809 learning rate: 0.000944, scenario: 0, slope: -0.21125354916523645, fluctuations: 0.0\n",
      "step: 69100 loss: 89.725243 time elapsed: 86.6944 learning rate: 0.000944, scenario: 0, slope: -0.2011819697480536, fluctuations: 0.0\n",
      "step: 69110 loss: 88.203107 time elapsed: 86.7087 learning rate: 0.000944, scenario: 0, slope: -0.18998615767886853, fluctuations: 0.0\n",
      "step: 69120 loss: 86.748870 time elapsed: 86.7219 learning rate: 0.000944, scenario: 0, slope: -0.180674728459732, fluctuations: 0.0\n",
      "step: 69130 loss: 85.358784 time elapsed: 86.7356 learning rate: 0.000944, scenario: 0, slope: -0.17204834311317677, fluctuations: 0.0\n",
      "step: 69140 loss: 84.029440 time elapsed: 86.7492 learning rate: 0.000944, scenario: 0, slope: -0.16400800429162565, fluctuations: 0.0\n",
      "step: 69150 loss: 82.757720 time elapsed: 86.7623 learning rate: 0.000944, scenario: 0, slope: -0.15647906312608173, fluctuations: 0.0\n",
      "step: 69160 loss: 81.540759 time elapsed: 86.7765 learning rate: 0.000944, scenario: 0, slope: -0.14940341859313364, fluctuations: 0.0\n",
      "step: 69170 loss: 80.375905 time elapsed: 86.7892 learning rate: 0.000944, scenario: 0, slope: -0.14273430511193447, fluctuations: 0.0\n",
      "step: 69180 loss: 79.260684 time elapsed: 86.8015 learning rate: 0.000944, scenario: 0, slope: -0.1364329798172755, fluctuations: 0.0\n",
      "step: 69190 loss: 78.192761 time elapsed: 86.8133 learning rate: 0.000944, scenario: 0, slope: -0.1304667246574774, fluctuations: 0.0\n",
      "step: 69200 loss: 77.169913 time elapsed: 86.8273 learning rate: 0.000944, scenario: 0, slope: -0.12536047170476328, fluctuations: 0.0\n",
      "step: 69210 loss: 76.190004 time elapsed: 86.8404 learning rate: 0.000944, scenario: 0, slope: -0.11943220850096414, fluctuations: 0.0\n",
      "step: 69220 loss: 75.250960 time elapsed: 86.8526 learning rate: 0.000944, scenario: 0, slope: -0.11432037957729946, fluctuations: 0.0\n",
      "step: 69230 loss: 74.350756 time elapsed: 86.8642 learning rate: 0.000944, scenario: 0, slope: -0.10945570033893445, fluctuations: 0.0\n",
      "step: 69240 loss: 73.487402 time elapsed: 86.8761 learning rate: 0.000944, scenario: 0, slope: -0.1048247389573245, fluctuations: 0.0\n",
      "step: 69250 loss: 72.658932 time elapsed: 86.8880 learning rate: 0.000944, scenario: 0, slope: -0.10041677825177332, fluctuations: 0.0\n",
      "step: 69260 loss: 71.863390 time elapsed: 86.9001 learning rate: 0.000944, scenario: 0, slope: -0.0962234514945966, fluctuations: 0.0\n",
      "step: 69270 loss: 71.098825 time elapsed: 86.9138 learning rate: 0.000944, scenario: 0, slope: -0.09223840120790447, fluctuations: 0.0\n",
      "step: 69280 loss: 70.363257 time elapsed: 86.9275 learning rate: 0.000944, scenario: 0, slope: -0.08845702348685557, fluctuations: 0.0\n",
      "step: 69290 loss: 69.654618 time elapsed: 86.9410 learning rate: 0.000944, scenario: 0, slope: -0.08487643753049481, fluctuations: 0.0\n",
      "step: 69300 loss: 68.970503 time elapsed: 86.9540 learning rate: 0.000944, scenario: 0, slope: -0.0818251211157207, fluctuations: 0.0\n",
      "step: 69310 loss: 68.306291 time elapsed: 86.9689 learning rate: 0.000944, scenario: 0, slope: -0.07832342832501064, fluctuations: 0.0\n",
      "step: 69320 loss: 67.611697 time elapsed: 86.9826 learning rate: 0.000944, scenario: 0, slope: -0.07546339987356886, fluctuations: 0.0\n",
      "step: 69330 loss: 66.714538 time elapsed: 86.9973 learning rate: 0.002225, scenario: 1, slope: -0.07348993506934565, fluctuations: 0.0\n",
      "step: 69340 loss: 65.205803 time elapsed: 87.0113 learning rate: 0.003583, scenario: 0, slope: -0.07573017126768927, fluctuations: 0.0\n",
      "step: 69350 loss: 62.725730 time elapsed: 87.0232 learning rate: 0.003583, scenario: 0, slope: -0.08485831373188651, fluctuations: 0.02\n",
      "step: 69360 loss: 60.650933 time elapsed: 87.0349 learning rate: 0.003583, scenario: 0, slope: -0.10083510444377006, fluctuations: 0.02\n",
      "step: 69370 loss: 59.035032 time elapsed: 87.0479 learning rate: 0.003583, scenario: 0, slope: -0.11832731310127738, fluctuations: 0.02\n",
      "step: 69380 loss: 57.611391 time elapsed: 87.0603 learning rate: 0.003583, scenario: 0, slope: -0.13459498099372086, fluctuations: 0.02\n",
      "step: 69390 loss: 56.370991 time elapsed: 87.0723 learning rate: 0.003583, scenario: 0, slope: -0.14762306021494911, fluctuations: 0.02\n",
      "step: 69400 loss: 55.266765 time elapsed: 87.0840 learning rate: 0.003583, scenario: 0, slope: -0.15531018766428625, fluctuations: 0.02\n",
      "step: 69410 loss: 54.268720 time elapsed: 87.0966 learning rate: 0.003583, scenario: 0, slope: -0.15808480613820192, fluctuations: 0.02\n",
      "step: 69420 loss: 53.355312 time elapsed: 87.1096 learning rate: 0.003583, scenario: 0, slope: -0.15311681663140708, fluctuations: 0.02\n",
      "step: 69430 loss: 52.509741 time elapsed: 87.1239 learning rate: 0.003583, scenario: 0, slope: -0.14044987266778847, fluctuations: 0.02\n",
      "step: 69440 loss: 51.718918 time elapsed: 87.1378 learning rate: 0.003583, scenario: 0, slope: -0.12461108073056182, fluctuations: 0.01\n",
      "step: 69450 loss: 50.972894 time elapsed: 87.1507 learning rate: 0.003583, scenario: 0, slope: -0.11103813847823253, fluctuations: 0.0\n",
      "step: 69460 loss: 50.263754 time elapsed: 87.1642 learning rate: 0.003583, scenario: 0, slope: -0.09991574257069298, fluctuations: 0.0\n",
      "step: 69470 loss: 49.584985 time elapsed: 87.1776 learning rate: 0.003583, scenario: 0, slope: -0.09121464117912877, fluctuations: 0.0\n",
      "step: 69480 loss: 48.923940 time elapsed: 87.1910 learning rate: 0.003583, scenario: 0, slope: -0.08434413887714279, fluctuations: 0.0\n",
      "step: 69490 loss: 48.844371 time elapsed: 87.2040 learning rate: 0.003583, scenario: 0, slope: -0.0783981861095335, fluctuations: 0.02\n",
      "step: 69500 loss: 47.841599 time elapsed: 87.2158 learning rate: 0.003583, scenario: 0, slope: -0.0744426789292741, fluctuations: 0.06\n",
      "step: 69510 loss: 47.037512 time elapsed: 87.2278 learning rate: 0.003583, scenario: 0, slope: -0.07037306665459042, fluctuations: 0.08\n",
      "step: 69520 loss: 46.410691 time elapsed: 87.2396 learning rate: 0.003583, scenario: 0, slope: -0.06772710302602249, fluctuations: 0.08\n",
      "step: 69530 loss: 45.822984 time elapsed: 87.2507 learning rate: 0.003583, scenario: 0, slope: -0.06579296213568728, fluctuations: 0.08\n",
      "step: 69540 loss: 45.267657 time elapsed: 87.2624 learning rate: 0.003583, scenario: 0, slope: -0.06417248108984099, fluctuations: 0.08\n",
      "step: 69550 loss: 44.739531 time elapsed: 87.2739 learning rate: 0.003583, scenario: 0, slope: -0.06266379042660342, fluctuations: 0.08\n",
      "step: 69560 loss: 44.232859 time elapsed: 87.2858 learning rate: 0.003583, scenario: 0, slope: -0.061104105640558855, fluctuations: 0.08\n",
      "step: 69570 loss: 43.714807 time elapsed: 87.2980 learning rate: 0.003583, scenario: 0, slope: -0.05950095577805952, fluctuations: 0.08\n",
      "step: 69580 loss: 43.225061 time elapsed: 87.3101 learning rate: 0.003583, scenario: 0, slope: -0.057704875827296796, fluctuations: 0.08\n",
      "step: 69590 loss: 42.747585 time elapsed: 87.3236 learning rate: 0.003583, scenario: 0, slope: -0.05550444234374208, fluctuations: 0.05\n",
      "step: 69600 loss: 42.279942 time elapsed: 87.3371 learning rate: 0.003583, scenario: 0, slope: -0.05352659956877928, fluctuations: 0.01\n",
      "step: 69610 loss: 41.821643 time elapsed: 87.3513 learning rate: 0.003583, scenario: 0, slope: -0.05146855037822274, fluctuations: 0.0\n",
      "step: 69620 loss: 41.376934 time elapsed: 87.3648 learning rate: 0.003583, scenario: 0, slope: -0.049931966968225765, fluctuations: 0.0\n",
      "step: 69630 loss: 40.951000 time elapsed: 87.3783 learning rate: 0.003583, scenario: 0, slope: -0.04857520929072109, fluctuations: 0.0\n",
      "step: 69640 loss: 40.499569 time elapsed: 87.3917 learning rate: 0.003583, scenario: 0, slope: -0.04746101098568626, fluctuations: 0.0\n",
      "step: 69650 loss: 40.066491 time elapsed: 87.4054 learning rate: 0.003583, scenario: 0, slope: -0.04646829102417994, fluctuations: 0.0\n",
      "step: 69660 loss: 39.645507 time elapsed: 87.4183 learning rate: 0.003583, scenario: 0, slope: -0.04551175108929996, fluctuations: 0.0\n",
      "step: 69670 loss: 39.232130 time elapsed: 87.4301 learning rate: 0.003583, scenario: 0, slope: -0.044674818760564644, fluctuations: 0.0\n",
      "step: 69680 loss: 38.824696 time elapsed: 87.4422 learning rate: 0.003583, scenario: 0, slope: -0.043903432226684524, fluctuations: 0.0\n",
      "step: 69690 loss: 38.424617 time elapsed: 87.4537 learning rate: 0.003583, scenario: 0, slope: -0.04318790459096255, fluctuations: 0.0\n",
      "step: 69700 loss: 38.031596 time elapsed: 87.4659 learning rate: 0.003583, scenario: 0, slope: -0.04256227874111761, fluctuations: 0.0\n",
      "step: 69710 loss: 37.647135 time elapsed: 87.4785 learning rate: 0.003583, scenario: 0, slope: -0.04181464867207258, fluctuations: 0.0\n",
      "step: 69720 loss: 64.315614 time elapsed: 87.4902 learning rate: 0.008449, scenario: 1, slope: -0.012313027240254201, fluctuations: 0.01\n",
      "step: 69730 loss: 1455.942165 time elapsed: 87.5018 learning rate: 0.003125, scenario: -1, slope: 10.89823923790108, fluctuations: 0.04\n",
      "step: 69740 loss: 669.974593 time elapsed: 87.5134 learning rate: 0.001089, scenario: -1, slope: 14.350962395658929, fluctuations: 0.06\n",
      "step: 69750 loss: 332.495026 time elapsed: 87.5255 learning rate: 0.000380, scenario: -1, slope: 12.874735346940165, fluctuations: 0.08\n",
      "step: 69760 loss: 212.237061 time elapsed: 87.5390 learning rate: 0.000132, scenario: -1, slope: 9.570009746754284, fluctuations: 0.09\n",
      "step: 69770 loss: 185.479437 time elapsed: 87.5528 learning rate: 0.000046, scenario: -1, slope: 5.720165798464144, fluctuations: 0.09\n",
      "step: 69780 loss: 178.360634 time elapsed: 87.5664 learning rate: 0.000016, scenario: -1, slope: 1.9551075736878918, fluctuations: 0.09\n",
      "step: 69790 loss: 176.804393 time elapsed: 87.5797 learning rate: 0.000010, scenario: 0, slope: -1.9118586508704036, fluctuations: 0.09\n",
      "step: 69800 loss: 175.805691 time elapsed: 87.5931 learning rate: 0.000010, scenario: 0, slope: -5.703445040633592, fluctuations: 0.09\n",
      "step: 69810 loss: 174.905767 time elapsed: 87.6068 learning rate: 0.000010, scenario: 0, slope: -11.133295333763384, fluctuations: 0.09\n",
      "step: 69820 loss: 174.066343 time elapsed: 87.6209 learning rate: 0.000010, scenario: 0, slope: -16.31883030462449, fluctuations: 0.08\n",
      "step: 69830 loss: 173.266414 time elapsed: 87.6341 learning rate: 0.000010, scenario: 0, slope: -6.172739403272469, fluctuations: 0.05\n",
      "step: 69840 loss: 172.493429 time elapsed: 87.6465 learning rate: 0.000010, scenario: 0, slope: -1.8354452098411933, fluctuations: 0.02\n",
      "step: 69850 loss: 171.739677 time elapsed: 87.6584 learning rate: 0.000010, scenario: 0, slope: -0.44410889515048707, fluctuations: 0.01\n",
      "step: 69860 loss: 171.000350 time elapsed: 87.6706 learning rate: 0.000012, scenario: 1, slope: -0.1948333924544964, fluctuations: 0.0\n",
      "step: 69870 loss: 169.729487 time elapsed: 87.6822 learning rate: 0.000030, scenario: 1, slope: -0.09925859619667358, fluctuations: 0.0\n",
      "step: 69880 loss: 166.544292 time elapsed: 87.6940 learning rate: 0.000077, scenario: 1, slope: -0.09182550579033051, fluctuations: 0.0\n",
      "step: 69890 loss: 158.856296 time elapsed: 87.7057 learning rate: 0.000201, scenario: 1, slope: -0.12064177045700737, fluctuations: 0.0\n",
      "step: 69900 loss: 141.929524 time elapsed: 87.7173 learning rate: 0.000391, scenario: 0, slope: -0.19463334321367595, fluctuations: 0.0\n",
      "step: 69910 loss: 123.543848 time elapsed: 87.7309 learning rate: 0.000391, scenario: 0, slope: -0.37206639065494684, fluctuations: 0.0\n",
      "step: 69920 loss: 109.199635 time elapsed: 87.7448 learning rate: 0.000391, scenario: 0, slope: -0.5763573352144934, fluctuations: 0.0\n",
      "step: 69930 loss: 97.911062 time elapsed: 87.7584 learning rate: 0.000391, scenario: 0, slope: -0.7828687175242282, fluctuations: 0.0\n",
      "step: 69940 loss: 88.930364 time elapsed: 87.7724 learning rate: 0.000391, scenario: 0, slope: -0.9631222924472477, fluctuations: 0.0\n",
      "step: 69950 loss: 81.700199 time elapsed: 87.7859 learning rate: 0.000391, scenario: 0, slope: -1.0952813369642431, fluctuations: 0.0\n",
      "step: 69960 loss: 75.827018 time elapsed: 87.7991 learning rate: 0.000391, scenario: 0, slope: -1.1623888246373986, fluctuations: 0.0\n",
      "step: 69970 loss: 71.026456 time elapsed: 87.8124 learning rate: 0.000391, scenario: 0, slope: -1.1523621668653827, fluctuations: 0.0\n",
      "step: 69980 loss: 67.087149 time elapsed: 87.8255 learning rate: 0.000391, scenario: 0, slope: -1.0630049613649635, fluctuations: 0.0\n",
      "step: 69990 loss: 63.847828 time elapsed: 87.8378 learning rate: 0.000391, scenario: 0, slope: -0.9083227706643784, fluctuations: 0.0\n",
      "step: 70000 loss: 61.180282 time elapsed: 87.8496 learning rate: 0.000391, scenario: 0, slope: -0.7491794856193497, fluctuations: 0.0\n",
      "step: 70010 loss: 58.978871 time elapsed: 87.8617 learning rate: 0.000391, scenario: 0, slope: -0.5902104408992594, fluctuations: 0.0\n",
      "step: 70020 loss: 57.155019 time elapsed: 87.8732 learning rate: 0.000391, scenario: 0, slope: -0.4789241403483033, fluctuations: 0.0\n",
      "step: 70030 loss: 55.634353 time elapsed: 87.8848 learning rate: 0.000391, scenario: 0, slope: -0.3910509630573145, fluctuations: 0.0\n",
      "step: 70040 loss: 54.355026 time elapsed: 87.8965 learning rate: 0.000391, scenario: 0, slope: -0.32085013726826556, fluctuations: 0.0\n",
      "step: 70050 loss: 53.266491 time elapsed: 87.9083 learning rate: 0.000391, scenario: 0, slope: -0.26431610306166037, fluctuations: 0.0\n",
      "step: 70060 loss: 52.328315 time elapsed: 87.9201 learning rate: 0.000391, scenario: 0, slope: -0.21862779940554303, fluctuations: 0.0\n",
      "step: 70070 loss: 51.508818 time elapsed: 87.9320 learning rate: 0.000391, scenario: 0, slope: -0.18171419985274914, fluctuations: 0.0\n",
      "step: 70080 loss: 50.783578 time elapsed: 87.9458 learning rate: 0.000391, scenario: 0, slope: -0.15198340429245322, fluctuations: 0.0\n",
      "step: 70090 loss: 50.133973 time elapsed: 87.9599 learning rate: 0.000391, scenario: 0, slope: -0.12814820099420743, fluctuations: 0.0\n",
      "step: 70100 loss: 49.545857 time elapsed: 87.9736 learning rate: 0.000391, scenario: 0, slope: -0.11083829452749366, fluctuations: 0.0\n",
      "step: 70110 loss: 49.008473 time elapsed: 87.9877 learning rate: 0.000391, scenario: 0, slope: -0.09398633982573422, fluctuations: 0.0\n",
      "step: 70120 loss: 48.513576 time elapsed: 88.0014 learning rate: 0.000391, scenario: 0, slope: -0.08194175713198489, fluctuations: 0.0\n",
      "step: 70130 loss: 48.054779 time elapsed: 88.0145 learning rate: 0.000391, scenario: 0, slope: -0.07232838822032646, fluctuations: 0.0\n",
      "step: 70140 loss: 47.627060 time elapsed: 88.0290 learning rate: 0.000391, scenario: 0, slope: -0.06460335139624805, fluctuations: 0.0\n",
      "step: 70150 loss: 47.226408 time elapsed: 88.0413 learning rate: 0.000391, scenario: 0, slope: -0.05833338617318575, fluctuations: 0.0\n",
      "step: 70160 loss: 46.849561 time elapsed: 88.0531 learning rate: 0.000391, scenario: 0, slope: -0.05318108385786674, fluctuations: 0.0\n",
      "step: 70170 loss: 46.407783 time elapsed: 88.0655 learning rate: 0.000839, scenario: 1, slope: -0.049017373527559804, fluctuations: 0.0\n",
      "step: 70180 loss: 45.412288 time elapsed: 88.0775 learning rate: 0.002175, scenario: 1, slope: -0.0474880080154668, fluctuations: 0.0\n",
      "step: 70190 loss: 43.367166 time elapsed: 88.0891 learning rate: 0.004239, scenario: 0, slope: -0.05268864725806301, fluctuations: 0.0\n",
      "step: 70200 loss: 41.367423 time elapsed: 88.1008 learning rate: 0.004239, scenario: 0, slope: -0.06483170243449746, fluctuations: 0.0\n",
      "step: 70210 loss: 39.893908 time elapsed: 88.1131 learning rate: 0.004239, scenario: 0, slope: -0.08383204723996979, fluctuations: 0.0\n",
      "step: 70220 loss: 38.754903 time elapsed: 88.1248 learning rate: 0.004239, scenario: 0, slope: -0.1010349864375141, fluctuations: 0.0\n",
      "step: 70230 loss: 37.874112 time elapsed: 88.1364 learning rate: 0.004239, scenario: 0, slope: -0.11530570384266219, fluctuations: 0.0\n",
      "step: 70240 loss: 37.166132 time elapsed: 88.1484 learning rate: 0.004239, scenario: 0, slope: -0.1246770687796999, fluctuations: 0.0\n",
      "step: 70250 loss: 36.571433 time elapsed: 88.1618 learning rate: 0.004239, scenario: 0, slope: -0.12781807587014604, fluctuations: 0.0\n",
      "step: 70260 loss: 36.057153 time elapsed: 88.1754 learning rate: 0.004239, scenario: 0, slope: -0.1237814157571442, fluctuations: 0.0\n",
      "step: 70270 loss: 35.599473 time elapsed: 88.1887 learning rate: 0.004239, scenario: 0, slope: -0.11200628485883501, fluctuations: 0.0\n",
      "step: 70280 loss: 35.182319 time elapsed: 88.2019 learning rate: 0.004239, scenario: 0, slope: -0.09416283537163793, fluctuations: 0.0\n",
      "step: 70290 loss: 34.795318 time elapsed: 88.2151 learning rate: 0.004239, scenario: 0, slope: -0.07586847372081948, fluctuations: 0.0\n",
      "step: 70300 loss: 34.431400 time elapsed: 88.2280 learning rate: 0.004239, scenario: 0, slope: -0.06378672216564095, fluctuations: 0.0\n",
      "step: 70310 loss: 34.085666 time elapsed: 88.2425 learning rate: 0.004239, scenario: 0, slope: -0.0534032332865028, fluctuations: 0.0\n",
      "step: 70320 loss: 33.754720 time elapsed: 88.2548 learning rate: 0.004239, scenario: 0, slope: -0.046911147737716694, fluctuations: 0.0\n",
      "step: 70330 loss: 33.436166 time elapsed: 88.2667 learning rate: 0.004239, scenario: 0, slope: -0.04231799222830001, fluctuations: 0.0\n",
      "step: 70340 loss: 33.128272 time elapsed: 88.2786 learning rate: 0.004239, scenario: 0, slope: -0.03896322020241057, fluctuations: 0.0\n",
      "step: 70350 loss: 32.829756 time elapsed: 88.2907 learning rate: 0.004663, scenario: 1, slope: -0.03644277777550966, fluctuations: 0.0\n",
      "step: 70360 loss: 32.370301 time elapsed: 88.3025 learning rate: 0.012095, scenario: 1, slope: -0.034825979454190435, fluctuations: 0.0\n",
      "step: 70370 loss: 280.366547 time elapsed: 88.3142 learning rate: 0.027222, scenario: -1, slope: 0.11342491471718236, fluctuations: 0.0\n",
      "step: 70380 loss: 115265.452318 time elapsed: 88.3260 learning rate: 0.009492, scenario: -1, slope: 472.97549095668563, fluctuations: 0.03\n",
      "step: 70390 loss: 36329.472279 time elapsed: 88.3376 learning rate: 0.003310, scenario: -1, slope: 678.5137009071723, fluctuations: 0.04\n",
      "step: 70400 loss: 26751.092069 time elapsed: 88.3492 learning rate: 0.001282, scenario: -1, slope: 666.6504814014676, fluctuations: 0.04\n",
      "step: 70410 loss: 22124.720630 time elapsed: 88.3634 learning rate: 0.000447, scenario: -1, slope: 573.9138830217605, fluctuations: 0.04\n",
      "step: 70420 loss: 21052.848311 time elapsed: 88.3773 learning rate: 0.000156, scenario: -1, slope: 454.78007565698084, fluctuations: 0.04\n",
      "step: 70430 loss: 20718.419270 time elapsed: 88.3912 learning rate: 0.000054, scenario: -1, slope: 310.75542709000683, fluctuations: 0.04\n",
      "step: 70440 loss: 20618.685008 time elapsed: 88.4043 learning rate: 0.000019, scenario: -1, slope: 140.81013575511432, fluctuations: 0.04\n",
      "step: 70450 loss: 20586.048224 time elapsed: 88.4179 learning rate: 0.000009, scenario: 0, slope: -60.00717207859175, fluctuations: 0.04\n",
      "step: 70460 loss: 20565.441824 time elapsed: 88.4317 learning rate: 0.000009, scenario: 0, slope: -299.5546978635634, fluctuations: 0.04\n",
      "step: 70470 loss: 20545.356873 time elapsed: 88.4461 learning rate: 0.000009, scenario: 0, slope: -589.0877802138649, fluctuations: 0.04\n",
      "step: 70480 loss: 20525.695200 time elapsed: 88.4585 learning rate: 0.000009, scenario: 0, slope: -242.4129148571804, fluctuations: 0.01\n",
      "step: 70490 loss: 20506.398915 time elapsed: 88.4712 learning rate: 0.000009, scenario: 0, slope: -68.5102240982301, fluctuations: 0.0\n",
      "step: 70500 loss: 20487.427573 time elapsed: 88.4833 learning rate: 0.000010, scenario: 1, slope: -24.179754987881797, fluctuations: 0.0\n",
      "step: 70510 loss: 20457.535903 time elapsed: 88.4961 learning rate: 0.000026, scenario: 1, slope: -7.428842548989558, fluctuations: 0.0\n",
      "step: 70520 loss: 20382.230764 time elapsed: 88.5079 learning rate: 0.000067, scenario: 1, slope: -3.48012291303226, fluctuations: 0.0\n",
      "step: 70530 loss: 20194.508310 time elapsed: 88.5198 learning rate: 0.000174, scenario: 1, slope: -3.165636407851425, fluctuations: 0.0\n",
      "step: 70540 loss: 19738.192724 time elapsed: 88.5318 learning rate: 0.000451, scenario: 1, slope: -5.151026878658165, fluctuations: 0.0\n",
      "step: 70550 loss: 18614.996368 time elapsed: 88.5437 learning rate: 0.001171, scenario: 1, slope: -10.915116737595, fluctuations: 0.0\n",
      "step: 70560 loss: 14742.129192 time elapsed: 88.5558 learning rate: 0.002074, scenario: 0, slope: -28.323584770006793, fluctuations: 0.0\n",
      "step: 70570 loss: 11913.400463 time elapsed: 88.5685 learning rate: 0.002074, scenario: 0, slope: -62.664314971225906, fluctuations: 0.0\n",
      "step: 70580 loss: 10081.304691 time elapsed: 88.5817 learning rate: 0.002074, scenario: 0, slope: -98.5018719161918, fluctuations: 0.0\n",
      "step: 70590 loss: 8769.852545 time elapsed: 88.5952 learning rate: 0.002074, scenario: 0, slope: -131.13630485048634, fluctuations: 0.0\n",
      "step: 70600 loss: 7865.905757 time elapsed: 88.6085 learning rate: 0.002074, scenario: 0, slope: -154.25326963495013, fluctuations: 0.0\n",
      "step: 70610 loss: 7125.077351 time elapsed: 88.6219 learning rate: 0.002074, scenario: 0, slope: -171.29378753922666, fluctuations: 0.0\n",
      "step: 70620 loss: 6475.043321 time elapsed: 88.6348 learning rate: 0.002074, scenario: 0, slope: -174.65257304654494, fluctuations: 0.0\n",
      "step: 70630 loss: 5868.595252 time elapsed: 88.6483 learning rate: 0.002074, scenario: 0, slope: -165.76543804584844, fluctuations: 0.0\n",
      "step: 70640 loss: 4821.135543 time elapsed: 88.6624 learning rate: 0.002074, scenario: 0, slope: -146.9265296774554, fluctuations: 0.0\n",
      "step: 70650 loss: 4242.421478 time elapsed: 88.6741 learning rate: 0.002074, scenario: 0, slope: -119.38166317156168, fluctuations: 0.0\n",
      "step: 70660 loss: 3967.829695 time elapsed: 88.6863 learning rate: 0.002074, scenario: 0, slope: -91.71249776272721, fluctuations: 0.0\n",
      "step: 70670 loss: 3735.613136 time elapsed: 88.6980 learning rate: 0.002074, scenario: 0, slope: -76.55229251363012, fluctuations: 0.0\n",
      "step: 70680 loss: 3564.565094 time elapsed: 88.7098 learning rate: 0.002074, scenario: 0, slope: -64.66334779191442, fluctuations: 0.0\n",
      "step: 70690 loss: 3402.873276 time elapsed: 88.7217 learning rate: 0.002074, scenario: 0, slope: -55.207482039520954, fluctuations: 0.0\n",
      "step: 70700 loss: 3237.687557 time elapsed: 88.7335 learning rate: 0.002074, scenario: 0, slope: -47.67022354318553, fluctuations: 0.0\n",
      "step: 70710 loss: 3071.147349 time elapsed: 88.7458 learning rate: 0.002074, scenario: 0, slope: -38.75920500651509, fluctuations: 0.0\n",
      "step: 70720 loss: 2920.417727 time elapsed: 88.7573 learning rate: 0.002074, scenario: 0, slope: -30.920933934632455, fluctuations: 0.0\n",
      "step: 70730 loss: 2789.033750 time elapsed: 88.7693 learning rate: 0.002074, scenario: 0, slope: -23.356100833977987, fluctuations: 0.0\n",
      "step: 70740 loss: 2638.362691 time elapsed: 88.7826 learning rate: 0.002074, scenario: 0, slope: -18.538879453017262, fluctuations: 0.0\n",
      "step: 70750 loss: 2370.422994 time elapsed: 88.7967 learning rate: 0.002074, scenario: 0, slope: -16.723991531942847, fluctuations: 0.0\n",
      "step: 70760 loss: 2028.759058 time elapsed: 88.8104 learning rate: 0.002074, scenario: 0, slope: -17.121223172862223, fluctuations: 0.01\n",
      "step: 70770 loss: 1869.247756 time elapsed: 88.8243 learning rate: 0.002074, scenario: 0, slope: -18.1596701501615, fluctuations: 0.01\n",
      "step: 70780 loss: 1755.943129 time elapsed: 88.8377 learning rate: 0.002074, scenario: 0, slope: -18.87440561180453, fluctuations: 0.01\n",
      "step: 70790 loss: 1681.411287 time elapsed: 88.8511 learning rate: 0.002074, scenario: 0, slope: -18.952782532866138, fluctuations: 0.01\n",
      "step: 70800 loss: 1634.138326 time elapsed: 88.8654 learning rate: 0.002074, scenario: 0, slope: -18.398249049681766, fluctuations: 0.01\n",
      "step: 70810 loss: 1599.478797 time elapsed: 88.8800 learning rate: 0.002074, scenario: 0, slope: -16.980985502313477, fluctuations: 0.01\n",
      "step: 70820 loss: 1572.394158 time elapsed: 88.8922 learning rate: 0.002074, scenario: 0, slope: -15.042611296294016, fluctuations: 0.01\n",
      "step: 70830 loss: 1548.999787 time elapsed: 88.9048 learning rate: 0.002074, scenario: 0, slope: -12.452801859751524, fluctuations: 0.01\n",
      "step: 70840 loss: 1527.783109 time elapsed: 88.9174 learning rate: 0.002074, scenario: 0, slope: -9.307527492974836, fluctuations: 0.01\n",
      "step: 70850 loss: 1508.107430 time elapsed: 88.9297 learning rate: 0.002074, scenario: 0, slope: -6.0605230969565556, fluctuations: 0.01\n",
      "step: 70860 loss: 1489.349207 time elapsed: 88.9418 learning rate: 0.002074, scenario: 0, slope: -4.335003503201996, fluctuations: 0.0\n",
      "step: 70870 loss: 1471.053551 time elapsed: 88.9542 learning rate: 0.002074, scenario: 0, slope: -3.250337426360792, fluctuations: 0.0\n",
      "step: 70880 loss: 1452.890796 time elapsed: 88.9666 learning rate: 0.002074, scenario: 0, slope: -2.606273720427981, fluctuations: 0.0\n",
      "step: 70890 loss: 1435.229474 time elapsed: 88.9793 learning rate: 0.002074, scenario: 0, slope: -2.2461082063461966, fluctuations: 0.0\n",
      "step: 70900 loss: 1418.199034 time elapsed: 88.9930 learning rate: 0.002074, scenario: 0, slope: -2.0580259481190546, fluctuations: 0.0\n",
      "step: 70910 loss: 1401.434076 time elapsed: 89.0074 learning rate: 0.002074, scenario: 0, slope: -1.9201793468334174, fluctuations: 0.0\n",
      "step: 70920 loss: 1384.851592 time elapsed: 89.0214 learning rate: 0.002074, scenario: 0, slope: -1.8412773079594085, fluctuations: 0.0\n",
      "step: 70930 loss: 1368.387737 time elapsed: 89.0344 learning rate: 0.002074, scenario: 0, slope: -1.7859599994969202, fluctuations: 0.0\n",
      "step: 70940 loss: 1351.988634 time elapsed: 89.0484 learning rate: 0.002074, scenario: 0, slope: -1.7452311960569833, fluctuations: 0.0\n",
      "step: 70950 loss: 1335.593002 time elapsed: 89.0621 learning rate: 0.002074, scenario: 0, slope: -1.7139237480830312, fluctuations: 0.0\n",
      "step: 70960 loss: 1319.128004 time elapsed: 89.0761 learning rate: 0.002074, scenario: 0, slope: -1.6889818770690321, fluctuations: 0.0\n",
      "step: 70970 loss: 1302.501236 time elapsed: 89.0892 learning rate: 0.002074, scenario: 0, slope: -1.6699390653677948, fluctuations: 0.0\n",
      "step: 70980 loss: 1285.701523 time elapsed: 89.1017 learning rate: 0.002074, scenario: 0, slope: -1.6583035810858542, fluctuations: 0.0\n",
      "step: 70990 loss: 1268.999141 time elapsed: 89.1135 learning rate: 0.002074, scenario: 0, slope: -1.6544424106193738, fluctuations: 0.0\n",
      "step: 71000 loss: 1252.274556 time elapsed: 89.1253 learning rate: 0.002074, scenario: 0, slope: -1.6544576465462442, fluctuations: 0.0\n",
      "step: 71010 loss: 1235.375994 time elapsed: 89.1384 learning rate: 0.002074, scenario: 0, slope: -1.6579949350351115, fluctuations: 0.0\n",
      "step: 71020 loss: 1218.197043 time elapsed: 89.1506 learning rate: 0.002074, scenario: 0, slope: -1.6640987021783562, fluctuations: 0.0\n",
      "step: 71030 loss: 1199.620298 time elapsed: 89.1627 learning rate: 0.002074, scenario: 0, slope: -1.6770012114043409, fluctuations: 0.0\n",
      "step: 71040 loss: 1180.682024 time elapsed: 89.1746 learning rate: 0.002074, scenario: 0, slope: -1.6978288515446527, fluctuations: 0.0\n",
      "step: 71050 loss: 1155.507621 time elapsed: 89.1867 learning rate: 0.002074, scenario: 0, slope: -1.7373637269688034, fluctuations: 0.0\n",
      "step: 71060 loss: 1128.949243 time elapsed: 89.2000 learning rate: 0.002074, scenario: 0, slope: -1.8136000038187505, fluctuations: 0.03\n",
      "step: 71070 loss: 1105.506661 time elapsed: 89.2139 learning rate: 0.002074, scenario: 0, slope: -1.9189654842745163, fluctuations: 0.05\n",
      "step: 71080 loss: 1085.850614 time elapsed: 89.2277 learning rate: 0.002074, scenario: 0, slope: -2.0265439622359165, fluctuations: 0.05\n",
      "step: 71090 loss: 1066.945927 time elapsed: 89.2414 learning rate: 0.002074, scenario: 0, slope: -2.103510634794398, fluctuations: 0.05\n",
      "step: 71100 loss: 1049.123301 time elapsed: 89.2547 learning rate: 0.002074, scenario: 0, slope: -2.142304884295277, fluctuations: 0.05\n",
      "step: 71110 loss: 1032.218886 time elapsed: 89.2696 learning rate: 0.002074, scenario: 0, slope: -2.1476724951721873, fluctuations: 0.05\n",
      "step: 71120 loss: 1016.051707 time elapsed: 89.2832 learning rate: 0.002074, scenario: 0, slope: -2.1092462462299713, fluctuations: 0.05\n",
      "step: 71130 loss: 1000.597459 time elapsed: 89.2961 learning rate: 0.002074, scenario: 0, slope: -2.030322906914666, fluctuations: 0.05\n",
      "step: 71140 loss: 985.779620 time elapsed: 89.3086 learning rate: 0.002074, scenario: 0, slope: -1.909667401428297, fluctuations: 0.05\n",
      "step: 71150 loss: 971.511987 time elapsed: 89.3205 learning rate: 0.002074, scenario: 0, slope: -1.75775707648356, fluctuations: 0.05\n",
      "step: 71160 loss: 957.712236 time elapsed: 89.3324 learning rate: 0.002074, scenario: 0, slope: -1.6679323999949254, fluctuations: 0.02\n",
      "step: 71170 loss: 944.307749 time elapsed: 89.3445 learning rate: 0.002074, scenario: 0, slope: -1.5960032743447239, fluctuations: 0.0\n",
      "step: 71180 loss: 931.241043 time elapsed: 89.3564 learning rate: 0.002074, scenario: 0, slope: -1.5285152652132232, fluctuations: 0.0\n",
      "step: 71190 loss: 918.502610 time elapsed: 89.3683 learning rate: 0.002074, scenario: 0, slope: -1.4697196506677357, fluctuations: 0.0\n",
      "step: 71200 loss: 906.142382 time elapsed: 89.3801 learning rate: 0.002074, scenario: 0, slope: -1.422386754177067, fluctuations: 0.0\n",
      "step: 71210 loss: 894.177116 time elapsed: 89.3926 learning rate: 0.002074, scenario: 0, slope: -1.3705315976410033, fluctuations: 0.0\n",
      "step: 71220 loss: 882.591515 time elapsed: 89.4051 learning rate: 0.002074, scenario: 0, slope: -1.3278655189772424, fluctuations: 0.0\n",
      "step: 71230 loss: 871.369490 time elapsed: 89.4184 learning rate: 0.002074, scenario: 0, slope: -1.2881878415602979, fluctuations: 0.0\n",
      "step: 71240 loss: 860.494987 time elapsed: 89.4322 learning rate: 0.002074, scenario: 0, slope: -1.250402071647255, fluctuations: 0.0\n",
      "step: 71250 loss: 849.954016 time elapsed: 89.4458 learning rate: 0.002074, scenario: 0, slope: -1.2136686490552653, fluctuations: 0.0\n",
      "step: 71260 loss: 840.694005 time elapsed: 89.4595 learning rate: 0.002074, scenario: 0, slope: -1.1759159523043952, fluctuations: 0.01\n",
      "step: 71270 loss: 829.972928 time elapsed: 89.4732 learning rate: 0.002074, scenario: 0, slope: -1.1372781870946704, fluctuations: 0.02\n",
      "step: 71280 loss: 820.577434 time elapsed: 89.4869 learning rate: 0.002074, scenario: 0, slope: -1.1000223795889055, fluctuations: 0.02\n",
      "step: 71290 loss: 811.149746 time elapsed: 89.5014 learning rate: 0.002074, scenario: 0, slope: -1.0658039458212218, fluctuations: 0.02\n",
      "step: 71300 loss: 801.879760 time elapsed: 89.5139 learning rate: 0.002074, scenario: 0, slope: -1.0373995207767375, fluctuations: 0.02\n",
      "step: 71310 loss: 791.884122 time elapsed: 89.5266 learning rate: 0.002074, scenario: 0, slope: -1.007701864486262, fluctuations: 0.02\n",
      "step: 71320 loss: 779.888233 time elapsed: 89.5384 learning rate: 0.002074, scenario: 0, slope: -0.9979850808215827, fluctuations: 0.02\n",
      "step: 71330 loss: 770.898962 time elapsed: 89.5503 learning rate: 0.002074, scenario: 0, slope: -0.9959370138827596, fluctuations: 0.02\n",
      "step: 71340 loss: 761.041239 time elapsed: 89.5621 learning rate: 0.002074, scenario: 0, slope: -0.9914141823272021, fluctuations: 0.02\n",
      "step: 71350 loss: 750.901812 time elapsed: 89.5738 learning rate: 0.002074, scenario: 0, slope: -0.9971457516666569, fluctuations: 0.02\n",
      "step: 71360 loss: 741.544951 time elapsed: 89.5856 learning rate: 0.002074, scenario: 0, slope: -0.9984113887322975, fluctuations: 0.01\n",
      "step: 71370 loss: 732.687431 time elapsed: 89.5977 learning rate: 0.002074, scenario: 0, slope: -0.9942246117425164, fluctuations: 0.0\n",
      "step: 71380 loss: 724.164971 time elapsed: 89.6100 learning rate: 0.002074, scenario: 0, slope: -0.9849740134733022, fluctuations: 0.0\n",
      "step: 71390 loss: 715.745409 time elapsed: 89.6238 learning rate: 0.002074, scenario: 0, slope: -0.9684536801339466, fluctuations: 0.0\n",
      "step: 71400 loss: 707.382770 time elapsed: 89.6373 learning rate: 0.002074, scenario: 0, slope: -0.9464433324877779, fluctuations: 0.0\n",
      "step: 71410 loss: 699.022856 time elapsed: 89.6522 learning rate: 0.002074, scenario: 0, slope: -0.9122853547176041, fluctuations: 0.0\n",
      "step: 71420 loss: 695.820799 time elapsed: 89.6662 learning rate: 0.002074, scenario: 0, slope: -0.883762885649358, fluctuations: 0.01\n",
      "step: 71430 loss: 683.639572 time elapsed: 89.6800 learning rate: 0.002074, scenario: 0, slope: -0.8631837284662333, fluctuations: 0.03\n",
      "step: 71440 loss: 673.893733 time elapsed: 89.6939 learning rate: 0.002074, scenario: 0, slope: -0.84340190468063, fluctuations: 0.03\n",
      "step: 71450 loss: 665.813034 time elapsed: 89.7072 learning rate: 0.002074, scenario: 0, slope: -0.8369369723520551, fluctuations: 0.03\n",
      "step: 71460 loss: 658.173268 time elapsed: 89.7196 learning rate: 0.002074, scenario: 0, slope: -0.8320202034767604, fluctuations: 0.03\n",
      "step: 71470 loss: 650.938386 time elapsed: 89.7316 learning rate: 0.002074, scenario: 0, slope: -0.8251636610047964, fluctuations: 0.03\n",
      "step: 71480 loss: 643.970316 time elapsed: 89.7435 learning rate: 0.002074, scenario: 0, slope: -0.8147893009380275, fluctuations: 0.03\n",
      "step: 71490 loss: 637.294561 time elapsed: 89.7556 learning rate: 0.002074, scenario: 0, slope: -0.7999256835176783, fluctuations: 0.03\n",
      "step: 71500 loss: 630.833362 time elapsed: 89.7670 learning rate: 0.002074, scenario: 0, slope: -0.7825373736483457, fluctuations: 0.03\n",
      "step: 71510 loss: 624.557317 time elapsed: 89.7790 learning rate: 0.002074, scenario: 0, slope: -0.756741125471592, fluctuations: 0.03\n",
      "step: 71520 loss: 618.440633 time elapsed: 89.7909 learning rate: 0.002074, scenario: 0, slope: -0.7231199982273028, fluctuations: 0.02\n",
      "step: 71530 loss: 612.729366 time elapsed: 89.8029 learning rate: 0.002074, scenario: 0, slope: -0.6913603286007357, fluctuations: 0.0\n",
      "step: 71540 loss: 923.294970 time elapsed: 89.8159 learning rate: 0.004041, scenario: 1, slope: -0.3318078249795111, fluctuations: 0.0\n",
      "step: 71550 loss: 715.286130 time elapsed: 89.8293 learning rate: 0.002728, scenario: -1, slope: 0.8026437325894442, fluctuations: 0.03\n",
      "step: 71560 loss: 627.562120 time elapsed: 89.8433 learning rate: 0.000951, scenario: -1, slope: 0.804197908237576, fluctuations: 0.06\n",
      "step: 71570 loss: 596.435039 time elapsed: 89.8565 learning rate: 0.000332, scenario: -1, slope: 0.5198569169687702, fluctuations: 0.08\n",
      "step: 71580 loss: 592.819111 time elapsed: 89.8702 learning rate: 0.000116, scenario: -1, slope: 0.14854349759236285, fluctuations: 0.09\n",
      "step: 71590 loss: 592.148662 time elapsed: 89.8835 learning rate: 0.000158, scenario: 1, slope: -0.19539624107846565, fluctuations: 0.1\n",
      "step: 71600 loss: 590.809470 time elapsed: 89.8974 learning rate: 0.000372, scenario: 1, slope: -0.5016832914552637, fluctuations: 0.1\n",
      "step: 71610 loss: 588.778548 time elapsed: 89.9119 learning rate: 0.000450, scenario: 0, slope: -0.8626321097501682, fluctuations: 0.1\n",
      "step: 71620 loss: 587.165865 time elapsed: 89.9245 learning rate: 0.000450, scenario: 0, slope: -1.2145994679487204, fluctuations: 0.1\n",
      "step: 71630 loss: 585.703035 time elapsed: 89.9366 learning rate: 0.000450, scenario: 0, slope: -1.6274466755015975, fluctuations: 0.1\n",
      "step: 71640 loss: 584.381336 time elapsed: 89.9498 learning rate: 0.000450, scenario: 0, slope: -1.895010374214334, fluctuations: 0.09\n",
      "step: 71650 loss: 583.036885 time elapsed: 89.9624 learning rate: 0.000724, scenario: 1, slope: -0.5515519182439723, fluctuations: 0.06\n",
      "step: 71660 loss: 580.223805 time elapsed: 89.9749 learning rate: 0.001879, scenario: 1, slope: -0.2310126233822195, fluctuations: 0.03\n",
      "step: 71670 loss: 573.354347 time elapsed: 89.9881 learning rate: 0.004874, scenario: 1, slope: -0.17920300593388366, fluctuations: 0.02\n",
      "step: 71680 loss: 613.515340 time elapsed: 90.0006 learning rate: 0.012642, scenario: 1, slope: -0.20755292868504296, fluctuations: 0.01\n",
      "step: 71690 loss: 7966.672876 time elapsed: 90.0130 learning rate: 0.004675, scenario: -1, slope: 64.20505841418044, fluctuations: 0.02\n",
      "step: 71700 loss: 4633.528622 time elapsed: 90.0269 learning rate: 0.001811, scenario: -1, slope: 76.93777462732932, fluctuations: 0.04\n",
      "step: 71710 loss: 3189.536241 time elapsed: 90.0421 learning rate: 0.000632, scenario: -1, slope: 74.48517468215022, fluctuations: 0.06\n",
      "step: 71720 loss: 2621.885812 time elapsed: 90.0562 learning rate: 0.000220, scenario: -1, slope: 60.26977316062013, fluctuations: 0.06\n",
      "step: 71730 loss: 2537.591679 time elapsed: 90.0702 learning rate: 0.000077, scenario: -1, slope: 43.892674890953096, fluctuations: 0.06\n",
      "step: 71740 loss: 2495.856469 time elapsed: 90.0839 learning rate: 0.000027, scenario: -1, slope: 25.8219941814907, fluctuations: 0.06\n",
      "step: 71750 loss: 2483.053277 time elapsed: 90.0980 learning rate: 0.000009, scenario: -1, slope: 5.596341657111014, fluctuations: 0.06\n",
      "step: 71760 loss: 2478.137311 time elapsed: 90.1123 learning rate: 0.000008, scenario: 0, slope: -17.641123323709817, fluctuations: 0.06\n",
      "step: 71770 loss: 2473.748732 time elapsed: 90.1252 learning rate: 0.000008, scenario: 0, slope: -45.18723195457154, fluctuations: 0.06\n",
      "step: 71780 loss: 2469.496032 time elapsed: 90.1376 learning rate: 0.000008, scenario: 0, slope: -78.93947081304503, fluctuations: 0.06\n",
      "step: 71790 loss: 2465.348885 time elapsed: 90.1497 learning rate: 0.000008, scenario: 0, slope: -26.545704959758602, fluctuations: 0.03\n",
      "step: 71800 loss: 2461.287330 time elapsed: 90.1616 learning rate: 0.000008, scenario: 0, slope: -8.266072005566862, fluctuations: 0.02\n",
      "step: 71810 loss: 2457.256980 time elapsed: 90.1751 learning rate: 0.000010, scenario: 1, slope: -2.4381789806277685, fluctuations: 0.0\n",
      "step: 71820 loss: 2449.713209 time elapsed: 90.1873 learning rate: 0.000026, scenario: 1, slope: -0.9859580960505298, fluctuations: 0.0\n",
      "step: 71830 loss: 2430.819077 time elapsed: 90.1999 learning rate: 0.000068, scenario: 1, slope: -0.619489725797424, fluctuations: 0.0\n",
      "step: 71840 loss: 2384.590031 time elapsed: 90.2123 learning rate: 0.000176, scenario: 1, slope: -0.6936902572769367, fluctuations: 0.0\n",
      "step: 71850 loss: 2273.323394 time elapsed: 90.2237 learning rate: 0.000455, scenario: 1, slope: -1.226093467681036, fluctuations: 0.0\n",
      "step: 71860 loss: 2030.091276 time elapsed: 90.2370 learning rate: 0.001074, scenario: 0, slope: -2.594537779046203, fluctuations: 0.0\n",
      "step: 71870 loss: 1759.034550 time elapsed: 90.2511 learning rate: 0.001074, scenario: 0, slope: -5.135823331950743, fluctuations: 0.0\n",
      "step: 71880 loss: 1596.058329 time elapsed: 90.2652 learning rate: 0.001074, scenario: 0, slope: -8.078899787203317, fluctuations: 0.0\n",
      "step: 71890 loss: 1483.800313 time elapsed: 90.2790 learning rate: 0.001074, scenario: 0, slope: -10.788157572796862, fluctuations: 0.0\n",
      "step: 71900 loss: 1397.803056 time elapsed: 90.2921 learning rate: 0.001074, scenario: 0, slope: -12.738891232399054, fluctuations: 0.0\n",
      "step: 71910 loss: 1324.835250 time elapsed: 90.3062 learning rate: 0.001074, scenario: 0, slope: -14.264370239921817, fluctuations: 0.0\n",
      "step: 71920 loss: 1257.184721 time elapsed: 90.3202 learning rate: 0.001074, scenario: 0, slope: -14.693967503185817, fluctuations: 0.0\n",
      "step: 71930 loss: 1186.710467 time elapsed: 90.3340 learning rate: 0.001074, scenario: 0, slope: -14.195284374198188, fluctuations: 0.0\n",
      "step: 71940 loss: 1125.018227 time elapsed: 90.3460 learning rate: 0.001074, scenario: 0, slope: -12.783092025018911, fluctuations: 0.0\n",
      "step: 71950 loss: 1071.479396 time elapsed: 90.3582 learning rate: 0.001074, scenario: 0, slope: -10.672780606240849, fluctuations: 0.0\n",
      "step: 71960 loss: 1024.627090 time elapsed: 90.3704 learning rate: 0.001074, scenario: 0, slope: -8.509691959592647, fluctuations: 0.0\n",
      "step: 71970 loss: 985.563589 time elapsed: 90.3825 learning rate: 0.001074, scenario: 0, slope: -7.119452945563675, fluctuations: 0.0\n",
      "step: 71980 loss: 954.287738 time elapsed: 90.3945 learning rate: 0.001074, scenario: 0, slope: -6.244728753697474, fluctuations: 0.0\n",
      "step: 71990 loss: 929.805415 time elapsed: 90.4067 learning rate: 0.001074, scenario: 0, slope: -5.5587948209338185, fluctuations: 0.0\n",
      "step: 72000 loss: 910.681236 time elapsed: 90.4191 learning rate: 0.001074, scenario: 0, slope: -4.981437247704294, fluctuations: 0.0\n",
      "step: 72010 loss: 895.358803 time elapsed: 90.4317 learning rate: 0.001074, scenario: 0, slope: -4.2644269335156055, fluctuations: 0.0\n",
      "step: 72020 loss: 882.460341 time elapsed: 90.4445 learning rate: 0.001074, scenario: 0, slope: -3.5965007290363435, fluctuations: 0.0\n",
      "step: 72030 loss: 871.049414 time elapsed: 90.4586 learning rate: 0.001074, scenario: 0, slope: -2.985531054686571, fluctuations: 0.0\n",
      "step: 72040 loss: 860.605711 time elapsed: 90.4736 learning rate: 0.001074, scenario: 0, slope: -2.451645900184002, fluctuations: 0.0\n",
      "step: 72050 loss: 850.843716 time elapsed: 90.4881 learning rate: 0.001074, scenario: 0, slope: -2.002598148934827, fluctuations: 0.0\n",
      "step: 72060 loss: 841.582567 time elapsed: 90.5019 learning rate: 0.001074, scenario: 0, slope: -1.6481730056930786, fluctuations: 0.0\n",
      "step: 72070 loss: 832.693292 time elapsed: 90.5160 learning rate: 0.001074, scenario: 0, slope: -1.3849119040622824, fluctuations: 0.0\n",
      "step: 72080 loss: 824.081493 time elapsed: 90.5299 learning rate: 0.001074, scenario: 0, slope: -1.1997978783008307, fluctuations: 0.0\n",
      "step: 72090 loss: 815.677855 time elapsed: 90.5437 learning rate: 0.001074, scenario: 0, slope: -1.0745420324053832, fluctuations: 0.0\n",
      "step: 72100 loss: 807.428458 time elapsed: 90.5577 learning rate: 0.001074, scenario: 0, slope: -0.9978891504699251, fluctuations: 0.0\n",
      "step: 72110 loss: 799.284508 time elapsed: 90.5708 learning rate: 0.001074, scenario: 0, slope: -0.9341687746949296, fluctuations: 0.0\n",
      "step: 72120 loss: 791.192342 time elapsed: 90.5840 learning rate: 0.001074, scenario: 0, slope: -0.8942328323165135, fluctuations: 0.0\n",
      "step: 72130 loss: 782.833777 time elapsed: 90.5962 learning rate: 0.001572, scenario: 1, slope: -0.8657620649280969, fluctuations: 0.0\n",
      "step: 72140 loss: 765.615603 time elapsed: 90.6083 learning rate: 0.003064, scenario: 0, slope: -0.8678324425000749, fluctuations: 0.0\n",
      "step: 72150 loss: 741.942766 time elapsed: 90.6204 learning rate: 0.003064, scenario: 0, slope: -0.9474348580542382, fluctuations: 0.0\n",
      "step: 72160 loss: 722.314911 time elapsed: 90.6322 learning rate: 0.003064, scenario: 0, slope: -1.0493705038592074, fluctuations: 0.03\n",
      "step: 72170 loss: 702.273351 time elapsed: 90.6439 learning rate: 0.003064, scenario: 0, slope: -1.2105661523938023, fluctuations: 0.06\n",
      "step: 72180 loss: 682.053618 time elapsed: 90.6573 learning rate: 0.003064, scenario: 0, slope: -1.4165334502008005, fluctuations: 0.06\n",
      "step: 72190 loss: 664.409013 time elapsed: 90.6719 learning rate: 0.003064, scenario: 0, slope: -1.5979221498213232, fluctuations: 0.06\n",
      "step: 72200 loss: 649.229582 time elapsed: 90.6855 learning rate: 0.003064, scenario: 0, slope: -1.7270334115131911, fluctuations: 0.06\n",
      "step: 72210 loss: 636.743789 time elapsed: 90.7000 learning rate: 0.003064, scenario: 0, slope: -1.8200797233205848, fluctuations: 0.06\n",
      "step: 72220 loss: 625.869741 time elapsed: 90.7141 learning rate: 0.003064, scenario: 0, slope: -1.826374255416084, fluctuations: 0.06\n",
      "step: 72230 loss: 615.968762 time elapsed: 90.7279 learning rate: 0.003064, scenario: 0, slope: -1.744398510038757, fluctuations: 0.06\n",
      "step: 72240 loss: 606.785537 time elapsed: 90.7415 learning rate: 0.003064, scenario: 0, slope: -1.583216242181457, fluctuations: 0.06\n",
      "step: 72250 loss: 598.131131 time elapsed: 90.7544 learning rate: 0.003064, scenario: 0, slope: -1.4042028416121481, fluctuations: 0.06\n",
      "step: 72260 loss: 589.900157 time elapsed: 90.7665 learning rate: 0.003064, scenario: 0, slope: -1.2628528691751362, fluctuations: 0.02\n",
      "step: 72270 loss: 582.034032 time elapsed: 90.7787 learning rate: 0.003064, scenario: 0, slope: -1.137205176381605, fluctuations: 0.0\n",
      "step: 72280 loss: 574.496036 time elapsed: 90.7912 learning rate: 0.003064, scenario: 0, slope: -1.020059735888025, fluctuations: 0.0\n",
      "step: 72290 loss: 567.254939 time elapsed: 90.8033 learning rate: 0.003064, scenario: 0, slope: -0.929788587203952, fluctuations: 0.0\n",
      "step: 72300 loss: 560.273710 time elapsed: 90.8159 learning rate: 0.003064, scenario: 0, slope: -0.8705888643720983, fluctuations: 0.0\n",
      "step: 72310 loss: 553.522609 time elapsed: 90.8291 learning rate: 0.003064, scenario: 0, slope: -0.8171737924094497, fluctuations: 0.0\n",
      "step: 72320 loss: 546.980022 time elapsed: 90.8408 learning rate: 0.003064, scenario: 0, slope: -0.7784002526658693, fluctuations: 0.0\n",
      "step: 72330 loss: 540.627131 time elapsed: 90.8535 learning rate: 0.003064, scenario: 0, slope: -0.7455693623222664, fluctuations: 0.0\n",
      "step: 72340 loss: 534.447279 time elapsed: 90.8676 learning rate: 0.003064, scenario: 0, slope: -0.7169621363298896, fluctuations: 0.0\n",
      "step: 72350 loss: 528.425399 time elapsed: 90.8827 learning rate: 0.003064, scenario: 0, slope: -0.691507295646953, fluctuations: 0.0\n",
      "step: 72360 loss: 522.546974 time elapsed: 90.8979 learning rate: 0.003064, scenario: 0, slope: -0.668668733719703, fluctuations: 0.0\n",
      "step: 72370 loss: 516.794993 time elapsed: 90.9126 learning rate: 0.003064, scenario: 0, slope: -0.6481199757240865, fluctuations: 0.0\n",
      "step: 72380 loss: 511.140514 time elapsed: 90.9268 learning rate: 0.003064, scenario: 0, slope: -0.6296501239350861, fluctuations: 0.0\n",
      "step: 72390 loss: 505.567886 time elapsed: 90.9413 learning rate: 0.003064, scenario: 0, slope: -0.6131566377125444, fluctuations: 0.0\n",
      "step: 72400 loss: 500.162651 time elapsed: 90.9560 learning rate: 0.003064, scenario: 0, slope: -0.599598499950074, fluctuations: 0.0\n",
      "step: 72410 loss: 494.867486 time elapsed: 90.9719 learning rate: 0.003064, scenario: 0, slope: -0.5842914840422327, fluctuations: 0.0\n",
      "step: 72420 loss: 489.678140 time elapsed: 90.9862 learning rate: 0.003064, scenario: 0, slope: -0.5713513773487461, fluctuations: 0.0\n",
      "step: 72430 loss: 484.588648 time elapsed: 90.9999 learning rate: 0.003064, scenario: 0, slope: -0.5591447478116647, fluctuations: 0.0\n",
      "step: 72440 loss: 479.593839 time elapsed: 91.0135 learning rate: 0.003064, scenario: 0, slope: -0.5475189791757848, fluctuations: 0.0\n",
      "step: 72450 loss: 474.689285 time elapsed: 91.0265 learning rate: 0.003064, scenario: 0, slope: -0.5363504725609922, fluctuations: 0.0\n",
      "step: 72460 loss: 469.870871 time elapsed: 91.0389 learning rate: 0.003064, scenario: 0, slope: -0.525549737827219, fluctuations: 0.0\n",
      "step: 72470 loss: 465.088053 time elapsed: 91.0520 learning rate: 0.004079, scenario: 1, slope: -0.5151028562719875, fluctuations: 0.0\n",
      "step: 72480 loss: 456.288366 time elapsed: 91.0653 learning rate: 0.007948, scenario: 0, slope: -0.514657497507611, fluctuations: 0.0\n",
      "step: 72490 loss: 832.052990 time elapsed: 91.0806 learning rate: 0.005541, scenario: -1, slope: 0.6844529567854848, fluctuations: 0.03\n",
      "step: 72500 loss: 464.642883 time elapsed: 91.0949 learning rate: 0.002147, scenario: -1, slope: 0.8852804746328262, fluctuations: 0.06\n",
      "step: 72510 loss: 446.548983 time elapsed: 91.1094 learning rate: 0.000748, scenario: -1, slope: 0.6344879394833878, fluctuations: 0.08\n",
      "step: 72520 loss: 444.694864 time elapsed: 91.1230 learning rate: 0.000261, scenario: -1, slope: 0.27642550830537255, fluctuations: 0.09\n",
      "step: 72530 loss: 443.645584 time elapsed: 91.1368 learning rate: 0.000130, scenario: 1, slope: -0.05731962286601091, fluctuations: 0.1\n",
      "step: 72540 loss: 443.144970 time elapsed: 91.1509 learning rate: 0.000338, scenario: 1, slope: -0.36841833131932844, fluctuations: 0.11\n",
      "step: 72550 loss: 441.696495 time elapsed: 91.1648 learning rate: 0.000450, scenario: 0, slope: -0.6808791283766301, fluctuations: 0.11\n",
      "step: 72560 loss: 440.442054 time elapsed: 91.1777 learning rate: 0.000450, scenario: 0, slope: -0.9978610233163254, fluctuations: 0.11\n",
      "step: 72570 loss: 439.389873 time elapsed: 91.1898 learning rate: 0.000450, scenario: 0, slope: -1.3550987640404795, fluctuations: 0.11\n",
      "step: 72580 loss: 438.451764 time elapsed: 91.2024 learning rate: 0.000450, scenario: 0, slope: -1.8150754261131234, fluctuations: 0.11\n",
      "step: 72590 loss: 437.581330 time elapsed: 91.2146 learning rate: 0.000495, scenario: 1, slope: -0.5827744381366747, fluctuations: 0.08\n",
      "step: 72600 loss: 436.372906 time elapsed: 91.2269 learning rate: 0.001062, scenario: 1, slope: -0.23917057204144343, fluctuations: 0.05\n",
      "step: 72610 loss: 433.784392 time elapsed: 91.2394 learning rate: 0.002754, scenario: 1, slope: -0.12998202577891188, fluctuations: 0.03\n",
      "step: 72620 loss: 427.923008 time elapsed: 91.2511 learning rate: 0.007143, scenario: 1, slope: -0.1318419344539138, fluctuations: 0.02\n",
      "step: 72630 loss: 414.523073 time elapsed: 91.2634 learning rate: 0.018528, scenario: 1, slope: -0.19062238048191252, fluctuations: 0.01\n",
      "step: 72640 loss: 53008.779797 time elapsed: 91.2760 learning rate: 0.012510, scenario: -1, slope: 127.86972090162452, fluctuations: 0.02\n",
      "step: 72650 loss: 18401.485478 time elapsed: 91.2898 learning rate: 0.004362, scenario: -1, slope: 237.84937235973715, fluctuations: 0.04\n",
      "step: 72660 loss: 8548.359370 time elapsed: 91.3039 learning rate: 0.001521, scenario: -1, slope: 240.2233379576107, fluctuations: 0.04\n",
      "step: 72670 loss: 6887.665108 time elapsed: 91.3180 learning rate: 0.000530, scenario: -1, slope: 204.90256374423313, fluctuations: 0.04\n",
      "step: 72680 loss: 6415.917602 time elapsed: 91.3318 learning rate: 0.000185, scenario: -1, slope: 158.45697756507826, fluctuations: 0.04\n",
      "step: 72690 loss: 6273.576880 time elapsed: 91.3455 learning rate: 0.000064, scenario: -1, slope: 105.26298644510709, fluctuations: 0.04\n",
      "step: 72700 loss: 6220.962190 time elapsed: 91.3597 learning rate: 0.000025, scenario: -1, slope: 51.30065968171592, fluctuations: 0.04\n",
      "step: 72710 loss: 6200.865206 time elapsed: 91.3739 learning rate: 0.000013, scenario: 0, slope: -24.181152249984738, fluctuations: 0.04\n",
      "step: 72720 loss: 6186.505380 time elapsed: 91.3873 learning rate: 0.000013, scenario: 0, slope: -104.48522986548573, fluctuations: 0.04\n",
      "step: 72730 loss: 6172.402297 time elapsed: 91.3995 learning rate: 0.000013, scenario: 0, slope: -199.58370921096133, fluctuations: 0.04\n",
      "step: 72740 loss: 6158.526037 time elapsed: 91.4119 learning rate: 0.000013, scenario: 0, slope: -157.25674417609682, fluctuations: 0.01\n",
      "step: 72750 loss: 6144.847070 time elapsed: 91.4237 learning rate: 0.000013, scenario: 0, slope: -40.02033624744585, fluctuations: 0.0\n",
      "step: 72760 loss: 6131.339165 time elapsed: 91.4361 learning rate: 0.000013, scenario: 0, slope: -10.157275312528855, fluctuations: 0.0\n",
      "step: 72770 loss: 6114.673640 time elapsed: 91.4489 learning rate: 0.000028, scenario: 1, slope: -3.5918459474186433, fluctuations: 0.0\n",
      "step: 72780 loss: 6073.911287 time elapsed: 91.4608 learning rate: 0.000074, scenario: 1, slope: -2.0728800023218805, fluctuations: 0.0\n",
      "step: 72790 loss: 5971.419835 time elapsed: 91.4731 learning rate: 0.000191, scenario: 1, slope: -1.9580289268777324, fluctuations: 0.0\n",
      "step: 72800 loss: 5718.347165 time elapsed: 91.4851 learning rate: 0.000451, scenario: 1, slope: -2.8465573547852503, fluctuations: 0.0\n",
      "step: 72810 loss: 5167.181848 time elapsed: 91.4984 learning rate: 0.001171, scenario: 1, slope: -6.018137137860857, fluctuations: 0.0\n",
      "step: 72820 loss: 4357.143285 time elapsed: 91.5120 learning rate: 0.001171, scenario: 0, slope: -12.31365491730842, fluctuations: 0.0\n",
      "step: 72830 loss: 3815.440734 time elapsed: 91.5258 learning rate: 0.001171, scenario: 0, slope: -20.434708216599788, fluctuations: 0.0\n",
      "step: 72840 loss: 3465.650686 time elapsed: 91.5397 learning rate: 0.001171, scenario: 0, slope: -28.358812734386014, fluctuations: 0.0\n",
      "step: 72850 loss: 3213.799255 time elapsed: 91.5534 learning rate: 0.001171, scenario: 0, slope: -34.85613320928279, fluctuations: 0.0\n",
      "step: 72860 loss: 3000.328538 time elapsed: 91.5671 learning rate: 0.001171, scenario: 0, slope: -39.28082921378694, fluctuations: 0.0\n",
      "step: 72870 loss: 2837.548448 time elapsed: 91.5803 learning rate: 0.001171, scenario: 0, slope: -41.14735586977024, fluctuations: 0.0\n",
      "step: 72880 loss: 2701.416373 time elapsed: 91.5941 learning rate: 0.001171, scenario: 0, slope: -40.122237393036144, fluctuations: 0.0\n",
      "step: 72890 loss: 2581.602544 time elapsed: 91.6068 learning rate: 0.001171, scenario: 0, slope: -36.23752079311501, fluctuations: 0.0\n",
      "step: 72900 loss: 2475.127533 time elapsed: 91.6192 learning rate: 0.001171, scenario: 0, slope: -30.716414560216748, fluctuations: 0.0\n",
      "step: 72910 loss: 2379.048577 time elapsed: 91.6321 learning rate: 0.001171, scenario: 0, slope: -22.96687545479071, fluctuations: 0.0\n",
      "step: 72920 loss: 2292.020815 time elapsed: 91.6438 learning rate: 0.001171, scenario: 0, slope: -17.726105735219505, fluctuations: 0.0\n",
      "step: 72930 loss: 2213.067552 time elapsed: 91.6560 learning rate: 0.001171, scenario: 0, slope: -14.469839990286443, fluctuations: 0.0\n",
      "step: 72940 loss: 2141.304269 time elapsed: 91.6679 learning rate: 0.001171, scenario: 0, slope: -12.300410832254126, fluctuations: 0.0\n",
      "step: 72950 loss: 2075.716100 time elapsed: 91.6802 learning rate: 0.001171, scenario: 0, slope: -10.684760572290624, fluctuations: 0.0\n",
      "step: 72960 loss: 2015.068698 time elapsed: 91.6926 learning rate: 0.001171, scenario: 0, slope: -9.460262161756143, fluctuations: 0.0\n",
      "step: 72970 loss: 1958.709318 time elapsed: 91.7058 learning rate: 0.001171, scenario: 0, slope: -8.522384124798347, fluctuations: 0.0\n",
      "step: 72980 loss: 1905.889233 time elapsed: 91.7194 learning rate: 0.001171, scenario: 0, slope: -7.736087443926619, fluctuations: 0.0\n",
      "step: 72990 loss: 1856.034019 time elapsed: 91.7337 learning rate: 0.001171, scenario: 0, slope: -7.06782859070972, fluctuations: 0.0\n",
      "step: 73000 loss: 1808.976417 time elapsed: 91.7470 learning rate: 0.001171, scenario: 0, slope: -6.547934312343318, fluctuations: 0.0\n",
      "step: 73010 loss: 1765.009198 time elapsed: 91.7609 learning rate: 0.001171, scenario: 0, slope: -6.000232979983409, fluctuations: 0.0\n",
      "step: 73020 loss: 1724.489775 time elapsed: 91.7746 learning rate: 0.001171, scenario: 0, slope: -5.570199879492286, fluctuations: 0.0\n",
      "step: 73030 loss: 1687.337011 time elapsed: 91.7884 learning rate: 0.001171, scenario: 0, slope: -5.1882005654818215, fluctuations: 0.0\n",
      "step: 73040 loss: 1653.070181 time elapsed: 91.8015 learning rate: 0.001171, scenario: 0, slope: -4.839977772414925, fluctuations: 0.0\n",
      "step: 73050 loss: 1621.161621 time elapsed: 91.8135 learning rate: 0.001171, scenario: 0, slope: -4.5144577434839634, fluctuations: 0.0\n",
      "step: 73060 loss: 1591.189928 time elapsed: 91.8251 learning rate: 0.001171, scenario: 0, slope: -4.2071492720518595, fluctuations: 0.0\n",
      "step: 73070 loss: 1562.824716 time elapsed: 91.8367 learning rate: 0.001171, scenario: 0, slope: -3.9163648347404565, fluctuations: 0.0\n",
      "step: 73080 loss: 1535.892764 time elapsed: 91.8481 learning rate: 0.001171, scenario: 0, slope: -3.6432848921248784, fluctuations: 0.0\n",
      "step: 73090 loss: 1509.969420 time elapsed: 91.8598 learning rate: 0.001171, scenario: 0, slope: -3.3927645420791577, fluctuations: 0.0\n",
      "step: 73100 loss: 1485.087142 time elapsed: 91.8714 learning rate: 0.001171, scenario: 0, slope: -3.1909509173553388, fluctuations: 0.0\n",
      "step: 73110 loss: 1461.006167 time elapsed: 91.8841 learning rate: 0.001171, scenario: 0, slope: -2.978696140842897, fluctuations: 0.0\n",
      "step: 73120 loss: 1437.630636 time elapsed: 91.8962 learning rate: 0.001171, scenario: 0, slope: -2.817977992125883, fluctuations: 0.0\n",
      "step: 73130 loss: 1414.863888 time elapsed: 91.9082 learning rate: 0.001171, scenario: 0, slope: -2.683730287126051, fluctuations: 0.0\n",
      "step: 73140 loss: 1392.633763 time elapsed: 91.9222 learning rate: 0.001171, scenario: 0, slope: -2.5709795079645064, fluctuations: 0.0\n",
      "step: 73150 loss: 1370.876198 time elapsed: 91.9364 learning rate: 0.001171, scenario: 0, slope: -2.475438865060241, fluctuations: 0.0\n",
      "step: 73160 loss: 1349.548002 time elapsed: 91.9504 learning rate: 0.001171, scenario: 0, slope: -2.3940291940843785, fluctuations: 0.0\n",
      "step: 73170 loss: 1328.582270 time elapsed: 91.9641 learning rate: 0.001171, scenario: 0, slope: -2.3241960240312856, fluctuations: 0.0\n",
      "step: 73180 loss: 1308.259870 time elapsed: 91.9779 learning rate: 0.001171, scenario: 0, slope: -2.2631064862204666, fluctuations: 0.0\n",
      "step: 73190 loss: 1287.773001 time elapsed: 91.9911 learning rate: 0.001171, scenario: 0, slope: -2.2096344978873925, fluctuations: 0.0\n",
      "step: 73200 loss: 1267.851222 time elapsed: 92.0049 learning rate: 0.001171, scenario: 0, slope: -2.166759885740337, fluctuations: 0.0\n",
      "step: 73210 loss: 1248.257194 time elapsed: 92.0181 learning rate: 0.001171, scenario: 0, slope: -2.119498259480803, fluctuations: 0.0\n",
      "step: 73220 loss: 1228.992903 time elapsed: 92.0303 learning rate: 0.001171, scenario: 0, slope: -2.080242529208842, fluctuations: 0.0\n",
      "step: 73230 loss: 1210.028130 time elapsed: 92.0422 learning rate: 0.001171, scenario: 0, slope: -2.0436359519359755, fluctuations: 0.0\n",
      "step: 73240 loss: 1191.366116 time elapsed: 92.0543 learning rate: 0.001171, scenario: 0, slope: -2.00900537740238, fluctuations: 0.0\n",
      "step: 73250 loss: 1172.996928 time elapsed: 92.0666 learning rate: 0.001171, scenario: 0, slope: -1.9757044202924534, fluctuations: 0.0\n",
      "step: 73260 loss: 1155.029169 time elapsed: 92.0790 learning rate: 0.001171, scenario: 0, slope: -1.9433560529065639, fluctuations: 0.0\n",
      "step: 73270 loss: 1137.133120 time elapsed: 92.0907 learning rate: 0.001171, scenario: 0, slope: -1.9119553364486364, fluctuations: 0.0\n",
      "step: 73280 loss: 1119.614686 time elapsed: 92.1029 learning rate: 0.001171, scenario: 0, slope: -1.8812627192170066, fluctuations: 0.0\n",
      "step: 73290 loss: 1102.381744 time elapsed: 92.1160 learning rate: 0.001171, scenario: 0, slope: -1.8511751686185516, fluctuations: 0.0\n",
      "step: 73300 loss: 1085.405303 time elapsed: 92.1295 learning rate: 0.001171, scenario: 0, slope: -1.824782388100705, fluctuations: 0.0\n",
      "step: 73310 loss: 1068.704190 time elapsed: 92.1441 learning rate: 0.001171, scenario: 0, slope: -1.7932411429666102, fluctuations: 0.0\n",
      "step: 73320 loss: 1052.313204 time elapsed: 92.1586 learning rate: 0.001171, scenario: 0, slope: -1.7650187445038605, fluctuations: 0.0\n",
      "step: 73330 loss: 1036.103504 time elapsed: 92.1726 learning rate: 0.001171, scenario: 0, slope: -1.737294673844006, fluctuations: 0.0\n",
      "step: 73340 loss: 1020.162770 time elapsed: 92.1863 learning rate: 0.001171, scenario: 0, slope: -1.7101337989941203, fluctuations: 0.0\n",
      "step: 73350 loss: 1004.498904 time elapsed: 92.1995 learning rate: 0.001171, scenario: 0, slope: -1.6834020231898859, fluctuations: 0.0\n",
      "step: 73360 loss: 989.101836 time elapsed: 92.2128 learning rate: 0.001171, scenario: 0, slope: -1.6567475975438481, fluctuations: 0.0\n",
      "step: 73370 loss: 974.000710 time elapsed: 92.2248 learning rate: 0.001171, scenario: 0, slope: -1.6301424947421082, fluctuations: 0.0\n",
      "step: 73380 loss: 959.112098 time elapsed: 92.2366 learning rate: 0.001171, scenario: 0, slope: -1.6036867083915958, fluctuations: 0.0\n",
      "step: 73390 loss: 944.546245 time elapsed: 92.2485 learning rate: 0.001171, scenario: 0, slope: -1.5772648063466856, fluctuations: 0.0\n",
      "step: 73400 loss: 930.264583 time elapsed: 92.2602 learning rate: 0.001171, scenario: 0, slope: -1.5534149954092034, fluctuations: 0.0\n",
      "step: 73410 loss: 916.294201 time elapsed: 92.2728 learning rate: 0.001171, scenario: 0, slope: -1.5236347812843152, fluctuations: 0.0\n",
      "step: 73420 loss: 902.563724 time elapsed: 92.2850 learning rate: 0.001171, scenario: 0, slope: -1.4962527045683394, fluctuations: 0.0\n",
      "step: 73430 loss: 889.172591 time elapsed: 92.2969 learning rate: 0.001171, scenario: 0, slope: -1.468376949181692, fluctuations: 0.0\n",
      "step: 73440 loss: 876.088084 time elapsed: 92.3086 learning rate: 0.001171, scenario: 0, slope: -1.44008465696565, fluctuations: 0.0\n",
      "step: 73450 loss: 863.317683 time elapsed: 92.3213 learning rate: 0.001171, scenario: 0, slope: -1.4113255286704198, fluctuations: 0.0\n",
      "step: 73460 loss: 850.970436 time elapsed: 92.3346 learning rate: 0.001171, scenario: 0, slope: -1.3815488534122546, fluctuations: 0.0\n",
      "step: 73470 loss: 838.722790 time elapsed: 92.3482 learning rate: 0.001171, scenario: 0, slope: -1.3516122678151525, fluctuations: 0.0\n",
      "step: 73480 loss: 826.877749 time elapsed: 92.3619 learning rate: 0.001171, scenario: 0, slope: -1.3212754616734832, fluctuations: 0.0\n",
      "step: 73490 loss: 815.365888 time elapsed: 92.3758 learning rate: 0.001171, scenario: 0, slope: -1.2906989088400986, fluctuations: 0.0\n",
      "step: 73500 loss: 804.165988 time elapsed: 92.3888 learning rate: 0.001171, scenario: 0, slope: -1.2630070791744312, fluctuations: 0.0\n",
      "step: 73510 loss: 793.281806 time elapsed: 92.4028 learning rate: 0.001171, scenario: 0, slope: -1.2285818410825284, fluctuations: 0.0\n",
      "step: 73520 loss: 782.722826 time elapsed: 92.4162 learning rate: 0.001171, scenario: 0, slope: -1.1972560779016133, fluctuations: 0.0\n",
      "step: 73530 loss: 772.571566 time elapsed: 92.4283 learning rate: 0.001171, scenario: 0, slope: -1.1655096578679023, fluctuations: 0.0\n",
      "step: 73540 loss: 762.521890 time elapsed: 92.4401 learning rate: 0.001171, scenario: 0, slope: -1.1340421885122824, fluctuations: 0.0\n",
      "step: 73550 loss: 752.884066 time elapsed: 92.4522 learning rate: 0.001171, scenario: 0, slope: -1.1027930050825512, fluctuations: 0.0\n",
      "step: 73560 loss: 743.542151 time elapsed: 92.4643 learning rate: 0.001171, scenario: 0, slope: -1.0712084587423345, fluctuations: 0.0\n",
      "step: 73570 loss: 734.483388 time elapsed: 92.4762 learning rate: 0.001171, scenario: 0, slope: -1.0400881160628042, fluctuations: 0.0\n",
      "step: 73580 loss: 725.721416 time elapsed: 92.4885 learning rate: 0.001171, scenario: 0, slope: -1.0093454368383539, fluctuations: 0.0\n",
      "step: 73590 loss: 717.298037 time elapsed: 92.5004 learning rate: 0.001171, scenario: 0, slope: -0.9788393878325883, fluctuations: 0.0\n",
      "step: 73600 loss: 708.986222 time elapsed: 92.5124 learning rate: 0.001171, scenario: 0, slope: -0.9521334053347449, fluctuations: 0.0\n",
      "step: 73610 loss: 700.964534 time elapsed: 92.5250 learning rate: 0.001171, scenario: 0, slope: -0.9204332353170933, fluctuations: 0.0\n",
      "step: 73620 loss: 693.191091 time elapsed: 92.5388 learning rate: 0.001171, scenario: 0, slope: -0.8925588220023953, fluctuations: 0.0\n",
      "step: 73630 loss: 685.637749 time elapsed: 92.5526 learning rate: 0.001171, scenario: 0, slope: -0.8652248113078261, fluctuations: 0.0\n",
      "step: 73640 loss: 678.366079 time elapsed: 92.5663 learning rate: 0.001171, scenario: 0, slope: -0.8389957341317074, fluctuations: 0.0\n",
      "step: 73650 loss: 671.176873 time elapsed: 92.5801 learning rate: 0.001171, scenario: 0, slope: -0.8138028071384872, fluctuations: 0.0\n",
      "step: 73660 loss: 664.220255 time elapsed: 92.5934 learning rate: 0.001171, scenario: 0, slope: -0.7900707862126954, fluctuations: 0.0\n",
      "step: 73670 loss: 657.427870 time elapsed: 92.6066 learning rate: 0.001171, scenario: 0, slope: -0.767618809498148, fluctuations: 0.0\n",
      "step: 73680 loss: 650.809226 time elapsed: 92.6203 learning rate: 0.001171, scenario: 0, slope: -0.7463363977146205, fluctuations: 0.0\n",
      "step: 73690 loss: 644.382856 time elapsed: 92.6325 learning rate: 0.001171, scenario: 0, slope: -0.7257507622980371, fluctuations: 0.0\n",
      "step: 73700 loss: 638.090609 time elapsed: 92.6442 learning rate: 0.001288, scenario: 1, slope: -0.7080024355374622, fluctuations: 0.0\n",
      "step: 73710 loss: 644.483965 time elapsed: 92.6566 learning rate: 0.003340, scenario: 1, slope: -0.6662466522974926, fluctuations: 0.02\n",
      "step: 73720 loss: 639.131064 time elapsed: 92.6687 learning rate: 0.008664, scenario: 1, slope: -0.6037999958001669, fluctuations: 0.05\n",
      "step: 73730 loss: 1296.515122 time elapsed: 92.6806 learning rate: 0.007150, scenario: -1, slope: 2.025921856679674, fluctuations: 0.07\n",
      "step: 73740 loss: 692.320181 time elapsed: 92.6926 learning rate: 0.002493, scenario: -1, slope: 2.1421156993326784, fluctuations: 0.11\n",
      "step: 73750 loss: 601.705178 time elapsed: 92.7049 learning rate: 0.000869, scenario: -1, slope: 1.4793375259595962, fluctuations: 0.13\n",
      "step: 73760 loss: 575.997260 time elapsed: 92.7168 learning rate: 0.000303, scenario: -1, slope: 0.5581347911036694, fluctuations: 0.14\n",
      "step: 73770 loss: 571.289901 time elapsed: 92.7294 learning rate: 0.000226, scenario: 1, slope: -0.31665075333677706, fluctuations: 0.14\n",
      "step: 73780 loss: 567.553782 time elapsed: 92.7426 learning rate: 0.000331, scenario: 0, slope: -1.0682304645672178, fluctuations: 0.14\n",
      "step: 73790 loss: 563.935148 time elapsed: 92.7561 learning rate: 0.000331, scenario: 0, slope: -1.8006635557064183, fluctuations: 0.14\n",
      "step: 73800 loss: 560.901779 time elapsed: 92.7700 learning rate: 0.000331, scenario: 0, slope: -2.5248637687109126, fluctuations: 0.14\n",
      "step: 73810 loss: 558.103037 time elapsed: 92.7842 learning rate: 0.000331, scenario: 0, slope: -3.2763015200884857, fluctuations: 0.12\n",
      "step: 73820 loss: 555.472501 time elapsed: 92.7975 learning rate: 0.000331, scenario: 0, slope: -3.680658660783635, fluctuations: 0.09\n",
      "step: 73830 loss: 552.985092 time elapsed: 92.8113 learning rate: 0.000331, scenario: 0, slope: -1.5313226030134124, fluctuations: 0.06\n",
      "step: 73840 loss: 550.550029 time elapsed: 92.8255 learning rate: 0.000485, scenario: 1, slope: -0.5543121671961861, fluctuations: 0.03\n",
      "step: 73850 loss: 545.860964 time elapsed: 92.8378 learning rate: 0.001258, scenario: 1, slope: -0.33761902724196496, fluctuations: 0.01\n",
      "step: 73860 loss: 535.231357 time elapsed: 92.8497 learning rate: 0.003263, scenario: 1, slope: -0.32095658787428133, fluctuations: 0.0\n",
      "step: 73870 loss: 514.247593 time elapsed: 92.8614 learning rate: 0.008465, scenario: 1, slope: -0.40663433515552055, fluctuations: 0.0\n",
      "step: 73880 loss: 472.885191 time elapsed: 92.8731 learning rate: 0.018145, scenario: 0, slope: -0.6264770917360029, fluctuations: 0.0\n",
      "step: 73890 loss: 7371.509968 time elapsed: 92.8846 learning rate: 0.013889, scenario: -1, slope: 15.551109723760295, fluctuations: 0.01\n",
      "step: 73900 loss: 2804.059622 time elapsed: 92.8960 learning rate: 0.005381, scenario: -1, slope: 25.779773618408445, fluctuations: 0.04\n",
      "step: 73910 loss: 1184.151081 time elapsed: 92.9085 learning rate: 0.001876, scenario: -1, slope: 24.79411361657557, fluctuations: 0.06\n",
      "step: 73920 loss: 884.624268 time elapsed: 92.9202 learning rate: 0.000654, scenario: -1, slope: 20.040621896939662, fluctuations: 0.08\n",
      "step: 73930 loss: 782.759935 time elapsed: 92.9319 learning rate: 0.000228, scenario: -1, slope: 12.926665623413813, fluctuations: 0.08\n",
      "step: 73940 loss: 773.091839 time elapsed: 92.9459 learning rate: 0.000080, scenario: -1, slope: 6.102959784936768, fluctuations: 0.08\n",
      "step: 73950 loss: 767.847994 time elapsed: 92.9598 learning rate: 0.000036, scenario: 0, slope: -0.7045510535987565, fluctuations: 0.08\n",
      "step: 73960 loss: 764.571412 time elapsed: 92.9735 learning rate: 0.000036, scenario: 0, slope: -7.935742803476417, fluctuations: 0.08\n",
      "step: 73970 loss: 761.651838 time elapsed: 92.9874 learning rate: 0.000036, scenario: 0, slope: -16.146210766290658, fluctuations: 0.08\n",
      "step: 73980 loss: 759.026858 time elapsed: 93.0008 learning rate: 0.000036, scenario: 0, slope: -26.15317812442966, fluctuations: 0.08\n",
      "step: 73990 loss: 756.578310 time elapsed: 93.0149 learning rate: 0.000036, scenario: 0, slope: -20.8987458192432, fluctuations: 0.06\n",
      "step: 74000 loss: 754.222579 time elapsed: 93.0289 learning rate: 0.000036, scenario: 0, slope: -5.880885436916822, fluctuations: 0.03\n",
      "step: 74010 loss: 751.911707 time elapsed: 93.0439 learning rate: 0.000036, scenario: 0, slope: -1.3143779110032623, fluctuations: 0.01\n",
      "step: 74020 loss: 749.048243 time elapsed: 93.0562 learning rate: 0.000077, scenario: 1, slope: -0.5041264619642729, fluctuations: 0.0\n",
      "step: 74030 loss: 741.778405 time elapsed: 93.0684 learning rate: 0.000201, scenario: 1, slope: -0.30941151474975637, fluctuations: 0.0\n",
      "step: 74040 loss: 721.611764 time elapsed: 93.0798 learning rate: 0.000521, scenario: 1, slope: -0.34514157260650147, fluctuations: 0.0\n",
      "step: 74050 loss: 691.631302 time elapsed: 93.0914 learning rate: 0.001352, scenario: 1, slope: -0.512333114913622, fluctuations: 0.0\n",
      "step: 74060 loss: 640.826060 time elapsed: 93.1030 learning rate: 0.002898, scenario: 0, slope: -0.8500255970114783, fluctuations: 0.0\n",
      "step: 74070 loss: 586.729944 time elapsed: 93.1153 learning rate: 0.002898, scenario: 0, slope: -1.394549188676022, fluctuations: 0.0\n",
      "step: 74080 loss: 542.919974 time elapsed: 93.1274 learning rate: 0.002898, scenario: 0, slope: -2.0349628696001973, fluctuations: 0.0\n",
      "step: 74090 loss: 509.339513 time elapsed: 93.1412 learning rate: 0.002898, scenario: 0, slope: -2.6565101887598446, fluctuations: 0.0\n",
      "step: 74100 loss: 481.474616 time elapsed: 93.1550 learning rate: 0.002898, scenario: 0, slope: -3.130580993804179, fluctuations: 0.0\n",
      "step: 74110 loss: 458.548086 time elapsed: 93.1703 learning rate: 0.002898, scenario: 0, slope: -3.5320261597275455, fluctuations: 0.0\n",
      "step: 74120 loss: 439.580179 time elapsed: 93.1853 learning rate: 0.002898, scenario: 0, slope: -3.6738603507115, fluctuations: 0.0\n",
      "step: 74130 loss: 423.584455 time elapsed: 93.2002 learning rate: 0.002898, scenario: 0, slope: -3.5770685228914587, fluctuations: 0.0\n",
      "step: 74140 loss: 410.027513 time elapsed: 93.2151 learning rate: 0.002898, scenario: 0, slope: -3.2676970007833477, fluctuations: 0.0\n",
      "step: 74150 loss: 399.030378 time elapsed: 93.2301 learning rate: 0.002898, scenario: 0, slope: -2.809027220458324, fluctuations: 0.0\n",
      "step: 74160 loss: 388.522333 time elapsed: 93.2442 learning rate: 0.002898, scenario: 0, slope: -2.3093110194823074, fluctuations: 0.0\n",
      "step: 74170 loss: 379.297528 time elapsed: 93.2566 learning rate: 0.002898, scenario: 0, slope: -1.9056889500750567, fluctuations: 0.0\n",
      "step: 74180 loss: 361.584403 time elapsed: 93.2707 learning rate: 0.002898, scenario: 0, slope: -1.6252709672911123, fluctuations: 0.0\n",
      "step: 74190 loss: 338.373797 time elapsed: 93.2839 learning rate: 0.002898, scenario: 0, slope: -1.498871814723719, fluctuations: 0.03\n",
      "step: 74200 loss: 326.155130 time elapsed: 93.2969 learning rate: 0.002898, scenario: 0, slope: -1.433075349099344, fluctuations: 0.06\n",
      "step: 74210 loss: 317.263620 time elapsed: 93.3106 learning rate: 0.002898, scenario: 0, slope: -1.388651473537747, fluctuations: 0.06\n",
      "step: 74220 loss: 310.678482 time elapsed: 93.3232 learning rate: 0.002898, scenario: 0, slope: -1.3499028529881376, fluctuations: 0.06\n",
      "step: 74230 loss: 305.231643 time elapsed: 93.3354 learning rate: 0.002898, scenario: 0, slope: -1.2969503115684675, fluctuations: 0.06\n",
      "step: 74240 loss: 300.247325 time elapsed: 93.3485 learning rate: 0.002898, scenario: 0, slope: -1.2237323376416271, fluctuations: 0.06\n",
      "step: 74250 loss: 295.606614 time elapsed: 93.3627 learning rate: 0.002898, scenario: 0, slope: -1.1209691092980716, fluctuations: 0.06\n",
      "step: 74260 loss: 291.131500 time elapsed: 93.3762 learning rate: 0.002898, scenario: 0, slope: -0.9843064610668307, fluctuations: 0.06\n",
      "step: 74270 loss: 286.904443 time elapsed: 93.3899 learning rate: 0.002898, scenario: 0, slope: -0.8023511696526274, fluctuations: 0.06\n",
      "step: 74280 loss: 283.228087 time elapsed: 93.4035 learning rate: 0.002898, scenario: 0, slope: -0.6108665134113138, fluctuations: 0.05\n",
      "step: 74290 loss: 278.987766 time elapsed: 93.4168 learning rate: 0.002898, scenario: 0, slope: -0.5255569159726275, fluctuations: 0.02\n",
      "step: 74300 loss: 275.064087 time elapsed: 93.4302 learning rate: 0.002898, scenario: 0, slope: -0.4812720098206729, fluctuations: 0.0\n",
      "step: 74310 loss: 271.358242 time elapsed: 93.4445 learning rate: 0.002898, scenario: 0, slope: -0.443065090763837, fluctuations: 0.0\n",
      "step: 74320 loss: 267.753953 time elapsed: 93.4567 learning rate: 0.002898, scenario: 0, slope: -0.4217170403741468, fluctuations: 0.0\n",
      "step: 74330 loss: 264.236821 time elapsed: 93.4691 learning rate: 0.002898, scenario: 0, slope: -0.40519723996324436, fluctuations: 0.0\n",
      "step: 74340 loss: 260.811296 time elapsed: 93.4812 learning rate: 0.002898, scenario: 0, slope: -0.39145855408461455, fluctuations: 0.0\n",
      "step: 74350 loss: 257.471979 time elapsed: 93.4929 learning rate: 0.002898, scenario: 0, slope: -0.37930198166000484, fluctuations: 0.0\n",
      "step: 74360 loss: 254.213140 time elapsed: 93.5047 learning rate: 0.002898, scenario: 0, slope: -0.36858629467248044, fluctuations: 0.0\n",
      "step: 74370 loss: 251.034008 time elapsed: 93.5170 learning rate: 0.002898, scenario: 0, slope: -0.358950774538905, fluctuations: 0.0\n",
      "step: 74380 loss: 247.988597 time elapsed: 93.5289 learning rate: 0.002898, scenario: 0, slope: -0.348258790709855, fluctuations: 0.0\n",
      "step: 74390 loss: 244.917673 time elapsed: 93.5405 learning rate: 0.002898, scenario: 0, slope: -0.3377735963470988, fluctuations: 0.0\n",
      "step: 74400 loss: 242.002518 time elapsed: 93.5522 learning rate: 0.002898, scenario: 0, slope: -0.32990018846994446, fluctuations: 0.0\n",
      "step: 74410 loss: 239.051077 time elapsed: 93.5658 learning rate: 0.002898, scenario: 0, slope: -0.32113456086441133, fluctuations: 0.0\n",
      "step: 74420 loss: 236.224986 time elapsed: 93.5790 learning rate: 0.002898, scenario: 0, slope: -0.31372820921885414, fluctuations: 0.0\n",
      "step: 74430 loss: 233.455908 time elapsed: 93.5924 learning rate: 0.002898, scenario: 0, slope: -0.3066483453394127, fluctuations: 0.0\n",
      "step: 74440 loss: 230.746410 time elapsed: 93.6061 learning rate: 0.002898, scenario: 0, slope: -0.299876465510419, fluctuations: 0.0\n",
      "step: 74450 loss: 228.096448 time elapsed: 93.6198 learning rate: 0.002898, scenario: 0, slope: -0.2933848296031387, fluctuations: 0.0\n",
      "step: 74460 loss: 225.500491 time elapsed: 93.6332 learning rate: 0.002898, scenario: 0, slope: -0.2871731481272402, fluctuations: 0.0\n",
      "step: 74470 loss: 222.955736 time elapsed: 93.6468 learning rate: 0.002898, scenario: 0, slope: -0.28123771353898946, fluctuations: 0.0\n",
      "step: 74480 loss: 220.459699 time elapsed: 93.6609 learning rate: 0.002898, scenario: 0, slope: -0.27547310110237244, fluctuations: 0.0\n",
      "step: 74490 loss: 218.009991 time elapsed: 93.6742 learning rate: 0.002898, scenario: 0, slope: -0.268642815393843, fluctuations: 0.0\n",
      "step: 74500 loss: 215.604535 time elapsed: 93.6865 learning rate: 0.002898, scenario: 0, slope: -0.26340304135721493, fluctuations: 0.0\n",
      "step: 74510 loss: 213.241184 time elapsed: 93.6994 learning rate: 0.002898, scenario: 0, slope: -0.25743039555556485, fluctuations: 0.0\n",
      "step: 74520 loss: 210.918320 time elapsed: 93.7117 learning rate: 0.002898, scenario: 0, slope: -0.25238379509350245, fluctuations: 0.0\n",
      "step: 74530 loss: 208.646680 time elapsed: 93.7244 learning rate: 0.002898, scenario: 0, slope: -0.24756527541655562, fluctuations: 0.0\n",
      "step: 74540 loss: 206.642592 time elapsed: 93.7364 learning rate: 0.002898, scenario: 0, slope: -0.2423185417806575, fluctuations: 0.0\n",
      "step: 74550 loss: 204.248222 time elapsed: 93.7492 learning rate: 0.002898, scenario: 0, slope: -0.2376834354257018, fluctuations: 0.0\n",
      "step: 74560 loss: 201.875905 time elapsed: 93.7631 learning rate: 0.002898, scenario: 0, slope: -0.23387316095416327, fluctuations: 0.0\n",
      "step: 74570 loss: 199.815167 time elapsed: 93.7783 learning rate: 0.002898, scenario: 0, slope: -0.2306199112352091, fluctuations: 0.0\n",
      "step: 74580 loss: 197.503652 time elapsed: 93.7924 learning rate: 0.002898, scenario: 0, slope: -0.22780884272492546, fluctuations: 0.0\n",
      "step: 74590 loss: 195.174944 time elapsed: 93.8057 learning rate: 0.002898, scenario: 0, slope: -0.22615146567150468, fluctuations: 0.0\n",
      "step: 74600 loss: 192.849265 time elapsed: 93.8197 learning rate: 0.002898, scenario: 0, slope: -0.2255011305575963, fluctuations: 0.0\n",
      "step: 74610 loss: 190.670019 time elapsed: 93.8345 learning rate: 0.002898, scenario: 0, slope: -0.22523540426889174, fluctuations: 0.0\n",
      "step: 74620 loss: 188.593949 time elapsed: 93.8492 learning rate: 0.002898, scenario: 0, slope: -0.22477756105969723, fluctuations: 0.0\n",
      "step: 74630 loss: 186.632374 time elapsed: 93.8640 learning rate: 0.002898, scenario: 0, slope: -0.2238328260468069, fluctuations: 0.0\n",
      "step: 74640 loss: 184.521264 time elapsed: 93.8775 learning rate: 0.002898, scenario: 0, slope: -0.22105519552500916, fluctuations: 0.0\n",
      "step: 74650 loss: 182.535422 time elapsed: 93.8900 learning rate: 0.002898, scenario: 0, slope: -0.21772522061696814, fluctuations: 0.0\n",
      "step: 74660 loss: 180.687191 time elapsed: 93.9019 learning rate: 0.002898, scenario: 0, slope: -0.21402276891451227, fluctuations: 0.0\n",
      "step: 74670 loss: 178.756643 time elapsed: 93.9136 learning rate: 0.002898, scenario: 0, slope: -0.21002193813182318, fluctuations: 0.0\n",
      "step: 74680 loss: 176.765747 time elapsed: 93.9255 learning rate: 0.002898, scenario: 0, slope: -0.20542681453023326, fluctuations: 0.0\n",
      "step: 74690 loss: 174.853863 time elapsed: 93.9373 learning rate: 0.002898, scenario: 0, slope: -0.20116052165862883, fluctuations: 0.0\n",
      "step: 74700 loss: 173.080671 time elapsed: 93.9489 learning rate: 0.002898, scenario: 0, slope: -0.19803354116808777, fluctuations: 0.0\n",
      "step: 74710 loss: 171.178317 time elapsed: 93.9610 learning rate: 0.002898, scenario: 0, slope: -0.19475718967319158, fluctuations: 0.0\n",
      "step: 74720 loss: 169.460822 time elapsed: 93.9737 learning rate: 0.002898, scenario: 0, slope: -0.19206732694312195, fluctuations: 0.0\n",
      "step: 74730 loss: 167.607900 time elapsed: 93.9875 learning rate: 0.002898, scenario: 0, slope: -0.18966837588496488, fluctuations: 0.0\n",
      "step: 74740 loss: 166.007772 time elapsed: 94.0008 learning rate: 0.002898, scenario: 0, slope: -0.18675545136549715, fluctuations: 0.0\n",
      "step: 74750 loss: 164.266933 time elapsed: 94.0145 learning rate: 0.002898, scenario: 0, slope: -0.18334226144270946, fluctuations: 0.0\n",
      "step: 74760 loss: 162.383103 time elapsed: 94.0280 learning rate: 0.002898, scenario: 0, slope: -0.18089189368855987, fluctuations: 0.0\n",
      "step: 74770 loss: 160.617825 time elapsed: 94.0420 learning rate: 0.002898, scenario: 0, slope: -0.17911704466249967, fluctuations: 0.0\n",
      "step: 74780 loss: 158.914041 time elapsed: 94.0557 learning rate: 0.002898, scenario: 0, slope: -0.17758396768855872, fluctuations: 0.0\n",
      "step: 74790 loss: 157.307431 time elapsed: 94.0692 learning rate: 0.002898, scenario: 0, slope: -0.1761625133545546, fluctuations: 0.0\n",
      "step: 74800 loss: 155.505563 time elapsed: 94.0814 learning rate: 0.002898, scenario: 0, slope: -0.17492614195116787, fluctuations: 0.0\n",
      "step: 74810 loss: 153.785437 time elapsed: 94.0941 learning rate: 0.002898, scenario: 0, slope: -0.17390985052737898, fluctuations: 0.0\n",
      "step: 74820 loss: 152.132198 time elapsed: 94.1063 learning rate: 0.002898, scenario: 0, slope: -0.1729952392245568, fluctuations: 0.0\n",
      "step: 74830 loss: 150.454670 time elapsed: 94.1178 learning rate: 0.002898, scenario: 0, slope: -0.17254989247606511, fluctuations: 0.0\n",
      "step: 74840 loss: 148.851744 time elapsed: 94.1299 learning rate: 0.002898, scenario: 0, slope: -0.17034917503232724, fluctuations: 0.01\n",
      "step: 74850 loss: 147.359820 time elapsed: 94.1418 learning rate: 0.002898, scenario: 0, slope: -0.1677997522616567, fluctuations: 0.02\n",
      "step: 74860 loss: 145.715384 time elapsed: 94.1540 learning rate: 0.002898, scenario: 0, slope: -0.1663207045881561, fluctuations: 0.02\n",
      "step: 74870 loss: 144.083485 time elapsed: 94.1657 learning rate: 0.002898, scenario: 0, slope: -0.16509166830547498, fluctuations: 0.02\n",
      "step: 74880 loss: 142.554142 time elapsed: 94.1788 learning rate: 0.002898, scenario: 0, slope: -0.16375585135246326, fluctuations: 0.02\n",
      "step: 74890 loss: 141.060846 time elapsed: 94.1928 learning rate: 0.002898, scenario: 0, slope: -0.16213612795403096, fluctuations: 0.02\n",
      "step: 74900 loss: 139.630482 time elapsed: 94.2067 learning rate: 0.002898, scenario: 0, slope: -0.16001788497608674, fluctuations: 0.02\n",
      "step: 74910 loss: 138.143351 time elapsed: 94.2220 learning rate: 0.002898, scenario: 0, slope: -0.15649915783822171, fluctuations: 0.02\n",
      "step: 74920 loss: 136.755095 time elapsed: 94.2368 learning rate: 0.002898, scenario: 0, slope: -0.15409956672625452, fluctuations: 0.02\n",
      "step: 74930 loss: 135.333780 time elapsed: 94.2517 learning rate: 0.002898, scenario: 0, slope: -0.15237182064914953, fluctuations: 0.02\n",
      "step: 74940 loss: 133.922326 time elapsed: 94.2666 learning rate: 0.002898, scenario: 0, slope: -0.14906182270862758, fluctuations: 0.01\n",
      "step: 74950 loss: 203.131050 time elapsed: 94.2814 learning rate: 0.006211, scenario: 1, slope: -0.10169965028511202, fluctuations: 0.0\n",
      "step: 74960 loss: 209.690229 time elapsed: 94.2948 learning rate: 0.002297, scenario: -1, slope: 0.692694328888444, fluctuations: 0.02\n",
      "step: 74970 loss: 153.472151 time elapsed: 94.3069 learning rate: 0.000801, scenario: -1, slope: 0.7146443535662516, fluctuations: 0.03\n",
      "step: 74980 loss: 137.247774 time elapsed: 94.3193 learning rate: 0.000279, scenario: -1, slope: 0.5667252809029442, fluctuations: 0.05\n",
      "step: 74990 loss: 134.243446 time elapsed: 94.3323 learning rate: 0.000097, scenario: -1, slope: 0.34932461315108465, fluctuations: 0.06\n",
      "step: 75000 loss: 133.285578 time elapsed: 94.3452 learning rate: 0.000038, scenario: -1, slope: 0.1342112719780114, fluctuations: 0.06\n",
      "step: 75010 loss: 133.100503 time elapsed: 94.3585 learning rate: 0.000042, scenario: 1, slope: -0.11412295722790221, fluctuations: 0.06\n",
      "step: 75020 loss: 133.001270 time elapsed: 94.3715 learning rate: 0.000046, scenario: 0, slope: -0.33904634745882634, fluctuations: 0.06\n",
      "step: 75030 loss: 132.896288 time elapsed: 94.3842 learning rate: 0.000046, scenario: 0, slope: -0.5753659980961433, fluctuations: 0.06\n",
      "step: 75040 loss: 132.810711 time elapsed: 94.3982 learning rate: 0.000046, scenario: 0, slope: -0.8367016213484688, fluctuations: 0.06\n",
      "step: 75050 loss: 132.735331 time elapsed: 94.4116 learning rate: 0.000046, scenario: 0, slope: -1.087711384930629, fluctuations: 0.06\n",
      "step: 75060 loss: 132.661318 time elapsed: 94.4247 learning rate: 0.000046, scenario: 0, slope: -0.23736522669452603, fluctuations: 0.04\n",
      "step: 75070 loss: 132.566740 time elapsed: 94.4380 learning rate: 0.000109, scenario: 1, slope: -0.058038862423768475, fluctuations: 0.02\n",
      "step: 75080 loss: 132.340798 time elapsed: 94.4514 learning rate: 0.000283, scenario: 1, slope: -0.01707886902432785, fluctuations: 0.01\n",
      "step: 75090 loss: 131.829807 time elapsed: 94.4648 learning rate: 0.000734, scenario: 1, slope: -0.013336765239581033, fluctuations: 0.0\n",
      "step: 75100 loss: 130.831315 time elapsed: 94.4787 learning rate: 0.001731, scenario: 1, slope: -0.015404983044195932, fluctuations: 0.0\n",
      "step: 75110 loss: 129.248687 time elapsed: 94.4919 learning rate: 0.004490, scenario: 1, slope: -0.02633245160594668, fluctuations: 0.0\n",
      "step: 75120 loss: 126.049928 time elapsed: 94.5047 learning rate: 0.011647, scenario: 1, slope: -0.046309863633023626, fluctuations: 0.0\n",
      "step: 75130 loss: 161278.647260 time elapsed: 94.5170 learning rate: 0.017548, scenario: -1, slope: 115.08442807133544, fluctuations: 0.0\n",
      "step: 75140 loss: 28336.614946 time elapsed: 94.5290 learning rate: 0.006119, scenario: -1, slope: 172.52333379564504, fluctuations: 0.04\n",
      "step: 75150 loss: 10721.806210 time elapsed: 94.5410 learning rate: 0.002133, scenario: -1, slope: 207.20825274558425, fluctuations: 0.06\n",
      "step: 75160 loss: 6660.983251 time elapsed: 94.5530 learning rate: 0.000744, scenario: -1, slope: 187.5342556538343, fluctuations: 0.06\n",
      "step: 75170 loss: 5682.809814 time elapsed: 94.5648 learning rate: 0.000259, scenario: -1, slope: 153.183782033293, fluctuations: 0.06\n",
      "step: 75180 loss: 5515.784265 time elapsed: 94.5768 learning rate: 0.000090, scenario: -1, slope: 112.64475549514734, fluctuations: 0.06\n",
      "step: 75190 loss: 5461.492763 time elapsed: 94.5896 learning rate: 0.000032, scenario: -1, slope: 66.90648579708622, fluctuations: 0.06\n",
      "step: 75200 loss: 5441.939464 time elapsed: 94.6030 learning rate: 0.000012, scenario: -1, slope: 19.957996104873704, fluctuations: 0.06\n",
      "step: 75210 loss: 5433.081248 time elapsed: 94.6167 learning rate: 0.000010, scenario: 0, slope: -47.71315392056327, fluctuations: 0.06\n",
      "step: 75220 loss: 5424.870546 time elapsed: 94.6295 learning rate: 0.000010, scenario: 0, slope: -123.12382967555271, fluctuations: 0.06\n",
      "step: 75230 loss: 5416.832968 time elapsed: 94.6429 learning rate: 0.000010, scenario: 0, slope: -179.2030890690114, fluctuations: 0.05\n",
      "step: 75240 loss: 5408.961924 time elapsed: 94.6563 learning rate: 0.000010, scenario: 0, slope: -51.41775768285126, fluctuations: 0.02\n",
      "step: 75250 loss: 5401.244482 time elapsed: 94.6700 learning rate: 0.000010, scenario: 0, slope: -15.544519280970073, fluctuations: 0.0\n",
      "step: 75260 loss: 5393.433747 time elapsed: 94.6841 learning rate: 0.000014, scenario: 1, slope: -4.443928666806943, fluctuations: 0.0\n",
      "step: 75270 loss: 5377.730687 time elapsed: 94.6982 learning rate: 0.000038, scenario: 1, slope: -1.5672243711667135, fluctuations: 0.0\n",
      "step: 75280 loss: 5338.732257 time elapsed: 94.7114 learning rate: 0.000097, scenario: 1, slope: -1.1108822243128196, fluctuations: 0.0\n",
      "step: 75290 loss: 5245.473818 time elapsed: 94.7237 learning rate: 0.000253, scenario: 1, slope: -1.3606158783903042, fluctuations: 0.0\n",
      "step: 75300 loss: 5029.966855 time elapsed: 94.7369 learning rate: 0.000596, scenario: 1, slope: -2.278328368221113, fluctuations: 0.0\n",
      "step: 75310 loss: 4604.675782 time elapsed: 94.7508 learning rate: 0.001546, scenario: 1, slope: -4.99190271993394, fluctuations: 0.0\n",
      "step: 75320 loss: 3991.247933 time elapsed: 94.7638 learning rate: 0.001701, scenario: 0, slope: -9.906738971351798, fluctuations: 0.0\n",
      "step: 75330 loss: 3550.176489 time elapsed: 94.7772 learning rate: 0.001701, scenario: 0, slope: -16.23700487069517, fluctuations: 0.0\n",
      "step: 75340 loss: 3202.093213 time elapsed: 94.7900 learning rate: 0.001701, scenario: 0, slope: -22.697776128670334, fluctuations: 0.0\n",
      "step: 75350 loss: 2917.596338 time elapsed: 94.8053 learning rate: 0.001701, scenario: 0, slope: -28.413535905446423, fluctuations: 0.0\n",
      "step: 75360 loss: 2673.495918 time elapsed: 94.8204 learning rate: 0.001701, scenario: 0, slope: -32.75244634256831, fluctuations: 0.0\n",
      "step: 75370 loss: 2456.305984 time elapsed: 94.8347 learning rate: 0.001701, scenario: 0, slope: -35.25339502921088, fluctuations: 0.0\n",
      "step: 75380 loss: 2252.231389 time elapsed: 94.8482 learning rate: 0.001701, scenario: 0, slope: -35.656779354094354, fluctuations: 0.0\n",
      "step: 75390 loss: 2076.937742 time elapsed: 94.8619 learning rate: 0.001701, scenario: 0, slope: -33.88654594686627, fluctuations: 0.0\n",
      "step: 75400 loss: 1925.363736 time elapsed: 94.8752 learning rate: 0.001701, scenario: 0, slope: -30.650899425726287, fluctuations: 0.0\n",
      "step: 75410 loss: 1804.138439 time elapsed: 94.8884 learning rate: 0.001701, scenario: 0, slope: -25.601932852409348, fluctuations: 0.0\n",
      "step: 75420 loss: 1706.689529 time elapsed: 94.9027 learning rate: 0.001701, scenario: 0, slope: -21.688159842174123, fluctuations: 0.0\n",
      "step: 75430 loss: 1623.348628 time elapsed: 94.9171 learning rate: 0.001701, scenario: 0, slope: -18.665048038338778, fluctuations: 0.0\n",
      "step: 75440 loss: 1550.486381 time elapsed: 94.9309 learning rate: 0.001701, scenario: 0, slope: -16.136846332186572, fluctuations: 0.0\n",
      "step: 75450 loss: 1486.270730 time elapsed: 94.9450 learning rate: 0.001701, scenario: 0, slope: -13.910467771142331, fluctuations: 0.0\n",
      "step: 75460 loss: 1429.130058 time elapsed: 94.9573 learning rate: 0.001701, scenario: 0, slope: -11.905095098862542, fluctuations: 0.0\n",
      "step: 75470 loss: 1377.868253 time elapsed: 94.9694 learning rate: 0.001701, scenario: 0, slope: -10.10734259539534, fluctuations: 0.0\n",
      "step: 75480 loss: 1331.560688 time elapsed: 94.9812 learning rate: 0.001701, scenario: 0, slope: -8.570180088499677, fluctuations: 0.0\n",
      "step: 75490 loss: 1289.448593 time elapsed: 94.9930 learning rate: 0.001701, scenario: 0, slope: -7.335487810032365, fluctuations: 0.0\n",
      "step: 75500 loss: 1250.911236 time elapsed: 95.0065 learning rate: 0.001701, scenario: 0, slope: -6.465684749640559, fluctuations: 0.0\n",
      "step: 75510 loss: 1215.430927 time elapsed: 95.0214 learning rate: 0.001701, scenario: 0, slope: -5.6486072840545205, fluctuations: 0.0\n",
      "step: 75520 loss: 1182.572935 time elapsed: 95.0353 learning rate: 0.001701, scenario: 0, slope: -5.058183777660346, fluctuations: 0.0\n",
      "step: 75530 loss: 1151.970795 time elapsed: 95.0491 learning rate: 0.001701, scenario: 0, slope: -4.5654716083627385, fluctuations: 0.0\n",
      "step: 75540 loss: 1123.315534 time elapsed: 95.0623 learning rate: 0.001701, scenario: 0, slope: -4.150528214506722, fluctuations: 0.0\n",
      "step: 75550 loss: 1096.346696 time elapsed: 95.0755 learning rate: 0.001701, scenario: 0, slope: -3.7987146493765236, fluctuations: 0.0\n",
      "step: 75560 loss: 1070.846171 time elapsed: 95.0892 learning rate: 0.001701, scenario: 0, slope: -3.4986178924933626, fluctuations: 0.0\n",
      "step: 75570 loss: 1047.070672 time elapsed: 95.1027 learning rate: 0.001701, scenario: 0, slope: -3.2400122596779073, fluctuations: 0.0\n",
      "step: 75580 loss: 1023.587675 time elapsed: 95.1180 learning rate: 0.001701, scenario: 0, slope: -3.0180104302357633, fluctuations: 0.0\n",
      "step: 75590 loss: 1001.535501 time elapsed: 95.1306 learning rate: 0.001701, scenario: 0, slope: -2.8268889504161963, fluctuations: 0.0\n",
      "step: 75600 loss: 980.377578 time elapsed: 95.1429 learning rate: 0.001701, scenario: 0, slope: -2.6768483426795187, fluctuations: 0.0\n",
      "step: 75610 loss: 960.020347 time elapsed: 95.1552 learning rate: 0.001701, scenario: 0, slope: -2.5171300848008955, fluctuations: 0.0\n",
      "step: 75620 loss: 940.377792 time elapsed: 95.1679 learning rate: 0.001701, scenario: 0, slope: -2.3909174677401435, fluctuations: 0.0\n",
      "step: 75630 loss: 921.372577 time elapsed: 95.1815 learning rate: 0.001701, scenario: 0, slope: -2.279917851234683, fluctuations: 0.0\n",
      "step: 75640 loss: 902.935178 time elapsed: 95.1948 learning rate: 0.001701, scenario: 0, slope: -2.181886240015558, fluctuations: 0.0\n",
      "step: 75650 loss: 885.002659 time elapsed: 95.2074 learning rate: 0.001701, scenario: 0, slope: -2.0950290810576835, fluctuations: 0.0\n",
      "step: 75660 loss: 867.511077 time elapsed: 95.2212 learning rate: 0.001701, scenario: 0, slope: -2.017945742265672, fluctuations: 0.0\n",
      "step: 75670 loss: 850.391930 time elapsed: 95.2363 learning rate: 0.001701, scenario: 0, slope: -1.9478894762138974, fluctuations: 0.0\n",
      "step: 75680 loss: 833.622294 time elapsed: 95.2516 learning rate: 0.001701, scenario: 0, slope: -1.885701024787773, fluctuations: 0.0\n",
      "step: 75690 loss: 817.133356 time elapsed: 95.2662 learning rate: 0.001701, scenario: 0, slope: -1.831020870141336, fluctuations: 0.0\n",
      "step: 75700 loss: 800.851112 time elapsed: 95.2809 learning rate: 0.001701, scenario: 0, slope: -1.787521257745053, fluctuations: 0.0\n",
      "step: 75710 loss: 784.682459 time elapsed: 95.2962 learning rate: 0.001701, scenario: 0, slope: -1.7413798183868336, fluctuations: 0.0\n",
      "step: 75720 loss: 768.510847 time elapsed: 95.3107 learning rate: 0.001701, scenario: 0, slope: -1.7062269699614432, fluctuations: 0.0\n",
      "step: 75730 loss: 752.186139 time elapsed: 95.3245 learning rate: 0.001701, scenario: 0, slope: -1.678118913611616, fluctuations: 0.0\n",
      "step: 75740 loss: 735.521059 time elapsed: 95.3375 learning rate: 0.001701, scenario: 0, slope: -1.6581253578384194, fluctuations: 0.0\n",
      "step: 75750 loss: 718.314082 time elapsed: 95.3502 learning rate: 0.001701, scenario: 0, slope: -1.647815039149392, fluctuations: 0.0\n",
      "step: 75760 loss: 700.448825 time elapsed: 95.3622 learning rate: 0.001701, scenario: 0, slope: -1.6488919091038872, fluctuations: 0.0\n",
      "step: 75770 loss: 682.101361 time elapsed: 95.3739 learning rate: 0.001701, scenario: 0, slope: -1.662015128377622, fluctuations: 0.0\n",
      "step: 75780 loss: 663.841593 time elapsed: 95.3860 learning rate: 0.001701, scenario: 0, slope: -1.6847875627176336, fluctuations: 0.0\n",
      "step: 75790 loss: 646.207026 time elapsed: 95.3981 learning rate: 0.001701, scenario: 0, slope: -1.7110816588955893, fluctuations: 0.0\n",
      "step: 75800 loss: 629.219666 time elapsed: 95.4098 learning rate: 0.001701, scenario: 0, slope: -1.731984295068958, fluctuations: 0.0\n",
      "step: 75810 loss: 610.739976 time elapsed: 95.4222 learning rate: 0.001701, scenario: 0, slope: -1.7547384952237648, fluctuations: 0.01\n",
      "step: 75820 loss: 591.403794 time elapsed: 95.4340 learning rate: 0.001701, scenario: 0, slope: -1.7782951185510467, fluctuations: 0.01\n",
      "step: 75830 loss: 573.070675 time elapsed: 95.4472 learning rate: 0.001701, scenario: 0, slope: -1.798352690096908, fluctuations: 0.01\n",
      "step: 75840 loss: 556.556202 time elapsed: 95.4611 learning rate: 0.001701, scenario: 0, slope: -1.8067044343617695, fluctuations: 0.01\n",
      "step: 75850 loss: 542.558169 time elapsed: 95.4746 learning rate: 0.001701, scenario: 0, slope: -1.792970771226894, fluctuations: 0.01\n",
      "step: 75860 loss: 530.251884 time elapsed: 95.4882 learning rate: 0.001701, scenario: 0, slope: -1.7523556026387057, fluctuations: 0.01\n",
      "step: 75870 loss: 518.600729 time elapsed: 95.5018 learning rate: 0.001701, scenario: 0, slope: -1.6883317737760843, fluctuations: 0.01\n",
      "step: 75880 loss: 507.646036 time elapsed: 95.5156 learning rate: 0.001701, scenario: 0, slope: -1.6060669929713873, fluctuations: 0.01\n",
      "step: 75890 loss: 497.123309 time elapsed: 95.5293 learning rate: 0.001701, scenario: 0, slope: -1.50784417851539, fluctuations: 0.01\n",
      "step: 75900 loss: 486.980657 time elapsed: 95.5444 learning rate: 0.001701, scenario: 0, slope: -1.4066446005581403, fluctuations: 0.01\n",
      "step: 75910 loss: 477.279251 time elapsed: 95.5586 learning rate: 0.001701, scenario: 0, slope: -1.2866404717991329, fluctuations: 0.0\n",
      "step: 75920 loss: 467.746230 time elapsed: 95.5707 learning rate: 0.001701, scenario: 0, slope: -1.1866439942177482, fluctuations: 0.0\n",
      "step: 75930 loss: 458.519191 time elapsed: 95.5832 learning rate: 0.001701, scenario: 0, slope: -1.1069197074487216, fluctuations: 0.0\n",
      "step: 75940 loss: 449.586853 time elapsed: 95.5961 learning rate: 0.001701, scenario: 0, slope: -1.048684126194249, fluctuations: 0.0\n",
      "step: 75950 loss: 440.914331 time elapsed: 95.6085 learning rate: 0.001701, scenario: 0, slope: -1.0055973590114926, fluctuations: 0.0\n",
      "step: 75960 loss: 432.659717 time elapsed: 95.6212 learning rate: 0.001701, scenario: 0, slope: -0.969432809664106, fluctuations: 0.0\n",
      "step: 75970 loss: 424.427998 time elapsed: 95.6345 learning rate: 0.001701, scenario: 0, slope: -0.9362436458149839, fluctuations: 0.0\n",
      "step: 75980 loss: 416.524093 time elapsed: 95.6477 learning rate: 0.001701, scenario: 0, slope: -0.9066159610057624, fluctuations: 0.0\n",
      "step: 75990 loss: 408.050427 time elapsed: 95.6622 learning rate: 0.001701, scenario: 0, slope: -0.8798345670947633, fluctuations: 0.0\n",
      "step: 76000 loss: 394.110068 time elapsed: 95.6775 learning rate: 0.001701, scenario: 0, slope: -0.8792444468254468, fluctuations: 0.01\n",
      "step: 76010 loss: 381.432917 time elapsed: 95.6924 learning rate: 0.001701, scenario: 0, slope: -0.9060491686673737, fluctuations: 0.01\n",
      "step: 76020 loss: 361.901402 time elapsed: 95.7067 learning rate: 0.001701, scenario: 0, slope: -0.9668444053340748, fluctuations: 0.01\n",
      "step: 76030 loss: 345.810851 time elapsed: 95.7214 learning rate: 0.001701, scenario: 0, slope: -1.0681187341703526, fluctuations: 0.01\n",
      "step: 76040 loss: 335.297754 time elapsed: 95.7361 learning rate: 0.001701, scenario: 0, slope: -1.1661382293125124, fluctuations: 0.01\n",
      "step: 76050 loss: 326.233484 time elapsed: 95.7499 learning rate: 0.001701, scenario: 0, slope: -1.2379707133351046, fluctuations: 0.01\n",
      "step: 76060 loss: 318.558938 time elapsed: 95.7634 learning rate: 0.001701, scenario: 0, slope: -1.2727496032327779, fluctuations: 0.01\n",
      "step: 76070 loss: 311.832193 time elapsed: 95.7758 learning rate: 0.001701, scenario: 0, slope: -1.2625631351460516, fluctuations: 0.01\n",
      "step: 76080 loss: 305.304409 time elapsed: 95.7882 learning rate: 0.001701, scenario: 0, slope: -1.2047408584102088, fluctuations: 0.01\n",
      "step: 76090 loss: 299.287095 time elapsed: 95.8005 learning rate: 0.001701, scenario: 0, slope: -1.0967533440539272, fluctuations: 0.01\n",
      "step: 76100 loss: 293.579365 time elapsed: 95.8122 learning rate: 0.001701, scenario: 0, slope: -0.9901018029288245, fluctuations: 0.0\n",
      "step: 76110 loss: 288.183988 time elapsed: 95.8245 learning rate: 0.001701, scenario: 0, slope: -0.8414274640978767, fluctuations: 0.0\n",
      "step: 76120 loss: 283.021045 time elapsed: 95.8365 learning rate: 0.001701, scenario: 0, slope: -0.7212227231027685, fluctuations: 0.0\n",
      "step: 76130 loss: 278.080208 time elapsed: 95.8482 learning rate: 0.001701, scenario: 0, slope: -0.6479278328250527, fluctuations: 0.0\n",
      "step: 76140 loss: 273.337569 time elapsed: 95.8598 learning rate: 0.001701, scenario: 0, slope: -0.5998539435771169, fluctuations: 0.0\n",
      "step: 76150 loss: 268.778967 time elapsed: 95.8732 learning rate: 0.001701, scenario: 0, slope: -0.5637660338148972, fluctuations: 0.0\n",
      "step: 76160 loss: 264.390955 time elapsed: 95.8872 learning rate: 0.001701, scenario: 0, slope: -0.5348889107951015, fluctuations: 0.0\n",
      "step: 76170 loss: 260.161730 time elapsed: 95.9009 learning rate: 0.001701, scenario: 0, slope: -0.5099926910556045, fluctuations: 0.0\n",
      "step: 76180 loss: 256.081149 time elapsed: 95.9142 learning rate: 0.001701, scenario: 0, slope: -0.4875170548957575, fluctuations: 0.0\n",
      "step: 76190 loss: 252.197250 time elapsed: 95.9279 learning rate: 0.001701, scenario: 0, slope: -0.4675543176563912, fluctuations: 0.0\n",
      "step: 76200 loss: 248.339972 time elapsed: 95.9412 learning rate: 0.001701, scenario: 0, slope: -0.4503789856208635, fluctuations: 0.0\n",
      "step: 76210 loss: 244.694663 time elapsed: 95.9553 learning rate: 0.001701, scenario: 0, slope: -0.43173411863407, fluctuations: 0.0\n",
      "step: 76220 loss: 241.086938 time elapsed: 95.9689 learning rate: 0.001701, scenario: 0, slope: -0.41627095230087224, fluctuations: 0.0\n",
      "step: 76230 loss: 237.623861 time elapsed: 95.9819 learning rate: 0.001701, scenario: 0, slope: -0.4018816829366805, fluctuations: 0.0\n",
      "step: 76240 loss: 234.259257 time elapsed: 95.9954 learning rate: 0.001701, scenario: 0, slope: -0.38847080688272717, fluctuations: 0.0\n",
      "step: 76250 loss: 230.995018 time elapsed: 96.0083 learning rate: 0.001701, scenario: 0, slope: -0.3759288029929073, fluctuations: 0.0\n",
      "step: 76260 loss: 227.820771 time elapsed: 96.0213 learning rate: 0.001701, scenario: 0, slope: -0.36418438720788693, fluctuations: 0.0\n",
      "step: 76270 loss: 224.733737 time elapsed: 96.0347 learning rate: 0.001701, scenario: 0, slope: -0.3531724042424782, fluctuations: 0.0\n",
      "step: 76280 loss: 221.730185 time elapsed: 96.0488 learning rate: 0.001701, scenario: 0, slope: -0.34283184242662096, fluctuations: 0.0\n",
      "step: 76290 loss: 218.807081 time elapsed: 96.0627 learning rate: 0.001701, scenario: 0, slope: -0.3330082381340505, fluctuations: 0.0\n",
      "step: 76300 loss: 215.962213 time elapsed: 96.0749 learning rate: 0.001701, scenario: 0, slope: -0.32377477908477825, fluctuations: 0.0\n",
      "step: 76310 loss: 213.264186 time elapsed: 96.0900 learning rate: 0.001701, scenario: 0, slope: -0.3134753589201795, fluctuations: 0.0\n",
      "step: 76320 loss: 210.513019 time elapsed: 96.1050 learning rate: 0.001701, scenario: 0, slope: -0.3038910964259276, fluctuations: 0.0\n",
      "step: 76330 loss: 207.917555 time elapsed: 96.1192 learning rate: 0.001701, scenario: 0, slope: -0.2954640951058971, fluctuations: 0.0\n",
      "step: 76340 loss: 205.329651 time elapsed: 96.1334 learning rate: 0.001701, scenario: 0, slope: -0.28771668743340795, fluctuations: 0.0\n",
      "step: 76350 loss: 202.824083 time elapsed: 96.1480 learning rate: 0.001701, scenario: 0, slope: -0.28030634972299845, fluctuations: 0.0\n",
      "step: 76360 loss: 200.398544 time elapsed: 96.1618 learning rate: 0.001701, scenario: 0, slope: -0.27319589350957235, fluctuations: 0.0\n",
      "step: 76370 loss: 198.032298 time elapsed: 96.1760 learning rate: 0.001701, scenario: 0, slope: -0.2663365140226113, fluctuations: 0.0\n",
      "step: 76380 loss: 195.727270 time elapsed: 96.1882 learning rate: 0.001701, scenario: 0, slope: -0.2597123274485927, fluctuations: 0.0\n",
      "step: 76390 loss: 193.481976 time elapsed: 96.2009 learning rate: 0.001701, scenario: 0, slope: -0.2533091135025029, fluctuations: 0.0\n",
      "step: 76400 loss: 191.294048 time elapsed: 96.2128 learning rate: 0.001701, scenario: 0, slope: -0.2477271965675396, fluctuations: 0.0\n",
      "step: 76410 loss: 189.161296 time elapsed: 96.2256 learning rate: 0.001701, scenario: 0, slope: -0.2410064769912167, fluctuations: 0.0\n",
      "step: 76420 loss: 187.082300 time elapsed: 96.2375 learning rate: 0.001701, scenario: 0, slope: -0.23399676447643525, fluctuations: 0.0\n",
      "step: 76430 loss: 185.059873 time elapsed: 96.2493 learning rate: 0.001701, scenario: 0, slope: -0.22769199602732831, fluctuations: 0.0\n",
      "step: 76440 loss: 183.271881 time elapsed: 96.2611 learning rate: 0.001701, scenario: 0, slope: -0.22141960206559408, fluctuations: 0.0\n",
      "step: 76450 loss: 181.213681 time elapsed: 96.2729 learning rate: 0.001701, scenario: 0, slope: -0.21547307889726963, fluctuations: 0.0\n",
      "step: 76460 loss: 179.231097 time elapsed: 96.2865 learning rate: 0.001701, scenario: 0, slope: -0.21024940810102977, fluctuations: 0.0\n",
      "step: 76470 loss: 177.315906 time elapsed: 96.3007 learning rate: 0.001701, scenario: 0, slope: -0.20553339927157377, fluctuations: 0.0\n",
      "step: 76480 loss: 175.307121 time elapsed: 96.3141 learning rate: 0.001701, scenario: 0, slope: -0.20167469045906466, fluctuations: 0.0\n",
      "step: 76490 loss: 173.207548 time elapsed: 96.3279 learning rate: 0.001701, scenario: 0, slope: -0.19966388384645803, fluctuations: 0.0\n",
      "step: 76500 loss: 171.509385 time elapsed: 96.3415 learning rate: 0.001701, scenario: 0, slope: -0.1977799863521297, fluctuations: 0.0\n",
      "step: 76510 loss: 169.651495 time elapsed: 96.3558 learning rate: 0.001701, scenario: 0, slope: -0.1956387220516778, fluctuations: 0.0\n",
      "step: 76520 loss: 167.993527 time elapsed: 96.3696 learning rate: 0.001701, scenario: 0, slope: -0.1932493211268131, fluctuations: 0.0\n",
      "step: 76530 loss: 166.390067 time elapsed: 96.3840 learning rate: 0.001701, scenario: 0, slope: -0.19009438763889275, fluctuations: 0.0\n",
      "step: 76540 loss: 164.820413 time elapsed: 96.3996 learning rate: 0.001701, scenario: 0, slope: -0.18563106095698242, fluctuations: 0.0\n",
      "step: 76550 loss: 163.291789 time elapsed: 96.4135 learning rate: 0.002264, scenario: 1, slope: -0.18010094496296766, fluctuations: 0.0\n",
      "step: 76560 loss: 262.317594 time elapsed: 96.4263 learning rate: 0.004169, scenario: -1, slope: 0.030112993976690348, fluctuations: 0.02\n",
      "step: 76570 loss: 170.444474 time elapsed: 96.4387 learning rate: 0.001454, scenario: -1, slope: 0.1008301934509744, fluctuations: 0.06\n",
      "step: 76580 loss: 159.338615 time elapsed: 96.4522 learning rate: 0.000507, scenario: -1, slope: 0.0697496390513957, fluctuations: 0.09\n",
      "step: 76590 loss: 158.318055 time elapsed: 96.4653 learning rate: 0.000207, scenario: 1, slope: -0.0026893121020285936, fluctuations: 0.1\n",
      "step: 76600 loss: 157.685398 time elapsed: 96.4777 learning rate: 0.000489, scenario: 1, slope: -0.05325612589288386, fluctuations: 0.12\n",
      "step: 76610 loss: 156.718541 time elapsed: 96.4915 learning rate: 0.001268, scenario: 1, slope: -0.12078884318241286, fluctuations: 0.13\n",
      "step: 76620 loss: 155.072432 time elapsed: 96.5062 learning rate: 0.002470, scenario: 0, slope: -0.1829631628743994, fluctuations: 0.13\n",
      "step: 76630 loss: 153.115622 time elapsed: 96.5212 learning rate: 0.002470, scenario: 0, slope: -0.2509089334110421, fluctuations: 0.13\n",
      "step: 76640 loss: 151.211070 time elapsed: 96.5352 learning rate: 0.002470, scenario: 0, slope: -0.33193524514092765, fluctuations: 0.13\n",
      "step: 76650 loss: 149.363929 time elapsed: 96.5489 learning rate: 0.002470, scenario: 0, slope: -0.43662795325955955, fluctuations: 0.13\n",
      "step: 76660 loss: 147.514031 time elapsed: 96.5626 learning rate: 0.002470, scenario: 0, slope: -0.2508034890127957, fluctuations: 0.11\n",
      "step: 76670 loss: 145.655553 time elapsed: 96.5767 learning rate: 0.002470, scenario: 0, slope: -0.17417633224778117, fluctuations: 0.06\n",
      "step: 76680 loss: 143.777911 time elapsed: 96.5904 learning rate: 0.002470, scenario: 0, slope: -0.16535092024267425, fluctuations: 0.04\n",
      "step: 76690 loss: 141.885959 time elapsed: 96.6045 learning rate: 0.002470, scenario: 0, slope: -0.17615562944950855, fluctuations: 0.02\n",
      "step: 76700 loss: 139.992613 time elapsed: 96.6170 learning rate: 0.002470, scenario: 0, slope: -0.183876249638175, fluctuations: 0.01\n",
      "step: 76710 loss: 138.116332 time elapsed: 96.6296 learning rate: 0.002470, scenario: 0, slope: -0.18761542140264667, fluctuations: 0.0\n",
      "step: 76720 loss: 136.275328 time elapsed: 96.6414 learning rate: 0.002470, scenario: 0, slope: -0.1874272410375843, fluctuations: 0.0\n",
      "step: 76730 loss: 134.483767 time elapsed: 96.6532 learning rate: 0.002470, scenario: 0, slope: -0.18693319583570772, fluctuations: 0.0\n",
      "step: 76740 loss: 132.751376 time elapsed: 96.6648 learning rate: 0.002470, scenario: 0, slope: -0.18622667853098065, fluctuations: 0.0\n",
      "step: 76750 loss: 131.118789 time elapsed: 96.6767 learning rate: 0.002470, scenario: 0, slope: -0.1846202819044935, fluctuations: 0.0\n",
      "step: 76760 loss: 129.498226 time elapsed: 96.6885 learning rate: 0.002470, scenario: 0, slope: -0.1819835864507001, fluctuations: 0.0\n",
      "step: 76770 loss: 127.964318 time elapsed: 96.7016 learning rate: 0.002470, scenario: 0, slope: -0.17837097604501634, fluctuations: 0.0\n",
      "step: 76780 loss: 126.468435 time elapsed: 96.7150 learning rate: 0.002470, scenario: 0, slope: -0.173874333738381, fluctuations: 0.0\n",
      "step: 76790 loss: 125.024198 time elapsed: 96.7287 learning rate: 0.002470, scenario: 0, slope: -0.16874147898555586, fluctuations: 0.0\n",
      "step: 76800 loss: 123.627065 time elapsed: 96.7428 learning rate: 0.002470, scenario: 0, slope: -0.16382708059221077, fluctuations: 0.0\n",
      "step: 76810 loss: 122.274166 time elapsed: 96.7571 learning rate: 0.002470, scenario: 0, slope: -0.15774540678287707, fluctuations: 0.0\n",
      "step: 76820 loss: 120.963238 time elapsed: 96.7709 learning rate: 0.002470, scenario: 0, slope: -0.15236629603118546, fluctuations: 0.0\n",
      "step: 76830 loss: 119.691530 time elapsed: 96.7856 learning rate: 0.002470, scenario: 0, slope: -0.14726707663751562, fluctuations: 0.0\n",
      "step: 76840 loss: 118.456842 time elapsed: 96.8002 learning rate: 0.002470, scenario: 0, slope: -0.14251247274218298, fluctuations: 0.0\n",
      "step: 76850 loss: 117.256838 time elapsed: 96.8141 learning rate: 0.002470, scenario: 0, slope: -0.13790573454704036, fluctuations: 0.0\n",
      "step: 76860 loss: 116.089135 time elapsed: 96.8275 learning rate: 0.002470, scenario: 0, slope: -0.1335357966601998, fluctuations: 0.0\n",
      "step: 76870 loss: 114.951344 time elapsed: 96.8399 learning rate: 0.002470, scenario: 0, slope: -0.12947745342900843, fluctuations: 0.0\n",
      "step: 76880 loss: 113.830184 time elapsed: 96.8529 learning rate: 0.003288, scenario: 1, slope: -0.12568187651409757, fluctuations: 0.0\n",
      "step: 76890 loss: 235.679763 time elapsed: 96.8657 learning rate: 0.008528, scenario: 1, slope: -0.0329389146794572, fluctuations: 0.0\n",
      "step: 76900 loss: 3186.225035 time elapsed: 96.8786 learning rate: 0.003504, scenario: -1, slope: 13.266226246988323, fluctuations: 0.03\n",
      "step: 76910 loss: 1116.849237 time elapsed: 96.8916 learning rate: 0.001222, scenario: -1, slope: 18.196982790084782, fluctuations: 0.06\n",
      "step: 76920 loss: 377.650341 time elapsed: 96.9050 learning rate: 0.000426, scenario: -1, slope: 15.472068828330396, fluctuations: 0.07\n",
      "step: 76930 loss: 334.749220 time elapsed: 96.9199 learning rate: 0.000149, scenario: -1, slope: 11.467582739021278, fluctuations: 0.08\n",
      "step: 76940 loss: 303.140914 time elapsed: 96.9351 learning rate: 0.000052, scenario: -1, slope: 6.750176860833278, fluctuations: 0.08\n",
      "step: 76950 loss: 299.657314 time elapsed: 96.9493 learning rate: 0.000018, scenario: -1, slope: 2.1284277854533733, fluctuations: 0.08\n",
      "step: 76960 loss: 298.151964 time elapsed: 96.9632 learning rate: 0.000012, scenario: 0, slope: -2.6439169435233714, fluctuations: 0.08\n",
      "step: 76970 loss: 296.940523 time elapsed: 96.9769 learning rate: 0.000012, scenario: 0, slope: -7.870451087487851, fluctuations: 0.08\n",
      "step: 76980 loss: 295.878570 time elapsed: 96.9901 learning rate: 0.000012, scenario: 0, slope: -13.93763034573025, fluctuations: 0.08\n",
      "step: 76990 loss: 294.952164 time elapsed: 97.0037 learning rate: 0.000012, scenario: 0, slope: -21.3000747732827, fluctuations: 0.08\n",
      "step: 77000 loss: 294.121120 time elapsed: 97.0167 learning rate: 0.000012, scenario: 0, slope: -7.79128186468856, fluctuations: 0.05\n",
      "step: 77010 loss: 293.351132 time elapsed: 97.0296 learning rate: 0.000012, scenario: 0, slope: -1.6215723859373394, fluctuations: 0.02\n",
      "step: 77020 loss: 292.619425 time elapsed: 97.0417 learning rate: 0.000012, scenario: 0, slope: -0.46656774896139536, fluctuations: 0.01\n",
      "step: 77030 loss: 291.792721 time elapsed: 97.0535 learning rate: 0.000023, scenario: 1, slope: -0.15852305951734763, fluctuations: 0.0\n",
      "step: 77040 loss: 289.877203 time elapsed: 97.0659 learning rate: 0.000060, scenario: 1, slope: -0.10211575588779898, fluctuations: 0.0\n",
      "step: 77050 loss: 285.231275 time elapsed: 97.0776 learning rate: 0.000155, scenario: 1, slope: -0.1063617267962845, fluctuations: 0.0\n",
      "step: 77060 loss: 274.670205 time elapsed: 97.0894 learning rate: 0.000403, scenario: 1, slope: -0.15079524968747812, fluctuations: 0.0\n",
      "step: 77070 loss: 254.047309 time elapsed: 97.1018 learning rate: 0.001045, scenario: 1, slope: -0.2677445290193563, fluctuations: 0.0\n",
      "step: 77080 loss: 231.385028 time elapsed: 97.1149 learning rate: 0.001150, scenario: 0, slope: -0.48112179015118045, fluctuations: 0.0\n",
      "step: 77090 loss: 220.038681 time elapsed: 97.1287 learning rate: 0.001150, scenario: 0, slope: -0.7183872205919991, fluctuations: 0.0\n",
      "step: 77100 loss: 212.651166 time elapsed: 97.1418 learning rate: 0.001150, scenario: 0, slope: -0.905254291221466, fluctuations: 0.0\n",
      "step: 77110 loss: 206.860483 time elapsed: 97.1561 learning rate: 0.001150, scenario: 0, slope: -1.0742497774613091, fluctuations: 0.0\n",
      "step: 77120 loss: 202.172082 time elapsed: 97.1697 learning rate: 0.001150, scenario: 0, slope: -1.1563625671803812, fluctuations: 0.0\n",
      "step: 77130 loss: 198.239434 time elapsed: 97.1838 learning rate: 0.001150, scenario: 0, slope: -1.1597494811263638, fluctuations: 0.0\n",
      "step: 77140 loss: 194.815722 time elapsed: 97.1971 learning rate: 0.001150, scenario: 0, slope: -1.0806343046487905, fluctuations: 0.0\n",
      "step: 77150 loss: 191.755074 time elapsed: 97.2112 learning rate: 0.001150, scenario: 0, slope: -0.9264077787172961, fluctuations: 0.0\n",
      "step: 77160 loss: 188.962666 time elapsed: 97.2244 learning rate: 0.001150, scenario: 0, slope: -0.7236985076100722, fluctuations: 0.0\n",
      "step: 77170 loss: 186.362251 time elapsed: 97.2372 learning rate: 0.001150, scenario: 0, slope: -0.5309570391530843, fluctuations: 0.0\n",
      "step: 77180 loss: 183.879151 time elapsed: 97.2496 learning rate: 0.001150, scenario: 0, slope: -0.41693866596614376, fluctuations: 0.0\n",
      "step: 77190 loss: 181.421630 time elapsed: 97.2622 learning rate: 0.001150, scenario: 0, slope: -0.355400167228546, fluctuations: 0.0\n",
      "step: 77200 loss: 178.845669 time elapsed: 97.2746 learning rate: 0.001150, scenario: 0, slope: -0.3186202455459637, fluctuations: 0.0\n",
      "step: 77210 loss: 175.861067 time elapsed: 97.2878 learning rate: 0.001150, scenario: 0, slope: -0.2899327022685824, fluctuations: 0.0\n",
      "step: 77220 loss: 171.718684 time elapsed: 97.3011 learning rate: 0.001150, scenario: 0, slope: -0.27929177806861927, fluctuations: 0.0\n",
      "step: 77230 loss: 163.959023 time elapsed: 97.3146 learning rate: 0.001150, scenario: 0, slope: -0.2917808332682827, fluctuations: 0.0\n",
      "step: 77240 loss: 145.216090 time elapsed: 97.3299 learning rate: 0.001150, scenario: 0, slope: -0.36147393573374054, fluctuations: 0.0\n",
      "step: 77250 loss: 130.080890 time elapsed: 97.3451 learning rate: 0.001150, scenario: 0, slope: -0.5125823151821711, fluctuations: 0.0\n",
      "step: 77260 loss: 125.561389 time elapsed: 97.3598 learning rate: 0.001150, scenario: 0, slope: -0.6468116954615293, fluctuations: 0.0\n",
      "step: 77270 loss: 123.276073 time elapsed: 97.3748 learning rate: 0.001150, scenario: 0, slope: -0.7444341174021596, fluctuations: 0.0\n",
      "step: 77280 loss: 121.580952 time elapsed: 97.3894 learning rate: 0.001150, scenario: 0, slope: -0.7926555064265705, fluctuations: 0.0\n",
      "step: 77290 loss: 120.345328 time elapsed: 97.4035 learning rate: 0.001150, scenario: 0, slope: -0.7894481133338722, fluctuations: 0.0\n",
      "step: 77300 loss: 119.363824 time elapsed: 97.4173 learning rate: 0.001150, scenario: 0, slope: -0.7423537881944042, fluctuations: 0.0\n",
      "step: 77310 loss: 118.531417 time elapsed: 97.4318 learning rate: 0.001150, scenario: 0, slope: -0.6305281413453823, fluctuations: 0.0\n",
      "step: 77320 loss: 117.798643 time elapsed: 97.4438 learning rate: 0.001150, scenario: 0, slope: -0.48370495047873885, fluctuations: 0.0\n",
      "step: 77330 loss: 117.137990 time elapsed: 97.4561 learning rate: 0.001150, scenario: 0, slope: -0.31164476263219554, fluctuations: 0.0\n",
      "step: 77340 loss: 116.525727 time elapsed: 97.4675 learning rate: 0.001150, scenario: 0, slope: -0.16727770639202894, fluctuations: 0.0\n",
      "step: 77350 loss: 115.913670 time elapsed: 97.4800 learning rate: 0.001851, scenario: 1, slope: -0.11586092217145022, fluctuations: 0.0\n",
      "step: 77360 loss: 114.661111 time elapsed: 97.4918 learning rate: 0.004802, scenario: 1, slope: -0.0912221716850555, fluctuations: 0.0\n",
      "step: 77370 loss: 111.749126 time elapsed: 97.5037 learning rate: 0.012455, scenario: 1, slope: -0.088265244791523, fluctuations: 0.0\n",
      "step: 77380 loss: 341.725882 time elapsed: 97.5156 learning rate: 0.028033, scenario: -1, slope: 0.0332151607416488, fluctuations: 0.0\n",
      "step: 77390 loss: 76366.848961 time elapsed: 97.5280 learning rate: 0.009774, scenario: -1, slope: 382.10046316247394, fluctuations: 0.03\n",
      "step: 77400 loss: 52409.169717 time elapsed: 97.5411 learning rate: 0.003787, scenario: -1, slope: 625.8651808053187, fluctuations: 0.04\n",
      "step: 77410 loss: 39831.149346 time elapsed: 97.5553 learning rate: 0.001320, scenario: -1, slope: 716.0544592174191, fluctuations: 0.04\n",
      "step: 77420 loss: 35526.118258 time elapsed: 97.5690 learning rate: 0.000460, scenario: -1, slope: 698.3194749594917, fluctuations: 0.04\n",
      "step: 77430 loss: 34199.433154 time elapsed: 97.5826 learning rate: 0.000161, scenario: -1, slope: 626.724937009775, fluctuations: 0.04\n",
      "step: 77440 loss: 33791.311617 time elapsed: 97.5963 learning rate: 0.000056, scenario: -1, slope: 513.3474285572781, fluctuations: 0.04\n",
      "step: 77450 loss: 33654.010222 time elapsed: 97.6100 learning rate: 0.000020, scenario: -1, slope: 358.4180943256075, fluctuations: 0.04\n",
      "step: 77460 loss: 33607.496184 time elapsed: 97.6234 learning rate: 0.000007, scenario: -1, slope: 157.07691407668972, fluctuations: 0.04\n",
      "step: 77470 loss: 33591.227960 time elapsed: 97.6386 learning rate: 0.000004, scenario: 0, slope: -99.60292731615407, fluctuations: 0.04\n",
      "step: 77480 loss: 33579.697107 time elapsed: 97.6518 learning rate: 0.000004, scenario: 0, slope: -425.34600079419437, fluctuations: 0.04\n",
      "step: 77490 loss: 33568.296364 time elapsed: 97.6639 learning rate: 0.000004, scenario: 0, slope: -237.25554231058095, fluctuations: 0.0\n",
      "step: 77500 loss: 33557.010696 time elapsed: 97.6759 learning rate: 0.000004, scenario: 0, slope: -86.2554971126672, fluctuations: 0.0\n",
      "step: 77510 loss: 33545.114642 time elapsed: 97.6889 learning rate: 0.000006, scenario: 1, slope: -25.97661950779319, fluctuations: 0.0\n",
      "step: 77520 loss: 33519.269389 time elapsed: 97.7010 learning rate: 0.000015, scenario: 1, slope: -8.478834941450064, fluctuations: 0.0\n",
      "step: 77530 loss: 33452.872167 time elapsed: 97.7134 learning rate: 0.000039, scenario: 1, slope: -3.514580134511317, fluctuations: 0.0\n",
      "step: 77540 loss: 33282.771801 time elapsed: 97.7255 learning rate: 0.000102, scenario: 1, slope: -2.674567788999648, fluctuations: 0.0\n",
      "step: 77550 loss: 32849.394942 time elapsed: 97.7382 learning rate: 0.000264, scenario: 1, slope: -4.352185668537684, fluctuations: 0.0\n",
      "step: 77560 loss: 31746.228048 time elapsed: 97.7537 learning rate: 0.000684, scenario: 1, slope: -9.929993173874559, fluctuations: 0.0\n",
      "step: 77570 loss: 28735.887501 time elapsed: 97.7685 learning rate: 0.001773, scenario: 1, slope: -24.96126790828834, fluctuations: 0.0\n",
      "step: 77580 loss: 20236.800272 time elapsed: 97.7834 learning rate: 0.002146, scenario: 0, slope: -69.45598589810578, fluctuations: 0.0\n",
      "step: 77590 loss: 14802.784366 time elapsed: 97.7982 learning rate: 0.002146, scenario: 0, slope: -142.84272344301257, fluctuations: 0.0\n",
      "step: 77600 loss: 12160.617094 time elapsed: 97.8128 learning rate: 0.002146, scenario: 0, slope: -208.37581393790194, fluctuations: 0.0\n",
      "step: 77610 loss: 10165.561517 time elapsed: 97.8278 learning rate: 0.002146, scenario: 0, slope: -275.4881694078703, fluctuations: 0.0\n",
      "step: 77620 loss: 8698.579195 time elapsed: 97.8422 learning rate: 0.002146, scenario: 0, slope: -317.88816023654346, fluctuations: 0.0\n",
      "step: 77630 loss: 7551.577171 time elapsed: 97.8561 learning rate: 0.002146, scenario: 0, slope: -338.34861043127785, fluctuations: 0.0\n",
      "step: 77640 loss: 6563.252034 time elapsed: 97.8696 learning rate: 0.002146, scenario: 0, slope: -334.7203932555594, fluctuations: 0.0\n",
      "step: 77650 loss: 5757.754697 time elapsed: 97.8822 learning rate: 0.002146, scenario: 0, slope: -306.1674904764839, fluctuations: 0.0\n",
      "step: 77660 loss: 5192.788641 time elapsed: 97.8948 learning rate: 0.002146, scenario: 0, slope: -253.71511901626877, fluctuations: 0.0\n",
      "step: 77670 loss: 4734.430565 time elapsed: 97.9069 learning rate: 0.002146, scenario: 0, slope: -184.45118412626076, fluctuations: 0.0\n",
      "step: 77680 loss: 4318.176932 time elapsed: 97.9188 learning rate: 0.002146, scenario: 0, slope: -126.3443502775054, fluctuations: 0.0\n",
      "step: 77690 loss: 3935.730564 time elapsed: 97.9304 learning rate: 0.002146, scenario: 0, slope: -96.3227200195183, fluctuations: 0.0\n",
      "step: 77700 loss: 3608.044710 time elapsed: 97.9421 learning rate: 0.002146, scenario: 0, slope: -78.36826211637324, fluctuations: 0.0\n",
      "step: 77710 loss: 3342.509444 time elapsed: 97.9541 learning rate: 0.002146, scenario: 0, slope: -62.24002582677955, fluctuations: 0.0\n",
      "step: 77720 loss: 3104.374872 time elapsed: 97.9682 learning rate: 0.002146, scenario: 0, slope: -51.25222098022531, fluctuations: 0.0\n",
      "step: 77730 loss: 2889.321402 time elapsed: 97.9818 learning rate: 0.002146, scenario: 0, slope: -42.55086123491847, fluctuations: 0.0\n",
      "step: 77740 loss: 2681.858592 time elapsed: 97.9951 learning rate: 0.002146, scenario: 0, slope: -35.89356853498949, fluctuations: 0.0\n",
      "step: 77750 loss: 2495.899504 time elapsed: 98.0087 learning rate: 0.002146, scenario: 0, slope: -31.156347317398104, fluctuations: 0.0\n",
      "step: 77760 loss: 2343.658260 time elapsed: 98.0223 learning rate: 0.002146, scenario: 0, slope: -27.531871708638604, fluctuations: 0.0\n",
      "step: 77770 loss: 2214.096138 time elapsed: 98.0361 learning rate: 0.002146, scenario: 0, slope: -24.304545478207356, fluctuations: 0.0\n",
      "step: 77780 loss: 2101.478748 time elapsed: 98.0497 learning rate: 0.002146, scenario: 0, slope: -21.393289470620314, fluctuations: 0.0\n",
      "step: 77790 loss: 2003.514517 time elapsed: 98.0628 learning rate: 0.002146, scenario: 0, slope: -18.852312663602838, fluctuations: 0.0\n",
      "step: 77800 loss: 1919.081904 time elapsed: 98.0751 learning rate: 0.002146, scenario: 0, slope: -16.881597527094158, fluctuations: 0.0\n",
      "step: 77810 loss: 1846.390198 time elapsed: 98.0876 learning rate: 0.002146, scenario: 0, slope: -14.696642142137968, fluctuations: 0.0\n",
      "step: 77820 loss: 1783.394835 time elapsed: 98.0996 learning rate: 0.002146, scenario: 0, slope: -12.825592432438754, fluctuations: 0.0\n",
      "step: 77830 loss: 1728.047299 time elapsed: 98.1118 learning rate: 0.002146, scenario: 0, slope: -11.074327262039757, fluctuations: 0.0\n",
      "step: 77840 loss: 1677.263299 time elapsed: 98.1236 learning rate: 0.002146, scenario: 0, slope: -9.519889413303982, fluctuations: 0.0\n",
      "step: 77850 loss: 1627.212995 time elapsed: 98.1355 learning rate: 0.002146, scenario: 0, slope: -8.250865637994409, fluctuations: 0.0\n",
      "step: 77860 loss: 1573.437418 time elapsed: 98.1479 learning rate: 0.002146, scenario: 0, slope: -7.260471559098201, fluctuations: 0.0\n",
      "step: 77870 loss: 1527.003308 time elapsed: 98.1604 learning rate: 0.002146, scenario: 0, slope: -6.487204020346008, fluctuations: 0.0\n",
      "step: 77880 loss: 1482.486368 time elapsed: 98.1731 learning rate: 0.002146, scenario: 0, slope: -5.88066724511029, fluctuations: 0.0\n",
      "step: 77890 loss: 1442.783972 time elapsed: 98.1875 learning rate: 0.002146, scenario: 0, slope: -5.409654703402763, fluctuations: 0.0\n",
      "step: 77900 loss: 1402.747135 time elapsed: 98.2025 learning rate: 0.002146, scenario: 0, slope: -5.081136236849328, fluctuations: 0.0\n",
      "step: 77910 loss: 1364.544495 time elapsed: 98.2180 learning rate: 0.002146, scenario: 0, slope: -4.770891710003516, fluctuations: 0.0\n",
      "step: 77920 loss: 1302.485329 time elapsed: 98.2326 learning rate: 0.002146, scenario: 0, slope: -4.5844034107439064, fluctuations: 0.0\n",
      "step: 77930 loss: 1192.830643 time elapsed: 98.2475 learning rate: 0.002146, scenario: 0, slope: -4.686111364950071, fluctuations: 0.03\n",
      "step: 77940 loss: 1124.952046 time elapsed: 98.2621 learning rate: 0.002146, scenario: 0, slope: -5.05092824847238, fluctuations: 0.04\n",
      "step: 77950 loss: 1069.938501 time elapsed: 98.2770 learning rate: 0.002146, scenario: 0, slope: -5.4369703777407405, fluctuations: 0.04\n",
      "step: 77960 loss: 1028.173335 time elapsed: 98.2902 learning rate: 0.002146, scenario: 0, slope: -5.736022517829324, fluctuations: 0.04\n",
      "step: 77970 loss: 994.194287 time elapsed: 98.3028 learning rate: 0.002146, scenario: 0, slope: -5.8982002985936655, fluctuations: 0.04\n",
      "step: 77980 loss: 965.152462 time elapsed: 98.3149 learning rate: 0.002146, scenario: 0, slope: -5.882563045265399, fluctuations: 0.04\n",
      "step: 77990 loss: 939.133736 time elapsed: 98.3269 learning rate: 0.002146, scenario: 0, slope: -5.663901504967714, fluctuations: 0.04\n",
      "step: 78000 loss: 908.021008 time elapsed: 98.3385 learning rate: 0.002146, scenario: 0, slope: -5.336537092189075, fluctuations: 0.05\n",
      "step: 78010 loss: 819.502717 time elapsed: 98.3508 learning rate: 0.002146, scenario: 0, slope: -4.9435956066420825, fluctuations: 0.09\n",
      "step: 78020 loss: 800.905941 time elapsed: 98.3628 learning rate: 0.002146, scenario: 0, slope: -4.323411918675418, fluctuations: 0.13\n",
      "step: 78030 loss: 775.625795 time elapsed: 98.3755 learning rate: 0.002146, scenario: 0, slope: -3.973442316667916, fluctuations: 0.12\n",
      "step: 78040 loss: 757.618213 time elapsed: 98.3892 learning rate: 0.002146, scenario: 0, slope: -3.717361365713278, fluctuations: 0.11\n",
      "step: 78050 loss: 741.348646 time elapsed: 98.4031 learning rate: 0.002146, scenario: 0, slope: -3.503043033737917, fluctuations: 0.11\n",
      "step: 78060 loss: 726.246958 time elapsed: 98.4166 learning rate: 0.002146, scenario: 0, slope: -3.28951311200915, fluctuations: 0.11\n",
      "step: 78070 loss: 713.934556 time elapsed: 98.4299 learning rate: 0.002146, scenario: 0, slope: -3.0250684635666185, fluctuations: 0.11\n",
      "step: 78080 loss: 699.007047 time elapsed: 98.4433 learning rate: 0.002146, scenario: 0, slope: -2.6871057433369865, fluctuations: 0.11\n",
      "step: 78090 loss: 685.385897 time elapsed: 98.4570 learning rate: 0.002146, scenario: 0, slope: -2.225186542797595, fluctuations: 0.11\n",
      "step: 78100 loss: 672.636177 time elapsed: 98.4706 learning rate: 0.002146, scenario: 0, slope: -1.7934026590043726, fluctuations: 0.09\n",
      "step: 78110 loss: 660.266918 time elapsed: 98.4841 learning rate: 0.002146, scenario: 0, slope: -1.5442900254138077, fluctuations: 0.05\n",
      "step: 78120 loss: 648.261905 time elapsed: 98.4968 learning rate: 0.002146, scenario: 0, slope: -1.4300500522798476, fluctuations: 0.02\n",
      "step: 78130 loss: 636.853768 time elapsed: 98.5090 learning rate: 0.002146, scenario: 0, slope: -1.3650520550773655, fluctuations: 0.0\n",
      "step: 78140 loss: 626.154727 time elapsed: 98.5214 learning rate: 0.002146, scenario: 0, slope: -1.3029869938281635, fluctuations: 0.0\n",
      "step: 78150 loss: 614.101298 time elapsed: 98.5335 learning rate: 0.002146, scenario: 0, slope: -1.2592436143382335, fluctuations: 0.0\n",
      "step: 78160 loss: 603.036287 time elapsed: 98.5464 learning rate: 0.002146, scenario: 0, slope: -1.2245895485340268, fluctuations: 0.0\n",
      "step: 78170 loss: 592.585107 time elapsed: 98.5594 learning rate: 0.002146, scenario: 0, slope: -1.1879185027641965, fluctuations: 0.0\n",
      "step: 78180 loss: 582.207709 time elapsed: 98.5722 learning rate: 0.002146, scenario: 0, slope: -1.1565633007726563, fluctuations: 0.0\n",
      "step: 78190 loss: 572.025259 time elapsed: 98.5872 learning rate: 0.002146, scenario: 0, slope: -1.1291638655919942, fluctuations: 0.0\n",
      "step: 78200 loss: 562.032442 time elapsed: 98.6024 learning rate: 0.002146, scenario: 0, slope: -1.1070839047232128, fluctuations: 0.0\n",
      "step: 78210 loss: 554.117267 time elapsed: 98.6173 learning rate: 0.002146, scenario: 0, slope: -1.0769837576703856, fluctuations: 0.0\n",
      "step: 78220 loss: 542.874023 time elapsed: 98.6318 learning rate: 0.002146, scenario: 0, slope: -1.0534587832579143, fluctuations: 0.0\n",
      "step: 78230 loss: 533.912836 time elapsed: 98.6465 learning rate: 0.002146, scenario: 0, slope: -1.0313710377356675, fluctuations: 0.0\n",
      "step: 78240 loss: 525.012455 time elapsed: 98.6612 learning rate: 0.002146, scenario: 0, slope: -1.0015525183264866, fluctuations: 0.0\n",
      "step: 78250 loss: 516.553291 time elapsed: 98.6755 learning rate: 0.002146, scenario: 0, slope: -0.974923786565722, fluctuations: 0.0\n",
      "step: 78260 loss: 508.382327 time elapsed: 98.6895 learning rate: 0.002146, scenario: 0, slope: -0.9490841217777408, fluctuations: 0.0\n",
      "step: 78270 loss: 500.530902 time elapsed: 98.7025 learning rate: 0.002146, scenario: 0, slope: -0.9227299084155616, fluctuations: 0.0\n",
      "step: 78280 loss: 492.938463 time elapsed: 98.7147 learning rate: 0.002146, scenario: 0, slope: -0.8951770014045082, fluctuations: 0.0\n",
      "step: 78290 loss: 485.581277 time elapsed: 98.7268 learning rate: 0.002146, scenario: 0, slope: -0.8667235922240887, fluctuations: 0.0\n",
      "step: 78300 loss: 478.542218 time elapsed: 98.7386 learning rate: 0.002146, scenario: 0, slope: -0.8417046379557287, fluctuations: 0.0\n",
      "step: 78310 loss: 471.611080 time elapsed: 98.7511 learning rate: 0.002146, scenario: 0, slope: -0.8001699130125963, fluctuations: 0.01\n",
      "step: 78320 loss: 464.962438 time elapsed: 98.7629 learning rate: 0.002146, scenario: 0, slope: -0.768601588129457, fluctuations: 0.02\n",
      "step: 78330 loss: 458.530576 time elapsed: 98.7749 learning rate: 0.002146, scenario: 0, slope: -0.7415693225228543, fluctuations: 0.02\n",
      "step: 78340 loss: 452.160858 time elapsed: 98.7869 learning rate: 0.002146, scenario: 0, slope: -0.7189560667356717, fluctuations: 0.02\n",
      "step: 78350 loss: 445.927686 time elapsed: 98.8000 learning rate: 0.002146, scenario: 0, slope: -0.6988805050122657, fluctuations: 0.02\n",
      "step: 78360 loss: 439.844405 time elapsed: 98.8142 learning rate: 0.002146, scenario: 0, slope: -0.680990058554315, fluctuations: 0.02\n",
      "step: 78370 loss: 433.901800 time elapsed: 98.8280 learning rate: 0.002146, scenario: 0, slope: -0.6648677334768776, fluctuations: 0.02\n",
      "step: 78380 loss: 428.081499 time elapsed: 98.8418 learning rate: 0.002146, scenario: 0, slope: -0.6502387582376099, fluctuations: 0.02\n",
      "step: 78390 loss: 422.389703 time elapsed: 98.8555 learning rate: 0.002146, scenario: 0, slope: -0.6369689865283904, fluctuations: 0.02\n",
      "step: 78400 loss: 417.130088 time elapsed: 98.8691 learning rate: 0.002146, scenario: 0, slope: -0.6256320861164073, fluctuations: 0.02\n",
      "step: 78410 loss: 411.449487 time elapsed: 98.8829 learning rate: 0.002146, scenario: 0, slope: -0.6034247774086761, fluctuations: 0.01\n",
      "step: 78420 loss: 406.125548 time elapsed: 98.8972 learning rate: 0.002146, scenario: 0, slope: -0.5879492189882061, fluctuations: 0.0\n",
      "step: 78430 loss: 400.878887 time elapsed: 98.9112 learning rate: 0.002146, scenario: 0, slope: -0.5742904253980201, fluctuations: 0.0\n",
      "step: 78440 loss: 396.130081 time elapsed: 98.9242 learning rate: 0.002146, scenario: 0, slope: -0.5612463963229751, fluctuations: 0.0\n",
      "step: 78450 loss: 390.781711 time elapsed: 98.9371 learning rate: 0.002146, scenario: 0, slope: -0.5472134838425289, fluctuations: 0.0\n",
      "step: 78460 loss: 385.871196 time elapsed: 98.9503 learning rate: 0.002146, scenario: 0, slope: -0.5359434903607042, fluctuations: 0.0\n",
      "step: 78470 loss: 380.947578 time elapsed: 98.9635 learning rate: 0.002146, scenario: 0, slope: -0.5260039981771617, fluctuations: 0.0\n",
      "step: 78480 loss: 376.030860 time elapsed: 98.9759 learning rate: 0.002146, scenario: 0, slope: -0.5173774258254629, fluctuations: 0.0\n",
      "step: 78490 loss: 371.105065 time elapsed: 98.9886 learning rate: 0.002146, scenario: 0, slope: -0.5102880531173025, fluctuations: 0.0\n",
      "step: 78500 loss: 366.128165 time elapsed: 99.0027 learning rate: 0.002146, scenario: 0, slope: -0.5051351890467282, fluctuations: 0.0\n",
      "step: 78510 loss: 361.022686 time elapsed: 99.0182 learning rate: 0.002146, scenario: 0, slope: -0.5005307415652678, fluctuations: 0.0\n",
      "step: 78520 loss: 355.890478 time elapsed: 99.0328 learning rate: 0.002146, scenario: 0, slope: -0.49949176508496734, fluctuations: 0.0\n",
      "step: 78530 loss: 349.963346 time elapsed: 99.0475 learning rate: 0.002146, scenario: 0, slope: -0.5028708296912993, fluctuations: 0.0\n",
      "step: 78540 loss: 343.921702 time elapsed: 99.0622 learning rate: 0.002146, scenario: 0, slope: -0.5114702368487263, fluctuations: 0.0\n",
      "step: 78550 loss: 337.361460 time elapsed: 99.0770 learning rate: 0.002146, scenario: 0, slope: -0.5237359805005423, fluctuations: 0.0\n",
      "step: 78560 loss: 330.442077 time elapsed: 99.0912 learning rate: 0.002146, scenario: 0, slope: -0.5437174367468207, fluctuations: 0.0\n",
      "step: 78570 loss: 323.406947 time elapsed: 99.1054 learning rate: 0.002146, scenario: 0, slope: -0.5691063448278072, fluctuations: 0.0\n",
      "step: 78580 loss: 316.615787 time elapsed: 99.1193 learning rate: 0.002146, scenario: 0, slope: -0.5965233049444462, fluctuations: 0.0\n",
      "step: 78590 loss: 310.452474 time elapsed: 99.1318 learning rate: 0.002146, scenario: 0, slope: -0.6208907880408939, fluctuations: 0.0\n",
      "step: 78600 loss: 305.109012 time elapsed: 99.1438 learning rate: 0.002146, scenario: 0, slope: -0.6355278643160235, fluctuations: 0.0\n",
      "step: 78610 loss: 300.554863 time elapsed: 99.1562 learning rate: 0.002146, scenario: 0, slope: -0.6387563820585209, fluctuations: 0.0\n",
      "step: 78620 loss: 296.622772 time elapsed: 99.1681 learning rate: 0.002146, scenario: 0, slope: -0.6245028256676928, fluctuations: 0.0\n",
      "step: 78630 loss: 293.127534 time elapsed: 99.1797 learning rate: 0.002146, scenario: 0, slope: -0.5943551522693898, fluctuations: 0.0\n",
      "step: 78640 loss: 289.924965 time elapsed: 99.1916 learning rate: 0.002146, scenario: 0, slope: -0.5521804811872797, fluctuations: 0.0\n",
      "step: 78650 loss: 286.920196 time elapsed: 99.2031 learning rate: 0.002146, scenario: 0, slope: -0.5021998257326507, fluctuations: 0.0\n",
      "step: 78660 loss: 284.054944 time elapsed: 99.2167 learning rate: 0.002146, scenario: 0, slope: -0.45001453285418547, fluctuations: 0.0\n",
      "step: 78670 loss: 281.293463 time elapsed: 99.2309 learning rate: 0.002146, scenario: 0, slope: -0.4013555892216686, fluctuations: 0.0\n",
      "step: 78680 loss: 278.612749 time elapsed: 99.2445 learning rate: 0.002146, scenario: 0, slope: -0.36029876150915846, fluctuations: 0.0\n",
      "step: 78690 loss: 275.996959 time elapsed: 99.2582 learning rate: 0.002146, scenario: 0, slope: -0.3285043877616397, fluctuations: 0.0\n",
      "step: 78700 loss: 273.436348 time elapsed: 99.2713 learning rate: 0.002146, scenario: 0, slope: -0.3072825864049889, fluctuations: 0.0\n",
      "step: 78710 loss: 654.638170 time elapsed: 99.2855 learning rate: 0.003233, scenario: -1, slope: 0.3247629783003232, fluctuations: 0.01\n",
      "step: 78720 loss: 458.402545 time elapsed: 99.2997 learning rate: 0.001127, scenario: -1, slope: 1.0691853303110568, fluctuations: 0.05\n",
      "step: 78730 loss: 323.416279 time elapsed: 99.3155 learning rate: 0.000393, scenario: -1, slope: 1.1693970361806876, fluctuations: 0.07\n",
      "step: 78740 loss: 288.612775 time elapsed: 99.3298 learning rate: 0.000137, scenario: -1, slope: 0.9181904995749108, fluctuations: 0.08\n",
      "step: 78750 loss: 282.555957 time elapsed: 99.3451 learning rate: 0.000048, scenario: -1, slope: 0.5651850425128035, fluctuations: 0.08\n",
      "step: 78760 loss: 281.186485 time elapsed: 99.3608 learning rate: 0.000017, scenario: -1, slope: 0.23055495774012205, fluctuations: 0.08\n",
      "step: 78770 loss: 280.697186 time elapsed: 99.3774 learning rate: 0.000012, scenario: 1, slope: -0.10488933110668086, fluctuations: 0.08\n",
      "step: 78780 loss: 280.317351 time elapsed: 99.3931 learning rate: 0.000020, scenario: 0, slope: -0.4622791606647097, fluctuations: 0.08\n",
      "step: 78790 loss: 280.060227 time elapsed: 99.4055 learning rate: 0.000020, scenario: 0, slope: -0.8689899704509797, fluctuations: 0.08\n",
      "step: 78800 loss: 279.827891 time elapsed: 99.4185 learning rate: 0.000020, scenario: 0, slope: -1.3074837669107335, fluctuations: 0.08\n",
      "step: 78810 loss: 279.592764 time elapsed: 99.4345 learning rate: 0.000020, scenario: 0, slope: -1.7657132983314465, fluctuations: 0.06\n",
      "step: 78820 loss: 279.366702 time elapsed: 99.4495 learning rate: 0.000020, scenario: 0, slope: -0.36982643096439183, fluctuations: 0.03\n",
      "step: 78830 loss: 279.050471 time elapsed: 99.4644 learning rate: 0.000052, scenario: 1, slope: -0.10756698670993696, fluctuations: 0.01\n",
      "step: 78840 loss: 278.277560 time elapsed: 99.4793 learning rate: 0.000135, scenario: 1, slope: -0.043222296529456236, fluctuations: 0.0\n",
      "step: 78850 loss: 276.521688 time elapsed: 99.4937 learning rate: 0.000350, scenario: 1, slope: -0.038426165914058945, fluctuations: 0.0\n",
      "step: 78860 loss: 273.215905 time elapsed: 99.5073 learning rate: 0.000907, scenario: 1, slope: -0.05215235622532134, fluctuations: 0.0\n",
      "step: 78870 loss: 268.734316 time elapsed: 99.5211 learning rate: 0.002352, scenario: 1, slope: -0.08542838288797498, fluctuations: 0.0\n",
      "step: 78880 loss: 263.553679 time elapsed: 99.5366 learning rate: 0.006101, scenario: 1, slope: -0.13521966630640225, fluctuations: 0.0\n",
      "step: 78890 loss: 257.352859 time elapsed: 99.5491 learning rate: 0.015824, scenario: 1, slope: -0.2075929954455412, fluctuations: 0.0\n",
      "step: 78900 loss: 25031.024690 time elapsed: 99.5619 learning rate: 0.007947, scenario: -1, slope: 114.97687251746508, fluctuations: 0.02\n",
      "step: 78910 loss: 7576.919877 time elapsed: 99.5749 learning rate: 0.002771, scenario: -1, slope: 144.26143038601816, fluctuations: 0.05\n",
      "step: 78920 loss: 4325.249450 time elapsed: 99.5880 learning rate: 0.000966, scenario: -1, slope: 132.5123714427459, fluctuations: 0.06\n",
      "step: 78930 loss: 3776.696788 time elapsed: 99.6001 learning rate: 0.000337, scenario: -1, slope: 106.1030732675397, fluctuations: 0.06\n",
      "step: 78940 loss: 3521.372224 time elapsed: 99.6134 learning rate: 0.000117, scenario: -1, slope: 75.73114484566143, fluctuations: 0.06\n",
      "step: 78950 loss: 3449.679098 time elapsed: 99.6267 learning rate: 0.000041, scenario: -1, slope: 42.470852135570915, fluctuations: 0.06\n",
      "step: 78960 loss: 3426.482442 time elapsed: 99.6408 learning rate: 0.000014, scenario: -1, slope: 5.548118974885517, fluctuations: 0.06\n",
      "step: 78970 loss: 3416.487294 time elapsed: 99.6540 learning rate: 0.000013, scenario: 0, slope: -36.63608278232437, fluctuations: 0.06\n",
      "step: 78980 loss: 3407.078116 time elapsed: 99.6670 learning rate: 0.000013, scenario: 0, slope: -86.43080405945086, fluctuations: 0.06\n",
      "step: 78990 loss: 3397.880829 time elapsed: 99.6810 learning rate: 0.000013, scenario: 0, slope: -147.29438167440503, fluctuations: 0.06\n",
      "step: 79000 loss: 3388.863723 time elapsed: 99.6946 learning rate: 0.000013, scenario: 0, slope: -55.22263588554302, fluctuations: 0.03\n",
      "step: 79010 loss: 3379.999951 time elapsed: 99.7089 learning rate: 0.000013, scenario: 0, slope: -12.921355451289681, fluctuations: 0.01\n",
      "step: 79020 loss: 3371.267571 time elapsed: 99.7223 learning rate: 0.000013, scenario: 0, slope: -4.870101879418416, fluctuations: 0.0\n",
      "step: 79030 loss: 3359.706356 time elapsed: 99.7363 learning rate: 0.000030, scenario: 1, slope: -1.9738068629453251, fluctuations: 0.0\n",
      "step: 79040 loss: 3330.956455 time elapsed: 99.7493 learning rate: 0.000079, scenario: 1, slope: -1.1981153174350656, fluctuations: 0.0\n",
      "step: 79050 loss: 3259.680106 time elapsed: 99.7627 learning rate: 0.000204, scenario: 1, slope: -1.268313802516873, fluctuations: 0.0\n",
      "step: 79060 loss: 3090.796235 time elapsed: 99.7757 learning rate: 0.000529, scenario: 1, slope: -2.046817016751492, fluctuations: 0.0\n",
      "step: 79070 loss: 2742.355468 time elapsed: 99.7884 learning rate: 0.001031, scenario: 0, slope: -4.047133553673681, fluctuations: 0.0\n",
      "step: 79080 loss: 2396.564335 time elapsed: 99.8018 learning rate: 0.001031, scenario: 0, slope: -7.515204778887459, fluctuations: 0.0\n",
      "step: 79090 loss: 2143.970084 time elapsed: 99.8150 learning rate: 0.001031, scenario: 0, slope: -11.568653446792315, fluctuations: 0.0\n",
      "step: 79100 loss: 1952.742773 time elapsed: 99.8284 learning rate: 0.001031, scenario: 0, slope: -15.096960724150692, fluctuations: 0.0\n",
      "step: 79110 loss: 1796.445502 time elapsed: 99.8440 learning rate: 0.001031, scenario: 0, slope: -18.703704909610273, fluctuations: 0.0\n",
      "step: 79120 loss: 1660.112666 time elapsed: 99.8575 learning rate: 0.001031, scenario: 0, slope: -20.958340537712775, fluctuations: 0.0\n",
      "step: 79130 loss: 1549.436852 time elapsed: 99.8722 learning rate: 0.001031, scenario: 0, slope: -21.95368924101794, fluctuations: 0.0\n",
      "step: 79140 loss: 1466.184079 time elapsed: 99.8861 learning rate: 0.001031, scenario: 0, slope: -21.460442496814974, fluctuations: 0.0\n",
      "step: 79150 loss: 1399.157141 time elapsed: 99.8998 learning rate: 0.001031, scenario: 0, slope: -19.47071768974084, fluctuations: 0.0\n",
      "step: 79160 loss: 1342.044652 time elapsed: 99.9138 learning rate: 0.001031, scenario: 0, slope: -16.34229900700306, fluctuations: 0.0\n",
      "step: 79170 loss: 1292.459493 time elapsed: 99.9271 learning rate: 0.001031, scenario: 0, slope: -13.038639075570284, fluctuations: 0.0\n",
      "step: 79180 loss: 1248.548933 time elapsed: 99.9409 learning rate: 0.001031, scenario: 0, slope: -10.525343402148387, fluctuations: 0.0\n",
      "step: 79190 loss: 1208.981036 time elapsed: 99.9541 learning rate: 0.001031, scenario: 0, slope: -8.639354885126325, fluctuations: 0.0\n",
      "step: 79200 loss: 1172.803557 time elapsed: 99.9659 learning rate: 0.001031, scenario: 0, slope: -7.293817590700573, fluctuations: 0.0\n",
      "step: 79210 loss: 1139.516863 time elapsed: 99.9783 learning rate: 0.001031, scenario: 0, slope: -5.979098015300183, fluctuations: 0.0\n",
      "step: 79220 loss: 1109.172224 time elapsed: 99.9904 learning rate: 0.001031, scenario: 0, slope: -5.073391017396087, fluctuations: 0.0\n",
      "step: 79230 loss: 1081.923866 time elapsed: 100.0024 learning rate: 0.001031, scenario: 0, slope: -4.423242584123069, fluctuations: 0.0\n",
      "step: 79240 loss: 1057.240045 time elapsed: 100.0139 learning rate: 0.001031, scenario: 0, slope: -3.9344738681377898, fluctuations: 0.0\n",
      "step: 79250 loss: 1034.356692 time elapsed: 100.0253 learning rate: 0.001031, scenario: 0, slope: -3.540846472642243, fluctuations: 0.0\n",
      "step: 79260 loss: 1012.945610 time elapsed: 100.0389 learning rate: 0.001031, scenario: 0, slope: -3.2112421497219112, fluctuations: 0.0\n",
      "step: 79270 loss: 992.833697 time elapsed: 100.0530 learning rate: 0.001031, scenario: 0, slope: -2.9287327329767296, fluctuations: 0.0\n",
      "step: 79280 loss: 973.841714 time elapsed: 100.0668 learning rate: 0.001031, scenario: 0, slope: -2.6828046544926596, fluctuations: 0.0\n",
      "step: 79290 loss: 955.796203 time elapsed: 100.0806 learning rate: 0.001031, scenario: 0, slope: -2.467575548654613, fluctuations: 0.0\n",
      "step: 79300 loss: 938.441956 time elapsed: 100.0943 learning rate: 0.001031, scenario: 0, slope: -2.2984375065624265, fluctuations: 0.0\n",
      "step: 79310 loss: 921.056185 time elapsed: 100.1087 learning rate: 0.001031, scenario: 0, slope: -2.1254034133966253, fluctuations: 0.0\n",
      "step: 79320 loss: 905.470642 time elapsed: 100.1221 learning rate: 0.001031, scenario: 0, slope: -1.997566697078733, fluctuations: 0.0\n",
      "step: 79330 loss: 890.802208 time elapsed: 100.1369 learning rate: 0.001031, scenario: 0, slope: -1.8882652126661739, fluctuations: 0.0\n",
      "step: 79340 loss: 876.933580 time elapsed: 100.1505 learning rate: 0.001031, scenario: 0, slope: -1.789952435497039, fluctuations: 0.0\n",
      "step: 79350 loss: 863.605217 time elapsed: 100.1641 learning rate: 0.001031, scenario: 0, slope: -1.6990035628245916, fluctuations: 0.0\n",
      "step: 79360 loss: 850.706764 time elapsed: 100.1771 learning rate: 0.001031, scenario: 0, slope: -1.6144230593316522, fluctuations: 0.0\n",
      "step: 79370 loss: 838.149356 time elapsed: 100.1895 learning rate: 0.001031, scenario: 0, slope: -1.5358573920729117, fluctuations: 0.0\n",
      "step: 79380 loss: 825.859612 time elapsed: 100.2029 learning rate: 0.001031, scenario: 0, slope: -1.4633349946046532, fluctuations: 0.0\n",
      "step: 79390 loss: 813.785675 time elapsed: 100.2160 learning rate: 0.001031, scenario: 0, slope: -1.397281501973773, fluctuations: 0.0\n",
      "step: 79400 loss: 801.887622 time elapsed: 100.2288 learning rate: 0.001031, scenario: 0, slope: -1.3442021685140115, fluctuations: 0.0\n",
      "step: 79410 loss: 790.139177 time elapsed: 100.2423 learning rate: 0.001031, scenario: 0, slope: -1.2914259180463399, fluctuations: 0.0\n",
      "step: 79420 loss: 778.528396 time elapsed: 100.2571 learning rate: 0.001031, scenario: 0, slope: -1.2559237240681247, fluctuations: 0.0\n",
      "step: 79430 loss: 767.056085 time elapsed: 100.2717 learning rate: 0.001031, scenario: 0, slope: -1.228017529956393, fluctuations: 0.0\n",
      "step: 79440 loss: 755.731831 time elapsed: 100.2854 learning rate: 0.001031, scenario: 0, slope: -1.2052867304856976, fluctuations: 0.0\n",
      "step: 79450 loss: 744.568671 time elapsed: 100.2987 learning rate: 0.001031, scenario: 0, slope: -1.1860064527737917, fluctuations: 0.0\n",
      "step: 79460 loss: 733.578865 time elapsed: 100.3127 learning rate: 0.001031, scenario: 0, slope: -1.16887389009434, fluctuations: 0.0\n",
      "step: 79470 loss: 722.772198 time elapsed: 100.3265 learning rate: 0.001031, scenario: 0, slope: -1.1528442789520406, fluctuations: 0.0\n",
      "step: 79480 loss: 712.156513 time elapsed: 100.3401 learning rate: 0.001031, scenario: 0, slope: -1.1371048584505636, fluctuations: 0.0\n",
      "step: 79490 loss: 701.739437 time elapsed: 100.3537 learning rate: 0.001031, scenario: 0, slope: -1.1210652593640649, fluctuations: 0.0\n",
      "step: 79500 loss: 691.530278 time elapsed: 100.3666 learning rate: 0.001031, scenario: 0, slope: -1.1060522409819533, fluctuations: 0.0\n",
      "step: 79510 loss: 681.541376 time elapsed: 100.3794 learning rate: 0.001031, scenario: 0, slope: -1.0867204885446824, fluctuations: 0.0\n",
      "step: 79520 loss: 671.788326 time elapsed: 100.3915 learning rate: 0.001031, scenario: 0, slope: -1.0680973551947321, fluctuations: 0.0\n",
      "step: 79530 loss: 662.288706 time elapsed: 100.4035 learning rate: 0.001031, scenario: 0, slope: -1.048414295836425, fluctuations: 0.0\n",
      "step: 79540 loss: 653.059449 time elapsed: 100.4155 learning rate: 0.001031, scenario: 0, slope: -1.0276093772153723, fluctuations: 0.0\n",
      "step: 79550 loss: 644.113676 time elapsed: 100.4273 learning rate: 0.001031, scenario: 0, slope: -1.0056049708082109, fluctuations: 0.0\n",
      "step: 79560 loss: 635.458133 time elapsed: 100.4391 learning rate: 0.001031, scenario: 0, slope: -0.9823337532174677, fluctuations: 0.0\n",
      "step: 79570 loss: 627.092123 time elapsed: 100.4529 learning rate: 0.001031, scenario: 0, slope: -0.9577860699490645, fluctuations: 0.0\n",
      "step: 79580 loss: 619.008081 time elapsed: 100.4672 learning rate: 0.001031, scenario: 0, slope: -0.9320545145162209, fluctuations: 0.0\n",
      "step: 79590 loss: 611.193285 time elapsed: 100.4808 learning rate: 0.001031, scenario: 0, slope: -0.9053573286976617, fluctuations: 0.0\n",
      "step: 79600 loss: 603.631939 time elapsed: 100.4944 learning rate: 0.001031, scenario: 0, slope: -0.8807817693267571, fluctuations: 0.0\n",
      "step: 79610 loss: 596.306979 time elapsed: 100.5091 learning rate: 0.001031, scenario: 0, slope: -0.8505057913473734, fluctuations: 0.0\n",
      "step: 79620 loss: 589.201263 time elapsed: 100.5231 learning rate: 0.001031, scenario: 0, slope: -0.8232367790882803, fluctuations: 0.0\n",
      "step: 79630 loss: 582.298012 time elapsed: 100.5377 learning rate: 0.001031, scenario: 0, slope: -0.7966643771880687, fluctuations: 0.0\n",
      "step: 79640 loss: 575.580404 time elapsed: 100.5531 learning rate: 0.001031, scenario: 0, slope: -0.7711596078322239, fluctuations: 0.0\n",
      "step: 79650 loss: 569.029927 time elapsed: 100.5694 learning rate: 0.001031, scenario: 0, slope: -0.7470048011228608, fluctuations: 0.0\n",
      "step: 79660 loss: 562.622508 time elapsed: 100.5828 learning rate: 0.001031, scenario: 0, slope: -0.7244090638162234, fluctuations: 0.0\n",
      "step: 79670 loss: 556.324163 time elapsed: 100.5960 learning rate: 0.001031, scenario: 0, slope: -0.7035637135172031, fluctuations: 0.0\n",
      "step: 79680 loss: 550.126538 time elapsed: 100.6086 learning rate: 0.001031, scenario: 0, slope: -0.6846367428391341, fluctuations: 0.0\n",
      "step: 79690 loss: 544.154654 time elapsed: 100.6216 learning rate: 0.001031, scenario: 0, slope: -0.6672852106946677, fluctuations: 0.0\n",
      "step: 79700 loss: 538.422025 time elapsed: 100.6342 learning rate: 0.001031, scenario: 0, slope: -0.6522759486172836, fluctuations: 0.0\n",
      "step: 79710 loss: 532.833313 time elapsed: 100.6480 learning rate: 0.001031, scenario: 0, slope: -0.634286281896877, fluctuations: 0.0\n",
      "step: 79720 loss: 527.363996 time elapsed: 100.6618 learning rate: 0.001031, scenario: 0, slope: -0.6182226088650956, fluctuations: 0.0\n",
      "step: 79730 loss: 522.010310 time elapsed: 100.6761 learning rate: 0.001031, scenario: 0, slope: -0.6024302464784831, fluctuations: 0.0\n",
      "step: 79740 loss: 516.765517 time elapsed: 100.6899 learning rate: 0.001031, scenario: 0, slope: -0.5869334542548384, fluctuations: 0.0\n",
      "step: 79750 loss: 511.624079 time elapsed: 100.7032 learning rate: 0.001031, scenario: 0, slope: -0.5718079654589729, fluctuations: 0.0\n",
      "step: 79760 loss: 506.029314 time elapsed: 100.7163 learning rate: 0.001826, scenario: 1, slope: -0.5578426511901449, fluctuations: 0.0\n",
      "step: 79770 loss: 493.966896 time elapsed: 100.7298 learning rate: 0.003234, scenario: 0, slope: -0.5648300689862913, fluctuations: 0.0\n",
      "step: 79780 loss: 479.387382 time elapsed: 100.7433 learning rate: 0.003234, scenario: 0, slope: -0.6193771954641786, fluctuations: 0.0\n",
      "step: 79790 loss: 465.664797 time elapsed: 100.7570 learning rate: 0.003234, scenario: 0, slope: -0.7097754274134116, fluctuations: 0.0\n",
      "step: 79800 loss: 452.750099 time elapsed: 100.7709 learning rate: 0.003234, scenario: 0, slope: -0.8086940558943856, fluctuations: 0.0\n",
      "step: 79810 loss: 440.575948 time elapsed: 100.7840 learning rate: 0.003234, scenario: 0, slope: -0.9364120853131829, fluctuations: 0.0\n",
      "step: 79820 loss: 429.074931 time elapsed: 100.7961 learning rate: 0.003234, scenario: 0, slope: -1.0454992102586012, fluctuations: 0.0\n",
      "step: 79830 loss: 418.171799 time elapsed: 100.8327 learning rate: 0.003234, scenario: 0, slope: -1.1357588065160606, fluctuations: 0.0\n",
      "step: 79840 loss: 407.241819 time elapsed: 100.8466 learning rate: 0.003234, scenario: 0, slope: -1.1972621608560112, fluctuations: 0.0\n",
      "step: 79850 loss: 396.726030 time elapsed: 100.8594 learning rate: 0.003234, scenario: 0, slope: -1.2237478574051905, fluctuations: 0.0\n",
      "step: 79860 loss: 386.741069 time elapsed: 100.8739 learning rate: 0.003234, scenario: 0, slope: -1.204426274392068, fluctuations: 0.0\n",
      "step: 79870 loss: 377.404013 time elapsed: 100.8890 learning rate: 0.003234, scenario: 0, slope: -1.1519085712262125, fluctuations: 0.0\n",
      "step: 79880 loss: 368.481248 time elapsed: 100.9037 learning rate: 0.003234, scenario: 0, slope: -1.1002272379530276, fluctuations: 0.0\n",
      "step: 79890 loss: 359.933737 time elapsed: 100.9184 learning rate: 0.003234, scenario: 0, slope: -1.0526682183747749, fluctuations: 0.0\n",
      "step: 79900 loss: 351.745354 time elapsed: 100.9329 learning rate: 0.003234, scenario: 0, slope: -1.012557984386344, fluctuations: 0.0\n",
      "step: 79910 loss: 343.887201 time elapsed: 100.9481 learning rate: 0.003234, scenario: 0, slope: -0.9658463731267813, fluctuations: 0.0\n",
      "step: 79920 loss: 336.337681 time elapsed: 100.9628 learning rate: 0.003234, scenario: 0, slope: -0.9245640223787032, fluctuations: 0.0\n",
      "step: 79930 loss: 329.072635 time elapsed: 100.9784 learning rate: 0.003234, scenario: 0, slope: -0.8835289530308129, fluctuations: 0.0\n",
      "step: 79940 loss: 322.087545 time elapsed: 100.9914 learning rate: 0.003234, scenario: 0, slope: -0.8427243398704313, fluctuations: 0.0\n",
      "step: 79950 loss: 315.301968 time elapsed: 101.0048 learning rate: 0.003234, scenario: 0, slope: -0.8066681372013305, fluctuations: 0.0\n",
      "step: 79960 loss: 308.722296 time elapsed: 101.0187 learning rate: 0.003234, scenario: 0, slope: -0.7743629146219114, fluctuations: 0.0\n",
      "step: 79970 loss: 302.238461 time elapsed: 101.0317 learning rate: 0.003234, scenario: 0, slope: -0.7452319983696932, fluctuations: 0.0\n",
      "step: 79980 loss: 295.763046 time elapsed: 101.0450 learning rate: 0.003234, scenario: 0, slope: -0.7195412230092146, fluctuations: 0.0\n",
      "step: 79990 loss: 289.151062 time elapsed: 101.0572 learning rate: 0.003234, scenario: 0, slope: -0.6982096846973029, fluctuations: 0.0\n",
      "step: 80000 loss: 281.853479 time elapsed: 101.0699 learning rate: 0.003234, scenario: 0, slope: -0.6847337314209928, fluctuations: 0.0\n",
      "step: 80010 loss: 273.320337 time elapsed: 101.0850 learning rate: 0.003234, scenario: 0, slope: -0.6805048872067483, fluctuations: 0.0\n",
      "step: 80020 loss: 262.549961 time elapsed: 101.0997 learning rate: 0.003234, scenario: 0, slope: -0.6965336858769795, fluctuations: 0.0\n",
      "step: 80030 loss: 249.514088 time elapsed: 101.1132 learning rate: 0.003234, scenario: 0, slope: -0.7405385003143052, fluctuations: 0.0\n",
      "step: 80040 loss: 236.490586 time elapsed: 101.1269 learning rate: 0.003234, scenario: 0, slope: -0.8108678554624819, fluctuations: 0.0\n",
      "step: 80050 loss: 225.993167 time elapsed: 101.1402 learning rate: 0.003234, scenario: 0, slope: -0.8903736500099891, fluctuations: 0.0\n",
      "step: 80060 loss: 217.639868 time elapsed: 101.1537 learning rate: 0.003234, scenario: 0, slope: -0.9572165913630825, fluctuations: 0.0\n",
      "step: 80070 loss: 211.246484 time elapsed: 101.1671 learning rate: 0.003234, scenario: 0, slope: -0.9972050904354645, fluctuations: 0.0\n",
      "step: 80080 loss: 205.905654 time elapsed: 101.1811 learning rate: 0.003234, scenario: 0, slope: -1.002856854665983, fluctuations: 0.0\n",
      "step: 80090 loss: 201.185676 time elapsed: 101.1941 learning rate: 0.003234, scenario: 0, slope: -0.9700536130195545, fluctuations: 0.0\n",
      "step: 80100 loss: 197.028096 time elapsed: 101.2063 learning rate: 0.003234, scenario: 0, slope: -0.9085495047371566, fluctuations: 0.0\n",
      "step: 80110 loss: 193.204425 time elapsed: 101.2189 learning rate: 0.003234, scenario: 0, slope: -0.799714371158997, fluctuations: 0.0\n",
      "step: 80120 loss: 189.657102 time elapsed: 101.2309 learning rate: 0.003234, scenario: 0, slope: -0.6840099346617645, fluctuations: 0.0\n",
      "step: 80130 loss: 186.329981 time elapsed: 101.2431 learning rate: 0.003234, scenario: 0, slope: -0.5748653016418709, fluctuations: 0.0\n",
      "step: 80140 loss: 183.182215 time elapsed: 101.2550 learning rate: 0.003234, scenario: 0, slope: -0.48954513766167945, fluctuations: 0.0\n",
      "step: 80150 loss: 180.184154 time elapsed: 101.2671 learning rate: 0.003234, scenario: 0, slope: -0.4295983482026739, fluctuations: 0.0\n",
      "step: 80160 loss: 177.312214 time elapsed: 101.2791 learning rate: 0.003234, scenario: 0, slope: -0.38682508957860334, fluctuations: 0.0\n",
      "step: 80170 loss: 174.547391 time elapsed: 101.2925 learning rate: 0.003234, scenario: 0, slope: -0.35521406651471665, fluctuations: 0.0\n",
      "step: 80180 loss: 171.873077 time elapsed: 101.3059 learning rate: 0.003234, scenario: 0, slope: -0.33166297663061906, fluctuations: 0.0\n",
      "step: 80190 loss: 169.273369 time elapsed: 101.3200 learning rate: 0.003234, scenario: 0, slope: -0.31315451288894647, fluctuations: 0.0\n",
      "step: 80200 loss: 166.730908 time elapsed: 101.3343 learning rate: 0.003234, scenario: 0, slope: -0.29948653316739554, fluctuations: 0.0\n",
      "step: 80210 loss: 164.223137 time elapsed: 101.3496 learning rate: 0.003234, scenario: 0, slope: -0.2857407587882239, fluctuations: 0.0\n",
      "step: 80220 loss: 161.714613 time elapsed: 101.3635 learning rate: 0.003234, scenario: 0, slope: -0.27565072684511555, fluctuations: 0.0\n",
      "step: 80230 loss: 159.144452 time elapsed: 101.3784 learning rate: 0.003234, scenario: 0, slope: -0.2678014856953216, fluctuations: 0.0\n",
      "step: 80240 loss: 156.453733 time elapsed: 101.3928 learning rate: 0.003234, scenario: 0, slope: -0.26250057085970535, fluctuations: 0.0\n",
      "step: 80250 loss: 153.786489 time elapsed: 101.4099 learning rate: 0.003234, scenario: 0, slope: -0.2597219074278496, fluctuations: 0.0\n",
      "step: 80260 loss: 151.335193 time elapsed: 101.4228 learning rate: 0.003234, scenario: 0, slope: -0.2581618388999845, fluctuations: 0.0\n",
      "step: 80270 loss: 149.023283 time elapsed: 101.4355 learning rate: 0.003234, scenario: 0, slope: -0.2565311178401909, fluctuations: 0.0\n",
      "step: 80280 loss: 146.805654 time elapsed: 101.4476 learning rate: 0.003234, scenario: 0, slope: -0.2541286068236473, fluctuations: 0.0\n",
      "step: 80290 loss: 144.644006 time elapsed: 101.4593 learning rate: 0.003234, scenario: 0, slope: -0.2505627500997947, fluctuations: 0.0\n",
      "step: 80300 loss: 142.508811 time elapsed: 101.4712 learning rate: 0.003234, scenario: 0, slope: -0.24627311818727204, fluctuations: 0.0\n",
      "step: 80310 loss: 140.393138 time elapsed: 101.4840 learning rate: 0.003234, scenario: 0, slope: -0.23976128441207836, fluctuations: 0.0\n",
      "step: 80320 loss: 138.238359 time elapsed: 101.4962 learning rate: 0.003234, scenario: 0, slope: -0.23271467161046328, fluctuations: 0.02\n",
      "step: 80330 loss: 136.019241 time elapsed: 101.5103 learning rate: 0.003234, scenario: 0, slope: -0.22558444956477208, fluctuations: 0.02\n",
      "step: 80340 loss: 133.787038 time elapsed: 101.5244 learning rate: 0.003234, scenario: 0, slope: -0.22051626211419756, fluctuations: 0.02\n",
      "step: 80350 loss: 131.336849 time elapsed: 101.5381 learning rate: 0.003234, scenario: 0, slope: -0.2188182411943768, fluctuations: 0.02\n",
      "step: 80360 loss: 128.743444 time elapsed: 101.5516 learning rate: 0.003234, scenario: 0, slope: -0.22052372056206937, fluctuations: 0.02\n",
      "step: 80370 loss: 125.867501 time elapsed: 101.5653 learning rate: 0.003234, scenario: 0, slope: -0.22575088164207135, fluctuations: 0.02\n",
      "step: 80380 loss: 123.171755 time elapsed: 101.5791 learning rate: 0.003234, scenario: 0, slope: -0.23349112584492188, fluctuations: 0.02\n",
      "step: 80390 loss: 120.574863 time elapsed: 101.5926 learning rate: 0.003234, scenario: 0, slope: -0.24196362849560188, fluctuations: 0.02\n",
      "step: 80400 loss: 118.806736 time elapsed: 101.6084 learning rate: 0.003234, scenario: 0, slope: -0.24837945587737104, fluctuations: 0.02\n",
      "step: 80410 loss: 116.241396 time elapsed: 101.6213 learning rate: 0.003234, scenario: 0, slope: -0.25258089592945876, fluctuations: 0.04\n",
      "step: 80420 loss: 114.065617 time elapsed: 101.6332 learning rate: 0.003234, scenario: 0, slope: -0.25137711104035243, fluctuations: 0.02\n",
      "step: 80430 loss: 112.198498 time elapsed: 101.6453 learning rate: 0.003234, scenario: 0, slope: -0.2469457148566374, fluctuations: 0.02\n",
      "step: 80440 loss: 110.409495 time elapsed: 101.6573 learning rate: 0.003234, scenario: 0, slope: -0.23863792672864637, fluctuations: 0.02\n",
      "step: 80450 loss: 108.831349 time elapsed: 101.6692 learning rate: 0.003234, scenario: 0, slope: -0.2270781634397912, fluctuations: 0.02\n",
      "step: 80460 loss: 107.314075 time elapsed: 101.6810 learning rate: 0.003234, scenario: 0, slope: -0.2127320761119506, fluctuations: 0.04\n",
      "step: 80470 loss: 105.557039 time elapsed: 101.6929 learning rate: 0.003234, scenario: 0, slope: -0.1987635568972532, fluctuations: 0.05\n",
      "step: 80480 loss: 104.036627 time elapsed: 101.7068 learning rate: 0.003234, scenario: 0, slope: -0.18649702285252337, fluctuations: 0.05\n",
      "step: 80490 loss: 102.584905 time elapsed: 101.7215 learning rate: 0.003234, scenario: 0, slope: -0.17655758355441653, fluctuations: 0.05\n",
      "step: 80500 loss: 101.177400 time elapsed: 101.7365 learning rate: 0.003234, scenario: 0, slope: -0.16958296289772098, fluctuations: 0.04\n",
      "step: 80510 loss: 100.452307 time elapsed: 101.7516 learning rate: 0.003234, scenario: 0, slope: -0.15920375467587705, fluctuations: 0.04\n",
      "step: 80520 loss: 98.510674 time elapsed: 101.7665 learning rate: 0.003234, scenario: 0, slope: -0.1526260325005615, fluctuations: 0.06\n",
      "step: 80530 loss: 97.329575 time elapsed: 101.7812 learning rate: 0.003234, scenario: 0, slope: -0.14756500264179503, fluctuations: 0.08\n",
      "step: 80540 loss: 95.996157 time elapsed: 101.7956 learning rate: 0.003234, scenario: 0, slope: -0.14334998280552558, fluctuations: 0.08\n",
      "step: 80550 loss: 94.701753 time elapsed: 101.8104 learning rate: 0.003234, scenario: 0, slope: -0.1399620795887761, fluctuations: 0.08\n",
      "step: 80560 loss: 93.604840 time elapsed: 101.8230 learning rate: 0.003234, scenario: 0, slope: -0.1352520099593551, fluctuations: 0.06\n",
      "step: 80570 loss: 92.900856 time elapsed: 101.8362 learning rate: 0.003234, scenario: 0, slope: -0.1296149414714837, fluctuations: 0.07\n",
      "step: 80580 loss: 91.501118 time elapsed: 101.8491 learning rate: 0.003234, scenario: 0, slope: -0.12620923208776, fluctuations: 0.09\n",
      "step: 80590 loss: 90.301948 time elapsed: 101.8613 learning rate: 0.003234, scenario: 0, slope: -0.1236740994233993, fluctuations: 0.1\n",
      "step: 80600 loss: 89.205596 time elapsed: 101.8734 learning rate: 0.003234, scenario: 0, slope: -0.121734313209377, fluctuations: 0.1\n",
      "step: 80610 loss: 88.174254 time elapsed: 101.8863 learning rate: 0.003234, scenario: 0, slope: -0.11644249107924767, fluctuations: 0.09\n",
      "step: 80620 loss: 87.218831 time elapsed: 101.8982 learning rate: 0.003234, scenario: 0, slope: -0.11304436874610088, fluctuations: 0.06\n",
      "step: 80630 loss: 86.153326 time elapsed: 101.9105 learning rate: 0.003234, scenario: 0, slope: -0.11020432190478359, fluctuations: 0.05\n",
      "step: 80640 loss: 85.185640 time elapsed: 101.9239 learning rate: 0.003234, scenario: 0, slope: -0.10808551419794432, fluctuations: 0.05\n",
      "step: 80650 loss: 85.647465 time elapsed: 101.9378 learning rate: 0.003234, scenario: 0, slope: -0.1031393686196152, fluctuations: 0.06\n",
      "step: 80660 loss: 83.806938 time elapsed: 101.9513 learning rate: 0.003234, scenario: 0, slope: -0.09994892851937226, fluctuations: 0.07\n",
      "step: 80670 loss: 82.537506 time elapsed: 101.9650 learning rate: 0.003234, scenario: 0, slope: -0.09665515206133367, fluctuations: 0.07\n",
      "step: 80680 loss: 81.557782 time elapsed: 101.9785 learning rate: 0.003234, scenario: 0, slope: -0.09502149206081326, fluctuations: 0.06\n",
      "step: 80690 loss: 80.629789 time elapsed: 101.9919 learning rate: 0.003234, scenario: 0, slope: -0.09432598714321261, fluctuations: 0.06\n",
      "step: 80700 loss: 79.776768 time elapsed: 102.0052 learning rate: 0.003234, scenario: 0, slope: -0.09411883565351324, fluctuations: 0.06\n",
      "step: 80710 loss: 79.065191 time elapsed: 102.0196 learning rate: 0.003234, scenario: 0, slope: -0.09254076673160821, fluctuations: 0.06\n",
      "step: 80720 loss: 78.102839 time elapsed: 102.0327 learning rate: 0.003234, scenario: 0, slope: -0.0921135624666985, fluctuations: 0.06\n",
      "step: 80730 loss: 77.244082 time elapsed: 102.0452 learning rate: 0.003234, scenario: 0, slope: -0.09225406742616929, fluctuations: 0.06\n",
      "step: 80740 loss: 76.312354 time elapsed: 102.0578 learning rate: 0.003234, scenario: 0, slope: -0.09295668958164528, fluctuations: 0.06\n",
      "step: 80750 loss: 75.482149 time elapsed: 102.0699 learning rate: 0.003234, scenario: 0, slope: -0.08991189265529156, fluctuations: 0.05\n",
      "step: 80760 loss: 75.313568 time elapsed: 102.0824 learning rate: 0.003234, scenario: 0, slope: -0.08453588506665326, fluctuations: 0.04\n",
      "step: 80770 loss: 73.963645 time elapsed: 102.0951 learning rate: 0.003234, scenario: 0, slope: -0.08267771252955816, fluctuations: 0.01\n",
      "step: 80780 loss: 73.102440 time elapsed: 102.1082 learning rate: 0.003234, scenario: 0, slope: -0.0821206328652904, fluctuations: 0.03\n",
      "step: 80790 loss: 72.283388 time elapsed: 102.1227 learning rate: 0.003234, scenario: 0, slope: -0.0823367074246577, fluctuations: 0.03\n",
      "step: 80800 loss: 71.501728 time elapsed: 102.1371 learning rate: 0.003234, scenario: 0, slope: -0.08257954787443632, fluctuations: 0.03\n",
      "step: 80810 loss: 70.913024 time elapsed: 102.1527 learning rate: 0.003234, scenario: 0, slope: -0.08155053455982464, fluctuations: 0.03\n",
      "step: 80820 loss: 70.118856 time elapsed: 102.1673 learning rate: 0.003234, scenario: 0, slope: -0.07962801651379463, fluctuations: 0.04\n",
      "step: 80830 loss: 69.427929 time elapsed: 102.1818 learning rate: 0.003234, scenario: 0, slope: -0.07889923041078804, fluctuations: 0.05\n",
      "step: 80840 loss: 68.652699 time elapsed: 102.1963 learning rate: 0.003234, scenario: 0, slope: -0.07847127521037782, fluctuations: 0.05\n",
      "step: 80850 loss: 67.992453 time elapsed: 102.2110 learning rate: 0.003234, scenario: 0, slope: -0.0782784899712393, fluctuations: 0.05\n",
      "step: 80860 loss: 67.403858 time elapsed: 102.2258 learning rate: 0.003558, scenario: 1, slope: -0.07478473931750403, fluctuations: 0.04\n",
      "step: 80870 loss: 387.930423 time elapsed: 102.2386 learning rate: 0.003588, scenario: -1, slope: 0.7240274660368111, fluctuations: 0.04\n",
      "step: 80880 loss: 119.556860 time elapsed: 102.2515 learning rate: 0.001251, scenario: -1, slope: 0.7586247323664628, fluctuations: 0.08\n",
      "step: 80890 loss: 76.292554 time elapsed: 102.2649 learning rate: 0.000436, scenario: -1, slope: 0.6771606230628917, fluctuations: 0.1\n",
      "step: 80900 loss: 72.283212 time elapsed: 102.2770 learning rate: 0.000169, scenario: -1, slope: 0.5021604549821538, fluctuations: 0.11\n",
      "step: 80910 loss: 71.576682 time elapsed: 102.2898 learning rate: 0.000059, scenario: -1, slope: 0.27070214108831475, fluctuations: 0.12\n",
      "step: 80920 loss: 71.331002 time elapsed: 102.3018 learning rate: 0.000021, scenario: -1, slope: 0.06585022545899956, fluctuations: 0.1\n",
      "step: 80930 loss: 71.244076 time elapsed: 102.3134 learning rate: 0.000019, scenario: 0, slope: -0.1391134148775573, fluctuations: 0.1\n",
      "step: 80940 loss: 71.156909 time elapsed: 102.3258 learning rate: 0.000019, scenario: 0, slope: -0.35581754587768777, fluctuations: 0.1\n",
      "step: 80950 loss: 71.102491 time elapsed: 102.3393 learning rate: 0.000019, scenario: 0, slope: -0.6036434375958598, fluctuations: 0.1\n",
      "step: 80960 loss: 71.057215 time elapsed: 102.3531 learning rate: 0.000019, scenario: 0, slope: -0.9106317598187016, fluctuations: 0.1\n",
      "step: 80970 loss: 71.013612 time elapsed: 102.3669 learning rate: 0.000019, scenario: 0, slope: -0.3623981739049274, fluctuations: 0.08\n",
      "step: 80980 loss: 70.972066 time elapsed: 102.3806 learning rate: 0.000019, scenario: 0, slope: -0.09984667824756367, fluctuations: 0.04\n",
      "step: 80990 loss: 70.913983 time elapsed: 102.3942 learning rate: 0.000050, scenario: 1, slope: -0.026343153531729092, fluctuations: 0.02\n",
      "step: 81000 loss: 70.771839 time elapsed: 102.4078 learning rate: 0.000117, scenario: 1, slope: -0.010714464390505857, fluctuations: 0.01\n",
      "step: 81010 loss: 70.467290 time elapsed: 102.4221 learning rate: 0.000303, scenario: 1, slope: -0.007050758974456478, fluctuations: 0.0\n",
      "step: 81020 loss: 69.827381 time elapsed: 102.4354 learning rate: 0.000787, scenario: 1, slope: -0.009780064026245499, fluctuations: 0.0\n",
      "step: 81030 loss: 68.738417 time elapsed: 102.4482 learning rate: 0.002042, scenario: 1, slope: -0.016502435263413835, fluctuations: 0.0\n",
      "step: 81040 loss: 67.376769 time elapsed: 102.4605 learning rate: 0.005296, scenario: 1, slope: -0.028178478842944985, fluctuations: 0.0\n",
      "step: 81050 loss: 65.609545 time elapsed: 102.4726 learning rate: 0.013736, scenario: 1, slope: -0.04594395169587332, fluctuations: 0.0\n",
      "step: 81060 loss: 31464.521451 time elapsed: 102.4850 learning rate: 0.006209, scenario: -1, slope: 65.05935230640677, fluctuations: 0.03\n",
      "step: 81070 loss: 9956.179580 time elapsed: 102.4979 learning rate: 0.002165, scenario: -1, slope: 107.4069624327659, fluctuations: 0.04\n",
      "step: 81080 loss: 4390.654055 time elapsed: 102.5107 learning rate: 0.000755, scenario: -1, slope: 106.72525202646993, fluctuations: 0.05\n",
      "step: 81090 loss: 3424.231158 time elapsed: 102.5236 learning rate: 0.000263, scenario: -1, slope: 95.03712830426434, fluctuations: 0.06\n",
      "step: 81100 loss: 3193.095562 time elapsed: 102.5378 learning rate: 0.000102, scenario: -1, slope: 76.5200575497562, fluctuations: 0.06\n",
      "step: 81110 loss: 3152.891030 time elapsed: 102.5525 learning rate: 0.000036, scenario: -1, slope: 50.80297972578129, fluctuations: 0.06\n",
      "step: 81120 loss: 3139.455302 time elapsed: 102.5678 learning rate: 0.000012, scenario: -1, slope: 23.97116096662364, fluctuations: 0.06\n",
      "step: 81130 loss: 3134.750533 time elapsed: 102.5828 learning rate: 0.000006, scenario: 0, slope: -7.3563835545702565, fluctuations: 0.06\n",
      "step: 81140 loss: 3131.691364 time elapsed: 102.5973 learning rate: 0.000006, scenario: 0, slope: -44.932200178032346, fluctuations: 0.06\n",
      "step: 81150 loss: 3128.667828 time elapsed: 102.6119 learning rate: 0.000006, scenario: 0, slope: -91.38355578166087, fluctuations: 0.06\n",
      "step: 81160 loss: 3125.680738 time elapsed: 102.6255 learning rate: 0.000006, scenario: 0, slope: -49.37941263180194, fluctuations: 0.03\n",
      "step: 81170 loss: 3122.727958 time elapsed: 102.6400 learning rate: 0.000006, scenario: 0, slope: -16.632631501148428, fluctuations: 0.01\n",
      "step: 81180 loss: 3119.806661 time elapsed: 102.6555 learning rate: 0.000006, scenario: 0, slope: -5.17431843585373, fluctuations: 0.0\n",
      "step: 81190 loss: 3115.925245 time elapsed: 102.6710 learning rate: 0.000014, scenario: 1, slope: -1.0422407028652205, fluctuations: 0.0\n",
      "step: 81200 loss: 3106.236765 time elapsed: 102.6872 learning rate: 0.000033, scenario: 1, slope: -0.5068590992873422, fluctuations: 0.0\n",
      "step: 81210 loss: 3084.151483 time elapsed: 102.7022 learning rate: 0.000086, scenario: 1, slope: -0.43878943578875007, fluctuations: 0.0\n",
      "step: 81220 loss: 3031.568063 time elapsed: 102.7168 learning rate: 0.000222, scenario: 1, slope: -0.6614433783549174, fluctuations: 0.0\n",
      "step: 81230 loss: 2910.705783 time elapsed: 102.7291 learning rate: 0.000575, scenario: 1, slope: -1.304116751294545, fluctuations: 0.0\n",
      "step: 81240 loss: 2646.426439 time elapsed: 102.7409 learning rate: 0.001492, scenario: 1, slope: -2.8254592011467135, fluctuations: 0.0\n",
      "step: 81250 loss: 2264.249291 time elapsed: 102.7548 learning rate: 0.001641, scenario: 0, slope: -5.852226008776348, fluctuations: 0.0\n",
      "step: 81260 loss: 1982.428033 time elapsed: 102.7681 learning rate: 0.001641, scenario: 0, slope: -9.820955655129131, fluctuations: 0.0\n",
      "step: 81270 loss: 1783.086988 time elapsed: 102.7814 learning rate: 0.001641, scenario: 0, slope: -13.836663284427393, fluctuations: 0.0\n",
      "step: 81280 loss: 1629.118861 time elapsed: 102.7945 learning rate: 0.001641, scenario: 0, slope: -17.28206256553134, fluctuations: 0.0\n",
      "step: 81290 loss: 1495.441905 time elapsed: 102.8078 learning rate: 0.001641, scenario: 0, slope: -19.803297872025734, fluctuations: 0.0\n",
      "step: 81300 loss: 1376.780098 time elapsed: 102.8205 learning rate: 0.001641, scenario: 0, slope: -21.0870368373219, fluctuations: 0.0\n",
      "step: 81310 loss: 1274.157421 time elapsed: 102.8341 learning rate: 0.001641, scenario: 0, slope: -21.204867475909428, fluctuations: 0.0\n",
      "step: 81320 loss: 1183.599706 time elapsed: 102.8484 learning rate: 0.001641, scenario: 0, slope: -19.868886954935753, fluctuations: 0.0\n",
      "step: 81330 loss: 1041.271803 time elapsed: 102.8611 learning rate: 0.001641, scenario: 0, slope: -17.487824065508992, fluctuations: 0.0\n",
      "step: 81340 loss: 915.488666 time elapsed: 102.8728 learning rate: 0.001641, scenario: 0, slope: -14.963585651887104, fluctuations: 0.0\n",
      "step: 81350 loss: 827.113675 time elapsed: 102.8845 learning rate: 0.001641, scenario: 0, slope: -13.062509440126775, fluctuations: 0.0\n",
      "step: 81360 loss: 769.410946 time elapsed: 102.8972 learning rate: 0.001641, scenario: 0, slope: -11.8285762452053, fluctuations: 0.0\n",
      "step: 81370 loss: 727.358515 time elapsed: 102.9101 learning rate: 0.001641, scenario: 0, slope: -10.841381056752654, fluctuations: 0.0\n",
      "step: 81380 loss: 691.136800 time elapsed: 102.9225 learning rate: 0.001641, scenario: 0, slope: -9.849975370867805, fluctuations: 0.0\n",
      "step: 81390 loss: 659.448701 time elapsed: 102.9351 learning rate: 0.001641, scenario: 0, slope: -8.792112162299498, fluctuations: 0.0\n",
      "step: 81400 loss: 631.036874 time elapsed: 102.9499 learning rate: 0.001641, scenario: 0, slope: -7.782100316720948, fluctuations: 0.0\n",
      "step: 81410 loss: 605.345572 time elapsed: 102.9652 learning rate: 0.001641, scenario: 0, slope: -6.471557106643171, fluctuations: 0.0\n",
      "step: 81420 loss: 581.974221 time elapsed: 102.9794 learning rate: 0.001641, scenario: 0, slope: -5.199928306560252, fluctuations: 0.0\n",
      "step: 81430 loss: 560.611830 time elapsed: 102.9943 learning rate: 0.001641, scenario: 0, slope: -4.014023439377229, fluctuations: 0.0\n",
      "step: 81440 loss: 540.984585 time elapsed: 103.0079 learning rate: 0.001641, scenario: 0, slope: -3.314998519039437, fluctuations: 0.0\n",
      "step: 81450 loss: 521.914912 time elapsed: 103.0222 learning rate: 0.001641, scenario: 0, slope: -2.8520837027436463, fluctuations: 0.0\n",
      "step: 81460 loss: 485.800867 time elapsed: 103.0369 learning rate: 0.001641, scenario: 0, slope: -2.5948153882911993, fluctuations: 0.0\n",
      "step: 81470 loss: 456.008962 time elapsed: 103.0509 learning rate: 0.001641, scenario: 0, slope: -2.5151968051069007, fluctuations: 0.0\n",
      "step: 81480 loss: 439.931291 time elapsed: 103.0654 learning rate: 0.001641, scenario: 0, slope: -2.452101358502941, fluctuations: 0.0\n",
      "step: 81490 loss: 424.793609 time elapsed: 103.0789 learning rate: 0.001641, scenario: 0, slope: -2.3858422613029564, fluctuations: 0.0\n",
      "step: 81500 loss: 403.108038 time elapsed: 103.0907 learning rate: 0.001641, scenario: 0, slope: -2.341660974956228, fluctuations: 0.0\n",
      "step: 81510 loss: 388.743362 time elapsed: 103.1033 learning rate: 0.001641, scenario: 0, slope: -2.2752980950261326, fluctuations: 0.0\n",
      "step: 81520 loss: 372.765953 time elapsed: 103.1165 learning rate: 0.001641, scenario: 0, slope: -2.1919562951136395, fluctuations: 0.0\n",
      "step: 81530 loss: 353.974483 time elapsed: 103.1286 learning rate: 0.001641, scenario: 0, slope: -2.0948824756855213, fluctuations: 0.0\n",
      "step: 81540 loss: 332.367165 time elapsed: 103.1403 learning rate: 0.001641, scenario: 0, slope: -1.9961706542018964, fluctuations: 0.0\n",
      "step: 81550 loss: 316.051032 time elapsed: 103.1522 learning rate: 0.001641, scenario: 0, slope: -1.875948886677588, fluctuations: 0.0\n",
      "step: 81560 loss: 304.788024 time elapsed: 103.1656 learning rate: 0.001641, scenario: 0, slope: -1.7534281862215253, fluctuations: 0.0\n",
      "step: 81570 loss: 296.057096 time elapsed: 103.1788 learning rate: 0.001641, scenario: 0, slope: -1.7021978589472377, fluctuations: 0.0\n",
      "step: 81580 loss: 288.802075 time elapsed: 103.1920 learning rate: 0.001641, scenario: 0, slope: -1.613886758461558, fluctuations: 0.0\n",
      "step: 81590 loss: 282.638129 time elapsed: 103.2057 learning rate: 0.001641, scenario: 0, slope: -1.479109636349972, fluctuations: 0.0\n",
      "step: 81600 loss: 277.284848 time elapsed: 103.2193 learning rate: 0.001641, scenario: 0, slope: -1.3471856350372926, fluctuations: 0.0\n",
      "step: 81610 loss: 272.479651 time elapsed: 103.2331 learning rate: 0.001641, scenario: 0, slope: -1.1618351100073636, fluctuations: 0.0\n",
      "step: 81620 loss: 268.055171 time elapsed: 103.2465 learning rate: 0.001641, scenario: 0, slope: -0.9723221566878084, fluctuations: 0.0\n",
      "step: 81630 loss: 263.901147 time elapsed: 103.2606 learning rate: 0.001641, scenario: 0, slope: -0.7878928081386029, fluctuations: 0.0\n",
      "step: 81640 loss: 259.957208 time elapsed: 103.2729 learning rate: 0.001641, scenario: 0, slope: -0.6456140648127814, fluctuations: 0.0\n",
      "step: 81650 loss: 256.188394 time elapsed: 103.2856 learning rate: 0.001641, scenario: 0, slope: -0.5541772195278507, fluctuations: 0.0\n",
      "step: 81660 loss: 252.571144 time elapsed: 103.2982 learning rate: 0.001641, scenario: 0, slope: -0.49351614435700236, fluctuations: 0.0\n",
      "step: 81670 loss: 249.088153 time elapsed: 103.3118 learning rate: 0.001641, scenario: 0, slope: -0.44964955666973566, fluctuations: 0.0\n",
      "step: 81680 loss: 245.726333 time elapsed: 103.3240 learning rate: 0.001641, scenario: 0, slope: -0.4173678027739029, fluctuations: 0.0\n",
      "step: 81690 loss: 242.475380 time elapsed: 103.3368 learning rate: 0.001641, scenario: 0, slope: -0.39305966817847376, fluctuations: 0.0\n",
      "step: 81700 loss: 239.326816 time elapsed: 103.3497 learning rate: 0.001641, scenario: 0, slope: -0.37569854915592776, fluctuations: 0.0\n",
      "step: 81710 loss: 236.273291 time elapsed: 103.3652 learning rate: 0.001641, scenario: 0, slope: -0.35817226126974966, fluctuations: 0.0\n",
      "step: 81720 loss: 233.307860 time elapsed: 103.3798 learning rate: 0.001641, scenario: 0, slope: -0.3445339199576631, fluctuations: 0.0\n",
      "step: 81730 loss: 230.422807 time elapsed: 103.3945 learning rate: 0.001641, scenario: 0, slope: -0.33239856656332345, fluctuations: 0.0\n",
      "step: 81740 loss: 227.606429 time elapsed: 103.4091 learning rate: 0.001641, scenario: 0, slope: -0.32143198415535884, fluctuations: 0.0\n",
      "step: 81750 loss: 224.829010 time elapsed: 103.4230 learning rate: 0.001641, scenario: 0, slope: -0.3115109678120992, fluctuations: 0.0\n",
      "step: 81760 loss: 221.927608 time elapsed: 103.4375 learning rate: 0.001641, scenario: 0, slope: -0.30297955217093947, fluctuations: 0.0\n",
      "step: 81770 loss: 215.445666 time elapsed: 103.4520 learning rate: 0.001641, scenario: 0, slope: -0.30271931709740596, fluctuations: 0.0\n",
      "step: 81780 loss: 209.301157 time elapsed: 103.4667 learning rate: 0.001641, scenario: 0, slope: -0.32888256194107307, fluctuations: 0.0\n",
      "step: 81790 loss: 205.170901 time elapsed: 103.4833 learning rate: 0.001641, scenario: 0, slope: -0.3576777074136622, fluctuations: 0.01\n",
      "step: 81800 loss: 201.224477 time elapsed: 103.4962 learning rate: 0.001641, scenario: 0, slope: -0.38361092515804196, fluctuations: 0.01\n",
      "step: 81810 loss: 197.311580 time elapsed: 103.5096 learning rate: 0.001641, scenario: 0, slope: -0.41088513563906953, fluctuations: 0.01\n",
      "step: 81820 loss: 193.402737 time elapsed: 103.5217 learning rate: 0.001641, scenario: 0, slope: -0.42981082942817556, fluctuations: 0.01\n",
      "step: 81830 loss: 189.470610 time elapsed: 103.5337 learning rate: 0.001641, scenario: 0, slope: -0.4418440061479184, fluctuations: 0.01\n",
      "step: 81840 loss: 185.558405 time elapsed: 103.5454 learning rate: 0.001641, scenario: 0, slope: -0.4453696916309328, fluctuations: 0.01\n",
      "step: 81850 loss: 181.745062 time elapsed: 103.5572 learning rate: 0.001641, scenario: 0, slope: -0.43832715112547505, fluctuations: 0.01\n",
      "step: 81860 loss: 178.129472 time elapsed: 103.5697 learning rate: 0.001641, scenario: 0, slope: -0.4185053861759031, fluctuations: 0.01\n",
      "step: 81870 loss: 174.796379 time elapsed: 103.5835 learning rate: 0.001641, scenario: 0, slope: -0.39075293439895287, fluctuations: 0.01\n",
      "step: 81880 loss: 171.785933 time elapsed: 103.5968 learning rate: 0.001641, scenario: 0, slope: -0.3797569141470385, fluctuations: 0.0\n",
      "step: 81890 loss: 169.087187 time elapsed: 103.6107 learning rate: 0.001641, scenario: 0, slope: -0.36882888246926976, fluctuations: 0.0\n",
      "step: 81900 loss: 166.653034 time elapsed: 103.6245 learning rate: 0.001641, scenario: 0, slope: -0.3553239218519752, fluctuations: 0.0\n",
      "step: 81910 loss: 164.420439 time elapsed: 103.6385 learning rate: 0.001641, scenario: 0, slope: -0.3345038893048413, fluctuations: 0.0\n",
      "step: 81920 loss: 162.627834 time elapsed: 103.6525 learning rate: 0.001641, scenario: 0, slope: -0.3125458388147522, fluctuations: 0.01\n",
      "step: 81930 loss: 160.615353 time elapsed: 103.6662 learning rate: 0.001641, scenario: 0, slope: -0.2889327675078694, fluctuations: 0.03\n",
      "step: 81940 loss: 158.569595 time elapsed: 103.6802 learning rate: 0.001641, scenario: 0, slope: -0.2643959470264933, fluctuations: 0.04\n",
      "step: 81950 loss: 156.561132 time elapsed: 103.6929 learning rate: 0.001641, scenario: 0, slope: -0.24248665618965182, fluctuations: 0.04\n",
      "step: 81960 loss: 154.647588 time elapsed: 103.7063 learning rate: 0.001641, scenario: 0, slope: -0.22520083021363077, fluctuations: 0.04\n",
      "step: 81970 loss: 152.914446 time elapsed: 103.7194 learning rate: 0.001641, scenario: 0, slope: -0.21185818142441162, fluctuations: 0.04\n",
      "step: 81980 loss: 151.284472 time elapsed: 103.7333 learning rate: 0.001641, scenario: 0, slope: -0.2013173343200721, fluctuations: 0.04\n",
      "step: 81990 loss: 149.766529 time elapsed: 103.7464 learning rate: 0.001641, scenario: 0, slope: -0.19254457384441337, fluctuations: 0.04\n",
      "step: 82000 loss: 148.330443 time elapsed: 103.7587 learning rate: 0.001641, scenario: 0, slope: -0.18535510522715878, fluctuations: 0.04\n",
      "step: 82010 loss: 146.958110 time elapsed: 103.7727 learning rate: 0.001641, scenario: 0, slope: -0.1766786046918725, fluctuations: 0.04\n",
      "step: 82020 loss: 145.641129 time elapsed: 103.7872 learning rate: 0.001641, scenario: 0, slope: -0.16791473045217092, fluctuations: 0.03\n",
      "step: 82030 loss: 144.371660 time elapsed: 103.8016 learning rate: 0.001805, scenario: 1, slope: -0.16017422047472243, fluctuations: 0.0\n",
      "step: 82040 loss: 196.885782 time elapsed: 103.8163 learning rate: 0.004683, scenario: 1, slope: -0.10375205284954325, fluctuations: 0.01\n",
      "step: 82050 loss: 182.575308 time elapsed: 103.8309 learning rate: 0.002644, scenario: -1, slope: 0.10847029866595263, fluctuations: 0.05\n",
      "step: 82060 loss: 147.542092 time elapsed: 103.8447 learning rate: 0.000922, scenario: -1, slope: 0.09430480590955331, fluctuations: 0.08\n",
      "step: 82070 loss: 140.916202 time elapsed: 103.8582 learning rate: 0.000321, scenario: -1, slope: 0.039846534277190065, fluctuations: 0.09\n",
      "step: 82080 loss: 139.778326 time elapsed: 103.8722 learning rate: 0.000196, scenario: 1, slope: -0.01546072208665943, fluctuations: 0.1\n",
      "step: 82090 loss: 139.106772 time elapsed: 103.8861 learning rate: 0.000509, scenario: 1, slope: -0.06729089265346427, fluctuations: 0.11\n",
      "step: 82100 loss: 138.061585 time elapsed: 103.8990 learning rate: 0.001201, scenario: 1, slope: -0.11379987466115865, fluctuations: 0.12\n",
      "step: 82110 loss: 136.584143 time elapsed: 103.9119 learning rate: 0.001758, scenario: 0, slope: -0.17874575302366508, fluctuations: 0.12\n",
      "step: 82120 loss: 135.433911 time elapsed: 103.9242 learning rate: 0.001758, scenario: 0, slope: -0.24559631138625682, fluctuations: 0.12\n",
      "step: 82130 loss: 134.266049 time elapsed: 103.9366 learning rate: 0.001758, scenario: 0, slope: -0.32755118644607395, fluctuations: 0.12\n",
      "step: 82140 loss: 133.199942 time elapsed: 103.9486 learning rate: 0.001758, scenario: 0, slope: -0.3795679237120961, fluctuations: 0.1\n",
      "step: 82150 loss: 132.147940 time elapsed: 103.9606 learning rate: 0.002340, scenario: 1, slope: -0.15147929464232446, fluctuations: 0.06\n",
      "step: 82160 loss: 130.233474 time elapsed: 103.9726 learning rate: 0.006070, scenario: 1, slope: -0.11885608968106053, fluctuations: 0.03\n",
      "step: 82170 loss: 10732.570852 time elapsed: 103.9865 learning rate: 0.011179, scenario: -1, slope: 7.459272504650122, fluctuations: 0.03\n",
      "step: 82180 loss: 3029.090676 time elapsed: 104.0005 learning rate: 0.003898, scenario: -1, slope: 26.328316693224885, fluctuations: 0.06\n",
      "step: 82190 loss: 1063.638922 time elapsed: 104.0136 learning rate: 0.001359, scenario: -1, slope: 29.866412396125874, fluctuations: 0.07\n",
      "step: 82200 loss: 610.887573 time elapsed: 104.0273 learning rate: 0.000527, scenario: -1, slope: 24.60034528677912, fluctuations: 0.06\n",
      "step: 82210 loss: 460.578301 time elapsed: 104.0415 learning rate: 0.000184, scenario: -1, slope: 17.540885448152796, fluctuations: 0.07\n",
      "step: 82220 loss: 444.148108 time elapsed: 104.0551 learning rate: 0.000064, scenario: -1, slope: 10.081258937637953, fluctuations: 0.07\n",
      "step: 82230 loss: 434.679103 time elapsed: 104.0687 learning rate: 0.000022, scenario: -1, slope: 2.572152715637919, fluctuations: 0.07\n",
      "step: 82240 loss: 431.188565 time elapsed: 104.0824 learning rate: 0.000016, scenario: 0, slope: -5.34702389918875, fluctuations: 0.07\n",
      "step: 82250 loss: 428.680892 time elapsed: 104.0960 learning rate: 0.000016, scenario: 0, slope: -14.113208033578845, fluctuations: 0.07\n",
      "step: 82260 loss: 426.674144 time elapsed: 104.1089 learning rate: 0.000016, scenario: 0, slope: -24.31770301017861, fluctuations: 0.07\n",
      "step: 82270 loss: 425.004812 time elapsed: 104.1224 learning rate: 0.000016, scenario: 0, slope: -47.40294834644384, fluctuations: 0.06\n",
      "step: 82280 loss: 423.539228 time elapsed: 104.1362 learning rate: 0.000016, scenario: 0, slope: -9.134773108173867, fluctuations: 0.03\n",
      "step: 82290 loss: 422.192816 time elapsed: 104.1491 learning rate: 0.000016, scenario: 0, slope: -2.475325179395206, fluctuations: 0.01\n",
      "step: 82300 loss: 420.918005 time elapsed: 104.1624 learning rate: 0.000016, scenario: 0, slope: -0.8365046658124939, fluctuations: 0.0\n",
      "step: 82310 loss: 419.274169 time elapsed: 104.1755 learning rate: 0.000038, scenario: 1, slope: -0.27748217082236487, fluctuations: 0.0\n",
      "step: 82320 loss: 415.303704 time elapsed: 104.1884 learning rate: 0.000100, scenario: 1, slope: -0.20197395626348594, fluctuations: 0.0\n",
      "step: 82330 loss: 405.961651 time elapsed: 104.2026 learning rate: 0.000258, scenario: 1, slope: -0.20323432270072228, fluctuations: 0.0\n",
      "step: 82340 loss: 386.105852 time elapsed: 104.2170 learning rate: 0.000669, scenario: 1, slope: -0.2895989882086794, fluctuations: 0.0\n",
      "step: 82350 loss: 351.719445 time elapsed: 104.2323 learning rate: 0.001305, scenario: 0, slope: -0.5030923101602638, fluctuations: 0.0\n",
      "step: 82360 loss: 324.640529 time elapsed: 104.2474 learning rate: 0.001305, scenario: 0, slope: -0.8330334606336661, fluctuations: 0.0\n",
      "step: 82370 loss: 308.081494 time elapsed: 104.2620 learning rate: 0.001305, scenario: 0, slope: -1.1747012448771468, fluctuations: 0.0\n",
      "step: 82380 loss: 296.499001 time elapsed: 104.2760 learning rate: 0.001305, scenario: 0, slope: -1.4664666387736058, fluctuations: 0.0\n",
      "step: 82390 loss: 287.664548 time elapsed: 104.2896 learning rate: 0.001305, scenario: 0, slope: -1.6735216364724415, fluctuations: 0.0\n",
      "step: 82400 loss: 280.377404 time elapsed: 104.3028 learning rate: 0.001305, scenario: 0, slope: -1.76936781002218, fluctuations: 0.0\n",
      "step: 82410 loss: 274.035009 time elapsed: 104.3156 learning rate: 0.001305, scenario: 0, slope: -1.7545836391047978, fluctuations: 0.0\n",
      "step: 82420 loss: 268.306195 time elapsed: 104.3274 learning rate: 0.001305, scenario: 0, slope: -1.613135364660891, fluctuations: 0.0\n",
      "step: 82430 loss: 263.018032 time elapsed: 104.3389 learning rate: 0.001305, scenario: 0, slope: -1.368274535212831, fluctuations: 0.0\n",
      "step: 82440 loss: 258.072032 time elapsed: 104.3512 learning rate: 0.001305, scenario: 0, slope: -1.0719229063051094, fluctuations: 0.0\n",
      "step: 82450 loss: 253.405056 time elapsed: 104.3634 learning rate: 0.001305, scenario: 0, slope: -0.8210964219722907, fluctuations: 0.0\n",
      "step: 82460 loss: 248.973868 time elapsed: 104.3752 learning rate: 0.001305, scenario: 0, slope: -0.6760630388773868, fluctuations: 0.0\n",
      "step: 82470 loss: 244.747395 time elapsed: 104.3869 learning rate: 0.001305, scenario: 0, slope: -0.589603620578563, fluctuations: 0.0\n",
      "step: 82480 loss: 240.702133 time elapsed: 104.4000 learning rate: 0.001305, scenario: 0, slope: -0.5333859739324235, fluctuations: 0.0\n",
      "step: 82490 loss: 236.819724 time elapsed: 104.4132 learning rate: 0.001305, scenario: 0, slope: -0.4936811393630707, fluctuations: 0.0\n",
      "step: 82500 loss: 233.085455 time elapsed: 104.4262 learning rate: 0.001305, scenario: 0, slope: -0.46609551165366714, fluctuations: 0.0\n",
      "step: 82510 loss: 229.487253 time elapsed: 104.4398 learning rate: 0.001305, scenario: 0, slope: -0.4387451769669176, fluctuations: 0.0\n",
      "step: 82520 loss: 226.015015 time elapsed: 104.4525 learning rate: 0.001305, scenario: 0, slope: -0.41785056420844746, fluctuations: 0.0\n",
      "step: 82530 loss: 222.660136 time elapsed: 104.4655 learning rate: 0.001305, scenario: 0, slope: -0.3995990737878552, fluctuations: 0.0\n",
      "step: 82540 loss: 219.415163 time elapsed: 104.4787 learning rate: 0.001305, scenario: 0, slope: -0.38332822614774453, fluctuations: 0.0\n",
      "step: 82550 loss: 216.273537 time elapsed: 104.4918 learning rate: 0.001305, scenario: 0, slope: -0.368603619837929, fluctuations: 0.0\n",
      "step: 82560 loss: 213.229396 time elapsed: 104.5053 learning rate: 0.001305, scenario: 0, slope: -0.35512566731219175, fluctuations: 0.0\n",
      "step: 82570 loss: 210.277419 time elapsed: 104.5175 learning rate: 0.001305, scenario: 0, slope: -0.342679292332515, fluctuations: 0.0\n",
      "step: 82580 loss: 207.412710 time elapsed: 104.5291 learning rate: 0.001305, scenario: 0, slope: -0.33110523643916373, fluctuations: 0.0\n",
      "step: 82590 loss: 204.630693 time elapsed: 104.5410 learning rate: 0.001305, scenario: 0, slope: -0.3202834074694086, fluctuations: 0.0\n",
      "step: 82600 loss: 201.927010 time elapsed: 104.5528 learning rate: 0.001305, scenario: 0, slope: -0.31111065040710695, fluctuations: 0.0\n",
      "step: 82610 loss: 199.297380 time elapsed: 104.5652 learning rate: 0.001305, scenario: 0, slope: -0.30055108926280993, fluctuations: 0.0\n",
      "step: 82620 loss: 196.737406 time elapsed: 104.5769 learning rate: 0.001305, scenario: 0, slope: -0.29151756298022685, fluctuations: 0.0\n",
      "step: 82630 loss: 194.242260 time elapsed: 104.5882 learning rate: 0.001305, scenario: 0, slope: -0.28298429587622453, fluctuations: 0.0\n",
      "step: 82640 loss: 191.806151 time elapsed: 104.6001 learning rate: 0.001305, scenario: 0, slope: -0.27493046464975723, fluctuations: 0.0\n",
      "step: 82650 loss: 189.421353 time elapsed: 104.6139 learning rate: 0.001305, scenario: 0, slope: -0.26735610097889667, fluctuations: 0.0\n",
      "step: 82660 loss: 187.076586 time elapsed: 104.6280 learning rate: 0.001305, scenario: 0, slope: -0.26029227575594066, fluctuations: 0.0\n",
      "step: 82670 loss: 184.755735 time elapsed: 104.6410 learning rate: 0.001305, scenario: 0, slope: -0.25381715454737225, fluctuations: 0.0\n",
      "step: 82680 loss: 182.447592 time elapsed: 104.6554 learning rate: 0.001305, scenario: 0, slope: -0.2480499401792924, fluctuations: 0.0\n",
      "step: 82690 loss: 180.200955 time elapsed: 104.6700 learning rate: 0.001305, scenario: 0, slope: -0.24296311472566767, fluctuations: 0.0\n",
      "step: 82700 loss: 178.068506 time elapsed: 104.6847 learning rate: 0.001305, scenario: 0, slope: -0.238594058785135, fluctuations: 0.0\n",
      "step: 82710 loss: 176.005157 time elapsed: 104.6999 learning rate: 0.001305, scenario: 0, slope: -0.23320817771022342, fluctuations: 0.0\n",
      "step: 82720 loss: 174.013441 time elapsed: 104.7150 learning rate: 0.001305, scenario: 0, slope: -0.22806887298548087, fluctuations: 0.0\n",
      "step: 82730 loss: 172.083052 time elapsed: 104.7284 learning rate: 0.001305, scenario: 0, slope: -0.22258432642497253, fluctuations: 0.0\n",
      "step: 82740 loss: 170.209333 time elapsed: 104.7407 learning rate: 0.001305, scenario: 0, slope: -0.21670788586745962, fluctuations: 0.0\n",
      "step: 82750 loss: 168.388336 time elapsed: 104.7541 learning rate: 0.001305, scenario: 0, slope: -0.21045848848808113, fluctuations: 0.0\n",
      "step: 82760 loss: 166.616765 time elapsed: 104.7672 learning rate: 0.001305, scenario: 0, slope: -0.2039358303884992, fluctuations: 0.0\n",
      "step: 82770 loss: 164.891983 time elapsed: 104.7806 learning rate: 0.001305, scenario: 0, slope: -0.19735334593066167, fluctuations: 0.0\n",
      "step: 82780 loss: 163.211766 time elapsed: 104.7931 learning rate: 0.001305, scenario: 0, slope: -0.19105985086474234, fluctuations: 0.0\n",
      "step: 82790 loss: 161.574179 time elapsed: 104.8058 learning rate: 0.001305, scenario: 0, slope: -0.18535510711434364, fluctuations: 0.0\n",
      "step: 82800 loss: 159.977509 time elapsed: 104.8193 learning rate: 0.001305, scenario: 0, slope: -0.18060216689117686, fluctuations: 0.0\n",
      "step: 82810 loss: 158.404836 time elapsed: 104.8336 learning rate: 0.001737, scenario: 1, slope: -0.17518027697996097, fluctuations: 0.0\n",
      "step: 82820 loss: 155.501805 time elapsed: 104.8469 learning rate: 0.004504, scenario: 1, slope: -0.1736944092399647, fluctuations: 0.0\n",
      "step: 82830 loss: 150.653297 time elapsed: 104.8607 learning rate: 0.004504, scenario: 0, slope: -0.18654805412190945, fluctuations: 0.0\n",
      "step: 82840 loss: 146.168496 time elapsed: 104.8744 learning rate: 0.004504, scenario: 0, slope: -0.21296737302913574, fluctuations: 0.0\n",
      "step: 82850 loss: 142.071337 time elapsed: 104.8885 learning rate: 0.004504, scenario: 0, slope: -0.24710178676620054, fluctuations: 0.0\n",
      "step: 82860 loss: 138.315830 time elapsed: 104.9021 learning rate: 0.004504, scenario: 0, slope: -0.28375136724478417, fluctuations: 0.0\n",
      "step: 82870 loss: 134.850498 time elapsed: 104.9156 learning rate: 0.004504, scenario: 0, slope: -0.3183746042421841, fluctuations: 0.0\n",
      "step: 82880 loss: 131.606489 time elapsed: 104.9294 learning rate: 0.004504, scenario: 0, slope: -0.3470859984660335, fluctuations: 0.0\n",
      "step: 82890 loss: 128.459833 time elapsed: 104.9425 learning rate: 0.004504, scenario: 0, slope: -0.3667897338216048, fluctuations: 0.0\n",
      "step: 82900 loss: 125.361422 time elapsed: 104.9544 learning rate: 0.004504, scenario: 0, slope: -0.3748748645659721, fluctuations: 0.0\n",
      "step: 82910 loss: 125.964687 time elapsed: 104.9668 learning rate: 0.004504, scenario: 0, slope: -0.36725487582520766, fluctuations: 0.0\n",
      "step: 82920 loss: 122.715111 time elapsed: 104.9789 learning rate: 0.004504, scenario: 0, slope: -0.32872648872325233, fluctuations: 0.03\n",
      "step: 82930 loss: 119.068049 time elapsed: 104.9910 learning rate: 0.004504, scenario: 0, slope: -0.2988478508959683, fluctuations: 0.06\n",
      "step: 82940 loss: 116.276191 time elapsed: 105.0026 learning rate: 0.004504, scenario: 0, slope: -0.27823229993230736, fluctuations: 0.08\n",
      "step: 82950 loss: 113.717910 time elapsed: 105.0145 learning rate: 0.004504, scenario: 0, slope: -0.264670378604818, fluctuations: 0.08\n",
      "step: 82960 loss: 111.334714 time elapsed: 105.0273 learning rate: 0.004504, scenario: 0, slope: -0.25609065046760277, fluctuations: 0.08\n",
      "step: 82970 loss: 109.172216 time elapsed: 105.0414 learning rate: 0.004504, scenario: 0, slope: -0.25011695606950113, fluctuations: 0.08\n",
      "step: 82980 loss: 107.991424 time elapsed: 105.0551 learning rate: 0.004504, scenario: 0, slope: -0.24490267340366254, fluctuations: 0.09\n",
      "step: 82990 loss: 105.659898 time elapsed: 105.0691 learning rate: 0.004504, scenario: 0, slope: -0.24151403095524787, fluctuations: 0.12\n",
      "step: 83000 loss: 103.705079 time elapsed: 105.0827 learning rate: 0.004504, scenario: 0, slope: -0.24068240573149494, fluctuations: 0.13\n",
      "step: 83010 loss: 101.841922 time elapsed: 105.0974 learning rate: 0.004504, scenario: 0, slope: -0.24863098203332074, fluctuations: 0.12\n",
      "step: 83020 loss: 99.365593 time elapsed: 105.1120 learning rate: 0.004504, scenario: 0, slope: -0.21905534879381083, fluctuations: 0.1\n",
      "step: 83030 loss: 97.841727 time elapsed: 105.1269 learning rate: 0.004504, scenario: 0, slope: -0.206644133957478, fluctuations: 0.11\n",
      "step: 83040 loss: 95.563417 time elapsed: 105.1412 learning rate: 0.004504, scenario: 0, slope: -0.1992185517599831, fluctuations: 0.11\n",
      "step: 83050 loss: 93.805357 time elapsed: 105.1546 learning rate: 0.004504, scenario: 0, slope: -0.19495708237965464, fluctuations: 0.11\n",
      "step: 83060 loss: 92.018366 time elapsed: 105.1674 learning rate: 0.004504, scenario: 0, slope: -0.19366116196234906, fluctuations: 0.11\n",
      "step: 83070 loss: 89.607777 time elapsed: 105.1803 learning rate: 0.004504, scenario: 0, slope: -0.19547266916224143, fluctuations: 0.11\n",
      "step: 83080 loss: 88.766905 time elapsed: 105.1930 learning rate: 0.004504, scenario: 0, slope: -0.1968055077738343, fluctuations: 0.11\n",
      "step: 83090 loss: 85.816638 time elapsed: 105.2056 learning rate: 0.004504, scenario: 0, slope: -0.1961507737630272, fluctuations: 0.13\n",
      "step: 83100 loss: 84.435270 time elapsed: 105.2188 learning rate: 0.004504, scenario: 0, slope: -0.1962145597992776, fluctuations: 0.16\n",
      "step: 83110 loss: 82.821913 time elapsed: 105.2321 learning rate: 0.004504, scenario: 0, slope: -0.19187451222569066, fluctuations: 0.19\n",
      "step: 83120 loss: 81.477051 time elapsed: 105.2460 learning rate: 0.004504, scenario: 0, slope: -0.18635979357130678, fluctuations: 0.2\n",
      "step: 83130 loss: 80.121765 time elapsed: 105.2602 learning rate: 0.004504, scenario: 0, slope: -0.18041307820784303, fluctuations: 0.17\n",
      "step: 83140 loss: 79.014358 time elapsed: 105.2739 learning rate: 0.004504, scenario: 0, slope: -0.17193082350103833, fluctuations: 0.18\n",
      "step: 83150 loss: 77.793534 time elapsed: 105.2872 learning rate: 0.004504, scenario: 0, slope: -0.16011570256473104, fluctuations: 0.2\n",
      "step: 83160 loss: 76.865855 time elapsed: 105.3009 learning rate: 0.004504, scenario: 0, slope: -0.1483734210977119, fluctuations: 0.23\n",
      "step: 83170 loss: 75.683624 time elapsed: 105.3148 learning rate: 0.004504, scenario: 0, slope: -0.1356901170950769, fluctuations: 0.24\n",
      "step: 83180 loss: 74.749485 time elapsed: 105.3280 learning rate: 0.004504, scenario: 0, slope: -0.12547568869765102, fluctuations: 0.21\n",
      "step: 83190 loss: 73.766417 time elapsed: 105.3413 learning rate: 0.004504, scenario: 0, slope: -0.11664326440208378, fluctuations: 0.17\n",
      "step: 83200 loss: 72.889325 time elapsed: 105.3552 learning rate: 0.004504, scenario: 0, slope: -0.11076650640385799, fluctuations: 0.15\n",
      "step: 83210 loss: 72.253295 time elapsed: 105.3681 learning rate: 0.004504, scenario: 0, slope: -0.1047896297986537, fluctuations: 0.13\n",
      "step: 83220 loss: 70.900909 time elapsed: 105.3814 learning rate: 0.004504, scenario: 0, slope: -0.1012998731012905, fluctuations: 0.12\n",
      "step: 83230 loss: 69.986579 time elapsed: 105.3934 learning rate: 0.004504, scenario: 0, slope: -0.09966801814619491, fluctuations: 0.12\n",
      "step: 83240 loss: 69.501776 time elapsed: 105.4054 learning rate: 0.004504, scenario: 0, slope: -0.09809336676110131, fluctuations: 0.12\n",
      "step: 83250 loss: 68.593779 time elapsed: 105.4179 learning rate: 0.004504, scenario: 0, slope: -0.09423994580132963, fluctuations: 0.12\n",
      "step: 83260 loss: 67.466311 time elapsed: 105.4302 learning rate: 0.004504, scenario: 0, slope: -0.0924933168043949, fluctuations: 0.11\n",
      "step: 83270 loss: 66.607091 time elapsed: 105.4444 learning rate: 0.004504, scenario: 0, slope: -0.09154663960891815, fluctuations: 0.11\n",
      "step: 83280 loss: 65.840582 time elapsed: 105.4584 learning rate: 0.004504, scenario: 0, slope: -0.09060650216788951, fluctuations: 0.11\n",
      "step: 83290 loss: 65.091226 time elapsed: 105.4722 learning rate: 0.004504, scenario: 0, slope: -0.08961683884820844, fluctuations: 0.11\n",
      "step: 83300 loss: 65.581610 time elapsed: 105.4851 learning rate: 0.004504, scenario: 0, slope: -0.08460024122499957, fluctuations: 0.09\n",
      "step: 83310 loss: 63.971016 time elapsed: 105.4991 learning rate: 0.004504, scenario: 0, slope: -0.08022322505294859, fluctuations: 0.09\n",
      "step: 83320 loss: 63.075691 time elapsed: 105.5149 learning rate: 0.004504, scenario: 0, slope: -0.0765919130730497, fluctuations: 0.1\n",
      "step: 83330 loss: 62.346920 time elapsed: 105.5293 learning rate: 0.004504, scenario: 0, slope: -0.07453717046328667, fluctuations: 0.1\n",
      "step: 83340 loss: 61.702470 time elapsed: 105.5436 learning rate: 0.004504, scenario: 0, slope: -0.07330022526918513, fluctuations: 0.09\n",
      "step: 83350 loss: 61.036402 time elapsed: 105.5642 learning rate: 0.004504, scenario: 0, slope: -0.07078522962240263, fluctuations: 0.07\n",
      "step: 83360 loss: 60.446765 time elapsed: 105.5777 learning rate: 0.004504, scenario: 0, slope: -0.06994174690550835, fluctuations: 0.05\n",
      "step: 83370 loss: 60.718036 time elapsed: 105.5891 learning rate: 0.004504, scenario: 0, slope: -0.0669363657620537, fluctuations: 0.06\n",
      "step: 83380 loss: 62.702004 time elapsed: 105.6023 learning rate: 0.007254, scenario: 1, slope: -0.06319225317997774, fluctuations: 0.07\n",
      "step: 83390 loss: 125.834207 time elapsed: 105.6158 learning rate: 0.003279, scenario: -1, slope: 0.41078456467349556, fluctuations: 0.09\n",
      "step: 83400 loss: 82.688316 time elapsed: 105.6295 learning rate: 0.001270, scenario: -1, slope: 0.4438384434202652, fluctuations: 0.11\n",
      "step: 83410 loss: 69.528223 time elapsed: 105.6427 learning rate: 0.000443, scenario: -1, slope: 0.3810881794176239, fluctuations: 0.11\n",
      "step: 83420 loss: 67.590057 time elapsed: 105.6564 learning rate: 0.000154, scenario: -1, slope: 0.28117245048920453, fluctuations: 0.1\n",
      "step: 83430 loss: 67.201371 time elapsed: 105.6707 learning rate: 0.000054, scenario: -1, slope: 0.1726936018558398, fluctuations: 0.1\n",
      "step: 83440 loss: 67.085980 time elapsed: 105.6848 learning rate: 0.000019, scenario: -1, slope: 0.06215349759523153, fluctuations: 0.1\n",
      "step: 83450 loss: 67.047834 time elapsed: 105.6987 learning rate: 0.000017, scenario: 1, slope: -0.05914155999524402, fluctuations: 0.1\n",
      "step: 83460 loss: 67.013251 time elapsed: 105.7124 learning rate: 0.000017, scenario: 0, slope: -0.2026918599585859, fluctuations: 0.1\n",
      "step: 83470 loss: 66.985045 time elapsed: 105.7263 learning rate: 0.000017, scenario: 0, slope: -0.3559593478891722, fluctuations: 0.09\n",
      "step: 83480 loss: 66.958755 time elapsed: 105.7397 learning rate: 0.000017, scenario: 0, slope: -0.53334584753618, fluctuations: 0.08\n",
      "step: 83490 loss: 66.932740 time elapsed: 105.7527 learning rate: 0.000017, scenario: 0, slope: -0.1186953456694999, fluctuations: 0.05\n",
      "step: 83500 loss: 66.895210 time elapsed: 105.7662 learning rate: 0.000040, scenario: 1, slope: -0.042376111483033684, fluctuations: 0.02\n",
      "step: 83510 loss: 66.808824 time elapsed: 105.7796 learning rate: 0.000105, scenario: 1, slope: -0.011301178708184811, fluctuations: 0.0\n",
      "step: 83520 loss: 66.596335 time elapsed: 105.7916 learning rate: 0.000272, scenario: 1, slope: -0.005277693371958035, fluctuations: 0.0\n",
      "step: 83530 loss: 66.101136 time elapsed: 105.8038 learning rate: 0.000705, scenario: 1, slope: -0.006452807331002742, fluctuations: 0.0\n",
      "step: 83540 loss: 65.103016 time elapsed: 105.8159 learning rate: 0.001828, scenario: 1, slope: -0.012018463302717516, fluctuations: 0.0\n",
      "step: 83550 loss: 63.533425 time elapsed: 105.8275 learning rate: 0.004743, scenario: 1, slope: -0.02333346962185088, fluctuations: 0.0\n",
      "step: 83560 loss: 61.501744 time elapsed: 105.8396 learning rate: 0.012301, scenario: 1, slope: -0.04151879903390497, fluctuations: 0.0\n",
      "step: 83570 loss: 16149.720654 time elapsed: 105.8511 learning rate: 0.010151, scenario: -1, slope: 58.707367938550604, fluctuations: 0.02\n",
      "step: 83580 loss: 13259.985990 time elapsed: 105.8641 learning rate: 0.003540, scenario: -1, slope: 145.36586521670583, fluctuations: 0.04\n",
      "step: 83590 loss: 7288.150113 time elapsed: 105.8783 learning rate: 0.001234, scenario: -1, slope: 157.9165345386044, fluctuations: 0.06\n",
      "step: 83600 loss: 4880.961814 time elapsed: 105.8918 learning rate: 0.000478, scenario: -1, slope: 141.3759699888222, fluctuations: 0.06\n",
      "step: 83610 loss: 4421.999121 time elapsed: 105.9055 learning rate: 0.000167, scenario: -1, slope: 110.98818043842971, fluctuations: 0.06\n",
      "step: 83620 loss: 4182.636518 time elapsed: 105.9188 learning rate: 0.000058, scenario: -1, slope: 77.86482173258862, fluctuations: 0.06\n",
      "step: 83630 loss: 4116.648984 time elapsed: 105.9323 learning rate: 0.000020, scenario: -1, slope: 40.052235687294846, fluctuations: 0.06\n",
      "step: 83640 loss: 4096.382088 time elapsed: 105.9459 learning rate: 0.000008, scenario: 0, slope: -3.574755720865157, fluctuations: 0.06\n",
      "step: 83650 loss: 4086.143679 time elapsed: 105.9594 learning rate: 0.000008, scenario: 0, slope: -55.24546453115307, fluctuations: 0.06\n",
      "step: 83660 loss: 4076.227718 time elapsed: 105.9725 learning rate: 0.000008, scenario: 0, slope: -118.39171232291655, fluctuations: 0.06\n",
      "step: 83670 loss: 4066.555662 time elapsed: 105.9843 learning rate: 0.000008, scenario: 0, slope: -135.40556963228582, fluctuations: 0.03\n",
      "step: 83680 loss: 4057.088526 time elapsed: 105.9962 learning rate: 0.000008, scenario: 0, slope: -35.674517360536896, fluctuations: 0.01\n",
      "step: 83690 loss: 4047.802673 time elapsed: 106.0084 learning rate: 0.000008, scenario: 0, slope: -11.458397771196115, fluctuations: 0.0\n",
      "step: 83700 loss: 4038.681158 time elapsed: 106.0200 learning rate: 0.000009, scenario: 1, slope: -4.838725697327274, fluctuations: 0.0\n",
      "step: 83710 loss: 4024.337186 time elapsed: 106.0328 learning rate: 0.000022, scenario: 1, slope: -1.921833677930359, fluctuations: 0.0\n",
      "step: 83720 loss: 3988.513877 time elapsed: 106.0450 learning rate: 0.000058, scenario: 1, slope: -1.2371728263991773, fluctuations: 0.0\n",
      "step: 83730 loss: 3901.920017 time elapsed: 106.0568 learning rate: 0.000151, scenario: 1, slope: -1.42123920543901, fluctuations: 0.0\n",
      "step: 83740 loss: 3710.822450 time elapsed: 106.0708 learning rate: 0.000391, scenario: 1, slope: -2.37630194308867, fluctuations: 0.0\n",
      "step: 83750 loss: 3355.761078 time elapsed: 106.0844 learning rate: 0.000838, scenario: 0, slope: -4.578400786017878, fluctuations: 0.0\n",
      "step: 83760 loss: 2979.595058 time elapsed: 106.0996 learning rate: 0.000838, scenario: 0, slope: -8.254439251056283, fluctuations: 0.0\n",
      "step: 83770 loss: 2640.930274 time elapsed: 106.1146 learning rate: 0.000838, scenario: 0, slope: -12.86113804287726, fluctuations: 0.0\n",
      "step: 83780 loss: 2414.303310 time elapsed: 106.1292 learning rate: 0.000838, scenario: 0, slope: -17.40272246108331, fluctuations: 0.0\n",
      "step: 83790 loss: 2248.083534 time elapsed: 106.1440 learning rate: 0.000838, scenario: 0, slope: -21.171438420668586, fluctuations: 0.0\n",
      "step: 83800 loss: 2108.423141 time elapsed: 106.1581 learning rate: 0.000838, scenario: 0, slope: -23.543724327557367, fluctuations: 0.0\n",
      "step: 83810 loss: 1990.832348 time elapsed: 106.1730 learning rate: 0.000838, scenario: 0, slope: -24.803880674023823, fluctuations: 0.0\n",
      "step: 83820 loss: 1888.381530 time elapsed: 106.1870 learning rate: 0.000838, scenario: 0, slope: -24.22984838033487, fluctuations: 0.0\n",
      "step: 83830 loss: 1798.088076 time elapsed: 106.2007 learning rate: 0.000838, scenario: 0, slope: -22.07512514962786, fluctuations: 0.0\n",
      "step: 83840 loss: 1718.532545 time elapsed: 106.2130 learning rate: 0.000838, scenario: 0, slope: -18.748870179802704, fluctuations: 0.0\n",
      "step: 83850 loss: 1648.086210 time elapsed: 106.2251 learning rate: 0.000838, scenario: 0, slope: -15.172102028054141, fluctuations: 0.0\n",
      "step: 83860 loss: 1576.515195 time elapsed: 106.2371 learning rate: 0.000838, scenario: 0, slope: -12.28675498713632, fluctuations: 0.0\n",
      "step: 83870 loss: 1360.237537 time elapsed: 106.2494 learning rate: 0.000838, scenario: 0, slope: -10.907243752063772, fluctuations: 0.0\n",
      "step: 83880 loss: 1232.494800 time elapsed: 106.2614 learning rate: 0.000838, scenario: 0, slope: -10.534722847128862, fluctuations: 0.02\n",
      "step: 83890 loss: 1162.301213 time elapsed: 106.2732 learning rate: 0.000838, scenario: 0, slope: -10.468557208002519, fluctuations: 0.02\n",
      "step: 83900 loss: 1097.919201 time elapsed: 106.2868 learning rate: 0.000838, scenario: 0, slope: -10.415919399890246, fluctuations: 0.02\n",
      "step: 83910 loss: 1043.834673 time elapsed: 106.3011 learning rate: 0.000838, scenario: 0, slope: -10.251318643708736, fluctuations: 0.02\n",
      "step: 83920 loss: 1005.364035 time elapsed: 106.3150 learning rate: 0.000838, scenario: 0, slope: -9.888016944718169, fluctuations: 0.02\n",
      "step: 83930 loss: 974.759204 time elapsed: 106.3285 learning rate: 0.000838, scenario: 0, slope: -9.250440359701285, fluctuations: 0.02\n",
      "step: 83940 loss: 948.048711 time elapsed: 106.3417 learning rate: 0.000838, scenario: 0, slope: -8.301575383910986, fluctuations: 0.02\n",
      "step: 83950 loss: 923.929048 time elapsed: 106.3548 learning rate: 0.000838, scenario: 0, slope: -7.00940182669996, fluctuations: 0.02\n",
      "step: 83960 loss: 901.724270 time elapsed: 106.3686 learning rate: 0.000838, scenario: 0, slope: -5.357539838999432, fluctuations: 0.02\n",
      "step: 83970 loss: 881.140567 time elapsed: 106.3825 learning rate: 0.000838, scenario: 0, slope: -4.084222209833466, fluctuations: 0.01\n",
      "step: 83980 loss: 861.894304 time elapsed: 106.3954 learning rate: 0.000838, scenario: 0, slope: -3.4111006011550034, fluctuations: 0.0\n",
      "step: 83990 loss: 843.791934 time elapsed: 106.4076 learning rate: 0.000838, scenario: 0, slope: -2.8848065362803386, fluctuations: 0.0\n",
      "step: 84000 loss: 826.685159 time elapsed: 106.4191 learning rate: 0.000838, scenario: 0, slope: -2.519839082777274, fluctuations: 0.0\n",
      "step: 84010 loss: 810.458111 time elapsed: 106.4315 learning rate: 0.000838, scenario: 0, slope: -2.22341026163677, fluctuations: 0.0\n",
      "step: 84020 loss: 795.020584 time elapsed: 106.4438 learning rate: 0.000838, scenario: 0, slope: -2.0430216709174114, fluctuations: 0.0\n",
      "step: 84030 loss: 780.300522 time elapsed: 106.4555 learning rate: 0.000838, scenario: 0, slope: -1.9043141038222529, fluctuations: 0.0\n",
      "step: 84040 loss: 766.240230 time elapsed: 106.4677 learning rate: 0.000838, scenario: 0, slope: -1.787878710205704, fluctuations: 0.0\n",
      "step: 84050 loss: 752.792663 time elapsed: 106.4811 learning rate: 0.000838, scenario: 0, slope: -1.6872812472604293, fluctuations: 0.0\n",
      "step: 84060 loss: 739.918319 time elapsed: 106.4962 learning rate: 0.000838, scenario: 0, slope: -1.5988981347841167, fluctuations: 0.0\n",
      "step: 84070 loss: 727.582773 time elapsed: 106.5105 learning rate: 0.000838, scenario: 0, slope: -1.5199183368978162, fluctuations: 0.0\n",
      "step: 84080 loss: 715.754805 time elapsed: 106.5243 learning rate: 0.000838, scenario: 0, slope: -1.448363766018834, fluctuations: 0.0\n",
      "step: 84090 loss: 704.405084 time elapsed: 106.5392 learning rate: 0.000838, scenario: 0, slope: -1.382789627239694, fluctuations: 0.0\n",
      "step: 84100 loss: 693.505351 time elapsed: 106.5542 learning rate: 0.000838, scenario: 0, slope: -1.3280135452623334, fluctuations: 0.0\n",
      "step: 84110 loss: 683.028028 time elapsed: 106.5692 learning rate: 0.000838, scenario: 0, slope: -1.2656813889486462, fluctuations: 0.0\n",
      "step: 84120 loss: 672.946129 time elapsed: 106.5839 learning rate: 0.000838, scenario: 0, slope: -1.212854745318695, fluctuations: 0.0\n",
      "step: 84130 loss: 663.233386 time elapsed: 106.5977 learning rate: 0.000838, scenario: 0, slope: -1.1633005474266953, fluctuations: 0.0\n",
      "step: 84140 loss: 653.864495 time elapsed: 106.6105 learning rate: 0.000838, scenario: 0, slope: -1.116771758854957, fluctuations: 0.0\n",
      "step: 84150 loss: 644.815396 time elapsed: 106.6224 learning rate: 0.000838, scenario: 0, slope: -1.0731001731815966, fluctuations: 0.0\n",
      "step: 84160 loss: 636.063530 time elapsed: 106.6354 learning rate: 0.000838, scenario: 0, slope: -1.0321621971213615, fluctuations: 0.0\n",
      "step: 84170 loss: 627.588030 time elapsed: 106.6481 learning rate: 0.000838, scenario: 0, slope: -0.9938526173811174, fluctuations: 0.0\n",
      "step: 84180 loss: 619.369814 time elapsed: 106.6606 learning rate: 0.000838, scenario: 0, slope: -0.9580664419589529, fluctuations: 0.0\n",
      "step: 84190 loss: 611.391589 time elapsed: 106.6721 learning rate: 0.000838, scenario: 0, slope: -0.9246881817343944, fluctuations: 0.0\n",
      "step: 84200 loss: 603.637761 time elapsed: 106.6842 learning rate: 0.000838, scenario: 0, slope: -0.8965990893189666, fluctuations: 0.0\n",
      "step: 84210 loss: 596.094295 time elapsed: 106.6983 learning rate: 0.000838, scenario: 0, slope: -0.8646197051775247, fluctuations: 0.0\n",
      "step: 84220 loss: 588.748532 time elapsed: 106.7124 learning rate: 0.000838, scenario: 0, slope: -0.8376297636446493, fluctuations: 0.0\n",
      "step: 84230 loss: 581.589026 time elapsed: 106.7260 learning rate: 0.000838, scenario: 0, slope: -0.8124577639762713, fluctuations: 0.0\n",
      "step: 84240 loss: 574.605380 time elapsed: 106.7395 learning rate: 0.000838, scenario: 0, slope: -0.7889449422935937, fluctuations: 0.0\n",
      "step: 84250 loss: 567.788132 time elapsed: 106.7529 learning rate: 0.000838, scenario: 0, slope: -0.7669388980343109, fluctuations: 0.0\n",
      "step: 84260 loss: 561.128665 time elapsed: 106.7666 learning rate: 0.000838, scenario: 0, slope: -0.7462974210181003, fluctuations: 0.0\n",
      "step: 84270 loss: 554.619134 time elapsed: 106.7798 learning rate: 0.000838, scenario: 0, slope: -0.7268906418625409, fluctuations: 0.0\n",
      "step: 84280 loss: 548.252425 time elapsed: 106.7934 learning rate: 0.000838, scenario: 0, slope: -0.7086015857174011, fluctuations: 0.0\n",
      "step: 84290 loss: 542.022110 time elapsed: 106.8061 learning rate: 0.000838, scenario: 0, slope: -0.6913254675294334, fluctuations: 0.0\n",
      "step: 84300 loss: 535.922406 time elapsed: 106.8180 learning rate: 0.000838, scenario: 0, slope: -0.6765650712253013, fluctuations: 0.0\n",
      "step: 84310 loss: 529.948143 time elapsed: 106.8303 learning rate: 0.000838, scenario: 0, slope: -0.6594446340272446, fluctuations: 0.0\n",
      "step: 84320 loss: 524.094712 time elapsed: 106.8422 learning rate: 0.000838, scenario: 0, slope: -0.6446768393246127, fluctuations: 0.0\n",
      "step: 84330 loss: 518.358022 time elapsed: 106.8539 learning rate: 0.000838, scenario: 0, slope: -0.6305928333105821, fluctuations: 0.0\n",
      "step: 84340 loss: 512.734448 time elapsed: 106.8658 learning rate: 0.000838, scenario: 0, slope: -0.6171257038706667, fluctuations: 0.0\n",
      "step: 84350 loss: 507.220772 time elapsed: 106.8773 learning rate: 0.000838, scenario: 0, slope: -0.604213201614011, fluctuations: 0.0\n",
      "step: 84360 loss: 501.814113 time elapsed: 106.8899 learning rate: 0.000838, scenario: 0, slope: -0.5917976951993061, fluctuations: 0.0\n",
      "step: 84370 loss: 496.511848 time elapsed: 106.9040 learning rate: 0.000838, scenario: 0, slope: -0.5798264246719915, fluctuations: 0.0\n",
      "step: 84380 loss: 491.311489 time elapsed: 106.9174 learning rate: 0.000838, scenario: 0, slope: -0.5682520650459615, fluctuations: 0.0\n",
      "step: 84390 loss: 486.210453 time elapsed: 106.9309 learning rate: 0.000838, scenario: 0, slope: -0.557033853466038, fluctuations: 0.0\n",
      "step: 84400 loss: 481.205438 time elapsed: 106.9441 learning rate: 0.000838, scenario: 0, slope: -0.5472157056126906, fluctuations: 0.0\n",
      "step: 84410 loss: 476.289900 time elapsed: 106.9593 learning rate: 0.000838, scenario: 0, slope: -0.5355591465352607, fluctuations: 0.0\n",
      "step: 84420 loss: 471.436408 time elapsed: 106.9740 learning rate: 0.000838, scenario: 0, slope: -0.5253491572036008, fluctuations: 0.0\n",
      "step: 84430 loss: 465.634899 time elapsed: 106.9886 learning rate: 0.001485, scenario: 1, slope: -0.5170152068095456, fluctuations: 0.0\n",
      "step: 84440 loss: 454.784094 time elapsed: 107.0033 learning rate: 0.001633, scenario: 0, slope: -0.5328293513835386, fluctuations: 0.0\n",
      "step: 84450 loss: 445.039951 time elapsed: 107.0168 learning rate: 0.001633, scenario: 0, slope: -0.5730590204268909, fluctuations: 0.0\n",
      "step: 84460 loss: 436.081001 time elapsed: 107.0283 learning rate: 0.001633, scenario: 0, slope: -0.6266962754258438, fluctuations: 0.0\n",
      "step: 84470 loss: 427.498116 time elapsed: 107.0394 learning rate: 0.001633, scenario: 0, slope: -0.6859711489180547, fluctuations: 0.0\n",
      "step: 84480 loss: 419.168846 time elapsed: 107.0513 learning rate: 0.001633, scenario: 0, slope: -0.7444785293241556, fluctuations: 0.0\n",
      "step: 84490 loss: 411.127291 time elapsed: 107.0630 learning rate: 0.001633, scenario: 0, slope: -0.7961716826015766, fluctuations: 0.0\n",
      "step: 84500 loss: 403.408019 time elapsed: 107.0740 learning rate: 0.001633, scenario: 0, slope: -0.831994203458179, fluctuations: 0.0\n",
      "step: 84510 loss: 396.115628 time elapsed: 107.0882 learning rate: 0.001633, scenario: 0, slope: -0.8553187261550288, fluctuations: 0.0\n",
      "step: 84520 loss: 389.295754 time elapsed: 107.1028 learning rate: 0.001633, scenario: 0, slope: -0.8506461435170679, fluctuations: 0.0\n",
      "step: 84530 loss: 382.913076 time elapsed: 107.1183 learning rate: 0.001633, scenario: 0, slope: -0.8171453784956462, fluctuations: 0.0\n",
      "step: 84540 loss: 376.878857 time elapsed: 107.1356 learning rate: 0.001633, scenario: 0, slope: -0.7770752417092139, fluctuations: 0.0\n",
      "step: 84550 loss: 371.109686 time elapsed: 107.1522 learning rate: 0.001633, scenario: 0, slope: -0.7398428827220457, fluctuations: 0.0\n",
      "step: 84560 loss: 365.553142 time elapsed: 107.1673 learning rate: 0.001633, scenario: 0, slope: -0.7032929366099439, fluctuations: 0.0\n",
      "step: 84570 loss: 360.177461 time elapsed: 107.1823 learning rate: 0.001633, scenario: 0, slope: -0.6675493815202956, fluctuations: 0.0\n",
      "step: 84580 loss: 354.958928 time elapsed: 107.1972 learning rate: 0.001633, scenario: 0, slope: -0.6336630734753125, fluctuations: 0.0\n",
      "step: 84590 loss: 349.879813 time elapsed: 107.2131 learning rate: 0.001633, scenario: 0, slope: -0.6027745213251484, fluctuations: 0.0\n",
      "step: 84600 loss: 344.928742 time elapsed: 107.2265 learning rate: 0.001633, scenario: 0, slope: -0.578339372888941, fluctuations: 0.0\n",
      "step: 84610 loss: 340.099798 time elapsed: 107.2394 learning rate: 0.001633, scenario: 0, slope: -0.5530968828173826, fluctuations: 0.0\n",
      "step: 84620 loss: 335.390657 time elapsed: 107.2535 learning rate: 0.001633, scenario: 0, slope: -0.5340136656025596, fluctuations: 0.0\n",
      "step: 84630 loss: 330.799906 time elapsed: 107.2671 learning rate: 0.001633, scenario: 0, slope: -0.517631360877719, fluctuations: 0.0\n",
      "step: 84640 loss: 326.324417 time elapsed: 107.2798 learning rate: 0.001633, scenario: 0, slope: -0.5030512132919901, fluctuations: 0.0\n",
      "step: 84650 loss: 321.958484 time elapsed: 107.2919 learning rate: 0.001633, scenario: 0, slope: -0.4896635727717915, fluctuations: 0.0\n",
      "step: 84660 loss: 317.695083 time elapsed: 107.3065 learning rate: 0.001633, scenario: 0, slope: -0.47711206900851605, fluctuations: 0.0\n",
      "step: 84670 loss: 313.527435 time elapsed: 107.3204 learning rate: 0.001633, scenario: 0, slope: -0.46518771386569074, fluctuations: 0.0\n",
      "step: 84680 loss: 309.449588 time elapsed: 107.3358 learning rate: 0.001633, scenario: 0, slope: -0.4537789045065747, fluctuations: 0.0\n",
      "step: 84690 loss: 305.456428 time elapsed: 107.3499 learning rate: 0.001633, scenario: 0, slope: -0.4428493455362777, fluctuations: 0.0\n",
      "step: 84700 loss: 301.543577 time elapsed: 107.3634 learning rate: 0.001633, scenario: 0, slope: -0.433431556590108, fluctuations: 0.0\n",
      "step: 84710 loss: 297.707262 time elapsed: 107.3777 learning rate: 0.001633, scenario: 0, slope: -0.422490298486492, fluctuations: 0.0\n",
      "step: 84720 loss: 293.944163 time elapsed: 107.3915 learning rate: 0.001633, scenario: 0, slope: -0.4131048501238178, fluctuations: 0.0\n",
      "step: 84730 loss: 290.251301 time elapsed: 107.4052 learning rate: 0.001633, scenario: 0, slope: -0.40424269649276245, fluctuations: 0.0\n",
      "step: 84740 loss: 286.625949 time elapsed: 107.4187 learning rate: 0.001633, scenario: 0, slope: -0.3958648154116378, fluctuations: 0.0\n",
      "step: 84750 loss: 283.065565 time elapsed: 107.4315 learning rate: 0.001633, scenario: 0, slope: -0.3879184820134451, fluctuations: 0.0\n",
      "step: 84760 loss: 279.567743 time elapsed: 107.4438 learning rate: 0.001633, scenario: 0, slope: -0.3803521286819284, fluctuations: 0.0\n",
      "step: 84770 loss: 276.130169 time elapsed: 107.4560 learning rate: 0.001633, scenario: 0, slope: -0.3731224772257356, fluctuations: 0.0\n",
      "step: 84780 loss: 272.750597 time elapsed: 107.4683 learning rate: 0.001633, scenario: 0, slope: -0.3661954825331092, fluctuations: 0.0\n",
      "step: 84790 loss: 269.426818 time elapsed: 107.4811 learning rate: 0.001633, scenario: 0, slope: -0.35954530883288427, fluctuations: 0.0\n",
      "step: 84800 loss: 266.156643 time elapsed: 107.4934 learning rate: 0.001633, scenario: 0, slope: -0.3537809650833077, fluctuations: 0.0\n",
      "step: 84810 loss: 262.937882 time elapsed: 107.5065 learning rate: 0.001633, scenario: 0, slope: -0.3470045141149068, fluctuations: 0.0\n",
      "step: 84820 loss: 259.768326 time elapsed: 107.5212 learning rate: 0.001633, scenario: 0, slope: -0.3410905963385626, fluctuations: 0.0\n",
      "step: 84830 loss: 256.645725 time elapsed: 107.5365 learning rate: 0.001633, scenario: 0, slope: -0.3354049000870928, fluctuations: 0.0\n",
      "step: 84840 loss: 253.567760 time elapsed: 107.5513 learning rate: 0.001633, scenario: 0, slope: -0.32994396053161723, fluctuations: 0.0\n",
      "step: 84850 loss: 250.532005 time elapsed: 107.5657 learning rate: 0.001633, scenario: 0, slope: -0.3247068135091647, fluctuations: 0.0\n",
      "step: 84860 loss: 247.535861 time elapsed: 107.5809 learning rate: 0.001633, scenario: 0, slope: -0.31969501994415706, fluctuations: 0.0\n",
      "step: 84870 loss: 244.576443 time elapsed: 107.5966 learning rate: 0.001633, scenario: 0, slope: -0.31491311795554117, fluctuations: 0.0\n",
      "step: 84880 loss: 241.650345 time elapsed: 107.6120 learning rate: 0.001633, scenario: 0, slope: -0.3103698300743121, fluctuations: 0.0\n",
      "step: 84890 loss: 238.753150 time elapsed: 107.6297 learning rate: 0.001633, scenario: 0, slope: -0.30608082187910857, fluctuations: 0.0\n",
      "step: 84900 loss: 235.878311 time elapsed: 107.6447 learning rate: 0.001633, scenario: 0, slope: -0.3024615328734508, fluctuations: 0.0\n",
      "step: 84910 loss: 233.014538 time elapsed: 107.6610 learning rate: 0.001633, scenario: 0, slope: -0.2984090495904606, fluctuations: 0.0\n",
      "step: 84920 loss: 230.141276 time elapsed: 107.6744 learning rate: 0.001633, scenario: 0, slope: -0.29519903012044196, fluctuations: 0.0\n",
      "step: 84930 loss: 227.241529 time elapsed: 107.6875 learning rate: 0.001633, scenario: 0, slope: -0.29263592067718214, fluctuations: 0.0\n",
      "step: 84940 loss: 224.376181 time elapsed: 107.7006 learning rate: 0.001633, scenario: 0, slope: -0.2907135125665532, fluctuations: 0.0\n",
      "step: 84950 loss: 221.564655 time elapsed: 107.7126 learning rate: 0.001633, scenario: 0, slope: -0.2891287641383194, fluctuations: 0.0\n",
      "step: 84960 loss: 220.385413 time elapsed: 107.7251 learning rate: 0.001633, scenario: 0, slope: -0.28169221803413474, fluctuations: 0.03\n",
      "step: 84970 loss: 216.482675 time elapsed: 107.7393 learning rate: 0.001633, scenario: 0, slope: -0.2785516361809511, fluctuations: 0.08\n",
      "step: 84980 loss: 213.693594 time elapsed: 107.7533 learning rate: 0.001633, scenario: 0, slope: -0.2764047505358955, fluctuations: 0.13\n",
      "step: 84990 loss: 211.148541 time elapsed: 107.7672 learning rate: 0.001633, scenario: 0, slope: -0.2737317052914419, fluctuations: 0.14\n",
      "step: 85000 loss: 208.536435 time elapsed: 107.7806 learning rate: 0.001633, scenario: 0, slope: -0.2716356450618442, fluctuations: 0.14\n",
      "step: 85010 loss: 205.974790 time elapsed: 107.7949 learning rate: 0.001633, scenario: 0, slope: -0.26914685261649635, fluctuations: 0.14\n",
      "step: 85020 loss: 203.441855 time elapsed: 107.8087 learning rate: 0.001633, scenario: 0, slope: -0.26696154011003076, fluctuations: 0.14\n",
      "step: 85030 loss: 200.950886 time elapsed: 107.8223 learning rate: 0.001633, scenario: 0, slope: -0.26513529300481264, fluctuations: 0.14\n",
      "step: 85040 loss: 198.493064 time elapsed: 107.8368 learning rate: 0.001633, scenario: 0, slope: -0.2640444311384357, fluctuations: 0.14\n",
      "step: 85050 loss: 196.068277 time elapsed: 107.8500 learning rate: 0.001633, scenario: 0, slope: -0.2640099269774744, fluctuations: 0.14\n",
      "step: 85060 loss: 193.675210 time elapsed: 107.8636 learning rate: 0.001633, scenario: 0, slope: -0.2554997155347268, fluctuations: 0.1\n",
      "step: 85070 loss: 191.311601 time elapsed: 107.8769 learning rate: 0.001633, scenario: 0, slope: -0.25075599479756616, fluctuations: 0.05\n",
      "step: 85080 loss: 188.975293 time elapsed: 107.8897 learning rate: 0.001633, scenario: 0, slope: -0.24785346687691126, fluctuations: 0.0\n",
      "step: 85090 loss: 186.664202 time elapsed: 107.9030 learning rate: 0.001633, scenario: 0, slope: -0.2442964210215025, fluctuations: 0.0\n",
      "step: 85100 loss: 184.376331 time elapsed: 107.9154 learning rate: 0.001633, scenario: 0, slope: -0.24142854743094752, fluctuations: 0.0\n",
      "step: 85110 loss: 182.109997 time elapsed: 107.9306 learning rate: 0.001633, scenario: 0, slope: -0.2381320245851455, fluctuations: 0.0\n",
      "step: 85120 loss: 179.863836 time elapsed: 107.9451 learning rate: 0.001633, scenario: 0, slope: -0.23532769039798387, fluctuations: 0.0\n",
      "step: 85130 loss: 177.636891 time elapsed: 107.9594 learning rate: 0.001633, scenario: 0, slope: -0.23269845883125015, fluctuations: 0.0\n",
      "step: 85140 loss: 175.428650 time elapsed: 107.9736 learning rate: 0.001633, scenario: 0, slope: -0.2302421698514195, fluctuations: 0.0\n",
      "step: 85150 loss: 173.239114 time elapsed: 107.9877 learning rate: 0.001633, scenario: 0, slope: -0.22794435770285112, fluctuations: 0.0\n",
      "step: 85160 loss: 171.068857 time elapsed: 108.0023 learning rate: 0.001633, scenario: 0, slope: -0.22578039723268942, fluctuations: 0.0\n",
      "step: 85170 loss: 168.919093 time elapsed: 108.0167 learning rate: 0.001633, scenario: 0, slope: -0.22371596170641345, fluctuations: 0.0\n",
      "step: 85180 loss: 166.806134 time elapsed: 108.0303 learning rate: 0.001633, scenario: 0, slope: -0.22169456825633124, fluctuations: 0.0\n",
      "step: 85190 loss: 165.138818 time elapsed: 108.0444 learning rate: 0.001633, scenario: 0, slope: -0.21548010995509612, fluctuations: 0.01\n",
      "step: 85200 loss: 162.732613 time elapsed: 108.0567 learning rate: 0.001633, scenario: 0, slope: -0.21228155422931055, fluctuations: 0.04\n",
      "step: 85210 loss: 160.792681 time elapsed: 108.0695 learning rate: 0.001633, scenario: 0, slope: -0.2095412780411233, fluctuations: 0.06\n",
      "step: 85220 loss: 158.851423 time elapsed: 108.0818 learning rate: 0.001633, scenario: 0, slope: -0.20735436222756926, fluctuations: 0.06\n",
      "step: 85230 loss: 156.932899 time elapsed: 108.0939 learning rate: 0.001633, scenario: 0, slope: -0.20528693297086123, fluctuations: 0.06\n",
      "step: 85240 loss: 155.061162 time elapsed: 108.1056 learning rate: 0.001633, scenario: 0, slope: -0.20314686362489445, fluctuations: 0.06\n",
      "step: 85250 loss: 153.233547 time elapsed: 108.1174 learning rate: 0.001633, scenario: 0, slope: -0.20092225257182844, fluctuations: 0.06\n",
      "step: 85260 loss: 151.440054 time elapsed: 108.1292 learning rate: 0.001633, scenario: 0, slope: -0.19865920883816904, fluctuations: 0.06\n",
      "step: 85270 loss: 149.688912 time elapsed: 108.1427 learning rate: 0.001633, scenario: 0, slope: -0.1964243902791453, fluctuations: 0.06\n",
      "step: 85280 loss: 147.981842 time elapsed: 108.1572 learning rate: 0.001633, scenario: 0, slope: -0.1942981085398921, fluctuations: 0.06\n",
      "step: 85290 loss: 146.320661 time elapsed: 108.1714 learning rate: 0.001633, scenario: 0, slope: -0.18775589912485094, fluctuations: 0.04\n",
      "step: 85300 loss: 144.706295 time elapsed: 108.1849 learning rate: 0.001633, scenario: 0, slope: -0.1817178700487728, fluctuations: 0.02\n",
      "step: 85310 loss: 143.139305 time elapsed: 108.1991 learning rate: 0.001633, scenario: 0, slope: -0.17696131345169025, fluctuations: 0.0\n",
      "step: 85320 loss: 141.619297 time elapsed: 108.2123 learning rate: 0.001633, scenario: 0, slope: -0.17239849895235473, fluctuations: 0.0\n",
      "step: 85330 loss: 140.144978 time elapsed: 108.2260 learning rate: 0.001633, scenario: 0, slope: -0.1679119196820461, fluctuations: 0.0\n",
      "step: 85340 loss: 138.714384 time elapsed: 108.2395 learning rate: 0.001633, scenario: 0, slope: -0.16339132806780382, fluctuations: 0.0\n",
      "step: 85350 loss: 137.325061 time elapsed: 108.2556 learning rate: 0.001633, scenario: 0, slope: -0.15884113416482346, fluctuations: 0.0\n",
      "step: 85360 loss: 135.974317 time elapsed: 108.2712 learning rate: 0.001633, scenario: 0, slope: -0.1543080969110953, fluctuations: 0.0\n",
      "step: 85370 loss: 134.659378 time elapsed: 108.2851 learning rate: 0.001633, scenario: 0, slope: -0.14984611671319878, fluctuations: 0.0\n",
      "step: 85380 loss: 465.007181 time elapsed: 108.2987 learning rate: 0.003341, scenario: -1, slope: 0.06459791043066682, fluctuations: 0.0\n",
      "step: 85390 loss: 174.717618 time elapsed: 108.3117 learning rate: 0.001165, scenario: -1, slope: 0.24967093028594736, fluctuations: 0.04\n",
      "step: 85400 loss: 141.835401 time elapsed: 108.3245 learning rate: 0.000451, scenario: -1, slope: 0.2737009183225193, fluctuations: 0.06\n",
      "step: 85410 loss: 133.171256 time elapsed: 108.3382 learning rate: 0.000157, scenario: -1, slope: 0.18719182916406635, fluctuations: 0.07\n",
      "step: 85420 loss: 133.486489 time elapsed: 108.3521 learning rate: 0.000055, scenario: -1, slope: 0.09495886315180369, fluctuations: 0.08\n",
      "step: 85430 loss: 132.632772 time elapsed: 108.3666 learning rate: 0.000022, scenario: 1, slope: -0.0048864175866025344, fluctuations: 0.08\n",
      "step: 85440 loss: 132.597067 time elapsed: 108.3818 learning rate: 0.000058, scenario: 1, slope: -0.09880904261227667, fluctuations: 0.08\n",
      "step: 85450 loss: 132.477346 time elapsed: 108.3959 learning rate: 0.000085, scenario: 0, slope: -0.19238781956304393, fluctuations: 0.09\n",
      "step: 85460 loss: 132.377988 time elapsed: 108.4096 learning rate: 0.000085, scenario: 0, slope: -0.29742080667642407, fluctuations: 0.09\n",
      "step: 85470 loss: 132.273279 time elapsed: 108.4239 learning rate: 0.000085, scenario: 0, slope: -0.4205252419404851, fluctuations: 0.09\n",
      "step: 85480 loss: 132.182000 time elapsed: 108.4375 learning rate: 0.000085, scenario: 0, slope: -0.6752145670642858, fluctuations: 0.08\n",
      "step: 85490 loss: 132.085684 time elapsed: 108.4515 learning rate: 0.000137, scenario: 1, slope: -0.1102228962699189, fluctuations: 0.05\n",
      "step: 85500 loss: 131.883166 time elapsed: 108.4664 learning rate: 0.000324, scenario: 1, slope: -0.03775879703756083, fluctuations: 0.03\n",
      "step: 85510 loss: 131.424495 time elapsed: 108.4793 learning rate: 0.000839, scenario: 1, slope: -0.016618567703412895, fluctuations: 0.02\n",
      "step: 85520 loss: 130.317667 time elapsed: 108.4914 learning rate: 0.002177, scenario: 1, slope: -0.017110356352372968, fluctuations: 0.01\n",
      "step: 85530 loss: 127.667250 time elapsed: 108.5032 learning rate: 0.005647, scenario: 1, slope: -0.02975099671842228, fluctuations: 0.01\n",
      "step: 85540 loss: 2581.001621 time elapsed: 108.5149 learning rate: 0.008509, scenario: -1, slope: 2.5579203062542035, fluctuations: 0.0\n",
      "step: 85550 loss: 641.415467 time elapsed: 108.5271 learning rate: 0.002967, scenario: -1, slope: 6.697708453469303, fluctuations: 0.04\n",
      "step: 85560 loss: 299.600332 time elapsed: 108.5390 learning rate: 0.001034, scenario: -1, slope: 6.915230037530404, fluctuations: 0.07\n",
      "step: 85570 loss: 199.443827 time elapsed: 108.5512 learning rate: 0.000361, scenario: -1, slope: 5.364456476939612, fluctuations: 0.08\n",
      "step: 85580 loss: 182.551630 time elapsed: 108.5652 learning rate: 0.000126, scenario: -1, slope: 3.5887874899025562, fluctuations: 0.09\n",
      "step: 85590 loss: 179.400830 time elapsed: 108.5802 learning rate: 0.000044, scenario: -1, slope: 1.6670878835819116, fluctuations: 0.09\n",
      "step: 85600 loss: 178.203797 time elapsed: 108.5949 learning rate: 0.000017, scenario: -1, slope: 0.008922202147427923, fluctuations: 0.09\n",
      "step: 85610 loss: 177.535279 time elapsed: 108.6097 learning rate: 0.000017, scenario: 0, slope: -2.0734232592834534, fluctuations: 0.09\n",
      "step: 85620 loss: 177.039676 time elapsed: 108.6240 learning rate: 0.000017, scenario: 0, slope: -4.169345139342447, fluctuations: 0.09\n",
      "step: 85630 loss: 176.668469 time elapsed: 108.6385 learning rate: 0.000017, scenario: 0, slope: -6.6557373118060035, fluctuations: 0.09\n",
      "step: 85640 loss: 176.352261 time elapsed: 108.6535 learning rate: 0.000017, scenario: 0, slope: -6.300487805947951, fluctuations: 0.09\n",
      "step: 85650 loss: 176.056308 time elapsed: 108.6686 learning rate: 0.000017, scenario: 0, slope: -1.779081571391272, fluctuations: 0.04\n",
      "step: 85660 loss: 175.769349 time elapsed: 108.6822 learning rate: 0.000017, scenario: 0, slope: -0.39837697764365376, fluctuations: 0.02\n",
      "step: 85670 loss: 175.440903 time elapsed: 108.6956 learning rate: 0.000033, scenario: 1, slope: -0.12521953338598635, fluctuations: 0.01\n",
      "step: 85680 loss: 174.675158 time elapsed: 108.7084 learning rate: 0.000086, scenario: 1, slope: -0.048600080454667224, fluctuations: 0.0\n",
      "step: 85690 loss: 172.823439 time elapsed: 108.7204 learning rate: 0.000223, scenario: 1, slope: -0.04541732400208275, fluctuations: 0.0\n",
      "step: 85700 loss: 168.651371 time elapsed: 108.7324 learning rate: 0.000525, scenario: 1, slope: -0.057881464104064355, fluctuations: 0.0\n",
      "step: 85710 loss: 160.299700 time elapsed: 108.7451 learning rate: 0.001362, scenario: 1, slope: -0.10569935986939585, fluctuations: 0.0\n",
      "step: 85720 loss: 146.199381 time elapsed: 108.7569 learning rate: 0.002655, scenario: 0, slope: -0.20703909857796793, fluctuations: 0.0\n",
      "step: 85730 loss: 137.536554 time elapsed: 108.7693 learning rate: 0.002655, scenario: 0, slope: -0.34060929836559833, fluctuations: 0.0\n",
      "step: 85740 loss: 132.370296 time elapsed: 108.7828 learning rate: 0.002655, scenario: 0, slope: -0.46791849349484566, fluctuations: 0.0\n",
      "step: 85750 loss: 128.748143 time elapsed: 108.7961 learning rate: 0.002655, scenario: 0, slope: -0.5690356033415774, fluctuations: 0.0\n",
      "step: 85760 loss: 125.868796 time elapsed: 108.8093 learning rate: 0.002655, scenario: 0, slope: -0.6341278725200232, fluctuations: 0.0\n",
      "step: 85770 loss: 123.343044 time elapsed: 108.8232 learning rate: 0.002655, scenario: 0, slope: -0.6569691944228112, fluctuations: 0.0\n",
      "step: 85780 loss: 121.045190 time elapsed: 108.8364 learning rate: 0.002655, scenario: 0, slope: -0.6347778906210783, fluctuations: 0.0\n",
      "step: 85790 loss: 118.907998 time elapsed: 108.8503 learning rate: 0.002655, scenario: 0, slope: -0.5692593635677585, fluctuations: 0.0\n",
      "step: 85800 loss: 116.871867 time elapsed: 108.8636 learning rate: 0.002655, scenario: 0, slope: -0.48072909579135403, fluctuations: 0.0\n",
      "step: 85810 loss: 114.904400 time elapsed: 108.8774 learning rate: 0.002655, scenario: 0, slope: -0.35786728379506283, fluctuations: 0.0\n",
      "step: 85820 loss: 112.983580 time elapsed: 108.8901 learning rate: 0.002655, scenario: 0, slope: -0.2799232901243721, fluctuations: 0.0\n",
      "step: 85830 loss: 111.094894 time elapsed: 108.9026 learning rate: 0.002655, scenario: 0, slope: -0.24062200494697072, fluctuations: 0.0\n",
      "step: 85840 loss: 109.226171 time elapsed: 108.9150 learning rate: 0.002655, scenario: 0, slope: -0.21974071921554345, fluctuations: 0.0\n",
      "step: 85850 loss: 107.367045 time elapsed: 108.9281 learning rate: 0.002655, scenario: 0, slope: -0.20712936802977006, fluctuations: 0.0\n",
      "step: 85860 loss: 105.514024 time elapsed: 108.9407 learning rate: 0.002655, scenario: 0, slope: -0.19910380844072306, fluctuations: 0.0\n",
      "step: 85870 loss: 103.681578 time elapsed: 108.9543 learning rate: 0.002655, scenario: 0, slope: -0.1937115436698717, fluctuations: 0.0\n",
      "step: 85880 loss: 101.896266 time elapsed: 108.9675 learning rate: 0.002655, scenario: 0, slope: -0.18996344083527655, fluctuations: 0.0\n",
      "step: 85890 loss: 100.157724 time elapsed: 108.9824 learning rate: 0.002655, scenario: 0, slope: -0.18702248529273663, fluctuations: 0.0\n",
      "step: 85900 loss: 98.426175 time elapsed: 108.9971 learning rate: 0.002655, scenario: 0, slope: -0.18468935438038614, fluctuations: 0.0\n",
      "step: 85910 loss: 96.772470 time elapsed: 109.0126 learning rate: 0.002655, scenario: 0, slope: -0.18207243265677142, fluctuations: 0.0\n",
      "step: 85920 loss: 95.303829 time elapsed: 109.0273 learning rate: 0.002655, scenario: 0, slope: -0.17429567080356803, fluctuations: 0.03\n",
      "step: 85930 loss: 94.206115 time elapsed: 109.0418 learning rate: 0.002655, scenario: 0, slope: -0.17005216296444112, fluctuations: 0.08\n",
      "step: 85940 loss: 92.795983 time elapsed: 109.0564 learning rate: 0.002655, scenario: 0, slope: -0.1645867143046732, fluctuations: 0.13\n",
      "step: 85950 loss: 91.446979 time elapsed: 109.0705 learning rate: 0.002655, scenario: 0, slope: -0.1573303660863529, fluctuations: 0.13\n",
      "step: 85960 loss: 90.090245 time elapsed: 109.0844 learning rate: 0.002655, scenario: 0, slope: -0.15155820223615232, fluctuations: 0.13\n",
      "step: 85970 loss: 88.779053 time elapsed: 109.0982 learning rate: 0.002655, scenario: 0, slope: -0.14659631995371378, fluctuations: 0.13\n",
      "step: 85980 loss: 87.548631 time elapsed: 109.1108 learning rate: 0.002655, scenario: 0, slope: -0.14227975830472997, fluctuations: 0.13\n",
      "step: 85990 loss: 86.328796 time elapsed: 109.1230 learning rate: 0.002655, scenario: 0, slope: -0.13860926298047138, fluctuations: 0.13\n",
      "step: 86000 loss: 85.132898 time elapsed: 109.1349 learning rate: 0.002655, scenario: 0, slope: -0.1360906147811929, fluctuations: 0.13\n",
      "step: 86010 loss: 83.959110 time elapsed: 109.1486 learning rate: 0.002655, scenario: 0, slope: -0.13495743254302442, fluctuations: 0.13\n",
      "step: 86020 loss: 82.806623 time elapsed: 109.1602 learning rate: 0.002655, scenario: 0, slope: -0.12869476571159275, fluctuations: 0.09\n",
      "step: 86030 loss: 81.676068 time elapsed: 109.1725 learning rate: 0.002655, scenario: 0, slope: -0.124428013573357, fluctuations: 0.04\n",
      "step: 86040 loss: 80.567825 time elapsed: 109.1849 learning rate: 0.002655, scenario: 0, slope: -0.12146912357551184, fluctuations: 0.0\n",
      "step: 86050 loss: 79.480777 time elapsed: 109.1985 learning rate: 0.002655, scenario: 0, slope: -0.11863286385682685, fluctuations: 0.0\n",
      "step: 86060 loss: 78.413062 time elapsed: 109.2127 learning rate: 0.002655, scenario: 0, slope: -0.1162577774952784, fluctuations: 0.0\n",
      "step: 86070 loss: 77.362656 time elapsed: 109.2263 learning rate: 0.002655, scenario: 0, slope: -0.11406384845830489, fluctuations: 0.0\n",
      "step: 86080 loss: 76.327919 time elapsed: 109.2396 learning rate: 0.002655, scenario: 0, slope: -0.11195797205608046, fluctuations: 0.0\n",
      "step: 86090 loss: 75.307617 time elapsed: 109.2534 learning rate: 0.002655, scenario: 0, slope: -0.10993988559266848, fluctuations: 0.0\n",
      "step: 86100 loss: 74.301046 time elapsed: 109.2669 learning rate: 0.002655, scenario: 0, slope: -0.10820226555091851, fluctuations: 0.0\n",
      "step: 86110 loss: 73.308473 time elapsed: 109.2812 learning rate: 0.002655, scenario: 0, slope: -0.1061944166561384, fluctuations: 0.0\n",
      "step: 86120 loss: 72.331699 time elapsed: 109.2954 learning rate: 0.002655, scenario: 0, slope: -0.1044815689448535, fluctuations: 0.0\n",
      "step: 86130 loss: 71.373850 time elapsed: 109.3088 learning rate: 0.002655, scenario: 0, slope: -0.10285145814933536, fluctuations: 0.0\n",
      "step: 86140 loss: 70.437372 time elapsed: 109.3220 learning rate: 0.002655, scenario: 0, slope: -0.10125542150428223, fluctuations: 0.0\n",
      "step: 86150 loss: 69.522112 time elapsed: 109.3353 learning rate: 0.002655, scenario: 0, slope: -0.09964013867422059, fluctuations: 0.0\n",
      "step: 86160 loss: 68.626601 time elapsed: 109.3486 learning rate: 0.002655, scenario: 0, slope: -0.09796794538579165, fluctuations: 0.0\n",
      "step: 86170 loss: 67.749800 time elapsed: 109.3620 learning rate: 0.002655, scenario: 0, slope: -0.0962217128535448, fluctuations: 0.0\n",
      "step: 86180 loss: 66.891018 time elapsed: 109.3740 learning rate: 0.002655, scenario: 0, slope: -0.09440178679880082, fluctuations: 0.0\n",
      "step: 86190 loss: 66.049448 time elapsed: 109.3873 learning rate: 0.002655, scenario: 0, slope: -0.09252422131916728, fluctuations: 0.0\n",
      "step: 86200 loss: 65.224066 time elapsed: 109.4015 learning rate: 0.002655, scenario: 0, slope: -0.09081021745087943, fluctuations: 0.0\n",
      "step: 86210 loss: 64.413571 time elapsed: 109.4163 learning rate: 0.002655, scenario: 0, slope: -0.08872990830472548, fluctuations: 0.0\n",
      "step: 86220 loss: 63.616254 time elapsed: 109.4310 learning rate: 0.002655, scenario: 0, slope: -0.08690006849092914, fluctuations: 0.0\n",
      "step: 86230 loss: 64.352942 time elapsed: 109.4457 learning rate: 0.002655, scenario: 0, slope: -0.08369764238652753, fluctuations: 0.0\n",
      "step: 86240 loss: 62.511923 time elapsed: 109.4604 learning rate: 0.002655, scenario: 0, slope: -0.07846965329893174, fluctuations: 0.03\n",
      "step: 86250 loss: 61.240510 time elapsed: 109.4746 learning rate: 0.002655, scenario: 0, slope: -0.077102972680264, fluctuations: 0.06\n",
      "step: 86260 loss: 60.333455 time elapsed: 109.4887 learning rate: 0.002655, scenario: 0, slope: -0.07765493004019634, fluctuations: 0.09\n",
      "step: 86270 loss: 59.467100 time elapsed: 109.5037 learning rate: 0.002655, scenario: 0, slope: -0.07948221216775284, fluctuations: 0.1\n",
      "step: 86280 loss: 58.629423 time elapsed: 109.5172 learning rate: 0.002655, scenario: 0, slope: -0.08136848529717533, fluctuations: 0.1\n",
      "step: 86290 loss: 57.839787 time elapsed: 109.5298 learning rate: 0.002655, scenario: 0, slope: -0.08320069116114087, fluctuations: 0.1\n",
      "step: 86300 loss: 57.105822 time elapsed: 109.5419 learning rate: 0.002655, scenario: 0, slope: -0.08452593814626384, fluctuations: 0.1\n",
      "step: 86310 loss: 56.408724 time elapsed: 109.5547 learning rate: 0.002655, scenario: 0, slope: -0.0856370333401269, fluctuations: 0.1\n",
      "step: 86320 loss: 55.737872 time elapsed: 109.5666 learning rate: 0.002655, scenario: 0, slope: -0.08613935143607616, fluctuations: 0.1\n",
      "step: 86330 loss: 55.082106 time elapsed: 109.5785 learning rate: 0.002655, scenario: 0, slope: -0.08639468568759337, fluctuations: 0.09\n",
      "step: 86340 loss: 54.438988 time elapsed: 109.5905 learning rate: 0.002655, scenario: 0, slope: -0.07659281506578056, fluctuations: 0.07\n",
      "step: 86350 loss: 53.805220 time elapsed: 109.6039 learning rate: 0.002655, scenario: 0, slope: -0.07302943212840987, fluctuations: 0.03\n",
      "step: 86360 loss: 53.178000 time elapsed: 109.6176 learning rate: 0.002655, scenario: 0, slope: -0.07021295056915343, fluctuations: 0.0\n",
      "step: 86370 loss: 52.553017 time elapsed: 109.6310 learning rate: 0.002655, scenario: 0, slope: -0.06784131059885956, fluctuations: 0.0\n",
      "step: 86380 loss: 51.917694 time elapsed: 109.6445 learning rate: 0.002655, scenario: 0, slope: -0.06590121961432488, fluctuations: 0.0\n",
      "step: 86390 loss: 51.359037 time elapsed: 109.6582 learning rate: 0.002655, scenario: 0, slope: -0.060066282410982295, fluctuations: 0.01\n",
      "step: 86400 loss: 51.387845 time elapsed: 109.6720 learning rate: 0.002655, scenario: 0, slope: -0.05779675700736645, fluctuations: 0.03\n",
      "step: 86410 loss: 50.157588 time elapsed: 109.6858 learning rate: 0.002655, scenario: 0, slope: -0.057537295478187964, fluctuations: 0.06\n",
      "step: 86420 loss: 49.508981 time elapsed: 109.6994 learning rate: 0.002655, scenario: 0, slope: -0.058284719532753146, fluctuations: 0.08\n",
      "step: 86430 loss: 48.793844 time elapsed: 109.7126 learning rate: 0.002655, scenario: 0, slope: -0.05969626868450711, fluctuations: 0.09\n",
      "step: 86440 loss: 49.175446 time elapsed: 109.7251 learning rate: 0.002655, scenario: 0, slope: -0.05635209888712528, fluctuations: 0.1\n",
      "step: 86450 loss: 57.145584 time elapsed: 109.7371 learning rate: 0.005173, scenario: 1, slope: -0.03816916209066958, fluctuations: 0.12\n",
      "step: 86460 loss: 185.468039 time elapsed: 109.7493 learning rate: 0.002338, scenario: -1, slope: 0.6021958241959718, fluctuations: 0.15\n",
      "step: 86470 loss: 70.548777 time elapsed: 109.7613 learning rate: 0.000815, scenario: -1, slope: 0.5577567976896813, fluctuations: 0.19\n",
      "step: 86480 loss: 55.515891 time elapsed: 109.7736 learning rate: 0.000284, scenario: -1, slope: 0.41343340954163343, fluctuations: 0.2\n",
      "step: 86490 loss: 51.721394 time elapsed: 109.7854 learning rate: 0.000099, scenario: -1, slope: 0.252806845576954, fluctuations: 0.2\n",
      "step: 86500 loss: 50.719240 time elapsed: 109.7986 learning rate: 0.000038, scenario: -1, slope: 0.11495229163086092, fluctuations: 0.18\n",
      "step: 86510 loss: 50.689289 time elapsed: 109.8131 learning rate: 0.000029, scenario: 1, slope: -0.042688056418818655, fluctuations: 0.16\n",
      "step: 86520 loss: 50.627228 time elapsed: 109.8267 learning rate: 0.000029, scenario: 0, slope: -0.2034023608362717, fluctuations: 0.15\n",
      "step: 86530 loss: 50.578047 time elapsed: 109.8405 learning rate: 0.000029, scenario: 0, slope: -0.37384413121591187, fluctuations: 0.14\n",
      "step: 86540 loss: 50.545957 time elapsed: 109.8542 learning rate: 0.000029, scenario: 0, slope: -0.5692776342212698, fluctuations: 0.13\n",
      "step: 86550 loss: 50.513254 time elapsed: 109.8680 learning rate: 0.000029, scenario: 0, slope: -0.7321240542884749, fluctuations: 0.11\n",
      "step: 86560 loss: 50.482863 time elapsed: 109.8822 learning rate: 0.000029, scenario: 0, slope: -0.1371556560768415, fluctuations: 0.07\n",
      "step: 86570 loss: 50.448878 time elapsed: 109.8971 learning rate: 0.000056, scenario: 1, slope: -0.03415738458974286, fluctuations: 0.04\n",
      "step: 86580 loss: 50.370842 time elapsed: 109.9119 learning rate: 0.000145, scenario: 1, slope: -0.012837877813993504, fluctuations: 0.02\n",
      "step: 86590 loss: 50.187225 time elapsed: 109.9248 learning rate: 0.000376, scenario: 1, slope: -0.006430350664534714, fluctuations: 0.01\n",
      "step: 86600 loss: 49.787800 time elapsed: 109.9374 learning rate: 0.000886, scenario: 1, slope: -0.00597654943018132, fluctuations: 0.01\n",
      "step: 86610 loss: 49.092671 time elapsed: 109.9516 learning rate: 0.002299, scenario: 1, slope: -0.010375609411606833, fluctuations: 0.0\n",
      "step: 86620 loss: 47.957261 time elapsed: 109.9646 learning rate: 0.005962, scenario: 1, slope: -0.01828328718157913, fluctuations: 0.0\n",
      "step: 86630 loss: 70.378467 time elapsed: 109.9788 learning rate: 0.015465, scenario: 1, slope: -0.017334365175557667, fluctuations: 0.0\n",
      "step: 86640 loss: 38202.298250 time elapsed: 109.9924 learning rate: 0.005719, scenario: -1, slope: 143.19578605900745, fluctuations: 0.03\n",
      "step: 86650 loss: 10991.621889 time elapsed: 110.0067 learning rate: 0.001994, scenario: -1, slope: 205.69603354104447, fluctuations: 0.04\n",
      "step: 86660 loss: 7611.886187 time elapsed: 110.0216 learning rate: 0.000695, scenario: -1, slope: 200.72963618634887, fluctuations: 0.04\n",
      "step: 86670 loss: 6356.999927 time elapsed: 110.0357 learning rate: 0.000242, scenario: -1, slope: 172.08348558259857, fluctuations: 0.04\n",
      "step: 86680 loss: 6044.376093 time elapsed: 110.0496 learning rate: 0.000085, scenario: -1, slope: 133.8807075983671, fluctuations: 0.04\n",
      "step: 86690 loss: 5963.586458 time elapsed: 110.0635 learning rate: 0.000029, scenario: -1, slope: 88.83254748446805, fluctuations: 0.04\n",
      "step: 86700 loss: 5938.530459 time elapsed: 110.0772 learning rate: 0.000011, scenario: -1, slope: 42.03388974946963, fluctuations: 0.04\n",
      "step: 86710 loss: 5929.287978 time elapsed: 110.0915 learning rate: 0.000006, scenario: 0, slope: -24.915158245479585, fluctuations: 0.04\n",
      "step: 86720 loss: 5922.744842 time elapsed: 110.1053 learning rate: 0.000006, scenario: 0, slope: -97.56428049766967, fluctuations: 0.04\n",
      "step: 86730 loss: 5916.335812 time elapsed: 110.1194 learning rate: 0.000006, scenario: 0, slope: -184.9244115303387, fluctuations: 0.04\n",
      "step: 86740 loss: 5910.040021 time elapsed: 110.1318 learning rate: 0.000006, scenario: 0, slope: -77.96442387516184, fluctuations: 0.01\n",
      "step: 86750 loss: 5903.846435 time elapsed: 110.1434 learning rate: 0.000006, scenario: 0, slope: -23.521903024755314, fluctuations: 0.0\n",
      "step: 86760 loss: 5897.747786 time elapsed: 110.1554 learning rate: 0.000007, scenario: 1, slope: -6.327035330296831, fluctuations: 0.0\n",
      "step: 86770 loss: 5887.252551 time elapsed: 110.1673 learning rate: 0.000019, scenario: 1, slope: -1.9671962616908898, fluctuations: 0.0\n",
      "step: 86780 loss: 5860.966322 time elapsed: 110.1794 learning rate: 0.000049, scenario: 1, slope: -1.0157979534797024, fluctuations: 0.0\n",
      "step: 86790 loss: 5798.039566 time elapsed: 110.1915 learning rate: 0.000128, scenario: 1, slope: -1.0275357061935593, fluctuations: 0.0\n",
      "step: 86800 loss: 5658.747847 time elapsed: 110.2033 learning rate: 0.000302, scenario: 1, slope: -1.5932977106234252, fluctuations: 0.0\n",
      "step: 86810 loss: 5391.072207 time elapsed: 110.2167 learning rate: 0.000784, scenario: 1, slope: -3.3124781522638775, fluctuations: 0.0\n",
      "step: 86820 loss: 4858.483096 time elapsed: 110.2303 learning rate: 0.001680, scenario: 0, slope: -6.666172694892951, fluctuations: 0.0\n",
      "step: 86830 loss: 4257.510343 time elapsed: 110.2433 learning rate: 0.001680, scenario: 0, slope: -12.4392369761827, fluctuations: 0.0\n",
      "step: 86840 loss: 3778.383840 time elapsed: 110.2562 learning rate: 0.001680, scenario: 0, slope: -19.37829690631055, fluctuations: 0.0\n",
      "step: 86850 loss: 3331.012592 time elapsed: 110.2706 learning rate: 0.001680, scenario: 0, slope: -26.51594198394731, fluctuations: 0.0\n",
      "step: 86860 loss: 2923.813505 time elapsed: 110.2842 learning rate: 0.001680, scenario: 0, slope: -33.17063071108327, fluctuations: 0.0\n",
      "step: 86870 loss: 2558.586025 time elapsed: 110.2979 learning rate: 0.001680, scenario: 0, slope: -38.57235974092239, fluctuations: 0.0\n",
      "step: 86880 loss: 2210.120336 time elapsed: 110.3112 learning rate: 0.001680, scenario: 0, slope: -42.188287935004276, fluctuations: 0.0\n",
      "step: 86890 loss: 1880.077061 time elapsed: 110.3249 learning rate: 0.001680, scenario: 0, slope: -43.709676174620256, fluctuations: 0.0\n",
      "step: 86900 loss: 1592.050959 time elapsed: 110.3375 learning rate: 0.001680, scenario: 0, slope: -43.166982960384, fluctuations: 0.0\n",
      "step: 86910 loss: 1365.397676 time elapsed: 110.3498 learning rate: 0.001680, scenario: 0, slope: -40.15229295464067, fluctuations: 0.0\n",
      "step: 86920 loss: 1192.578601 time elapsed: 110.3616 learning rate: 0.001680, scenario: 0, slope: -36.09564774909699, fluctuations: 0.0\n",
      "step: 86930 loss: 1066.842015 time elapsed: 110.3736 learning rate: 0.001680, scenario: 0, slope: -32.24597174698017, fluctuations: 0.0\n",
      "step: 86940 loss: 956.948749 time elapsed: 110.3854 learning rate: 0.001680, scenario: 0, slope: -28.40265860409269, fluctuations: 0.0\n",
      "step: 86950 loss: 833.451397 time elapsed: 110.3971 learning rate: 0.001680, scenario: 0, slope: -24.637843451046596, fluctuations: 0.0\n",
      "step: 86960 loss: 777.263647 time elapsed: 110.4089 learning rate: 0.001680, scenario: 0, slope: -21.048987571974028, fluctuations: 0.0\n",
      "step: 86970 loss: 732.692938 time elapsed: 110.4215 learning rate: 0.001680, scenario: 0, slope: -17.44676860667214, fluctuations: 0.0\n",
      "step: 86980 loss: 697.852080 time elapsed: 110.4350 learning rate: 0.001680, scenario: 0, slope: -14.037902328938134, fluctuations: 0.0\n",
      "step: 86990 loss: 667.440210 time elapsed: 110.4483 learning rate: 0.001680, scenario: 0, slope: -11.054456957945046, fluctuations: 0.0\n",
      "step: 87000 loss: 639.966601 time elapsed: 110.4616 learning rate: 0.001680, scenario: 0, slope: -8.878474339750676, fluctuations: 0.0\n",
      "step: 87010 loss: 614.341607 time elapsed: 110.4752 learning rate: 0.001680, scenario: 0, slope: -6.817829599603729, fluctuations: 0.0\n",
      "step: 87020 loss: 590.296004 time elapsed: 110.4889 learning rate: 0.001680, scenario: 0, slope: -5.407973650352749, fluctuations: 0.0\n",
      "step: 87030 loss: 567.845610 time elapsed: 110.5028 learning rate: 0.001680, scenario: 0, slope: -4.287316948431089, fluctuations: 0.0\n",
      "step: 87040 loss: 547.013658 time elapsed: 110.5161 learning rate: 0.001680, scenario: 0, slope: -3.3845867432491596, fluctuations: 0.0\n",
      "step: 87050 loss: 527.666585 time elapsed: 110.5299 learning rate: 0.001680, scenario: 0, slope: -2.824630485123892, fluctuations: 0.0\n",
      "step: 87060 loss: 509.601285 time elapsed: 110.5430 learning rate: 0.001680, scenario: 0, slope: -2.56290153249702, fluctuations: 0.0\n",
      "step: 87070 loss: 492.652385 time elapsed: 110.5554 learning rate: 0.001680, scenario: 0, slope: -2.3426057581687307, fluctuations: 0.0\n",
      "step: 87080 loss: 476.697998 time elapsed: 110.5678 learning rate: 0.001680, scenario: 0, slope: -2.17875514255145, fluctuations: 0.0\n",
      "step: 87090 loss: 461.648549 time elapsed: 110.5794 learning rate: 0.001680, scenario: 0, slope: -2.0315582955371934, fluctuations: 0.0\n",
      "step: 87100 loss: 447.434966 time elapsed: 110.5915 learning rate: 0.001680, scenario: 0, slope: -1.9119579207429156, fluctuations: 0.0\n",
      "step: 87110 loss: 433.999575 time elapsed: 110.6037 learning rate: 0.001680, scenario: 0, slope: -1.776985216065, fluctuations: 0.0\n",
      "step: 87120 loss: 421.290836 time elapsed: 110.6157 learning rate: 0.001680, scenario: 0, slope: -1.6658902061870327, fluctuations: 0.0\n",
      "step: 87130 loss: 409.260237 time elapsed: 110.6287 learning rate: 0.001680, scenario: 0, slope: -1.5655228084545427, fluctuations: 0.0\n",
      "step: 87140 loss: 397.860794 time elapsed: 110.6424 learning rate: 0.001680, scenario: 0, slope: -1.4747840963509125, fluctuations: 0.0\n",
      "step: 87150 loss: 387.046776 time elapsed: 110.6556 learning rate: 0.001680, scenario: 0, slope: -1.3919373888183677, fluctuations: 0.0\n",
      "step: 87160 loss: 376.774098 time elapsed: 110.6687 learning rate: 0.001680, scenario: 0, slope: -1.3155980359145245, fluctuations: 0.0\n",
      "step: 87170 loss: 367.000949 time elapsed: 110.6824 learning rate: 0.001680, scenario: 0, slope: -1.2448570661935485, fluctuations: 0.0\n",
      "step: 87180 loss: 357.688325 time elapsed: 110.6962 learning rate: 0.001680, scenario: 0, slope: -1.1791557665786394, fluctuations: 0.0\n",
      "step: 87190 loss: 348.800328 time elapsed: 110.7099 learning rate: 0.001680, scenario: 0, slope: -1.1181375402540326, fluctuations: 0.0\n",
      "step: 87200 loss: 340.304241 time elapsed: 110.7231 learning rate: 0.001680, scenario: 0, slope: -1.0670073139329659, fluctuations: 0.0\n",
      "step: 87210 loss: 332.170458 time elapsed: 110.7374 learning rate: 0.001680, scenario: 0, slope: -1.0091272952020784, fluctuations: 0.0\n",
      "step: 87220 loss: 324.372321 time elapsed: 110.7513 learning rate: 0.001680, scenario: 0, slope: -0.9606590966097884, fluctuations: 0.0\n",
      "step: 87230 loss: 316.885927 time elapsed: 110.7650 learning rate: 0.001680, scenario: 0, slope: -0.9158750798660189, fluctuations: 0.0\n",
      "step: 87240 loss: 309.689899 time elapsed: 110.7774 learning rate: 0.001680, scenario: 0, slope: -0.8744977436536194, fluctuations: 0.0\n",
      "step: 87250 loss: 302.765148 time elapsed: 110.7897 learning rate: 0.001680, scenario: 0, slope: -0.8362407521500468, fluctuations: 0.0\n",
      "step: 87260 loss: 296.094643 time elapsed: 110.8039 learning rate: 0.001680, scenario: 0, slope: -0.8008193273818245, fluctuations: 0.0\n",
      "step: 87270 loss: 289.663191 time elapsed: 110.8169 learning rate: 0.001680, scenario: 0, slope: -0.7679590768660174, fluctuations: 0.0\n",
      "step: 87280 loss: 283.457235 time elapsed: 110.8298 learning rate: 0.001680, scenario: 0, slope: -0.7374019559638311, fluctuations: 0.0\n",
      "step: 87290 loss: 277.464668 time elapsed: 110.8447 learning rate: 0.001680, scenario: 0, slope: -0.7089095313887182, fluctuations: 0.0\n",
      "step: 87300 loss: 271.674654 time elapsed: 110.8587 learning rate: 0.001680, scenario: 0, slope: -0.6848514651892592, fluctuations: 0.0\n",
      "step: 87310 loss: 266.077469 time elapsed: 110.8738 learning rate: 0.001680, scenario: 0, slope: -0.657269843448631, fluctuations: 0.0\n",
      "step: 87320 loss: 260.664343 time elapsed: 110.8876 learning rate: 0.001680, scenario: 0, slope: -0.6337502176233148, fluctuations: 0.0\n",
      "step: 87330 loss: 255.427319 time elapsed: 110.9011 learning rate: 0.001680, scenario: 0, slope: -0.611549026041181, fluctuations: 0.0\n",
      "step: 87340 loss: 250.359123 time elapsed: 110.9145 learning rate: 0.001680, scenario: 0, slope: -0.5905282088769439, fluctuations: 0.0\n",
      "step: 87350 loss: 245.453044 time elapsed: 110.9274 learning rate: 0.001680, scenario: 0, slope: -0.5705668035835462, fluctuations: 0.0\n",
      "step: 87360 loss: 240.702828 time elapsed: 110.9407 learning rate: 0.001680, scenario: 0, slope: -0.551559725575306, fluctuations: 0.0\n",
      "step: 87370 loss: 236.102586 time elapsed: 110.9539 learning rate: 0.001680, scenario: 0, slope: -0.5334165781025608, fluctuations: 0.0\n",
      "step: 87380 loss: 231.646719 time elapsed: 110.9667 learning rate: 0.001680, scenario: 0, slope: -0.5160604604462654, fluctuations: 0.0\n",
      "step: 87390 loss: 227.329849 time elapsed: 110.9794 learning rate: 0.001680, scenario: 0, slope: -0.49942674428787603, fluctuations: 0.0\n",
      "step: 87400 loss: 223.146780 time elapsed: 110.9913 learning rate: 0.001680, scenario: 0, slope: -0.48502952272892313, fluctuations: 0.0\n",
      "step: 87410 loss: 219.092458 time elapsed: 111.0044 learning rate: 0.001680, scenario: 0, slope: -0.46812167466533094, fluctuations: 0.0\n",
      "step: 87420 loss: 215.161953 time elapsed: 111.0162 learning rate: 0.001680, scenario: 0, slope: -0.45337071495136844, fluctuations: 0.0\n",
      "step: 87430 loss: 211.350451 time elapsed: 111.0280 learning rate: 0.001680, scenario: 0, slope: -0.4391801823033533, fluctuations: 0.0\n",
      "step: 87440 loss: 207.653254 time elapsed: 111.0395 learning rate: 0.001680, scenario: 0, slope: -0.4255268610284454, fluctuations: 0.0\n",
      "step: 87450 loss: 204.065791 time elapsed: 111.0525 learning rate: 0.001680, scenario: 0, slope: -0.41239170664402663, fluctuations: 0.0\n",
      "step: 87460 loss: 200.583641 time elapsed: 111.0666 learning rate: 0.001680, scenario: 0, slope: -0.3997585594063721, fluctuations: 0.0\n",
      "step: 87470 loss: 197.202549 time elapsed: 111.0808 learning rate: 0.001680, scenario: 0, slope: -0.3876129499825161, fluctuations: 0.0\n",
      "step: 87480 loss: 193.918462 time elapsed: 111.0941 learning rate: 0.001680, scenario: 0, slope: -0.37594101728871654, fluctuations: 0.0\n",
      "step: 87490 loss: 190.727561 time elapsed: 111.1072 learning rate: 0.001680, scenario: 0, slope: -0.3647285523499444, fluctuations: 0.0\n",
      "step: 87500 loss: 187.626292 time elapsed: 111.1206 learning rate: 0.001680, scenario: 0, slope: -0.3550175051227389, fluctuations: 0.0\n",
      "step: 87510 loss: 184.611403 time elapsed: 111.1349 learning rate: 0.001680, scenario: 0, slope: -0.34361867500698473, fluctuations: 0.0\n",
      "step: 87520 loss: 181.679968 time elapsed: 111.1483 learning rate: 0.001680, scenario: 0, slope: -0.33368446924428846, fluctuations: 0.0\n",
      "step: 87530 loss: 178.829410 time elapsed: 111.1618 learning rate: 0.001680, scenario: 0, slope: -0.3241353097041884, fluctuations: 0.0\n",
      "step: 87540 loss: 176.057500 time elapsed: 111.1755 learning rate: 0.001680, scenario: 0, slope: -0.3149461778059136, fluctuations: 0.0\n",
      "step: 87550 loss: 173.362337 time elapsed: 111.1894 learning rate: 0.001680, scenario: 0, slope: -0.30608948561001353, fluctuations: 0.0\n",
      "step: 87560 loss: 170.742304 time elapsed: 111.2012 learning rate: 0.001680, scenario: 0, slope: -0.29753562160616953, fluctuations: 0.0\n",
      "step: 87570 loss: 168.195996 time elapsed: 111.2129 learning rate: 0.001680, scenario: 0, slope: -0.2892538837544001, fluctuations: 0.0\n",
      "step: 87580 loss: 165.722131 time elapsed: 111.2259 learning rate: 0.001680, scenario: 0, slope: -0.2812137869738884, fluctuations: 0.0\n",
      "step: 87590 loss: 163.319451 time elapsed: 111.2382 learning rate: 0.001680, scenario: 0, slope: -0.2733866441486153, fluctuations: 0.0\n",
      "step: 87600 loss: 160.986652 time elapsed: 111.2506 learning rate: 0.001680, scenario: 0, slope: -0.26650332922192693, fluctuations: 0.0\n",
      "step: 87610 loss: 158.722346 time elapsed: 111.2644 learning rate: 0.001680, scenario: 0, slope: -0.25827505196761685, fluctuations: 0.0\n",
      "step: 87620 loss: 156.525097 time elapsed: 111.2774 learning rate: 0.001680, scenario: 0, slope: -0.25095531688190525, fluctuations: 0.0\n",
      "step: 87630 loss: 154.393509 time elapsed: 111.2906 learning rate: 0.001680, scenario: 0, slope: -0.24377846790983226, fluctuations: 0.0\n",
      "step: 87640 loss: 152.326376 time elapsed: 111.3041 learning rate: 0.001680, scenario: 0, slope: -0.23673890095194997, fluctuations: 0.0\n",
      "step: 87650 loss: 150.322800 time elapsed: 111.3180 learning rate: 0.001680, scenario: 0, slope: -0.22983257444615907, fluctuations: 0.0\n",
      "step: 87660 loss: 148.382212 time elapsed: 111.3314 learning rate: 0.001680, scenario: 0, slope: -0.2230543844091868, fluctuations: 0.0\n",
      "step: 87670 loss: 146.504147 time elapsed: 111.3452 learning rate: 0.001680, scenario: 0, slope: -0.21639656592621034, fluctuations: 0.0\n",
      "step: 87680 loss: 144.687793 time elapsed: 111.3588 learning rate: 0.001680, scenario: 0, slope: -0.20984955636813574, fluctuations: 0.0\n",
      "step: 87690 loss: 142.931504 time elapsed: 111.3730 learning rate: 0.001680, scenario: 0, slope: -0.20340583536145446, fluctuations: 0.0\n",
      "step: 87700 loss: 141.232656 time elapsed: 111.3857 learning rate: 0.001680, scenario: 0, slope: -0.19769443140729426, fluctuations: 0.0\n",
      "step: 87710 loss: 139.588027 time elapsed: 111.3979 learning rate: 0.001680, scenario: 0, slope: -0.19083858031357948, fluctuations: 0.0\n",
      "step: 87720 loss: 137.994398 time elapsed: 111.4106 learning rate: 0.001680, scenario: 0, slope: -0.18474762148886598, fluctuations: 0.0\n",
      "step: 87730 loss: 136.448912 time elapsed: 111.4230 learning rate: 0.001680, scenario: 0, slope: -0.1788214966677279, fluctuations: 0.0\n",
      "step: 87740 loss: 134.949085 time elapsed: 111.4362 learning rate: 0.001680, scenario: 0, slope: -0.17309194825643748, fluctuations: 0.0\n",
      "step: 87750 loss: 133.492671 time elapsed: 111.4491 learning rate: 0.001680, scenario: 0, slope: -0.16758839090510444, fluctuations: 0.0\n",
      "step: 87760 loss: 132.077551 time elapsed: 111.4623 learning rate: 0.001680, scenario: 0, slope: -0.162333236406999, fluctuations: 0.0\n",
      "step: 87770 loss: 130.701684 time elapsed: 111.4769 learning rate: 0.001680, scenario: 0, slope: -0.1573381042259396, fluctuations: 0.0\n",
      "step: 87780 loss: 129.363097 time elapsed: 111.4910 learning rate: 0.001680, scenario: 0, slope: -0.15260233616728155, fluctuations: 0.0\n",
      "step: 87790 loss: 128.059883 time elapsed: 111.5054 learning rate: 0.001680, scenario: 0, slope: -0.1481150632630643, fluctuations: 0.0\n",
      "step: 87800 loss: 126.790218 time elapsed: 111.5195 learning rate: 0.001680, scenario: 0, slope: -0.14427574914830174, fluctuations: 0.0\n",
      "step: 87810 loss: 125.552367 time elapsed: 111.5345 learning rate: 0.001680, scenario: 0, slope: -0.13982202303128646, fluctuations: 0.0\n",
      "step: 87820 loss: 123.935267 time elapsed: 111.5497 learning rate: 0.003961, scenario: 1, slope: -0.1366635081287105, fluctuations: 0.0\n",
      "step: 87830 loss: 120.665084 time elapsed: 111.5644 learning rate: 0.005799, scenario: 0, slope: -0.1413422255348419, fluctuations: 0.0\n",
      "step: 87840 loss: 129.801086 time elapsed: 111.5782 learning rate: 0.002912, scenario: -1, slope: 0.08450069913612154, fluctuations: 0.03\n",
      "step: 87850 loss: 125.085221 time elapsed: 111.5916 learning rate: 0.001016, scenario: -1, slope: 0.09597323867487316, fluctuations: 0.05\n",
      "step: 87860 loss: 120.315362 time elapsed: 111.6041 learning rate: 0.000354, scenario: -1, slope: 0.049314584418494636, fluctuations: 0.07\n",
      "step: 87870 loss: 117.945363 time elapsed: 111.6164 learning rate: 0.000264, scenario: 1, slope: -0.023949079598596407, fluctuations: 0.07\n",
      "step: 87880 loss: 117.246666 time elapsed: 111.6283 learning rate: 0.000686, scenario: 1, slope: -0.082045500782575, fluctuations: 0.08\n",
      "step: 87890 loss: 116.073425 time elapsed: 111.6402 learning rate: 0.001470, scenario: 0, slope: -0.13817133356268513, fluctuations: 0.09\n",
      "step: 87900 loss: 114.858616 time elapsed: 111.6521 learning rate: 0.001470, scenario: 0, slope: -0.19128336552324862, fluctuations: 0.09\n",
      "step: 87910 loss: 113.787093 time elapsed: 111.6648 learning rate: 0.001470, scenario: 0, slope: -0.2582295742737485, fluctuations: 0.09\n",
      "step: 87920 loss: 112.801668 time elapsed: 111.6766 learning rate: 0.001470, scenario: 0, slope: -0.32609863806502876, fluctuations: 0.09\n",
      "step: 87930 loss: 111.880909 time elapsed: 111.6904 learning rate: 0.001470, scenario: 0, slope: -0.4172559350052964, fluctuations: 0.09\n",
      "step: 87940 loss: 111.005392 time elapsed: 111.7046 learning rate: 0.001470, scenario: 0, slope: -0.16573655193855302, fluctuations: 0.06\n",
      "step: 87950 loss: 110.022001 time elapsed: 111.7180 learning rate: 0.002864, scenario: 1, slope: -0.11375623851939677, fluctuations: 0.03\n",
      "step: 87960 loss: 107.774620 time elapsed: 111.7311 learning rate: 0.007430, scenario: 1, slope: -0.10263676580296731, fluctuations: 0.02\n",
      "step: 87970 loss: 2210.211467 time elapsed: 111.7449 learning rate: 0.016722, scenario: -1, slope: 1.1848692208052112, fluctuations: 0.01\n",
      "step: 87980 loss: 11587.003707 time elapsed: 111.7586 learning rate: 0.005831, scenario: -1, slope: 57.36886785893327, fluctuations: 0.03\n",
      "step: 87990 loss: 4599.457080 time elapsed: 111.7726 learning rate: 0.002033, scenario: -1, slope: 74.49371963300678, fluctuations: 0.05\n",
      "step: 88000 loss: 2994.138986 time elapsed: 111.7864 learning rate: 0.000788, scenario: -1, slope: 72.49862609826741, fluctuations: 0.06\n",
      "step: 88010 loss: 2400.889217 time elapsed: 111.8010 learning rate: 0.000275, scenario: -1, slope: 61.50665381261375, fluctuations: 0.07\n",
      "step: 88020 loss: 2312.588832 time elapsed: 111.8141 learning rate: 0.000096, scenario: -1, slope: 46.61115436187203, fluctuations: 0.07\n",
      "step: 88030 loss: 2273.759599 time elapsed: 111.8268 learning rate: 0.000033, scenario: -1, slope: 29.799532229387793, fluctuations: 0.07\n",
      "step: 88040 loss: 2260.445989 time elapsed: 111.8397 learning rate: 0.000012, scenario: -1, slope: 10.491629907133635, fluctuations: 0.07\n",
      "step: 88050 loss: 2255.675942 time elapsed: 111.8533 learning rate: 0.000008, scenario: 0, slope: -12.32430983227296, fluctuations: 0.07\n",
      "step: 88060 loss: 2251.806657 time elapsed: 111.8666 learning rate: 0.000008, scenario: 0, slope: -40.22883438443713, fluctuations: 0.07\n",
      "step: 88070 loss: 2248.031107 time elapsed: 111.8802 learning rate: 0.000008, scenario: 0, slope: -74.10451500419673, fluctuations: 0.07\n",
      "step: 88080 loss: 2244.333832 time elapsed: 111.8937 learning rate: 0.000008, scenario: 0, slope: -28.66514803490992, fluctuations: 0.03\n",
      "step: 88090 loss: 2240.703203 time elapsed: 111.9082 learning rate: 0.000008, scenario: 0, slope: -8.477053427749928, fluctuations: 0.01\n",
      "step: 88100 loss: 2237.129732 time elapsed: 111.9226 learning rate: 0.000008, scenario: 0, slope: -3.1121715581031504, fluctuations: 0.0\n",
      "step: 88110 loss: 2232.003734 time elapsed: 111.9375 learning rate: 0.000020, scenario: 1, slope: -0.9126803634838938, fluctuations: 0.0\n",
      "step: 88120 loss: 2219.141537 time elapsed: 111.9521 learning rate: 0.000051, scenario: 1, slope: -0.5522415370233218, fluctuations: 0.0\n",
      "step: 88130 loss: 2187.466832 time elapsed: 111.9669 learning rate: 0.000133, scenario: 1, slope: -0.550781441938185, fluctuations: 0.0\n",
      "step: 88140 loss: 2110.586059 time elapsed: 111.9803 learning rate: 0.000346, scenario: 1, slope: -0.8940456491883387, fluctuations: 0.0\n",
      "step: 88150 loss: 1926.670922 time elapsed: 111.9941 learning rate: 0.000897, scenario: 1, slope: -1.8578413228786794, fluctuations: 0.0\n",
      "step: 88160 loss: 1603.639824 time elapsed: 112.0080 learning rate: 0.001085, scenario: 0, slope: -4.041852529058385, fluctuations: 0.0\n",
      "step: 88170 loss: 1248.994211 time elapsed: 112.0232 learning rate: 0.001085, scenario: 0, slope: -7.445706808311568, fluctuations: 0.0\n",
      "step: 88180 loss: 871.750218 time elapsed: 112.0361 learning rate: 0.001085, scenario: 0, slope: -12.083300206412853, fluctuations: 0.0\n",
      "step: 88190 loss: 716.290491 time elapsed: 112.0482 learning rate: 0.001085, scenario: 0, slope: -16.481622401134334, fluctuations: 0.0\n",
      "step: 88200 loss: 616.615598 time elapsed: 112.0601 learning rate: 0.001085, scenario: 0, slope: -19.541753415886774, fluctuations: 0.0\n",
      "step: 88210 loss: 547.858651 time elapsed: 112.0728 learning rate: 0.001085, scenario: 0, slope: -21.69183023323991, fluctuations: 0.0\n",
      "step: 88220 loss: 505.783739 time elapsed: 112.0849 learning rate: 0.001085, scenario: 0, slope: -21.935884717941235, fluctuations: 0.0\n",
      "step: 88230 loss: 475.815991 time elapsed: 112.0971 learning rate: 0.001085, scenario: 0, slope: -20.46690877479017, fluctuations: 0.0\n",
      "step: 88240 loss: 453.070848 time elapsed: 112.1103 learning rate: 0.001085, scenario: 0, slope: -17.42560475931796, fluctuations: 0.0\n",
      "step: 88250 loss: 434.619631 time elapsed: 112.1243 learning rate: 0.001085, scenario: 0, slope: -13.306525711315794, fluctuations: 0.0\n",
      "step: 88260 loss: 419.139641 time elapsed: 112.1380 learning rate: 0.001085, scenario: 0, slope: -9.155595323053, fluctuations: 0.0\n",
      "step: 88270 loss: 405.700600 time elapsed: 112.1517 learning rate: 0.001085, scenario: 0, slope: -5.6828905705018595, fluctuations: 0.0\n",
      "step: 88280 loss: 393.795579 time elapsed: 112.1659 learning rate: 0.001085, scenario: 0, slope: -3.7606703756561397, fluctuations: 0.0\n",
      "step: 88290 loss: 383.061093 time elapsed: 112.1795 learning rate: 0.001085, scenario: 0, slope: -2.70368295136023, fluctuations: 0.0\n",
      "step: 88300 loss: 373.262050 time elapsed: 112.1940 learning rate: 0.001085, scenario: 0, slope: -2.1069028470382487, fluctuations: 0.0\n",
      "step: 88310 loss: 364.217500 time elapsed: 112.2085 learning rate: 0.001085, scenario: 0, slope: -1.6369932670183704, fluctuations: 0.0\n",
      "step: 88320 loss: 355.771376 time elapsed: 112.2228 learning rate: 0.001085, scenario: 0, slope: -1.381630265614281, fluctuations: 0.0\n",
      "step: 88330 loss: 347.745039 time elapsed: 112.2354 learning rate: 0.001085, scenario: 0, slope: -1.2051568927199867, fluctuations: 0.0\n",
      "step: 88340 loss: 339.967030 time elapsed: 112.2483 learning rate: 0.001085, scenario: 0, slope: -1.077993971986941, fluctuations: 0.0\n",
      "step: 88350 loss: 332.489367 time elapsed: 112.2619 learning rate: 0.001085, scenario: 0, slope: -0.9825256782639917, fluctuations: 0.0\n",
      "step: 88360 loss: 325.366617 time elapsed: 112.2745 learning rate: 0.001085, scenario: 0, slope: -0.9089556836710875, fluctuations: 0.0\n",
      "step: 88370 loss: 318.501195 time elapsed: 112.2876 learning rate: 0.001085, scenario: 0, slope: -0.850797132341477, fluctuations: 0.0\n",
      "step: 88380 loss: 311.849570 time elapsed: 112.3011 learning rate: 0.001085, scenario: 0, slope: -0.8040394980736718, fluctuations: 0.0\n",
      "step: 88390 loss: 305.373415 time elapsed: 112.3157 learning rate: 0.001085, scenario: 0, slope: -0.7658789401193717, fluctuations: 0.0\n",
      "step: 88400 loss: 299.042459 time elapsed: 112.3297 learning rate: 0.001085, scenario: 0, slope: -0.7371512423960868, fluctuations: 0.0\n",
      "step: 88410 loss: 292.847633 time elapsed: 112.3434 learning rate: 0.001085, scenario: 0, slope: -0.7074067326500753, fluctuations: 0.0\n",
      "step: 88420 loss: 286.813423 time elapsed: 112.3575 learning rate: 0.001085, scenario: 0, slope: -0.6839279758255847, fluctuations: 0.0\n",
      "step: 88430 loss: 281.001869 time elapsed: 112.3713 learning rate: 0.001085, scenario: 0, slope: -0.6625155177943317, fluctuations: 0.0\n",
      "step: 88440 loss: 275.489105 time elapsed: 112.3845 learning rate: 0.001085, scenario: 0, slope: -0.6423651915350655, fluctuations: 0.0\n",
      "step: 88450 loss: 270.320364 time elapsed: 112.3975 learning rate: 0.001085, scenario: 0, slope: -0.62268531852484, fluctuations: 0.0\n",
      "step: 88460 loss: 265.481623 time elapsed: 112.4111 learning rate: 0.001085, scenario: 0, slope: -0.6020957539597293, fluctuations: 0.0\n",
      "step: 88470 loss: 260.916753 time elapsed: 112.4238 learning rate: 0.001085, scenario: 0, slope: -0.5797325377314534, fluctuations: 0.0\n",
      "step: 88480 loss: 256.566964 time elapsed: 112.4362 learning rate: 0.001085, scenario: 0, slope: -0.5555308763833021, fluctuations: 0.0\n",
      "step: 88490 loss: 252.392357 time elapsed: 112.4485 learning rate: 0.001085, scenario: 0, slope: -0.5299453548803338, fluctuations: 0.0\n",
      "step: 88500 loss: 248.369842 time elapsed: 112.4600 learning rate: 0.001085, scenario: 0, slope: -0.5064320788584757, fluctuations: 0.0\n",
      "step: 88510 loss: 244.484570 time elapsed: 112.4721 learning rate: 0.001085, scenario: 0, slope: -0.4782531483363299, fluctuations: 0.0\n",
      "step: 88520 loss: 240.724594 time elapsed: 112.4840 learning rate: 0.001085, scenario: 0, slope: -0.45433286856383187, fluctuations: 0.0\n",
      "step: 88530 loss: 237.078510 time elapsed: 112.4958 learning rate: 0.001085, scenario: 0, slope: -0.4329153410076188, fluctuations: 0.0\n",
      "step: 88540 loss: 233.533362 time elapsed: 112.5087 learning rate: 0.001085, scenario: 0, slope: -0.41434784925340806, fluctuations: 0.0\n",
      "step: 88550 loss: 230.069388 time elapsed: 112.5216 learning rate: 0.001085, scenario: 0, slope: -0.39845652824385513, fluctuations: 0.0\n",
      "step: 88560 loss: 226.643097 time elapsed: 112.5357 learning rate: 0.001085, scenario: 0, slope: -0.3848577535932252, fluctuations: 0.0\n",
      "step: 88570 loss: 223.206382 time elapsed: 112.5493 learning rate: 0.001085, scenario: 0, slope: -0.37341410916874285, fluctuations: 0.0\n",
      "step: 88580 loss: 220.057770 time elapsed: 112.5628 learning rate: 0.001085, scenario: 0, slope: -0.3631419898960479, fluctuations: 0.0\n",
      "step: 88590 loss: 217.021121 time elapsed: 112.5763 learning rate: 0.001085, scenario: 0, slope: -0.3531873457792702, fluctuations: 0.0\n",
      "step: 88600 loss: 214.085849 time elapsed: 112.5897 learning rate: 0.001085, scenario: 0, slope: -0.3442573993992676, fluctuations: 0.0\n",
      "step: 88610 loss: 211.247777 time elapsed: 112.6035 learning rate: 0.001085, scenario: 0, slope: -0.33320892248416034, fluctuations: 0.0\n",
      "step: 88620 loss: 208.498277 time elapsed: 112.6174 learning rate: 0.001085, scenario: 0, slope: -0.3229548823886999, fluctuations: 0.0\n",
      "step: 88630 loss: 205.833335 time elapsed: 112.6312 learning rate: 0.001085, scenario: 0, slope: -0.3124794826659656, fluctuations: 0.0\n",
      "step: 88640 loss: 203.249915 time elapsed: 112.6445 learning rate: 0.001085, scenario: 0, slope: -0.3018214547407298, fluctuations: 0.0\n",
      "step: 88650 loss: 200.745420 time elapsed: 112.6570 learning rate: 0.001085, scenario: 0, slope: -0.291109232599581, fluctuations: 0.0\n",
      "step: 88660 loss: 198.317509 time elapsed: 112.6697 learning rate: 0.001085, scenario: 0, slope: -0.280670445864174, fluctuations: 0.0\n",
      "step: 88670 loss: 195.963896 time elapsed: 112.6826 learning rate: 0.001085, scenario: 0, slope: -0.27125687982230706, fluctuations: 0.0\n",
      "step: 88680 loss: 193.682303 time elapsed: 112.6967 learning rate: 0.001085, scenario: 0, slope: -0.26272686555645236, fluctuations: 0.0\n",
      "step: 88690 loss: 191.470433 time elapsed: 112.7099 learning rate: 0.001085, scenario: 0, slope: -0.25460482271604346, fluctuations: 0.0\n",
      "step: 88700 loss: 189.325968 time elapsed: 112.7236 learning rate: 0.001085, scenario: 0, slope: -0.24756182605796007, fluctuations: 0.0\n",
      "step: 88710 loss: 187.246556 time elapsed: 112.7385 learning rate: 0.001085, scenario: 0, slope: -0.23924452496547668, fluctuations: 0.0\n",
      "step: 88720 loss: 185.229828 time elapsed: 112.7530 learning rate: 0.001085, scenario: 0, slope: -0.23193739640289712, fluctuations: 0.0\n",
      "step: 88730 loss: 183.273404 time elapsed: 112.7679 learning rate: 0.001085, scenario: 0, slope: -0.22486452363696266, fluctuations: 0.0\n",
      "step: 88740 loss: 181.374906 time elapsed: 112.7827 learning rate: 0.001085, scenario: 0, slope: -0.21802465004349986, fluctuations: 0.0\n",
      "step: 88750 loss: 179.531967 time elapsed: 112.7975 learning rate: 0.001085, scenario: 0, slope: -0.21141821282144754, fluctuations: 0.0\n",
      "step: 88760 loss: 177.742243 time elapsed: 112.8115 learning rate: 0.001085, scenario: 0, slope: -0.20504625091430403, fluctuations: 0.0\n",
      "step: 88770 loss: 176.003427 time elapsed: 112.8248 learning rate: 0.001085, scenario: 0, slope: -0.19890955443083544, fluctuations: 0.0\n",
      "step: 88780 loss: 174.313255 time elapsed: 112.8376 learning rate: 0.001313, scenario: 1, slope: -0.1930082604542662, fluctuations: 0.0\n",
      "step: 88790 loss: 171.454671 time elapsed: 112.8505 learning rate: 0.003405, scenario: 1, slope: -0.18991676664155152, fluctuations: 0.0\n",
      "step: 88800 loss: 170.527913 time elapsed: 112.8619 learning rate: 0.005484, scenario: 0, slope: -0.19218384127270363, fluctuations: 0.02\n",
      "step: 88810 loss: 159.090010 time elapsed: 112.8745 learning rate: 0.005484, scenario: 0, slope: -0.21598614184012266, fluctuations: 0.06\n",
      "step: 88820 loss: 152.752005 time elapsed: 112.8864 learning rate: 0.005484, scenario: 0, slope: -0.2597202598663298, fluctuations: 0.11\n",
      "step: 88830 loss: 147.388499 time elapsed: 112.8978 learning rate: 0.005484, scenario: 0, slope: -0.3362140318107035, fluctuations: 0.12\n",
      "step: 88840 loss: 142.589613 time elapsed: 112.9096 learning rate: 0.005484, scenario: 0, slope: -0.4038899559578447, fluctuations: 0.12\n",
      "step: 88850 loss: 138.172410 time elapsed: 112.9212 learning rate: 0.005484, scenario: 0, slope: -0.4567638140322942, fluctuations: 0.12\n",
      "step: 88860 loss: 134.079465 time elapsed: 112.9342 learning rate: 0.005484, scenario: 0, slope: -0.4947229871895179, fluctuations: 0.12\n",
      "step: 88870 loss: 130.306323 time elapsed: 112.9480 learning rate: 0.005484, scenario: 0, slope: -0.5158311437688747, fluctuations: 0.12\n",
      "step: 88880 loss: 126.767569 time elapsed: 112.9617 learning rate: 0.005484, scenario: 0, slope: -0.5154378776909312, fluctuations: 0.12\n",
      "step: 88890 loss: 123.433925 time elapsed: 112.9751 learning rate: 0.005484, scenario: 0, slope: -0.4874372144587546, fluctuations: 0.12\n",
      "step: 88900 loss: 120.271666 time elapsed: 112.9879 learning rate: 0.005484, scenario: 0, slope: -0.4501837313942588, fluctuations: 0.09\n",
      "step: 88910 loss: 117.259823 time elapsed: 113.0020 learning rate: 0.005484, scenario: 0, slope: -0.40033907335347646, fluctuations: 0.05\n",
      "step: 88920 loss: 114.385640 time elapsed: 113.0158 learning rate: 0.005484, scenario: 0, slope: -0.3762571923739105, fluctuations: 0.0\n",
      "step: 88930 loss: 111.646266 time elapsed: 113.0294 learning rate: 0.005484, scenario: 0, slope: -0.3509831729740513, fluctuations: 0.0\n",
      "step: 88940 loss: 109.036174 time elapsed: 113.0435 learning rate: 0.005484, scenario: 0, slope: -0.3300084813632965, fluctuations: 0.0\n",
      "step: 88950 loss: 106.537280 time elapsed: 113.0561 learning rate: 0.005484, scenario: 0, slope: -0.3120277562162897, fluctuations: 0.0\n",
      "step: 88960 loss: 104.129589 time elapsed: 113.0683 learning rate: 0.005484, scenario: 0, slope: -0.29625703212827725, fluctuations: 0.0\n",
      "step: 88970 loss: 101.793483 time elapsed: 113.0800 learning rate: 0.005484, scenario: 0, slope: -0.2822355609393937, fluctuations: 0.0\n",
      "step: 88980 loss: 99.506238 time elapsed: 113.0918 learning rate: 0.005484, scenario: 0, slope: -0.26975886065806337, fluctuations: 0.0\n",
      "step: 88990 loss: 97.240100 time elapsed: 113.1036 learning rate: 0.005484, scenario: 0, slope: -0.2587939125340229, fluctuations: 0.0\n",
      "step: 89000 loss: 94.960147 time elapsed: 113.1153 learning rate: 0.005484, scenario: 0, slope: -0.25033380226522817, fluctuations: 0.0\n",
      "step: 89010 loss: 92.626897 time elapsed: 113.1277 learning rate: 0.005484, scenario: 0, slope: -0.24213266679673398, fluctuations: 0.0\n",
      "step: 89020 loss: 90.217273 time elapsed: 113.1407 learning rate: 0.005484, scenario: 0, slope: -0.2370977346627745, fluctuations: 0.0\n",
      "step: 89030 loss: 87.766112 time elapsed: 113.1546 learning rate: 0.005484, scenario: 0, slope: -0.23448968061643594, fluctuations: 0.0\n",
      "step: 89040 loss: 89.369213 time elapsed: 113.1684 learning rate: 0.005484, scenario: 0, slope: -0.2305554758464635, fluctuations: 0.0\n",
      "step: 89050 loss: 83.534331 time elapsed: 113.1820 learning rate: 0.005484, scenario: 0, slope: -0.22663204752280966, fluctuations: 0.03\n",
      "step: 89060 loss: 82.271314 time elapsed: 113.1959 learning rate: 0.005484, scenario: 0, slope: -0.22314628599015535, fluctuations: 0.05\n",
      "step: 89070 loss: 79.587549 time elapsed: 113.2103 learning rate: 0.005484, scenario: 0, slope: -0.22118365859794756, fluctuations: 0.07\n",
      "step: 89080 loss: 78.007755 time elapsed: 113.2249 learning rate: 0.005484, scenario: 0, slope: -0.2171764356357946, fluctuations: 0.08\n",
      "step: 89090 loss: 76.414890 time elapsed: 113.2396 learning rate: 0.005484, scenario: 0, slope: -0.21079133603184846, fluctuations: 0.08\n",
      "step: 89100 loss: 74.934554 time elapsed: 113.2544 learning rate: 0.005484, scenario: 0, slope: -0.2033248681589791, fluctuations: 0.08\n",
      "step: 89110 loss: 73.519469 time elapsed: 113.2683 learning rate: 0.005484, scenario: 0, slope: -0.19264504691677165, fluctuations: 0.08\n",
      "step: 89120 loss: 72.122688 time elapsed: 113.2818 learning rate: 0.005484, scenario: 0, slope: -0.18251213983391448, fluctuations: 0.08\n",
      "step: 89130 loss: 70.797598 time elapsed: 113.2949 learning rate: 0.005484, scenario: 0, slope: -0.17319840680322443, fluctuations: 0.08\n",
      "step: 89140 loss: 73.291445 time elapsed: 113.3068 learning rate: 0.005484, scenario: 0, slope: -0.15254442766376766, fluctuations: 0.08\n",
      "step: 89150 loss: 69.528689 time elapsed: 113.3195 learning rate: 0.005484, scenario: 0, slope: -0.1356395557403559, fluctuations: 0.09\n",
      "step: 89160 loss: 67.989248 time elapsed: 113.3327 learning rate: 0.005484, scenario: 0, slope: -0.125357297133512, fluctuations: 0.09\n",
      "step: 89170 loss: 66.298736 time elapsed: 113.3455 learning rate: 0.005484, scenario: 0, slope: -0.12208518096545738, fluctuations: 0.09\n",
      "step: 89180 loss: 65.149240 time elapsed: 113.3597 learning rate: 0.005484, scenario: 0, slope: -0.12073915836901934, fluctuations: 0.09\n",
      "step: 89190 loss: 64.012430 time elapsed: 113.3737 learning rate: 0.005484, scenario: 0, slope: -0.12060828192115125, fluctuations: 0.09\n",
      "step: 89200 loss: 62.941705 time elapsed: 113.3872 learning rate: 0.005484, scenario: 0, slope: -0.12103495184984456, fluctuations: 0.09\n",
      "step: 89210 loss: 61.905845 time elapsed: 113.4016 learning rate: 0.005484, scenario: 0, slope: -0.12222329113848684, fluctuations: 0.09\n",
      "step: 89220 loss: 66.529825 time elapsed: 113.4148 learning rate: 0.005484, scenario: 0, slope: -0.11574890573965009, fluctuations: 0.09\n",
      "step: 89230 loss: 61.046477 time elapsed: 113.4281 learning rate: 0.005484, scenario: 0, slope: -0.11365792385106593, fluctuations: 0.11\n",
      "step: 89240 loss: 59.719208 time elapsed: 113.4413 learning rate: 0.005484, scenario: 0, slope: -0.10245908019096818, fluctuations: 0.11\n",
      "step: 89250 loss: 58.252955 time elapsed: 113.4547 learning rate: 0.005484, scenario: 0, slope: -0.09640181252278655, fluctuations: 0.11\n",
      "step: 89260 loss: 57.136047 time elapsed: 113.4687 learning rate: 0.005484, scenario: 0, slope: -0.09726295221773909, fluctuations: 0.08\n",
      "step: 89270 loss: 56.940577 time elapsed: 113.4811 learning rate: 0.005484, scenario: 0, slope: -0.09638022238197107, fluctuations: 0.08\n",
      "step: 89280 loss: 55.642697 time elapsed: 113.4934 learning rate: 0.005484, scenario: 0, slope: -0.097884210637396, fluctuations: 0.1\n",
      "step: 89290 loss: 54.257031 time elapsed: 113.5058 learning rate: 0.005484, scenario: 0, slope: -0.10064501592011968, fluctuations: 0.1\n",
      "step: 89300 loss: 53.255880 time elapsed: 113.5176 learning rate: 0.005484, scenario: 0, slope: -0.1018677241240411, fluctuations: 0.11\n",
      "step: 89310 loss: 52.492884 time elapsed: 113.5302 learning rate: 0.005484, scenario: 0, slope: -0.10578989240543132, fluctuations: 0.12\n",
      "step: 89320 loss: 53.449616 time elapsed: 113.5426 learning rate: 0.005484, scenario: 0, slope: -0.10082126256406379, fluctuations: 0.11\n",
      "step: 89330 loss: 51.222422 time elapsed: 113.5556 learning rate: 0.005484, scenario: 0, slope: -0.08291098852205626, fluctuations: 0.11\n",
      "step: 89340 loss: 50.616920 time elapsed: 113.5685 learning rate: 0.005484, scenario: 0, slope: -0.07809734233255418, fluctuations: 0.11\n",
      "step: 89350 loss: 50.424828 time elapsed: 113.5822 learning rate: 0.005484, scenario: 0, slope: -0.07750733859758628, fluctuations: 0.1\n",
      "step: 89360 loss: 48.983188 time elapsed: 113.5963 learning rate: 0.005484, scenario: 0, slope: -0.07953048115697972, fluctuations: 0.11\n",
      "step: 89370 loss: 47.638624 time elapsed: 113.6104 learning rate: 0.005484, scenario: 0, slope: -0.07803602380900926, fluctuations: 0.1\n",
      "step: 89380 loss: 47.553587 time elapsed: 113.6240 learning rate: 0.005484, scenario: 0, slope: -0.07721766424424374, fluctuations: 0.11\n",
      "step: 89390 loss: 47.613688 time elapsed: 113.6389 learning rate: 0.005484, scenario: 0, slope: -0.07985270566799522, fluctuations: 0.14\n",
      "step: 89400 loss: 46.554264 time elapsed: 113.6539 learning rate: 0.005484, scenario: 0, slope: -0.0793513234726189, fluctuations: 0.14\n",
      "step: 89410 loss: 44.474664 time elapsed: 113.6696 learning rate: 0.005484, scenario: 0, slope: -0.08296917617145036, fluctuations: 0.16\n",
      "step: 89420 loss: 43.571071 time elapsed: 113.6845 learning rate: 0.005484, scenario: 0, slope: -0.08917767415722745, fluctuations: 0.17\n",
      "step: 89430 loss: 44.425653 time elapsed: 113.6983 learning rate: 0.005484, scenario: 0, slope: -0.0585636505557024, fluctuations: 0.18\n",
      "step: 89440 loss: 58.177726 time elapsed: 113.7125 learning rate: 0.008832, scenario: 1, slope: -0.032254152903240435, fluctuations: 0.18\n",
      "step: 89450 loss: 358.258539 time elapsed: 113.7266 learning rate: 0.003266, scenario: -1, slope: 1.7398262851906197, fluctuations: 0.19\n",
      "step: 89460 loss: 104.890361 time elapsed: 113.7391 learning rate: 0.001139, scenario: -1, slope: 1.837608039239169, fluctuations: 0.21\n",
      "step: 89470 loss: 70.681318 time elapsed: 113.7516 learning rate: 0.000397, scenario: -1, slope: 1.4354149739903852, fluctuations: 0.2\n",
      "step: 89480 loss: 63.063335 time elapsed: 113.7651 learning rate: 0.000138, scenario: -1, slope: 0.9426399383147068, fluctuations: 0.17\n",
      "step: 89490 loss: 61.067935 time elapsed: 113.7790 learning rate: 0.000048, scenario: -1, slope: 0.45456506504513833, fluctuations: 0.15\n",
      "step: 89500 loss: 60.798998 time elapsed: 113.7936 learning rate: 0.000019, scenario: -1, slope: 0.01731530928670714, fluctuations: 0.13\n",
      "step: 89510 loss: 60.612108 time elapsed: 113.8087 learning rate: 0.000019, scenario: 0, slope: -0.4979654887322877, fluctuations: 0.1\n",
      "step: 89520 loss: 60.473842 time elapsed: 113.8228 learning rate: 0.000019, scenario: 0, slope: -1.008207505564191, fluctuations: 0.08\n",
      "step: 89530 loss: 60.378941 time elapsed: 113.8371 learning rate: 0.000019, scenario: 0, slope: -1.5963110879388926, fluctuations: 0.07\n",
      "step: 89540 loss: 60.300208 time elapsed: 113.8517 learning rate: 0.000019, scenario: 0, slope: -2.080398698886295, fluctuations: 0.05\n",
      "step: 89550 loss: 60.225508 time elapsed: 113.8664 learning rate: 0.000019, scenario: 0, slope: -0.6220697014875234, fluctuations: 0.01\n",
      "step: 89560 loss: 60.152568 time elapsed: 113.8813 learning rate: 0.000019, scenario: 0, slope: -0.1442482964727916, fluctuations: 0.0\n",
      "step: 89570 loss: 60.073382 time elapsed: 113.8942 learning rate: 0.000033, scenario: 1, slope: -0.03934001835821152, fluctuations: 0.0\n",
      "step: 89580 loss: 59.895830 time elapsed: 113.9061 learning rate: 0.000086, scenario: 1, slope: -0.014118284702732053, fluctuations: 0.0\n",
      "step: 89590 loss: 59.462631 time elapsed: 113.9187 learning rate: 0.000223, scenario: 1, slope: -0.011442446276427098, fluctuations: 0.0\n",
      "step: 89600 loss: 58.475117 time elapsed: 113.9312 learning rate: 0.000526, scenario: 1, slope: -0.014164905572354726, fluctuations: 0.0\n",
      "step: 89610 loss: 56.608127 time elapsed: 113.9442 learning rate: 0.001363, scenario: 1, slope: -0.02503932506680095, fluctuations: 0.0\n",
      "step: 89620 loss: 53.129398 time elapsed: 113.9563 learning rate: 0.003536, scenario: 1, slope: -0.047240454929372136, fluctuations: 0.0\n",
      "step: 89630 loss: 47.819527 time elapsed: 113.9698 learning rate: 0.004706, scenario: 0, slope: -0.08880311440449161, fluctuations: 0.0\n",
      "step: 89640 loss: 46.480435 time elapsed: 113.9840 learning rate: 0.004706, scenario: 0, slope: -0.13390112161637027, fluctuations: 0.01\n",
      "step: 89650 loss: 45.530990 time elapsed: 113.9983 learning rate: 0.004706, scenario: 0, slope: -0.16851799564508604, fluctuations: 0.05\n",
      "step: 89660 loss: 43.666579 time elapsed: 114.0123 learning rate: 0.004706, scenario: 0, slope: -0.19928992478049165, fluctuations: 0.1\n",
      "step: 89670 loss: 42.529448 time elapsed: 114.0261 learning rate: 0.004706, scenario: 0, slope: -0.2212560824457744, fluctuations: 0.14\n",
      "step: 89680 loss: 41.510674 time elapsed: 114.0399 learning rate: 0.004706, scenario: 0, slope: -0.22479084360475787, fluctuations: 0.16\n",
      "step: 89690 loss: 40.713957 time elapsed: 114.0530 learning rate: 0.004706, scenario: 0, slope: -0.21052399101328148, fluctuations: 0.16\n",
      "step: 89700 loss: 39.907267 time elapsed: 114.0664 learning rate: 0.004706, scenario: 0, slope: -0.18855283930566039, fluctuations: 0.16\n",
      "step: 89710 loss: 39.199577 time elapsed: 114.0804 learning rate: 0.004706, scenario: 0, slope: -0.1529059916026465, fluctuations: 0.16\n",
      "step: 89720 loss: 38.584535 time elapsed: 114.0963 learning rate: 0.004706, scenario: 0, slope: -0.11749551802287903, fluctuations: 0.16\n",
      "step: 89730 loss: 38.021821 time elapsed: 114.1090 learning rate: 0.004706, scenario: 0, slope: -0.09427556600636015, fluctuations: 0.16\n",
      "step: 89740 loss: 37.489932 time elapsed: 114.1209 learning rate: 0.004706, scenario: 0, slope: -0.08372098255585288, fluctuations: 0.14\n",
      "step: 89750 loss: 36.980201 time elapsed: 114.1328 learning rate: 0.004706, scenario: 0, slope: -0.07358953708024368, fluctuations: 0.1\n",
      "step: 89760 loss: 36.490053 time elapsed: 114.1450 learning rate: 0.004706, scenario: 0, slope: -0.06641185351411666, fluctuations: 0.06\n",
      "step: 89770 loss: 36.016272 time elapsed: 114.1567 learning rate: 0.004706, scenario: 0, slope: -0.06177371075042219, fluctuations: 0.02\n",
      "step: 89780 loss: 35.555098 time elapsed: 114.1682 learning rate: 0.004706, scenario: 0, slope: -0.057894342613307934, fluctuations: 0.0\n",
      "step: 89790 loss: 35.108788 time elapsed: 114.1812 learning rate: 0.004706, scenario: 0, slope: -0.05409363552324184, fluctuations: 0.0\n",
      "step: 89800 loss: 34.675452 time elapsed: 114.1951 learning rate: 0.004706, scenario: 0, slope: -0.05129335453104372, fluctuations: 0.0\n",
      "step: 89810 loss: 34.253308 time elapsed: 114.2094 learning rate: 0.004706, scenario: 0, slope: -0.048760623080509165, fluctuations: 0.0\n",
      "step: 89820 loss: 33.840921 time elapsed: 114.2232 learning rate: 0.004706, scenario: 0, slope: -0.04701153511121111, fluctuations: 0.0\n",
      "step: 89830 loss: 33.437247 time elapsed: 114.2367 learning rate: 0.004706, scenario: 0, slope: -0.0455284412778958, fluctuations: 0.0\n",
      "step: 89840 loss: 33.041631 time elapsed: 114.2502 learning rate: 0.004706, scenario: 0, slope: -0.0442110067391439, fluctuations: 0.0\n",
      "step: 89850 loss: 32.654357 time elapsed: 114.2636 learning rate: 0.004706, scenario: 0, slope: -0.043024410719190935, fluctuations: 0.0\n",
      "step: 89860 loss: 32.276809 time elapsed: 114.2775 learning rate: 0.004706, scenario: 0, slope: -0.04193740398233249, fluctuations: 0.0\n",
      "step: 89870 loss: 31.909815 time elapsed: 114.2918 learning rate: 0.004706, scenario: 0, slope: -0.04091590433924874, fluctuations: 0.0\n",
      "step: 89880 loss: 31.551785 time elapsed: 114.3048 learning rate: 0.004706, scenario: 0, slope: -0.039942878501976815, fluctuations: 0.0\n",
      "step: 89890 loss: 31.201436 time elapsed: 114.3169 learning rate: 0.004706, scenario: 0, slope: -0.03901627138098958, fluctuations: 0.0\n",
      "step: 89900 loss: 30.858440 time elapsed: 114.3287 learning rate: 0.004706, scenario: 0, slope: -0.03820411247729496, fluctuations: 0.0\n",
      "step: 89910 loss: 30.522445 time elapsed: 114.3412 learning rate: 0.004706, scenario: 0, slope: -0.03723512550864241, fluctuations: 0.0\n",
      "step: 89920 loss: 30.193171 time elapsed: 114.3533 learning rate: 0.004706, scenario: 0, slope: -0.036376766344086243, fluctuations: 0.0\n",
      "step: 89930 loss: 29.870353 time elapsed: 114.3654 learning rate: 0.004706, scenario: 0, slope: -0.035545166753046385, fluctuations: 0.0\n",
      "step: 89940 loss: 29.553764 time elapsed: 114.3773 learning rate: 0.004706, scenario: 0, slope: -0.0347494637840142, fluctuations: 0.0\n",
      "step: 89950 loss: 29.243194 time elapsed: 114.3903 learning rate: 0.004706, scenario: 0, slope: -0.03399895769722968, fluctuations: 0.0\n",
      "step: 89960 loss: 28.938446 time elapsed: 114.4041 learning rate: 0.004706, scenario: 0, slope: -0.03329563457349031, fluctuations: 0.0\n",
      "step: 89970 loss: 28.639328 time elapsed: 114.4180 learning rate: 0.004706, scenario: 0, slope: -0.03263003575476627, fluctuations: 0.0\n",
      "step: 89980 loss: 28.345656 time elapsed: 114.4315 learning rate: 0.004706, scenario: 0, slope: -0.03199143587092014, fluctuations: 0.0\n",
      "step: 89990 loss: 28.057253 time elapsed: 114.4441 learning rate: 0.004706, scenario: 0, slope: -0.031376198597073995, fluctuations: 0.0\n",
      "step: 90000 loss: 27.773944 time elapsed: 114.4573 learning rate: 0.005177, scenario: 1, slope: -0.030840972180557417, fluctuations: 0.0\n",
      "step: 90010 loss: 27.329448 time elapsed: 114.4708 learning rate: 0.012207, scenario: 0, slope: -0.030544854560183618, fluctuations: 0.0\n",
      "step: 90020 loss: 9539.793554 time elapsed: 114.4839 learning rate: 0.006812, scenario: -1, slope: 23.925199989752933, fluctuations: 0.01\n",
      "step: 90030 loss: 1967.547490 time elapsed: 114.4971 learning rate: 0.002375, scenario: -1, slope: 31.183525117857155, fluctuations: 0.04\n",
      "step: 90040 loss: 906.093158 time elapsed: 114.5103 learning rate: 0.000828, scenario: -1, slope: 29.354956657309756, fluctuations: 0.05\n",
      "step: 90050 loss: 658.090011 time elapsed: 114.5227 learning rate: 0.000289, scenario: -1, slope: 24.719148809943874, fluctuations: 0.07\n",
      "step: 90060 loss: 590.450289 time elapsed: 114.5348 learning rate: 0.000101, scenario: -1, slope: 17.280744392242443, fluctuations: 0.07\n",
      "step: 90070 loss: 582.737040 time elapsed: 114.5467 learning rate: 0.000035, scenario: -1, slope: 9.766723843037102, fluctuations: 0.07\n",
      "step: 90080 loss: 579.137897 time elapsed: 114.5583 learning rate: 0.000012, scenario: -1, slope: 1.8803307940561782, fluctuations: 0.07\n",
      "step: 90090 loss: 577.649982 time elapsed: 114.5700 learning rate: 0.000010, scenario: 0, slope: -6.780314240085638, fluctuations: 0.07\n",
      "step: 90100 loss: 576.369572 time elapsed: 114.5817 learning rate: 0.000010, scenario: 0, slope: -15.65468556712529, fluctuations: 0.07\n",
      "step: 90110 loss: 575.206126 time elapsed: 114.5946 learning rate: 0.000010, scenario: 0, slope: -28.666181262222434, fluctuations: 0.07\n",
      "step: 90120 loss: 574.139823 time elapsed: 114.6081 learning rate: 0.000010, scenario: 0, slope: -16.314741089839757, fluctuations: 0.05\n",
      "step: 90130 loss: 573.146907 time elapsed: 114.6222 learning rate: 0.000010, scenario: 0, slope: -3.5335463769932005, fluctuations: 0.03\n",
      "step: 90140 loss: 572.207938 time elapsed: 114.6358 learning rate: 0.000010, scenario: 0, slope: -0.9749738226587147, fluctuations: 0.01\n",
      "step: 90150 loss: 571.005722 time elapsed: 114.6495 learning rate: 0.000023, scenario: 1, slope: -0.2779298896877607, fluctuations: 0.0\n",
      "step: 90160 loss: 568.131128 time elapsed: 114.6633 learning rate: 0.000061, scenario: 1, slope: -0.1505125998638264, fluctuations: 0.0\n",
      "step: 90170 loss: 561.349474 time elapsed: 114.6766 learning rate: 0.000157, scenario: 1, slope: -0.14478114728811217, fluctuations: 0.0\n",
      "step: 90180 loss: 545.505762 time elapsed: 114.6902 learning rate: 0.000408, scenario: 1, slope: -0.21083288951708518, fluctuations: 0.0\n",
      "step: 90190 loss: 508.982284 time elapsed: 114.7039 learning rate: 0.001058, scenario: 1, slope: -0.40153780353293533, fluctuations: 0.0\n",
      "step: 90200 loss: 441.227471 time elapsed: 114.7175 learning rate: 0.001549, scenario: 0, slope: -0.7828694245526886, fluctuations: 0.0\n",
      "step: 90210 loss: 384.106926 time elapsed: 114.7301 learning rate: 0.001549, scenario: 0, slope: -1.5087121078318344, fluctuations: 0.0\n",
      "step: 90220 loss: 338.968381 time elapsed: 114.7420 learning rate: 0.001549, scenario: 0, slope: -2.251969637895028, fluctuations: 0.0\n",
      "step: 90230 loss: 301.458287 time elapsed: 114.7538 learning rate: 0.001549, scenario: 0, slope: -2.9640261913615586, fluctuations: 0.0\n",
      "step: 90240 loss: 274.071785 time elapsed: 114.7655 learning rate: 0.001549, scenario: 0, slope: -3.5456874128980167, fluctuations: 0.0\n",
      "step: 90250 loss: 253.005853 time elapsed: 114.7773 learning rate: 0.001549, scenario: 0, slope: -3.916955786895831, fluctuations: 0.0\n",
      "step: 90260 loss: 235.901983 time elapsed: 114.7890 learning rate: 0.001549, scenario: 0, slope: -4.03072082721002, fluctuations: 0.0\n",
      "step: 90270 loss: 221.584693 time elapsed: 114.8018 learning rate: 0.001549, scenario: 0, slope: -3.8671675457007035, fluctuations: 0.0\n",
      "step: 90280 loss: 209.724772 time elapsed: 114.8156 learning rate: 0.001549, scenario: 0, slope: -3.4415009249294743, fluctuations: 0.0\n",
      "step: 90290 loss: 199.648018 time elapsed: 114.8292 learning rate: 0.001549, scenario: 0, slope: -2.8363261736521754, fluctuations: 0.0\n",
      "step: 90300 loss: 190.943808 time elapsed: 114.8423 learning rate: 0.001549, scenario: 0, slope: -2.3031490360892803, fluctuations: 0.0\n",
      "step: 90310 loss: 183.301476 time elapsed: 114.8557 learning rate: 0.001549, scenario: 0, slope: -1.8035106937145995, fluctuations: 0.0\n",
      "step: 90320 loss: 176.487390 time elapsed: 114.8695 learning rate: 0.001549, scenario: 0, slope: -1.4580836489122468, fluctuations: 0.0\n",
      "step: 90330 loss: 170.301617 time elapsed: 114.8827 learning rate: 0.001549, scenario: 0, slope: -1.2044829406870456, fluctuations: 0.0\n",
      "step: 90340 loss: 164.328134 time elapsed: 114.8959 learning rate: 0.001549, scenario: 0, slope: -1.0205947965582671, fluctuations: 0.0\n",
      "step: 90350 loss: 158.752939 time elapsed: 114.9088 learning rate: 0.001549, scenario: 0, slope: -0.8826011003690073, fluctuations: 0.0\n",
      "step: 90360 loss: 153.611677 time elapsed: 114.9218 learning rate: 0.001549, scenario: 0, slope: -0.776594452189813, fluctuations: 0.0\n",
      "step: 90370 loss: 149.105208 time elapsed: 114.9339 learning rate: 0.001549, scenario: 0, slope: -0.6936499973983087, fluctuations: 0.0\n",
      "step: 90380 loss: 144.914578 time elapsed: 114.9453 learning rate: 0.001549, scenario: 0, slope: -0.6278609314182335, fluctuations: 0.0\n",
      "step: 90390 loss: 140.892812 time elapsed: 114.9570 learning rate: 0.001549, scenario: 0, slope: -0.5739820752875495, fluctuations: 0.0\n",
      "step: 90400 loss: 136.779145 time elapsed: 114.9688 learning rate: 0.001549, scenario: 0, slope: -0.5337887560899696, fluctuations: 0.0\n",
      "step: 90410 loss: 131.663404 time elapsed: 114.9810 learning rate: 0.001549, scenario: 0, slope: -0.4963778581216211, fluctuations: 0.0\n",
      "step: 90420 loss: 123.878622 time elapsed: 114.9926 learning rate: 0.001549, scenario: 0, slope: -0.4838188907529868, fluctuations: 0.0\n",
      "step: 90430 loss: 115.716931 time elapsed: 115.0046 learning rate: 0.001549, scenario: 0, slope: -0.4935255283810759, fluctuations: 0.02\n",
      "step: 90440 loss: 107.432453 time elapsed: 115.0187 learning rate: 0.001549, scenario: 0, slope: -0.5265759770022642, fluctuations: 0.03\n",
      "step: 90450 loss: 101.156544 time elapsed: 115.0326 learning rate: 0.001549, scenario: 0, slope: -0.5695024532556056, fluctuations: 0.03\n",
      "step: 90460 loss: 96.931282 time elapsed: 115.0462 learning rate: 0.001549, scenario: 0, slope: -0.6041179735837676, fluctuations: 0.03\n",
      "step: 90470 loss: 93.816691 time elapsed: 115.0597 learning rate: 0.001549, scenario: 0, slope: -0.6202749088470545, fluctuations: 0.03\n",
      "step: 90480 loss: 91.317290 time elapsed: 115.0730 learning rate: 0.001549, scenario: 0, slope: -0.6130676434226219, fluctuations: 0.03\n",
      "step: 90490 loss: 89.193251 time elapsed: 115.0858 learning rate: 0.001549, scenario: 0, slope: -0.5802734481398902, fluctuations: 0.03\n",
      "step: 90500 loss: 87.327146 time elapsed: 115.0990 learning rate: 0.001549, scenario: 0, slope: -0.528115939478862, fluctuations: 0.03\n",
      "step: 90510 loss: 85.663812 time elapsed: 115.1128 learning rate: 0.001549, scenario: 0, slope: -0.4376641904506376, fluctuations: 0.03\n",
      "step: 90520 loss: 84.156078 time elapsed: 115.1263 learning rate: 0.001549, scenario: 0, slope: -0.3516586120148375, fluctuations: 0.02\n",
      "step: 90530 loss: 82.773771 time elapsed: 115.1384 learning rate: 0.001549, scenario: 0, slope: -0.28232222404101487, fluctuations: 0.0\n",
      "step: 90540 loss: 81.493823 time elapsed: 115.1505 learning rate: 0.001549, scenario: 0, slope: -0.22764186497659517, fluctuations: 0.0\n",
      "step: 90550 loss: 80.300123 time elapsed: 115.1627 learning rate: 0.001549, scenario: 0, slope: -0.19161755857357918, fluctuations: 0.0\n",
      "step: 90560 loss: 79.179962 time elapsed: 115.1745 learning rate: 0.001549, scenario: 0, slope: -0.16777805743274568, fluctuations: 0.0\n",
      "step: 90570 loss: 78.124615 time elapsed: 115.1866 learning rate: 0.001549, scenario: 0, slope: -0.15067894713779806, fluctuations: 0.0\n",
      "step: 90580 loss: 77.423497 time elapsed: 115.1985 learning rate: 0.001549, scenario: 0, slope: -0.1375310101586282, fluctuations: 0.01\n",
      "step: 90590 loss: 76.268410 time elapsed: 115.2113 learning rate: 0.001549, scenario: 0, slope: -0.12620178486900832, fluctuations: 0.01\n",
      "step: 90600 loss: 75.279913 time elapsed: 115.2252 learning rate: 0.001549, scenario: 0, slope: -0.11811683463426483, fluctuations: 0.01\n",
      "step: 90610 loss: 74.405556 time elapsed: 115.2396 learning rate: 0.001549, scenario: 0, slope: -0.11001333487340782, fluctuations: 0.01\n",
      "step: 90620 loss: 73.574541 time elapsed: 115.2533 learning rate: 0.001549, scenario: 0, slope: -0.10385152859693893, fluctuations: 0.01\n",
      "step: 90630 loss: 72.770481 time elapsed: 115.2661 learning rate: 0.001549, scenario: 0, slope: -0.09856156530929323, fluctuations: 0.01\n",
      "step: 90640 loss: 71.995589 time elapsed: 115.2794 learning rate: 0.001549, scenario: 0, slope: -0.09395653408908525, fluctuations: 0.01\n",
      "step: 90650 loss: 71.246798 time elapsed: 115.2927 learning rate: 0.001549, scenario: 0, slope: -0.08990461477347102, fluctuations: 0.01\n",
      "step: 90660 loss: 70.521734 time elapsed: 115.3063 learning rate: 0.001549, scenario: 0, slope: -0.08630811475924292, fluctuations: 0.01\n",
      "step: 90670 loss: 69.818211 time elapsed: 115.3202 learning rate: 0.001549, scenario: 0, slope: -0.08309008521161626, fluctuations: 0.01\n",
      "step: 90680 loss: 69.134358 time elapsed: 115.3337 learning rate: 0.001549, scenario: 0, slope: -0.07990908051034379, fluctuations: 0.0\n",
      "step: 90690 loss: 68.468689 time elapsed: 115.3456 learning rate: 0.001549, scenario: 0, slope: -0.07673084972090455, fluctuations: 0.0\n",
      "step: 90700 loss: 67.710534 time elapsed: 115.3576 learning rate: 0.002745, scenario: 1, slope: -0.07442750226621278, fluctuations: 0.0\n",
      "step: 90710 loss: 113.744424 time elapsed: 115.3702 learning rate: 0.002768, scenario: -1, slope: 0.0850835617455237, fluctuations: 0.02\n",
      "step: 90720 loss: 74.788198 time elapsed: 115.3820 learning rate: 0.000965, scenario: -1, slope: 0.09375359963665413, fluctuations: 0.05\n",
      "step: 90730 loss: 67.945580 time elapsed: 115.3938 learning rate: 0.000337, scenario: -1, slope: 0.0737204505530366, fluctuations: 0.07\n",
      "step: 90740 loss: 66.938913 time elapsed: 115.4056 learning rate: 0.000117, scenario: -1, slope: 0.032823161753055094, fluctuations: 0.07\n",
      "step: 90750 loss: 66.658898 time elapsed: 115.4183 learning rate: 0.000059, scenario: 1, slope: -0.0049757741677097945, fluctuations: 0.07\n",
      "step: 90760 loss: 66.494998 time elapsed: 115.4314 learning rate: 0.000152, scenario: 1, slope: -0.04035363980269191, fluctuations: 0.07\n",
      "step: 90770 loss: 66.201291 time elapsed: 115.4454 learning rate: 0.000326, scenario: 0, slope: -0.0759423681978341, fluctuations: 0.07\n",
      "step: 90780 loss: 65.864426 time elapsed: 115.4591 learning rate: 0.000326, scenario: 0, slope: -0.11510601454342136, fluctuations: 0.07\n",
      "step: 90790 loss: 65.588855 time elapsed: 115.4728 learning rate: 0.000326, scenario: 0, slope: -0.16054925369918888, fluctuations: 0.07\n",
      "step: 90800 loss: 65.362767 time elapsed: 115.4864 learning rate: 0.000326, scenario: 0, slope: -0.2100122651905352, fluctuations: 0.07\n",
      "step: 90810 loss: 65.169345 time elapsed: 115.5006 learning rate: 0.000326, scenario: 0, slope: -0.0874533418107982, fluctuations: 0.04\n",
      "step: 90820 loss: 64.921865 time elapsed: 115.5141 learning rate: 0.000846, scenario: 1, slope: -0.04103038707728675, fluctuations: 0.01\n",
      "step: 90830 loss: 64.374790 time elapsed: 115.5276 learning rate: 0.002194, scenario: 1, slope: -0.027748181886934026, fluctuations: 0.0\n",
      "step: 90840 loss: 63.185513 time elapsed: 115.5406 learning rate: 0.005690, scenario: 1, slope: -0.02985024595634109, fluctuations: 0.0\n",
      "step: 90850 loss: 64.777454 time elapsed: 115.5540 learning rate: 0.014759, scenario: 1, slope: -0.03956575692970979, fluctuations: 0.0\n",
      "step: 90860 loss: 37662.883279 time elapsed: 115.5699 learning rate: 0.006671, scenario: -1, slope: 122.92183218309954, fluctuations: 0.02\n",
      "step: 90870 loss: 6328.995030 time elapsed: 115.5824 learning rate: 0.002326, scenario: -1, slope: 160.0446461912153, fluctuations: 0.04\n",
      "step: 90880 loss: 3865.510693 time elapsed: 115.5949 learning rate: 0.000811, scenario: -1, slope: 147.44717226566988, fluctuations: 0.06\n",
      "step: 90890 loss: 2342.963842 time elapsed: 115.6072 learning rate: 0.000283, scenario: -1, slope: 111.11349618290672, fluctuations: 0.06\n",
      "step: 90900 loss: 2182.470553 time elapsed: 115.6186 learning rate: 0.000110, scenario: -1, slope: 76.39276498812379, fluctuations: 0.06\n",
      "step: 90910 loss: 2080.697854 time elapsed: 115.6330 learning rate: 0.000038, scenario: -1, slope: 32.988829099812634, fluctuations: 0.06\n",
      "step: 90920 loss: 2046.700033 time elapsed: 115.6470 learning rate: 0.000018, scenario: 0, slope: -8.65226271799503, fluctuations: 0.06\n",
      "step: 90930 loss: 2026.369978 time elapsed: 115.6606 learning rate: 0.000018, scenario: 0, slope: -54.10860671374062, fluctuations: 0.06\n",
      "step: 90940 loss: 2007.492028 time elapsed: 115.6748 learning rate: 0.000018, scenario: 0, slope: -105.74586660483234, fluctuations: 0.06\n",
      "step: 90950 loss: 1989.779964 time elapsed: 115.6884 learning rate: 0.000018, scenario: 0, slope: -166.84679089998832, fluctuations: 0.06\n",
      "step: 90960 loss: 1972.993759 time elapsed: 115.7017 learning rate: 0.000018, scenario: 0, slope: -84.41737276813166, fluctuations: 0.03\n",
      "step: 90970 loss: 1956.953157 time elapsed: 115.7147 learning rate: 0.000018, scenario: 0, slope: -19.22944789382464, fluctuations: 0.01\n",
      "step: 90980 loss: 1941.527603 time elapsed: 115.7277 learning rate: 0.000018, scenario: 0, slope: -6.535143037569158, fluctuations: 0.0\n",
      "step: 90990 loss: 1926.623131 time elapsed: 115.7414 learning rate: 0.000018, scenario: 0, slope: -2.8716950860319206, fluctuations: 0.0\n",
      "step: 91000 loss: 1911.729722 time elapsed: 115.7539 learning rate: 0.000024, scenario: 1, slope: -2.0566262547378065, fluctuations: 0.0\n",
      "step: 91010 loss: 1884.734070 time elapsed: 115.7661 learning rate: 0.000063, scenario: 1, slope: -1.7210124098426254, fluctuations: 0.0\n",
      "step: 91020 loss: 1820.072580 time elapsed: 115.7782 learning rate: 0.000164, scenario: 1, slope: -1.8276330411331583, fluctuations: 0.0\n",
      "step: 91030 loss: 1693.440072 time elapsed: 115.7897 learning rate: 0.000240, scenario: 0, slope: -2.3917308278323093, fluctuations: 0.0\n",
      "step: 91040 loss: 1582.021632 time elapsed: 115.8015 learning rate: 0.000240, scenario: 0, slope: -3.4137491530562785, fluctuations: 0.0\n",
      "step: 91050 loss: 1497.569072 time elapsed: 115.8135 learning rate: 0.000240, scenario: 0, slope: -4.60709895239009, fluctuations: 0.0\n",
      "step: 91060 loss: 1429.639422 time elapsed: 115.8254 learning rate: 0.000240, scenario: 0, slope: -5.75786113241582, fluctuations: 0.0\n",
      "step: 91070 loss: 1367.219364 time elapsed: 115.8380 learning rate: 0.000240, scenario: 0, slope: -6.736142248019936, fluctuations: 0.0\n",
      "step: 91080 loss: 1300.347554 time elapsed: 115.8520 learning rate: 0.000240, scenario: 0, slope: -7.4837285460973195, fluctuations: 0.0\n",
      "step: 91090 loss: 1245.058127 time elapsed: 115.8675 learning rate: 0.000240, scenario: 0, slope: -7.934364527870801, fluctuations: 0.0\n",
      "step: 91100 loss: 1207.247643 time elapsed: 115.8825 learning rate: 0.000240, scenario: 0, slope: -7.933233998758988, fluctuations: 0.0\n",
      "step: 91110 loss: 1172.385328 time elapsed: 115.8970 learning rate: 0.000240, scenario: 0, slope: -7.379490132044315, fluctuations: 0.0\n",
      "step: 91120 loss: 1139.897236 time elapsed: 115.9107 learning rate: 0.000240, scenario: 0, slope: -6.470967061243884, fluctuations: 0.0\n",
      "step: 91130 loss: 1109.613819 time elapsed: 115.9239 learning rate: 0.000240, scenario: 0, slope: -5.546446268169649, fluctuations: 0.0\n",
      "step: 91140 loss: 1081.000025 time elapsed: 115.9382 learning rate: 0.000240, scenario: 0, slope: -4.844098280217837, fluctuations: 0.0\n",
      "step: 91150 loss: 1053.918513 time elapsed: 115.9513 learning rate: 0.000240, scenario: 0, slope: -4.283341675715435, fluctuations: 0.0\n",
      "step: 91160 loss: 1028.189799 time elapsed: 115.9645 learning rate: 0.000240, scenario: 0, slope: -3.7964024261955878, fluctuations: 0.0\n",
      "step: 91170 loss: 1003.721426 time elapsed: 115.9766 learning rate: 0.000240, scenario: 0, slope: -3.3635583725285056, fluctuations: 0.0\n",
      "step: 91180 loss: 980.415490 time elapsed: 115.9886 learning rate: 0.000240, scenario: 0, slope: -3.026919133325276, fluctuations: 0.0\n",
      "step: 91190 loss: 958.193859 time elapsed: 116.0003 learning rate: 0.000240, scenario: 0, slope: -2.8281269155890962, fluctuations: 0.0\n",
      "step: 91200 loss: 936.981878 time elapsed: 116.0114 learning rate: 0.000240, scenario: 0, slope: -2.6811041903251573, fluctuations: 0.0\n",
      "step: 91210 loss: 916.709864 time elapsed: 116.0238 learning rate: 0.000240, scenario: 0, slope: -2.529127692667083, fluctuations: 0.0\n",
      "step: 91220 loss: 897.307418 time elapsed: 116.0356 learning rate: 0.000240, scenario: 0, slope: -2.4038380058058806, fluctuations: 0.0\n",
      "step: 91230 loss: 878.691968 time elapsed: 116.0489 learning rate: 0.000240, scenario: 0, slope: -2.2895598607894754, fluctuations: 0.0\n",
      "step: 91240 loss: 860.737877 time elapsed: 116.0625 learning rate: 0.000240, scenario: 0, slope: -2.1841566723407477, fluctuations: 0.0\n",
      "step: 91250 loss: 843.184063 time elapsed: 116.0765 learning rate: 0.000240, scenario: 0, slope: -2.0877123854906823, fluctuations: 0.0\n",
      "step: 91260 loss: 825.376893 time elapsed: 116.0897 learning rate: 0.000240, scenario: 0, slope: -2.002384846774028, fluctuations: 0.0\n",
      "step: 91270 loss: 805.966941 time elapsed: 116.1034 learning rate: 0.000240, scenario: 0, slope: -1.9353137532152975, fluctuations: 0.0\n",
      "step: 91280 loss: 785.561461 time elapsed: 116.1170 learning rate: 0.000240, scenario: 0, slope: -1.8958303500196745, fluctuations: 0.0\n",
      "step: 91290 loss: 768.771878 time elapsed: 116.1301 learning rate: 0.000240, scenario: 0, slope: -1.868375953425371, fluctuations: 0.0\n",
      "step: 91300 loss: 752.976432 time elapsed: 116.1433 learning rate: 0.000240, scenario: 0, slope: -1.845448485555882, fluctuations: 0.0\n",
      "step: 91310 loss: 738.250299 time elapsed: 116.1577 learning rate: 0.000240, scenario: 0, slope: -1.8120427580637093, fluctuations: 0.0\n",
      "step: 91320 loss: 724.632667 time elapsed: 116.1721 learning rate: 0.000240, scenario: 0, slope: -1.7706889626136324, fluctuations: 0.0\n",
      "step: 91330 loss: 712.005516 time elapsed: 116.1847 learning rate: 0.000240, scenario: 0, slope: -1.714594599245724, fluctuations: 0.0\n",
      "step: 91340 loss: 700.292744 time elapsed: 116.1972 learning rate: 0.000240, scenario: 0, slope: -1.6409687154224533, fluctuations: 0.0\n",
      "step: 91350 loss: 689.414179 time elapsed: 116.2086 learning rate: 0.000240, scenario: 0, slope: -1.548921204262787, fluctuations: 0.0\n",
      "step: 91360 loss: 679.285302 time elapsed: 116.2213 learning rate: 0.000240, scenario: 0, slope: -1.4412992572874104, fluctuations: 0.0\n",
      "step: 91370 loss: 669.817821 time elapsed: 116.2334 learning rate: 0.000240, scenario: 0, slope: -1.3290347089249412, fluctuations: 0.0\n",
      "step: 91380 loss: 660.926045 time elapsed: 116.2451 learning rate: 0.000240, scenario: 0, slope: -1.2311860522372409, fluctuations: 0.0\n",
      "step: 91390 loss: 652.530158 time elapsed: 116.2578 learning rate: 0.000240, scenario: 0, slope: -1.1443203714175751, fluctuations: 0.0\n",
      "step: 91400 loss: 644.560268 time elapsed: 116.2718 learning rate: 0.000240, scenario: 0, slope: -1.0735104054799423, fluctuations: 0.0\n",
      "step: 91410 loss: 636.958318 time elapsed: 116.2864 learning rate: 0.000240, scenario: 0, slope: -0.9959743469522363, fluctuations: 0.0\n",
      "step: 91420 loss: 629.677555 time elapsed: 116.3003 learning rate: 0.000240, scenario: 0, slope: -0.9336908599271286, fluctuations: 0.0\n",
      "step: 91430 loss: 622.681145 time elapsed: 116.3140 learning rate: 0.000240, scenario: 0, slope: -0.8788293824219429, fluctuations: 0.0\n",
      "step: 91440 loss: 615.940204 time elapsed: 116.3274 learning rate: 0.000240, scenario: 0, slope: -0.8308344498960836, fluctuations: 0.0\n",
      "step: 91450 loss: 609.431877 time elapsed: 116.3410 learning rate: 0.000240, scenario: 0, slope: -0.7889775360277974, fluctuations: 0.0\n",
      "step: 91460 loss: 603.137746 time elapsed: 116.3545 learning rate: 0.000240, scenario: 0, slope: -0.7524237137047195, fluctuations: 0.0\n",
      "step: 91470 loss: 597.042606 time elapsed: 116.3684 learning rate: 0.000240, scenario: 0, slope: -0.7203112576525859, fluctuations: 0.0\n",
      "step: 91480 loss: 591.133581 time elapsed: 116.3815 learning rate: 0.000240, scenario: 0, slope: -0.6918339724651782, fluctuations: 0.0\n",
      "step: 91490 loss: 585.399502 time elapsed: 116.3947 learning rate: 0.000240, scenario: 0, slope: -0.6662908182232928, fluctuations: 0.0\n",
      "step: 91500 loss: 579.830480 time elapsed: 116.4088 learning rate: 0.000240, scenario: 0, slope: -0.645334724145646, fluctuations: 0.0\n",
      "step: 91510 loss: 571.978643 time elapsed: 116.4212 learning rate: 0.000621, scenario: 1, slope: -0.626262742335965, fluctuations: 0.0\n",
      "step: 91520 loss: 554.270773 time elapsed: 116.4329 learning rate: 0.001001, scenario: 0, slope: -0.6525263466293884, fluctuations: 0.0\n",
      "step: 91530 loss: 535.044994 time elapsed: 116.4452 learning rate: 0.001001, scenario: 0, slope: -0.7476399829704132, fluctuations: 0.0\n",
      "step: 91540 loss: 517.821200 time elapsed: 116.4579 learning rate: 0.001001, scenario: 0, slope: -0.8868287731430382, fluctuations: 0.0\n",
      "step: 91550 loss: 502.287506 time elapsed: 116.4711 learning rate: 0.001001, scenario: 0, slope: -1.0451136697340457, fluctuations: 0.0\n",
      "step: 91560 loss: 488.138339 time elapsed: 116.4853 learning rate: 0.001001, scenario: 0, slope: -1.2012679354018212, fluctuations: 0.0\n",
      "step: 91570 loss: 475.118898 time elapsed: 116.4991 learning rate: 0.001001, scenario: 0, slope: -1.337228115806021, fluctuations: 0.0\n",
      "step: 91580 loss: 463.024662 time elapsed: 116.5122 learning rate: 0.001001, scenario: 0, slope: -1.4374840206468718, fluctuations: 0.0\n",
      "step: 91590 loss: 451.701852 time elapsed: 116.5257 learning rate: 0.001001, scenario: 0, slope: -1.4885411382601865, fluctuations: 0.0\n",
      "step: 91600 loss: 441.053985 time elapsed: 116.5391 learning rate: 0.001001, scenario: 0, slope: -1.4825011959144008, fluctuations: 0.0\n",
      "step: 91610 loss: 431.025917 time elapsed: 116.5530 learning rate: 0.001001, scenario: 0, slope: -1.4010432492149094, fluctuations: 0.0\n",
      "step: 91620 loss: 421.536408 time elapsed: 116.5665 learning rate: 0.001001, scenario: 0, slope: -1.2921558150752193, fluctuations: 0.0\n",
      "step: 91630 loss: 412.470143 time elapsed: 116.5802 learning rate: 0.001001, scenario: 0, slope: -1.1973966380723766, fluctuations: 0.0\n",
      "step: 91640 loss: 403.753721 time elapsed: 116.5934 learning rate: 0.001001, scenario: 0, slope: -1.1177690540637335, fluctuations: 0.0\n",
      "step: 91650 loss: 395.340642 time elapsed: 116.6056 learning rate: 0.001001, scenario: 0, slope: -1.0504678931395544, fluctuations: 0.0\n",
      "step: 91660 loss: 387.192313 time elapsed: 116.6177 learning rate: 0.001001, scenario: 0, slope: -0.9931924453812768, fluctuations: 0.0\n",
      "step: 91670 loss: 379.280534 time elapsed: 116.6295 learning rate: 0.001001, scenario: 0, slope: -0.9441747475673704, fluctuations: 0.0\n",
      "step: 91680 loss: 371.583159 time elapsed: 116.6413 learning rate: 0.001001, scenario: 0, slope: -0.9020914065677247, fluctuations: 0.0\n",
      "step: 91690 loss: 364.082426 time elapsed: 116.6530 learning rate: 0.001001, scenario: 0, slope: -0.8659512661116635, fluctuations: 0.0\n",
      "step: 91700 loss: 356.763902 time elapsed: 116.6643 learning rate: 0.001001, scenario: 0, slope: -0.8378059504356117, fluctuations: 0.0\n",
      "step: 91710 loss: 349.615753 time elapsed: 116.6773 learning rate: 0.001001, scenario: 0, slope: -0.808010435714414, fluctuations: 0.0\n",
      "step: 91720 loss: 342.628269 time elapsed: 116.6910 learning rate: 0.001001, scenario: 0, slope: -0.7842359617378643, fluctuations: 0.0\n",
      "step: 91730 loss: 335.793510 time elapsed: 116.7043 learning rate: 0.001001, scenario: 0, slope: -0.7627934065745897, fluctuations: 0.0\n",
      "step: 91740 loss: 329.105064 time elapsed: 116.7179 learning rate: 0.001001, scenario: 0, slope: -0.743186716741127, fluctuations: 0.0\n",
      "step: 91750 loss: 322.557878 time elapsed: 116.7315 learning rate: 0.001001, scenario: 0, slope: -0.7250400679522399, fluctuations: 0.0\n",
      "step: 91760 loss: 316.148140 time elapsed: 116.7446 learning rate: 0.001001, scenario: 0, slope: -0.7080658305976515, fluctuations: 0.0\n",
      "step: 91770 loss: 309.873183 time elapsed: 116.7576 learning rate: 0.001001, scenario: 0, slope: -0.6920392601182951, fluctuations: 0.0\n",
      "step: 91780 loss: 303.731406 time elapsed: 116.7709 learning rate: 0.001001, scenario: 0, slope: -0.676774409622334, fluctuations: 0.0\n",
      "step: 91790 loss: 297.722180 time elapsed: 116.7847 learning rate: 0.001001, scenario: 0, slope: -0.662112940430269, fluctuations: 0.0\n",
      "step: 91800 loss: 291.845748 time elapsed: 116.7988 learning rate: 0.001001, scenario: 0, slope: -0.6493186122376879, fluctuations: 0.0\n",
      "step: 91810 loss: 286.103070 time elapsed: 116.8118 learning rate: 0.001001, scenario: 0, slope: -0.6340608594776291, fluctuations: 0.0\n",
      "step: 91820 loss: 280.495609 time elapsed: 116.8238 learning rate: 0.001001, scenario: 0, slope: -0.620436225138479, fluctuations: 0.0\n",
      "step: 91830 loss: 275.024975 time elapsed: 116.8355 learning rate: 0.001001, scenario: 0, slope: -0.6069444489584891, fluctuations: 0.0\n",
      "step: 91840 loss: 269.692255 time elapsed: 116.8475 learning rate: 0.001001, scenario: 0, slope: -0.5935029859011857, fluctuations: 0.0\n",
      "step: 91850 loss: 264.496486 time elapsed: 116.8594 learning rate: 0.001001, scenario: 0, slope: -0.5800522233591328, fluctuations: 0.0\n",
      "step: 91860 loss: 259.430054 time elapsed: 116.8711 learning rate: 0.001001, scenario: 0, slope: -0.5665766989419412, fluctuations: 0.0\n",
      "step: 91870 loss: 254.461114 time elapsed: 116.8837 learning rate: 0.001001, scenario: 0, slope: -0.553176925434388, fluctuations: 0.0\n",
      "step: 91880 loss: 249.466868 time elapsed: 116.8974 learning rate: 0.001001, scenario: 0, slope: -0.5403460622842451, fluctuations: 0.0\n",
      "step: 91890 loss: 244.254877 time elapsed: 116.9110 learning rate: 0.001001, scenario: 0, slope: -0.5295381648507673, fluctuations: 0.0\n",
      "step: 91900 loss: 238.895162 time elapsed: 116.9245 learning rate: 0.001001, scenario: 0, slope: -0.5220072731984688, fluctuations: 0.0\n",
      "step: 91910 loss: 232.203906 time elapsed: 116.9381 learning rate: 0.001001, scenario: 0, slope: -0.522406609252174, fluctuations: 0.0\n",
      "step: 91920 loss: 226.930072 time elapsed: 116.9510 learning rate: 0.001001, scenario: 0, slope: -0.5272331170845467, fluctuations: 0.0\n",
      "step: 91930 loss: 222.117741 time elapsed: 116.9644 learning rate: 0.001001, scenario: 0, slope: -0.5316178230330544, fluctuations: 0.0\n",
      "step: 91940 loss: 217.688477 time elapsed: 116.9777 learning rate: 0.001001, scenario: 0, slope: -0.5324319615763268, fluctuations: 0.0\n",
      "step: 91950 loss: 213.519040 time elapsed: 116.9915 learning rate: 0.001001, scenario: 0, slope: -0.5281382202479202, fluctuations: 0.0\n",
      "step: 91960 loss: 209.567974 time elapsed: 117.0053 learning rate: 0.001001, scenario: 0, slope: -0.5177873346936821, fluctuations: 0.0\n",
      "step: 91970 loss: 205.817398 time elapsed: 117.0177 learning rate: 0.001001, scenario: 0, slope: -0.5007625291836572, fluctuations: 0.0\n",
      "step: 91980 loss: 202.232461 time elapsed: 117.0295 learning rate: 0.001001, scenario: 0, slope: -0.4771974409125965, fluctuations: 0.0\n",
      "step: 91990 loss: 198.796522 time elapsed: 117.0417 learning rate: 0.001001, scenario: 0, slope: -0.44881745062013517, fluctuations: 0.0\n",
      "step: 92000 loss: 195.500480 time elapsed: 117.0538 learning rate: 0.001001, scenario: 0, slope: -0.42044768293574214, fluctuations: 0.0\n",
      "step: 92010 loss: 192.336704 time elapsed: 117.0666 learning rate: 0.001001, scenario: 0, slope: -0.3917280814898622, fluctuations: 0.0\n",
      "step: 92020 loss: 189.297760 time elapsed: 117.0783 learning rate: 0.001001, scenario: 0, slope: -0.3711300379265007, fluctuations: 0.0\n",
      "step: 92030 loss: 186.379211 time elapsed: 117.0911 learning rate: 0.001001, scenario: 0, slope: -0.3538516252182737, fluctuations: 0.0\n",
      "step: 92040 loss: 183.566025 time elapsed: 117.1049 learning rate: 0.001001, scenario: 0, slope: -0.3383887202762929, fluctuations: 0.0\n",
      "step: 92050 loss: 180.858058 time elapsed: 117.1188 learning rate: 0.001001, scenario: 0, slope: -0.324263359646117, fluctuations: 0.0\n",
      "step: 92060 loss: 178.244247 time elapsed: 117.1325 learning rate: 0.001001, scenario: 0, slope: -0.311247875879583, fluctuations: 0.0\n",
      "step: 92070 loss: 175.441355 time elapsed: 117.1457 learning rate: 0.001001, scenario: 0, slope: -0.29954095046515167, fluctuations: 0.0\n",
      "step: 92080 loss: 171.950939 time elapsed: 117.1593 learning rate: 0.001001, scenario: 0, slope: -0.2939252215673902, fluctuations: 0.0\n",
      "step: 92090 loss: 169.056063 time elapsed: 117.1728 learning rate: 0.001001, scenario: 0, slope: -0.2910378078973406, fluctuations: 0.0\n",
      "step: 92100 loss: 166.348239 time elapsed: 117.1864 learning rate: 0.001001, scenario: 0, slope: -0.28914978687280973, fluctuations: 0.0\n",
      "step: 92110 loss: 163.584739 time elapsed: 117.2007 learning rate: 0.001001, scenario: 0, slope: -0.28771295509508593, fluctuations: 0.0\n",
      "step: 92120 loss: 160.670156 time elapsed: 117.2142 learning rate: 0.001001, scenario: 0, slope: -0.2875032713409128, fluctuations: 0.0\n",
      "step: 92130 loss: 157.567074 time elapsed: 117.2268 learning rate: 0.001001, scenario: 0, slope: -0.2888565115705145, fluctuations: 0.0\n",
      "step: 92140 loss: 154.496384 time elapsed: 117.2386 learning rate: 0.001001, scenario: 0, slope: -0.2915039982470314, fluctuations: 0.0\n",
      "step: 92150 loss: 152.003923 time elapsed: 117.2508 learning rate: 0.001001, scenario: 0, slope: -0.2926031320992911, fluctuations: 0.0\n",
      "step: 92160 loss: 150.118008 time elapsed: 117.2624 learning rate: 0.001001, scenario: 0, slope: -0.28768554716860834, fluctuations: 0.0\n",
      "step: 92170 loss: 148.297832 time elapsed: 117.2740 learning rate: 0.001001, scenario: 0, slope: -0.2761107857105125, fluctuations: 0.0\n",
      "step: 92180 loss: 146.600598 time elapsed: 117.2861 learning rate: 0.001001, scenario: 0, slope: -0.26345244819234426, fluctuations: 0.0\n",
      "step: 92190 loss: 145.003792 time elapsed: 117.2988 learning rate: 0.001001, scenario: 0, slope: -0.24852109593224336, fluctuations: 0.0\n",
      "step: 92200 loss: 143.450034 time elapsed: 117.3119 learning rate: 0.001001, scenario: 0, slope: -0.23249381488792253, fluctuations: 0.0\n",
      "step: 92210 loss: 141.954939 time elapsed: 117.3257 learning rate: 0.001001, scenario: 0, slope: -0.21082718731516695, fluctuations: 0.0\n",
      "step: 92220 loss: 140.502697 time elapsed: 117.3395 learning rate: 0.001001, scenario: 0, slope: -0.19104755816932065, fluctuations: 0.0\n",
      "step: 92230 loss: 139.090142 time elapsed: 117.3530 learning rate: 0.001001, scenario: 0, slope: -0.17388074766795286, fluctuations: 0.0\n",
      "step: 92240 loss: 137.770812 time elapsed: 117.3664 learning rate: 0.001001, scenario: 0, slope: -0.1616015973105077, fluctuations: 0.0\n",
      "step: 92250 loss: 136.385845 time elapsed: 117.3792 learning rate: 0.001001, scenario: 0, slope: -0.15421080978192783, fluctuations: 0.0\n",
      "step: 92260 loss: 135.024256 time elapsed: 117.3922 learning rate: 0.001465, scenario: 1, slope: -0.14865024441067165, fluctuations: 0.0\n",
      "step: 92270 loss: 141.857383 time elapsed: 117.4058 learning rate: 0.003800, scenario: 1, slope: -0.12586279456106625, fluctuations: 0.02\n",
      "step: 92280 loss: 1542.719389 time elapsed: 117.4196 learning rate: 0.006998, scenario: -1, slope: 1.0996263440058365, fluctuations: 0.04\n",
      "step: 92290 loss: 295.402092 time elapsed: 117.4319 learning rate: 0.002440, scenario: -1, slope: 1.2801481900704625, fluctuations: 0.08\n",
      "step: 92300 loss: 160.268527 time elapsed: 117.4435 learning rate: 0.000945, scenario: -1, slope: 1.2746096482512665, fluctuations: 0.11\n",
      "step: 92310 loss: 150.552509 time elapsed: 117.4558 learning rate: 0.000330, scenario: -1, slope: 0.9380069526927108, fluctuations: 0.12\n",
      "step: 92320 loss: 142.789495 time elapsed: 117.4676 learning rate: 0.000115, scenario: -1, slope: 0.5933962480855121, fluctuations: 0.13\n",
      "step: 92330 loss: 141.922130 time elapsed: 117.4794 learning rate: 0.000040, scenario: -1, slope: 0.2553962297124614, fluctuations: 0.14\n",
      "step: 92340 loss: 141.557607 time elapsed: 117.4912 learning rate: 0.000024, scenario: 1, slope: -0.10682333189173554, fluctuations: 0.14\n",
      "step: 92350 loss: 141.386993 time elapsed: 117.5035 learning rate: 0.000024, scenario: 0, slope: -0.4992416099811227, fluctuations: 0.14\n",
      "step: 92360 loss: 141.244934 time elapsed: 117.5175 learning rate: 0.000024, scenario: 0, slope: -0.9805782917236381, fluctuations: 0.14\n",
      "step: 92370 loss: 141.108775 time elapsed: 117.5312 learning rate: 0.000024, scenario: 0, slope: -1.3389229528875792, fluctuations: 0.11\n",
      "step: 92380 loss: 140.980011 time elapsed: 117.5450 learning rate: 0.000024, scenario: 0, slope: -1.3973376306379721, fluctuations: 0.09\n",
      "step: 92390 loss: 140.856934 time elapsed: 117.5587 learning rate: 0.000024, scenario: 0, slope: -0.3328437820401565, fluctuations: 0.05\n",
      "step: 92400 loss: 140.715140 time elapsed: 117.5720 learning rate: 0.000043, scenario: 1, slope: -0.0992894206316492, fluctuations: 0.03\n",
      "step: 92410 loss: 140.413308 time elapsed: 117.5862 learning rate: 0.000112, scenario: 1, slope: -0.03261472607759336, fluctuations: 0.01\n",
      "step: 92420 loss: 139.678241 time elapsed: 117.5995 learning rate: 0.000292, scenario: 1, slope: -0.02025983350920811, fluctuations: 0.0\n",
      "step: 92430 loss: 137.997382 time elapsed: 117.6127 learning rate: 0.000757, scenario: 1, slope: -0.02443729216869805, fluctuations: 0.0\n",
      "step: 92440 loss: 134.607192 time elapsed: 117.6266 learning rate: 0.001963, scenario: 1, slope: -0.04305482577403982, fluctuations: 0.0\n",
      "step: 92450 loss: 128.896249 time elapsed: 117.6388 learning rate: 0.005091, scenario: 1, slope: -0.08192783989361666, fluctuations: 0.0\n",
      "step: 92460 loss: 119.540360 time elapsed: 117.6510 learning rate: 0.010913, scenario: 0, slope: -0.15067232374553066, fluctuations: 0.0\n",
      "step: 92470 loss: 281.255369 time elapsed: 117.6649 learning rate: 0.006766, scenario: -1, slope: 0.8508148675529839, fluctuations: 0.02\n",
      "step: 92480 loss: 151.256631 time elapsed: 117.6774 learning rate: 0.002359, scenario: -1, slope: 1.0469812793752735, fluctuations: 0.06\n",
      "step: 92490 loss: 127.548033 time elapsed: 117.6894 learning rate: 0.000823, scenario: -1, slope: 0.8000388282258182, fluctuations: 0.08\n",
      "step: 92500 loss: 118.950792 time elapsed: 117.7013 learning rate: 0.000319, scenario: -1, slope: 0.4773093712791431, fluctuations: 0.09\n",
      "step: 92510 loss: 117.967628 time elapsed: 117.7147 learning rate: 0.000111, scenario: -1, slope: 0.09795311362165224, fluctuations: 0.1\n",
      "step: 92520 loss: 117.187000 time elapsed: 117.7286 learning rate: 0.000139, scenario: 0, slope: -0.2224241750539802, fluctuations: 0.1\n",
      "step: 92530 loss: 116.630816 time elapsed: 117.7428 learning rate: 0.000139, scenario: 0, slope: -0.5097697461365013, fluctuations: 0.1\n",
      "step: 92540 loss: 116.118688 time elapsed: 117.7564 learning rate: 0.000139, scenario: 0, slope: -0.7988298607535437, fluctuations: 0.1\n",
      "step: 92550 loss: 115.680215 time elapsed: 117.7700 learning rate: 0.000139, scenario: 0, slope: -1.1332797450856138, fluctuations: 0.1\n",
      "step: 92560 loss: 115.253509 time elapsed: 117.7837 learning rate: 0.000139, scenario: 0, slope: -1.5809158608850815, fluctuations: 0.1\n",
      "step: 92570 loss: 114.849039 time elapsed: 117.7975 learning rate: 0.000139, scenario: 0, slope: -0.6937677064761024, fluctuations: 0.07\n",
      "step: 92580 loss: 114.457389 time elapsed: 117.8113 learning rate: 0.000139, scenario: 0, slope: -0.16536149190143692, fluctuations: 0.04\n",
      "step: 92590 loss: 113.906114 time elapsed: 117.8249 learning rate: 0.000361, scenario: 1, slope: -0.07289220388079377, fluctuations: 0.02\n",
      "step: 92600 loss: 112.562492 time elapsed: 117.8383 learning rate: 0.000851, scenario: 1, slope: -0.05226196575621355, fluctuations: 0.01\n",
      "step: 92610 loss: 109.722716 time elapsed: 117.8518 learning rate: 0.002207, scenario: 1, slope: -0.05890088261649358, fluctuations: 0.0\n",
      "step: 92620 loss: 103.907141 time elapsed: 117.8638 learning rate: 0.005723, scenario: 1, slope: -0.08762738794283684, fluctuations: 0.0\n",
      "step: 92630 loss: 96.126703 time elapsed: 117.8758 learning rate: 0.009217, scenario: 0, slope: -0.14629254630455768, fluctuations: 0.0\n",
      "step: 92640 loss: 1080.871832 time elapsed: 117.8876 learning rate: 0.006985, scenario: -1, slope: 2.215934494063591, fluctuations: 0.02\n",
      "step: 92650 loss: 461.831941 time elapsed: 117.8993 learning rate: 0.002435, scenario: -1, slope: 3.5741421682185206, fluctuations: 0.05\n",
      "step: 92660 loss: 120.296941 time elapsed: 117.9109 learning rate: 0.000849, scenario: -1, slope: 2.9961712509958325, fluctuations: 0.08\n",
      "step: 92670 loss: 105.437295 time elapsed: 117.9234 learning rate: 0.000296, scenario: -1, slope: 2.002611042641486, fluctuations: 0.09\n",
      "step: 92680 loss: 99.949959 time elapsed: 117.9373 learning rate: 0.000103, scenario: -1, slope: 1.0395598258087868, fluctuations: 0.1\n",
      "step: 92690 loss: 99.921794 time elapsed: 117.9508 learning rate: 0.000036, scenario: -1, slope: 0.12489158893034184, fluctuations: 0.11\n",
      "step: 92700 loss: 99.473026 time elapsed: 117.9635 learning rate: 0.000034, scenario: 0, slope: -0.713463055008645, fluctuations: 0.11\n",
      "step: 92710 loss: 99.163361 time elapsed: 117.9769 learning rate: 0.000034, scenario: 0, slope: -1.7366758280540004, fluctuations: 0.11\n",
      "step: 92720 loss: 99.063402 time elapsed: 117.9905 learning rate: 0.000034, scenario: 0, slope: -2.777801801178737, fluctuations: 0.11\n",
      "step: 92730 loss: 98.967885 time elapsed: 118.0041 learning rate: 0.000034, scenario: 0, slope: -4.079170615739819, fluctuations: 0.11\n",
      "step: 92740 loss: 98.864679 time elapsed: 118.0176 learning rate: 0.000034, scenario: 0, slope: -2.0644065357302304, fluctuations: 0.09\n",
      "step: 92750 loss: 98.769767 time elapsed: 118.0485 learning rate: 0.000034, scenario: 0, slope: -0.42134887359396306, fluctuations: 0.05\n",
      "step: 92760 loss: 98.677244 time elapsed: 118.0675 learning rate: 0.000046, scenario: 1, slope: -0.10353277197030816, fluctuations: 0.03\n",
      "step: 92770 loss: 98.502179 time elapsed: 118.0801 learning rate: 0.000118, scenario: 1, slope: -0.038005992272024385, fluctuations: 0.02\n",
      "step: 92780 loss: 98.062142 time elapsed: 118.0926 learning rate: 0.000306, scenario: 1, slope: -0.015227767682709132, fluctuations: 0.01\n",
      "step: 92790 loss: 96.994668 time elapsed: 118.1045 learning rate: 0.000794, scenario: 1, slope: -0.018125605230066327, fluctuations: 0.0\n",
      "step: 92800 loss: 94.583020 time elapsed: 118.1165 learning rate: 0.001873, scenario: 1, slope: -0.026775124418760465, fluctuations: 0.0\n",
      "step: 92810 loss: 90.100998 time elapsed: 118.1296 learning rate: 0.004858, scenario: 1, slope: -0.05592606076344751, fluctuations: 0.0\n",
      "step: 92820 loss: 82.400214 time elapsed: 118.1432 learning rate: 0.010413, scenario: 0, slope: -0.11129503470769309, fluctuations: 0.0\n",
      "step: 92830 loss: 74.770399 time elapsed: 118.1568 learning rate: 0.010413, scenario: 0, slope: -0.1952397762257384, fluctuations: 0.0\n",
      "step: 92840 loss: 67.659181 time elapsed: 118.1704 learning rate: 0.010413, scenario: 0, slope: -0.2817812128564469, fluctuations: 0.01\n",
      "step: 92850 loss: 61.965763 time elapsed: 118.1839 learning rate: 0.010413, scenario: 0, slope: -0.3759223048288778, fluctuations: 0.03\n",
      "step: 92860 loss: 113.305317 time elapsed: 118.1977 learning rate: 0.010413, scenario: 0, slope: -0.3122478920886147, fluctuations: 0.04\n",
      "step: 92870 loss: 99.291508 time elapsed: 118.2110 learning rate: 0.010413, scenario: 0, slope: -0.2749304486937614, fluctuations: 0.06\n",
      "step: 92880 loss: 62.363689 time elapsed: 118.2245 learning rate: 0.010413, scenario: 0, slope: -0.27119054255458325, fluctuations: 0.07\n",
      "step: 92890 loss: 63.632207 time elapsed: 118.2383 learning rate: 0.010413, scenario: 0, slope: -0.23520028023905235, fluctuations: 0.08\n",
      "step: 92900 loss: 51.818010 time elapsed: 118.2532 learning rate: 0.010413, scenario: 0, slope: -0.24872918784987924, fluctuations: 0.09\n",
      "step: 92910 loss: 57.081766 time elapsed: 118.2663 learning rate: 0.010413, scenario: 0, slope: -0.24674564179597483, fluctuations: 0.11\n",
      "step: 92920 loss: 47.496736 time elapsed: 118.2784 learning rate: 0.010413, scenario: 0, slope: -0.27196923477225043, fluctuations: 0.11\n",
      "step: 92930 loss: 79.442884 time elapsed: 118.2905 learning rate: 0.010413, scenario: 0, slope: -0.2914543021609181, fluctuations: 0.11\n",
      "step: 92940 loss: 3217.892067 time elapsed: 118.3026 learning rate: 0.008767, scenario: -1, slope: 3.5366212369867607, fluctuations: 0.1\n",
      "step: 92950 loss: 411.055628 time elapsed: 118.3148 learning rate: 0.003057, scenario: -1, slope: 2.7891827043564192, fluctuations: 0.13\n",
      "step: 92960 loss: 55.863826 time elapsed: 118.3270 learning rate: 0.001066, scenario: -1, slope: 2.3377532796926173, fluctuations: 0.15\n",
      "step: 92970 loss: 64.870744 time elapsed: 118.3404 learning rate: 0.000372, scenario: -1, slope: 1.6701588324705103, fluctuations: 0.15\n",
      "step: 92980 loss: 53.208939 time elapsed: 118.3543 learning rate: 0.000130, scenario: -1, slope: 0.7769800833095347, fluctuations: 0.14\n",
      "step: 92990 loss: 50.851620 time elapsed: 118.3679 learning rate: 0.000045, scenario: -1, slope: 0.05610863394209723, fluctuations: 0.14\n",
      "step: 93000 loss: 50.503600 time elapsed: 118.3812 learning rate: 0.000048, scenario: 0, slope: -0.6473184775555321, fluctuations: 0.13\n",
      "step: 93010 loss: 50.480255 time elapsed: 118.3954 learning rate: 0.000048, scenario: 0, slope: -1.4112899419693998, fluctuations: 0.12\n",
      "step: 93020 loss: 50.359685 time elapsed: 118.4085 learning rate: 0.000048, scenario: 0, slope: -2.303942490386118, fluctuations: 0.12\n",
      "step: 93030 loss: 50.291011 time elapsed: 118.4221 learning rate: 0.000048, scenario: 0, slope: -3.3605955321042056, fluctuations: 0.12\n",
      "step: 93040 loss: 50.233224 time elapsed: 118.4350 learning rate: 0.000048, scenario: 0, slope: -1.8433347571079612, fluctuations: 0.1\n",
      "step: 93050 loss: 50.172334 time elapsed: 118.4483 learning rate: 0.000048, scenario: 0, slope: -0.37159274953145194, fluctuations: 0.06\n",
      "step: 93060 loss: 50.114793 time elapsed: 118.4621 learning rate: 0.000048, scenario: 0, slope: -0.10268043739174945, fluctuations: 0.04\n",
      "step: 93070 loss: 50.038918 time elapsed: 118.4746 learning rate: 0.000112, scenario: 1, slope: -0.02769883518081913, fluctuations: 0.02\n",
      "step: 93080 loss: 49.851098 time elapsed: 118.4862 learning rate: 0.000292, scenario: 1, slope: -0.014497642061000147, fluctuations: 0.01\n",
      "step: 93090 loss: 49.394931 time elapsed: 118.4980 learning rate: 0.000757, scenario: 1, slope: -0.008918274313814468, fluctuations: 0.01\n",
      "step: 93100 loss: 48.381779 time elapsed: 118.5097 learning rate: 0.001784, scenario: 1, slope: -0.012982036802120737, fluctuations: 0.01\n",
      "step: 93110 loss: 46.697317 time elapsed: 118.5216 learning rate: 0.004627, scenario: 1, slope: -0.024388635991979248, fluctuations: 0.0\n",
      "step: 93120 loss: 44.601749 time elapsed: 118.5332 learning rate: 0.012002, scenario: 1, slope: -0.04272563133787579, fluctuations: 0.0\n",
      "step: 93130 loss: 661.339282 time elapsed: 118.5457 learning rate: 0.016773, scenario: -1, slope: 0.3265208619694289, fluctuations: 0.0\n",
      "step: 93140 loss: 6810.036291 time elapsed: 118.5594 learning rate: 0.005849, scenario: -1, slope: 80.82155405230893, fluctuations: 0.03\n",
      "step: 93150 loss: 3494.985200 time elapsed: 118.5725 learning rate: 0.002039, scenario: -1, slope: 90.47926923824261, fluctuations: 0.05\n",
      "step: 93160 loss: 1852.305729 time elapsed: 118.5856 learning rate: 0.000711, scenario: -1, slope: 78.09269582286777, fluctuations: 0.06\n",
      "step: 93170 loss: 1534.782542 time elapsed: 118.5988 learning rate: 0.000248, scenario: -1, slope: 57.51165149872951, fluctuations: 0.06\n",
      "step: 93180 loss: 1459.293937 time elapsed: 118.6123 learning rate: 0.000086, scenario: -1, slope: 36.48910615149352, fluctuations: 0.06\n",
      "step: 93190 loss: 1433.662225 time elapsed: 118.6260 learning rate: 0.000030, scenario: -1, slope: 14.557038973196342, fluctuations: 0.06\n",
      "step: 93200 loss: 1426.273122 time elapsed: 118.6392 learning rate: 0.000016, scenario: 0, slope: -6.576932649015068, fluctuations: 0.06\n",
      "step: 93210 loss: 1421.671878 time elapsed: 118.6531 learning rate: 0.000016, scenario: 0, slope: -35.392042505935066, fluctuations: 0.06\n",
      "step: 93220 loss: 1417.545907 time elapsed: 118.6678 learning rate: 0.000016, scenario: 0, slope: -65.97654081559354, fluctuations: 0.06\n",
      "step: 93230 loss: 1413.779915 time elapsed: 118.6799 learning rate: 0.000016, scenario: 0, slope: -102.42700409048012, fluctuations: 0.06\n",
      "step: 93240 loss: 1410.284170 time elapsed: 118.6924 learning rate: 0.000016, scenario: 0, slope: -22.34067420988425, fluctuations: 0.03\n",
      "step: 93250 loss: 1406.991927 time elapsed: 118.7042 learning rate: 0.000016, scenario: 0, slope: -5.7746264714695155, fluctuations: 0.01\n",
      "step: 93260 loss: 1403.854370 time elapsed: 118.7162 learning rate: 0.000019, scenario: 1, slope: -1.5118038393306223, fluctuations: 0.0\n",
      "step: 93270 loss: 1398.610733 time elapsed: 118.7281 learning rate: 0.000050, scenario: 1, slope: -0.7270746232461164, fluctuations: 0.0\n",
      "step: 93280 loss: 1385.937702 time elapsed: 118.7395 learning rate: 0.000130, scenario: 1, slope: -0.47014721693546424, fluctuations: 0.0\n",
      "step: 93290 loss: 1356.250636 time elapsed: 118.7529 learning rate: 0.000338, scenario: 1, slope: -0.5224761467187585, fluctuations: 0.0\n",
      "step: 93300 loss: 1287.573611 time elapsed: 118.7668 learning rate: 0.000797, scenario: 1, slope: -0.798206071195636, fluctuations: 0.0\n",
      "step: 93310 loss: 1152.512019 time elapsed: 118.7812 learning rate: 0.001554, scenario: 0, slope: -1.642244363631272, fluctuations: 0.0\n",
      "step: 93320 loss: 1012.212165 time elapsed: 118.7943 learning rate: 0.001554, scenario: 0, slope: -3.023862848596296, fluctuations: 0.0\n",
      "step: 93330 loss: 908.619106 time elapsed: 118.8077 learning rate: 0.001554, scenario: 0, slope: -4.652558635559204, fluctuations: 0.0\n",
      "step: 93340 loss: 829.009349 time elapsed: 118.8214 learning rate: 0.001554, scenario: 0, slope: -6.231051975018851, fluctuations: 0.0\n",
      "step: 93350 loss: 765.385449 time elapsed: 118.8347 learning rate: 0.001554, scenario: 0, slope: -7.551941190150348, fluctuations: 0.0\n",
      "step: 93360 loss: 713.301820 time elapsed: 118.8482 learning rate: 0.001554, scenario: 0, slope: -8.46186263863447, fluctuations: 0.0\n",
      "step: 93370 loss: 669.343965 time elapsed: 118.8615 learning rate: 0.001554, scenario: 0, slope: -8.84870870357026, fluctuations: 0.0\n",
      "step: 93380 loss: 627.588839 time elapsed: 118.8746 learning rate: 0.001554, scenario: 0, slope: -8.667138127425043, fluctuations: 0.0\n",
      "step: 93390 loss: 571.674497 time elapsed: 118.8869 learning rate: 0.001554, scenario: 0, slope: -7.981825948309816, fluctuations: 0.0\n",
      "step: 93400 loss: 538.789844 time elapsed: 118.8987 learning rate: 0.001554, scenario: 0, slope: -7.062625085622116, fluctuations: 0.01\n",
      "step: 93410 loss: 512.138011 time elapsed: 118.9114 learning rate: 0.001554, scenario: 0, slope: -5.782913375067262, fluctuations: 0.01\n",
      "step: 93420 loss: 488.975155 time elapsed: 118.9232 learning rate: 0.001554, scenario: 0, slope: -4.913570323460975, fluctuations: 0.01\n",
      "step: 93430 loss: 469.041208 time elapsed: 118.9351 learning rate: 0.001554, scenario: 0, slope: -4.2639594628320046, fluctuations: 0.01\n",
      "step: 93440 loss: 451.255370 time elapsed: 118.9466 learning rate: 0.001554, scenario: 0, slope: -3.7406363939418923, fluctuations: 0.01\n",
      "step: 93450 loss: 434.303207 time elapsed: 118.9592 learning rate: 0.001554, scenario: 0, slope: -3.29130753915137, fluctuations: 0.01\n",
      "step: 93460 loss: 414.183517 time elapsed: 118.9726 learning rate: 0.001554, scenario: 0, slope: -2.8931062341741804, fluctuations: 0.01\n",
      "step: 93470 loss: 334.649734 time elapsed: 118.9869 learning rate: 0.001554, scenario: 0, slope: -2.695525525481245, fluctuations: 0.02\n",
      "step: 93480 loss: 311.848414 time elapsed: 119.0006 learning rate: 0.001554, scenario: 0, slope: -2.647448374359094, fluctuations: 0.04\n",
      "step: 93490 loss: 295.213336 time elapsed: 119.0138 learning rate: 0.001554, scenario: 0, slope: -2.658806771493994, fluctuations: 0.05\n",
      "step: 93500 loss: 281.392341 time elapsed: 119.0273 learning rate: 0.001554, scenario: 0, slope: -2.715253827148816, fluctuations: 0.05\n",
      "step: 93510 loss: 268.641751 time elapsed: 119.0415 learning rate: 0.001554, scenario: 0, slope: -2.728489619180463, fluctuations: 0.05\n",
      "step: 93520 loss: 257.613699 time elapsed: 119.0550 learning rate: 0.001554, scenario: 0, slope: -2.666212292602244, fluctuations: 0.05\n",
      "step: 93530 loss: 247.289393 time elapsed: 119.0685 learning rate: 0.001554, scenario: 0, slope: -2.5215659844184852, fluctuations: 0.05\n",
      "step: 93540 loss: 237.232488 time elapsed: 119.0822 learning rate: 0.001554, scenario: 0, slope: -2.2856514971963366, fluctuations: 0.05\n",
      "step: 93550 loss: 227.566730 time elapsed: 119.0959 learning rate: 0.001554, scenario: 0, slope: -1.9479661560756523, fluctuations: 0.05\n",
      "step: 93560 loss: 218.895926 time elapsed: 119.1079 learning rate: 0.001554, scenario: 0, slope: -1.494940168689421, fluctuations: 0.05\n",
      "step: 93570 loss: 211.127237 time elapsed: 119.1215 learning rate: 0.001554, scenario: 0, slope: -1.1669712185242544, fluctuations: 0.04\n",
      "step: 93580 loss: 203.828749 time elapsed: 119.1335 learning rate: 0.001554, scenario: 0, slope: -1.0506099466048018, fluctuations: 0.01\n",
      "step: 93590 loss: 196.999598 time elapsed: 119.1455 learning rate: 0.001554, scenario: 0, slope: -0.9644894049909366, fluctuations: 0.0\n",
      "step: 93600 loss: 190.594354 time elapsed: 119.1577 learning rate: 0.001554, scenario: 0, slope: -0.9024089170148044, fluctuations: 0.0\n",
      "step: 93610 loss: 184.599191 time elapsed: 119.1721 learning rate: 0.001554, scenario: 0, slope: -0.8345989263738675, fluctuations: 0.0\n",
      "step: 93620 loss: 178.993666 time elapsed: 119.1863 learning rate: 0.001554, scenario: 0, slope: -0.7767918710866047, fluctuations: 0.0\n",
      "step: 93630 loss: 173.755818 time elapsed: 119.1992 learning rate: 0.001554, scenario: 0, slope: -0.721225405432324, fluctuations: 0.0\n",
      "step: 93640 loss: 168.860318 time elapsed: 119.2125 learning rate: 0.001554, scenario: 0, slope: -0.6687276641034041, fluctuations: 0.0\n",
      "step: 93650 loss: 164.281391 time elapsed: 119.2255 learning rate: 0.001554, scenario: 0, slope: -0.6236648004708277, fluctuations: 0.0\n",
      "step: 93660 loss: 159.980696 time elapsed: 119.2387 learning rate: 0.001554, scenario: 0, slope: -0.5832678791764098, fluctuations: 0.0\n",
      "step: 93670 loss: 155.921747 time elapsed: 119.2527 learning rate: 0.001554, scenario: 0, slope: -0.5459724811263068, fluctuations: 0.0\n",
      "step: 93680 loss: 152.076540 time elapsed: 119.2663 learning rate: 0.001554, scenario: 0, slope: -0.5113213579083765, fluctuations: 0.0\n",
      "step: 93690 loss: 148.420646 time elapsed: 119.2798 learning rate: 0.001554, scenario: 0, slope: -0.47938099630503206, fluctuations: 0.0\n",
      "step: 93700 loss: 144.932168 time elapsed: 119.2936 learning rate: 0.001554, scenario: 0, slope: -0.45302747318302683, fluctuations: 0.0\n",
      "step: 93710 loss: 141.590497 time elapsed: 119.3063 learning rate: 0.001554, scenario: 0, slope: -0.4239565174430646, fluctuations: 0.0\n",
      "step: 93720 loss: 138.376049 time elapsed: 119.3187 learning rate: 0.001554, scenario: 0, slope: -0.4004816845575852, fluctuations: 0.0\n",
      "step: 93730 loss: 135.280481 time elapsed: 119.3305 learning rate: 0.001554, scenario: 0, slope: -0.37967949502836307, fluctuations: 0.0\n",
      "step: 93740 loss: 132.316995 time elapsed: 119.3425 learning rate: 0.001554, scenario: 0, slope: -0.3612392770771074, fluctuations: 0.0\n",
      "step: 93750 loss: 129.493999 time elapsed: 119.3543 learning rate: 0.001554, scenario: 0, slope: -0.34466982718684475, fluctuations: 0.0\n",
      "step: 93760 loss: 126.805105 time elapsed: 119.3665 learning rate: 0.001554, scenario: 0, slope: -0.3294665073690878, fluctuations: 0.0\n",
      "step: 93770 loss: 124.237643 time elapsed: 119.3794 learning rate: 0.001554, scenario: 0, slope: -0.315250279467235, fluctuations: 0.0\n",
      "step: 93780 loss: 121.778533 time elapsed: 119.3932 learning rate: 0.001554, scenario: 0, slope: -0.30176570596984376, fluctuations: 0.0\n",
      "step: 93790 loss: 119.417523 time elapsed: 119.4069 learning rate: 0.001554, scenario: 0, slope: -0.28885616719929147, fluctuations: 0.0\n",
      "step: 93800 loss: 117.146595 time elapsed: 119.4198 learning rate: 0.001554, scenario: 0, slope: -0.277666911655178, fluctuations: 0.0\n",
      "step: 93810 loss: 114.959141 time elapsed: 119.4338 learning rate: 0.001554, scenario: 0, slope: -0.2645454425007585, fluctuations: 0.0\n",
      "step: 93820 loss: 112.849426 time elapsed: 119.4474 learning rate: 0.001554, scenario: 0, slope: -0.25323497117438365, fluctuations: 0.0\n",
      "step: 93830 loss: 110.812268 time elapsed: 119.4606 learning rate: 0.001554, scenario: 0, slope: -0.2426654960608994, fluctuations: 0.0\n",
      "step: 93840 loss: 108.842819 time elapsed: 119.4737 learning rate: 0.001554, scenario: 0, slope: -0.23293262402924106, fluctuations: 0.0\n",
      "step: 93850 loss: 106.936341 time elapsed: 119.4866 learning rate: 0.001554, scenario: 0, slope: -0.22400338554468846, fluctuations: 0.0\n",
      "step: 93860 loss: 105.087709 time elapsed: 119.5001 learning rate: 0.001554, scenario: 0, slope: -0.2157854033844038, fluctuations: 0.0\n",
      "step: 93870 loss: 103.289523 time elapsed: 119.5121 learning rate: 0.001554, scenario: 0, slope: -0.20819365940918566, fluctuations: 0.0\n",
      "step: 93880 loss: 101.517833 time elapsed: 119.5236 learning rate: 0.001554, scenario: 0, slope: -0.20121857257035813, fluctuations: 0.0\n",
      "step: 93890 loss: 98.721714 time elapsed: 119.5357 learning rate: 0.001554, scenario: 0, slope: -0.19632801072906572, fluctuations: 0.0\n",
      "step: 93900 loss: 95.877204 time elapsed: 119.5474 learning rate: 0.001554, scenario: 0, slope: -0.1994946504256236, fluctuations: 0.0\n",
      "step: 93910 loss: 93.763558 time elapsed: 119.5597 learning rate: 0.001554, scenario: 0, slope: -0.20617610339189016, fluctuations: 0.0\n",
      "step: 93920 loss: 92.126672 time elapsed: 119.5714 learning rate: 0.001554, scenario: 0, slope: -0.2106316534423878, fluctuations: 0.0\n",
      "step: 93930 loss: 90.672867 time elapsed: 119.5835 learning rate: 0.001554, scenario: 0, slope: -0.21181260576493077, fluctuations: 0.0\n",
      "step: 93940 loss: 89.297938 time elapsed: 119.5970 learning rate: 0.001554, scenario: 0, slope: -0.2093128593557097, fluctuations: 0.0\n",
      "step: 93950 loss: 87.991125 time elapsed: 119.6108 learning rate: 0.001554, scenario: 0, slope: -0.20300371708731038, fluctuations: 0.0\n",
      "step: 93960 loss: 86.733088 time elapsed: 119.6243 learning rate: 0.001554, scenario: 0, slope: -0.1929153513576062, fluctuations: 0.0\n",
      "step: 93970 loss: 85.516345 time elapsed: 119.6374 learning rate: 0.001554, scenario: 0, slope: -0.17916499402180963, fluctuations: 0.0\n",
      "step: 93980 loss: 84.337318 time elapsed: 119.6511 learning rate: 0.001554, scenario: 0, slope: -0.16197299152068573, fluctuations: 0.0\n",
      "step: 93990 loss: 83.192936 time elapsed: 119.6644 learning rate: 0.001554, scenario: 0, slope: -0.14319514812249118, fluctuations: 0.0\n",
      "step: 94000 loss: 82.081130 time elapsed: 119.6774 learning rate: 0.001554, scenario: 0, slope: -0.13282860249124678, fluctuations: 0.0\n",
      "step: 94010 loss: 81.000082 time elapsed: 119.6916 learning rate: 0.001554, scenario: 0, slope: -0.12532392378309726, fluctuations: 0.0\n",
      "step: 94020 loss: 79.948245 time elapsed: 119.7054 learning rate: 0.001554, scenario: 0, slope: -0.12061910725371057, fluctuations: 0.0\n",
      "step: 94030 loss: 78.924236 time elapsed: 119.7180 learning rate: 0.001554, scenario: 0, slope: -0.1166612807102992, fluctuations: 0.0\n",
      "step: 94040 loss: 77.926814 time elapsed: 119.7300 learning rate: 0.001554, scenario: 0, slope: -0.11314078718687472, fluctuations: 0.0\n",
      "step: 94050 loss: 76.954850 time elapsed: 119.7417 learning rate: 0.001554, scenario: 0, slope: -0.10989435807843147, fluctuations: 0.0\n",
      "step: 94060 loss: 76.007301 time elapsed: 119.7540 learning rate: 0.001554, scenario: 0, slope: -0.10684742655658008, fluctuations: 0.0\n",
      "step: 94070 loss: 75.083203 time elapsed: 119.7658 learning rate: 0.001554, scenario: 0, slope: -0.10396299339928973, fluctuations: 0.0\n",
      "step: 94080 loss: 74.181657 time elapsed: 119.7775 learning rate: 0.001554, scenario: 0, slope: -0.10121807694641638, fluctuations: 0.0\n",
      "step: 94090 loss: 73.301825 time elapsed: 119.7903 learning rate: 0.001554, scenario: 0, slope: -0.09859721631150575, fluctuations: 0.0\n",
      "step: 94100 loss: 72.442921 time elapsed: 119.8040 learning rate: 0.001554, scenario: 0, slope: -0.09633480706481531, fluctuations: 0.0\n",
      "step: 94110 loss: 71.604210 time elapsed: 119.8183 learning rate: 0.001554, scenario: 0, slope: -0.09368313924022875, fluctuations: 0.0\n",
      "step: 94120 loss: 70.785001 time elapsed: 119.8318 learning rate: 0.001554, scenario: 0, slope: -0.09137250926220504, fluctuations: 0.0\n",
      "step: 94130 loss: 69.984649 time elapsed: 119.8453 learning rate: 0.001554, scenario: 0, slope: -0.0891500285788398, fluctuations: 0.0\n",
      "step: 94140 loss: 69.202550 time elapsed: 119.8589 learning rate: 0.001554, scenario: 0, slope: -0.08700968864965028, fluctuations: 0.0\n",
      "step: 94150 loss: 68.438141 time elapsed: 119.8723 learning rate: 0.001554, scenario: 0, slope: -0.08494606427624762, fluctuations: 0.0\n",
      "step: 94160 loss: 67.690901 time elapsed: 119.8855 learning rate: 0.001554, scenario: 0, slope: -0.08295416117960076, fluctuations: 0.0\n",
      "step: 94170 loss: 66.960345 time elapsed: 119.8990 learning rate: 0.001554, scenario: 0, slope: -0.08102930693224088, fluctuations: 0.0\n",
      "step: 94180 loss: 66.246031 time elapsed: 119.9132 learning rate: 0.001554, scenario: 0, slope: -0.07916707067642506, fluctuations: 0.0\n",
      "step: 94190 loss: 65.547549 time elapsed: 119.9254 learning rate: 0.001554, scenario: 0, slope: -0.07736320694176006, fluctuations: 0.0\n",
      "step: 94200 loss: 64.864529 time elapsed: 119.9369 learning rate: 0.001554, scenario: 0, slope: -0.07578624970771412, fluctuations: 0.0\n",
      "step: 94210 loss: 64.196630 time elapsed: 119.9493 learning rate: 0.001554, scenario: 0, slope: -0.07391434274546274, fluctuations: 0.0\n",
      "step: 94220 loss: 63.543545 time elapsed: 119.9613 learning rate: 0.001554, scenario: 0, slope: -0.07226154257237502, fluctuations: 0.0\n",
      "step: 94230 loss: 62.904993 time elapsed: 119.9731 learning rate: 0.001554, scenario: 0, slope: -0.07065152890206175, fluctuations: 0.0\n",
      "step: 94240 loss: 62.280717 time elapsed: 119.9846 learning rate: 0.001709, scenario: 1, slope: -0.06908078150544537, fluctuations: 0.0\n",
      "step: 94250 loss: 61.313519 time elapsed: 119.9973 learning rate: 0.004434, scenario: 1, slope: -0.06824792251842825, fluctuations: 0.0\n",
      "step: 94260 loss: 59.688570 time elapsed: 120.0111 learning rate: 0.004434, scenario: 0, slope: -0.07171502873012064, fluctuations: 0.0\n",
      "step: 94270 loss: 59.538377 time elapsed: 120.0250 learning rate: 0.004434, scenario: 0, slope: -0.07206472907416354, fluctuations: 0.02\n",
      "step: 94280 loss: 57.319063 time elapsed: 120.0386 learning rate: 0.004434, scenario: 0, slope: -0.07959646052802952, fluctuations: 0.05\n",
      "step: 94290 loss: 55.614752 time elapsed: 120.0519 learning rate: 0.004434, scenario: 0, slope: -0.09192527153732813, fluctuations: 0.07\n",
      "step: 94300 loss: 54.372887 time elapsed: 120.0649 learning rate: 0.004434, scenario: 0, slope: -0.10439914577187219, fluctuations: 0.07\n",
      "step: 94310 loss: 53.185597 time elapsed: 120.0790 learning rate: 0.004434, scenario: 0, slope: -0.11692926158722132, fluctuations: 0.07\n",
      "step: 94320 loss: 52.135059 time elapsed: 120.0922 learning rate: 0.004434, scenario: 0, slope: -0.12505488756768895, fluctuations: 0.07\n",
      "step: 94330 loss: 51.159097 time elapsed: 120.1060 learning rate: 0.004434, scenario: 0, slope: -0.12945108862044485, fluctuations: 0.07\n",
      "step: 94340 loss: 50.225944 time elapsed: 120.1198 learning rate: 0.004434, scenario: 0, slope: -0.12938847245880916, fluctuations: 0.07\n",
      "step: 94350 loss: 49.306594 time elapsed: 120.1343 learning rate: 0.004434, scenario: 0, slope: -0.1246653592860265, fluctuations: 0.07\n",
      "step: 94360 loss: 48.362185 time elapsed: 120.1466 learning rate: 0.004434, scenario: 0, slope: -0.11937089720126652, fluctuations: 0.07\n",
      "step: 94370 loss: 47.324739 time elapsed: 120.1581 learning rate: 0.004434, scenario: 0, slope: -0.10872107505257404, fluctuations: 0.04\n",
      "step: 94380 loss: 46.120253 time elapsed: 120.1700 learning rate: 0.004434, scenario: 0, slope: -0.10300647271964286, fluctuations: 0.01\n",
      "step: 94390 loss: 45.436977 time elapsed: 120.1819 learning rate: 0.004434, scenario: 0, slope: -0.09818077201709367, fluctuations: 0.02\n",
      "step: 94400 loss: 44.812146 time elapsed: 120.1938 learning rate: 0.004434, scenario: 0, slope: -0.09630022301807382, fluctuations: 0.05\n",
      "step: 94410 loss: 43.269821 time elapsed: 120.2085 learning rate: 0.004434, scenario: 0, slope: -0.09610654155003881, fluctuations: 0.08\n",
      "step: 94420 loss: 43.064111 time elapsed: 120.2220 learning rate: 0.004434, scenario: 0, slope: -0.09581048882538044, fluctuations: 0.1\n",
      "step: 94430 loss: 41.663674 time elapsed: 120.2358 learning rate: 0.004434, scenario: 0, slope: -0.09535843130443769, fluctuations: 0.12\n",
      "step: 94440 loss: 41.159386 time elapsed: 120.2500 learning rate: 0.004434, scenario: 0, slope: -0.09359270129012684, fluctuations: 0.14\n",
      "step: 94450 loss: 40.722157 time elapsed: 120.2635 learning rate: 0.004434, scenario: 0, slope: -0.09025522047436951, fluctuations: 0.16\n",
      "step: 94460 loss: 39.976146 time elapsed: 120.2772 learning rate: 0.004434, scenario: 0, slope: -0.08552539034025043, fluctuations: 0.17\n",
      "step: 94470 loss: 39.163216 time elapsed: 120.2904 learning rate: 0.004434, scenario: 0, slope: -0.08057191217564241, fluctuations: 0.18\n",
      "step: 94480 loss: 38.634995 time elapsed: 120.3033 learning rate: 0.004434, scenario: 0, slope: -0.07615940569077492, fluctuations: 0.2\n",
      "step: 94490 loss: 38.320561 time elapsed: 120.3166 learning rate: 0.004434, scenario: 0, slope: -0.07012244238205312, fluctuations: 0.19\n",
      "step: 94500 loss: 37.654838 time elapsed: 120.3314 learning rate: 0.004434, scenario: 0, slope: -0.06538914818486656, fluctuations: 0.17\n",
      "step: 94510 loss: 37.533006 time elapsed: 120.3450 learning rate: 0.004434, scenario: 0, slope: -0.06149470651027378, fluctuations: 0.14\n",
      "step: 94520 loss: 36.656393 time elapsed: 120.3575 learning rate: 0.004434, scenario: 0, slope: -0.05894707009953659, fluctuations: 0.15\n",
      "step: 94530 loss: 36.038543 time elapsed: 120.3701 learning rate: 0.004434, scenario: 0, slope: -0.05724362026147607, fluctuations: 0.15\n",
      "step: 94540 loss: 36.583039 time elapsed: 120.3818 learning rate: 0.004434, scenario: 0, slope: -0.051179771741107424, fluctuations: 0.13\n",
      "step: 94550 loss: 36.060618 time elapsed: 120.3935 learning rate: 0.004434, scenario: 0, slope: -0.04810405417356647, fluctuations: 0.14\n",
      "step: 94560 loss: 34.854566 time elapsed: 120.4056 learning rate: 0.004434, scenario: 0, slope: -0.04699897809053911, fluctuations: 0.15\n",
      "step: 94570 loss: 34.328115 time elapsed: 120.4189 learning rate: 0.004434, scenario: 0, slope: -0.04728075282958684, fluctuations: 0.16\n",
      "step: 94580 loss: 33.633074 time elapsed: 120.4326 learning rate: 0.004434, scenario: 0, slope: -0.04751747354963579, fluctuations: 0.15\n",
      "step: 94590 loss: 33.903292 time elapsed: 120.4466 learning rate: 0.004434, scenario: 0, slope: -0.04751470333023462, fluctuations: 0.13\n",
      "step: 94600 loss: 34.273770 time elapsed: 120.4596 learning rate: 0.004434, scenario: 0, slope: -0.03915894935676608, fluctuations: 0.13\n",
      "step: 94610 loss: 33.526724 time elapsed: 120.4733 learning rate: 0.004877, scenario: 1, slope: -0.03743522387553668, fluctuations: 0.13\n",
      "step: 94620 loss: 35.886590 time elapsed: 120.4867 learning rate: 0.005902, scenario: 1, slope: -0.034030624723276726, fluctuations: 0.12\n",
      "step: 94630 loss: 297.258757 time elapsed: 120.5004 learning rate: 0.003260, scenario: -1, slope: 0.842967816255163, fluctuations: 0.13\n",
      "step: 94640 loss: 59.894477 time elapsed: 120.5140 learning rate: 0.001137, scenario: -1, slope: 0.8446530988164541, fluctuations: 0.15\n",
      "step: 94650 loss: 44.283948 time elapsed: 120.5274 learning rate: 0.000396, scenario: -1, slope: 0.7205713640465155, fluctuations: 0.15\n",
      "step: 94660 loss: 41.075547 time elapsed: 120.5410 learning rate: 0.000138, scenario: -1, slope: 0.5041980601100701, fluctuations: 0.14\n",
      "step: 94670 loss: 40.832939 time elapsed: 120.5540 learning rate: 0.000048, scenario: -1, slope: 0.29239543408997537, fluctuations: 0.14\n",
      "step: 94680 loss: 40.376229 time elapsed: 120.5662 learning rate: 0.000017, scenario: -1, slope: 0.07016916125798067, fluctuations: 0.13\n",
      "step: 94690 loss: 40.306308 time elapsed: 120.5779 learning rate: 0.000013, scenario: 0, slope: -0.16703129939680425, fluctuations: 0.13\n",
      "step: 94700 loss: 40.272953 time elapsed: 120.5895 learning rate: 0.000013, scenario: 0, slope: -0.37105235066706366, fluctuations: 0.12\n",
      "step: 94710 loss: 40.245509 time elapsed: 120.6018 learning rate: 0.000013, scenario: 0, slope: -0.6192358739426205, fluctuations: 0.1\n",
      "step: 94720 loss: 40.219172 time elapsed: 120.6138 learning rate: 0.000013, scenario: 0, slope: -0.9526315022650604, fluctuations: 0.1\n",
      "step: 94730 loss: 40.193671 time elapsed: 120.6274 learning rate: 0.000013, scenario: 0, slope: -0.28992044024330327, fluctuations: 0.07\n",
      "step: 94740 loss: 40.168913 time elapsed: 120.6413 learning rate: 0.000013, scenario: 0, slope: -0.07411545408826162, fluctuations: 0.04\n",
      "step: 94750 loss: 40.136428 time elapsed: 120.6556 learning rate: 0.000030, scenario: 1, slope: -0.01846704062035929, fluctuations: 0.02\n",
      "step: 94760 loss: 40.056311 time elapsed: 120.6694 learning rate: 0.000079, scenario: 1, slope: -0.007019460086409735, fluctuations: 0.01\n",
      "step: 94770 loss: 39.860663 time elapsed: 120.6839 learning rate: 0.000205, scenario: 1, slope: -0.004621502464818935, fluctuations: 0.0\n",
      "step: 94780 loss: 39.408489 time elapsed: 120.6989 learning rate: 0.000532, scenario: 1, slope: -0.00581389900196499, fluctuations: 0.0\n",
      "step: 94790 loss: 38.507052 time elapsed: 120.7138 learning rate: 0.001380, scenario: 1, slope: -0.010986011128456276, fluctuations: 0.0\n",
      "step: 94800 loss: 37.300327 time elapsed: 120.7284 learning rate: 0.003254, scenario: 1, slope: -0.01959081044325866, fluctuations: 0.0\n",
      "step: 94810 loss: 35.880375 time elapsed: 120.7436 learning rate: 0.008439, scenario: 1, slope: -0.034962597945809616, fluctuations: 0.0\n",
      "step: 94820 loss: 36.626043 time elapsed: 120.7574 learning rate: 0.011232, scenario: 0, slope: -0.05157399622993183, fluctuations: 0.0\n",
      "step: 94830 loss: 8352.702879 time elapsed: 120.7711 learning rate: 0.005077, scenario: -1, slope: 33.866390058500244, fluctuations: 0.02\n",
      "step: 94840 loss: 1189.019623 time elapsed: 120.7841 learning rate: 0.001770, scenario: -1, slope: 37.08385366260098, fluctuations: 0.05\n",
      "step: 94850 loss: 614.531833 time elapsed: 120.7972 learning rate: 0.000617, scenario: -1, slope: 31.32063523385264, fluctuations: 0.06\n",
      "step: 94860 loss: 385.230827 time elapsed: 120.8109 learning rate: 0.000215, scenario: -1, slope: 22.92738221350432, fluctuations: 0.07\n",
      "step: 94870 loss: 350.869559 time elapsed: 120.8236 learning rate: 0.000075, scenario: -1, slope: 13.29091878851676, fluctuations: 0.07\n",
      "step: 94880 loss: 333.509650 time elapsed: 120.8378 learning rate: 0.000026, scenario: -1, slope: 3.8167680358272738, fluctuations: 0.07\n",
      "step: 94890 loss: 328.423224 time elapsed: 120.8519 learning rate: 0.000020, scenario: 0, slope: -5.8874529585804485, fluctuations: 0.07\n",
      "step: 94900 loss: 324.798138 time elapsed: 120.8669 learning rate: 0.000020, scenario: 0, slope: -15.232883983312055, fluctuations: 0.07\n",
      "step: 94910 loss: 321.780507 time elapsed: 120.8822 learning rate: 0.000020, scenario: 0, slope: -28.14025231591198, fluctuations: 0.07\n",
      "step: 94920 loss: 319.110123 time elapsed: 120.8971 learning rate: 0.000020, scenario: 0, slope: -42.21686189947912, fluctuations: 0.07\n",
      "step: 94930 loss: 316.642018 time elapsed: 120.9116 learning rate: 0.000020, scenario: 0, slope: -14.224852251687436, fluctuations: 0.04\n",
      "step: 94940 loss: 314.302918 time elapsed: 120.9255 learning rate: 0.000020, scenario: 0, slope: -3.4087554251773025, fluctuations: 0.02\n",
      "step: 94950 loss: 312.056302 time elapsed: 120.9398 learning rate: 0.000020, scenario: 0, slope: -1.2106147018167572, fluctuations: 0.0\n",
      "step: 94960 loss: 309.882776 time elapsed: 120.9544 learning rate: 0.000020, scenario: 0, slope: -0.4817507895697792, fluctuations: 0.0\n",
      "step: 94970 loss: 307.637070 time elapsed: 120.9675 learning rate: 0.000032, scenario: 1, slope: -0.3078544707734419, fluctuations: 0.0\n",
      "step: 94980 loss: 302.898425 time elapsed: 120.9798 learning rate: 0.000084, scenario: 1, slope: -0.26369454898158523, fluctuations: 0.0\n",
      "step: 94990 loss: 291.545166 time elapsed: 120.9918 learning rate: 0.000218, scenario: 1, slope: -0.2829253126449862, fluctuations: 0.0\n",
      "step: 95000 loss: 268.248629 time elapsed: 121.0035 learning rate: 0.000351, scenario: 0, slope: -0.371578639003342, fluctuations: 0.0\n",
      "step: 95010 loss: 245.681685 time elapsed: 121.0161 learning rate: 0.000351, scenario: 0, slope: -0.5870041279111093, fluctuations: 0.0\n",
      "step: 95020 loss: 227.433632 time elapsed: 121.0280 learning rate: 0.000351, scenario: 0, slope: -0.8324471969273212, fluctuations: 0.0\n",
      "step: 95030 loss: 212.573744 time elapsed: 121.0393 learning rate: 0.000351, scenario: 0, slope: -1.080562790793675, fluctuations: 0.0\n",
      "step: 95040 loss: 200.298109 time elapsed: 121.0526 learning rate: 0.000351, scenario: 0, slope: -1.2979952798599819, fluctuations: 0.0\n",
      "step: 95050 loss: 189.953164 time elapsed: 121.0658 learning rate: 0.000351, scenario: 0, slope: -1.4589912411592465, fluctuations: 0.0\n",
      "step: 95060 loss: 181.081610 time elapsed: 121.0789 learning rate: 0.000351, scenario: 0, slope: -1.5434093621571754, fluctuations: 0.0\n",
      "step: 95070 loss: 173.366624 time elapsed: 121.0923 learning rate: 0.000351, scenario: 0, slope: -1.5352493154608124, fluctuations: 0.0\n",
      "step: 95080 loss: 166.580460 time elapsed: 121.1059 learning rate: 0.000351, scenario: 0, slope: -1.4290903473521888, fluctuations: 0.0\n",
      "step: 95090 loss: 160.556142 time elapsed: 121.1194 learning rate: 0.000351, scenario: 0, slope: -1.245953142610036, fluctuations: 0.0\n",
      "step: 95100 loss: 155.167542 time elapsed: 121.1326 learning rate: 0.000351, scenario: 0, slope: -1.0666819290621357, fluctuations: 0.0\n",
      "step: 95110 loss: 150.316175 time elapsed: 121.1467 learning rate: 0.000351, scenario: 0, slope: -0.8921815074127081, fluctuations: 0.0\n",
      "step: 95120 loss: 145.923163 time elapsed: 121.1603 learning rate: 0.000351, scenario: 0, slope: -0.7699073813156804, fluctuations: 0.0\n",
      "step: 95130 loss: 141.923984 time elapsed: 121.1739 learning rate: 0.000351, scenario: 0, slope: -0.6731747133546304, fluctuations: 0.0\n",
      "step: 95140 loss: 138.264952 time elapsed: 121.1862 learning rate: 0.000351, scenario: 0, slope: -0.5951255072257348, fluctuations: 0.0\n",
      "step: 95150 loss: 134.900830 time elapsed: 121.1982 learning rate: 0.000351, scenario: 0, slope: -0.5309079455332087, fluctuations: 0.0\n",
      "step: 95160 loss: 131.793138 time elapsed: 121.2098 learning rate: 0.000351, scenario: 0, slope: -0.4772288939939549, fluctuations: 0.0\n",
      "step: 95170 loss: 128.908912 time elapsed: 121.2218 learning rate: 0.000351, scenario: 0, slope: -0.4318010685987138, fluctuations: 0.0\n",
      "step: 95180 loss: 126.219773 time elapsed: 121.2338 learning rate: 0.000351, scenario: 0, slope: -0.39299483313536454, fluctuations: 0.0\n",
      "step: 95190 loss: 123.701188 time elapsed: 121.2457 learning rate: 0.000351, scenario: 0, slope: -0.3596162446876371, fluctuations: 0.0\n",
      "step: 95200 loss: 121.331863 time elapsed: 121.2590 learning rate: 0.000351, scenario: 0, slope: -0.3334639003762979, fluctuations: 0.0\n",
      "step: 95210 loss: 119.093229 time elapsed: 121.2732 learning rate: 0.000351, scenario: 0, slope: -0.3057209438958131, fluctuations: 0.0\n",
      "step: 95220 loss: 116.968963 time elapsed: 121.2863 learning rate: 0.000351, scenario: 0, slope: -0.2839326821313931, fluctuations: 0.0\n",
      "step: 95230 loss: 114.944478 time elapsed: 121.2988 learning rate: 0.000351, scenario: 0, slope: -0.26493565806442426, fluctuations: 0.0\n",
      "step: 95240 loss: 113.006235 time elapsed: 121.3118 learning rate: 0.000351, scenario: 0, slope: -0.2483522927270877, fluctuations: 0.0\n",
      "step: 95250 loss: 111.140297 time elapsed: 121.3258 learning rate: 0.000351, scenario: 0, slope: -0.23387628109133798, fluctuations: 0.0\n",
      "step: 95260 loss: 109.327539 time elapsed: 121.3391 learning rate: 0.000351, scenario: 0, slope: -0.22127940176079616, fluctuations: 0.0\n",
      "step: 95270 loss: 107.517021 time elapsed: 121.3527 learning rate: 0.000351, scenario: 0, slope: -0.2104901970114655, fluctuations: 0.0\n",
      "step: 95280 loss: 105.501081 time elapsed: 121.3670 learning rate: 0.000351, scenario: 0, slope: -0.20210106990979035, fluctuations: 0.0\n",
      "step: 95290 loss: 103.691303 time elapsed: 121.3822 learning rate: 0.000351, scenario: 0, slope: -0.19620848329833326, fluctuations: 0.0\n",
      "step: 95300 loss: 102.045224 time elapsed: 121.3953 learning rate: 0.000351, scenario: 0, slope: -0.19167334908649608, fluctuations: 0.0\n",
      "step: 95310 loss: 100.471685 time elapsed: 121.4077 learning rate: 0.000351, scenario: 0, slope: -0.18643048411419838, fluctuations: 0.0\n",
      "step: 95320 loss: 98.963243 time elapsed: 121.4209 learning rate: 0.000351, scenario: 0, slope: -0.18160721679110384, fluctuations: 0.0\n",
      "step: 95330 loss: 97.493683 time elapsed: 121.4341 learning rate: 0.000351, scenario: 0, slope: -0.17653855681185965, fluctuations: 0.0\n",
      "step: 95340 loss: 96.065606 time elapsed: 121.4481 learning rate: 0.000351, scenario: 0, slope: -0.17109145370349835, fluctuations: 0.0\n",
      "step: 95350 loss: 94.680881 time elapsed: 121.4615 learning rate: 0.000351, scenario: 0, slope: -0.16515182638084366, fluctuations: 0.0\n",
      "step: 95360 loss: 93.339763 time elapsed: 121.4756 learning rate: 0.000351, scenario: 0, slope: -0.15865883802792732, fluctuations: 0.0\n",
      "step: 95370 loss: 92.037298 time elapsed: 121.4900 learning rate: 0.000351, scenario: 0, slope: -0.15175144915078, fluctuations: 0.0\n",
      "step: 95380 loss: 90.765997 time elapsed: 121.5050 learning rate: 0.000351, scenario: 0, slope: -0.14544745755160263, fluctuations: 0.0\n",
      "step: 95390 loss: 89.518184 time elapsed: 121.5195 learning rate: 0.000351, scenario: 0, slope: -0.1407501435059741, fluctuations: 0.0\n",
      "step: 95400 loss: 88.287076 time elapsed: 121.5341 learning rate: 0.000351, scenario: 0, slope: -0.1371164323171871, fluctuations: 0.0\n",
      "step: 95410 loss: 87.066427 time elapsed: 121.5488 learning rate: 0.000351, scenario: 0, slope: -0.13318078885685827, fluctuations: 0.0\n",
      "step: 95420 loss: 85.849918 time elapsed: 121.5622 learning rate: 0.000351, scenario: 0, slope: -0.1300610445605721, fluctuations: 0.0\n",
      "step: 95430 loss: 84.630775 time elapsed: 121.5754 learning rate: 0.000351, scenario: 0, slope: -0.1274456428071542, fluctuations: 0.0\n",
      "step: 95440 loss: 83.401841 time elapsed: 121.5889 learning rate: 0.000351, scenario: 0, slope: -0.12542658632631548, fluctuations: 0.0\n",
      "step: 95450 loss: 82.156500 time elapsed: 121.6033 learning rate: 0.000351, scenario: 0, slope: -0.12406626809740602, fluctuations: 0.0\n",
      "step: 95460 loss: 80.891234 time elapsed: 121.6163 learning rate: 0.000351, scenario: 0, slope: -0.12337716905552078, fluctuations: 0.0\n",
      "step: 95470 loss: 79.610908 time elapsed: 121.6294 learning rate: 0.000351, scenario: 0, slope: -0.12331003916567862, fluctuations: 0.0\n",
      "step: 95480 loss: 78.336397 time elapsed: 121.6416 learning rate: 0.000351, scenario: 0, slope: -0.12371517084841215, fluctuations: 0.0\n",
      "step: 95490 loss: 77.108123 time elapsed: 121.6537 learning rate: 0.000351, scenario: 0, slope: -0.12427221361800943, fluctuations: 0.0\n",
      "step: 95500 loss: 75.971274 time elapsed: 121.6660 learning rate: 0.000351, scenario: 0, slope: -0.12447780174897387, fluctuations: 0.0\n",
      "step: 95510 loss: 74.943798 time elapsed: 121.6804 learning rate: 0.000351, scenario: 0, slope: -0.12370164557895122, fluctuations: 0.0\n",
      "step: 95520 loss: 74.008276 time elapsed: 121.6942 learning rate: 0.000351, scenario: 0, slope: -0.12157141267366915, fluctuations: 0.0\n",
      "step: 95530 loss: 73.141164 time elapsed: 121.7079 learning rate: 0.000351, scenario: 0, slope: -0.11793981119902429, fluctuations: 0.0\n",
      "step: 95540 loss: 72.328555 time elapsed: 121.7210 learning rate: 0.000351, scenario: 0, slope: -0.1129112637606232, fluctuations: 0.0\n",
      "step: 95550 loss: 71.560469 time elapsed: 121.7341 learning rate: 0.000351, scenario: 0, slope: -0.10677670637576372, fluctuations: 0.0\n",
      "step: 95560 loss: 70.828599 time elapsed: 121.7470 learning rate: 0.000351, scenario: 0, slope: -0.09998595023578252, fluctuations: 0.0\n",
      "step: 95570 loss: 70.127072 time elapsed: 121.7609 learning rate: 0.000351, scenario: 0, slope: -0.09309383375638976, fluctuations: 0.0\n",
      "step: 95580 loss: 69.451783 time elapsed: 121.7743 learning rate: 0.000351, scenario: 0, slope: -0.08665784620152418, fluctuations: 0.0\n",
      "step: 95590 loss: 68.799635 time elapsed: 121.7876 learning rate: 0.000351, scenario: 0, slope: -0.0810858241373249, fluctuations: 0.0\n",
      "step: 95600 loss: 68.168192 time elapsed: 121.8009 learning rate: 0.000351, scenario: 0, slope: -0.07691608104887072, fluctuations: 0.0\n",
      "step: 95610 loss: 67.405321 time elapsed: 121.8138 learning rate: 0.000753, scenario: 1, slope: -0.07297456824651713, fluctuations: 0.0\n",
      "step: 95620 loss: 65.605956 time elapsed: 121.8259 learning rate: 0.001953, scenario: 1, slope: -0.0736049767581984, fluctuations: 0.0\n",
      "step: 95630 loss: 62.711330 time elapsed: 121.8377 learning rate: 0.001953, scenario: 0, slope: -0.08456973797681003, fluctuations: 0.0\n",
      "step: 95640 loss: 60.122411 time elapsed: 121.8497 learning rate: 0.001953, scenario: 0, slope: -0.10435053517038745, fluctuations: 0.0\n",
      "step: 95650 loss: 57.798316 time elapsed: 121.8612 learning rate: 0.001953, scenario: 0, slope: -0.12876234560127095, fluctuations: 0.0\n",
      "step: 95660 loss: 55.705386 time elapsed: 121.8739 learning rate: 0.001953, scenario: 0, slope: -0.15425519479645858, fluctuations: 0.0\n",
      "step: 95670 loss: 53.855119 time elapsed: 121.8873 learning rate: 0.001953, scenario: 0, slope: -0.17760096490388275, fluctuations: 0.0\n",
      "step: 95680 loss: 52.235603 time elapsed: 121.9004 learning rate: 0.001953, scenario: 0, slope: -0.1958199440939629, fluctuations: 0.0\n",
      "step: 95690 loss: 50.819604 time elapsed: 121.9141 learning rate: 0.001953, scenario: 0, slope: -0.20635711152799968, fluctuations: 0.0\n",
      "step: 95700 loss: 49.579340 time elapsed: 121.9271 learning rate: 0.001953, scenario: 0, slope: -0.20746491272044296, fluctuations: 0.0\n",
      "step: 95710 loss: 48.486578 time elapsed: 121.9412 learning rate: 0.001953, scenario: 0, slope: -0.19629291946948124, fluctuations: 0.0\n",
      "step: 95720 loss: 47.517782 time elapsed: 121.9547 learning rate: 0.001953, scenario: 0, slope: -0.17659219250708819, fluctuations: 0.0\n",
      "step: 95730 loss: 46.653379 time elapsed: 121.9678 learning rate: 0.001953, scenario: 0, slope: -0.15618950763961853, fluctuations: 0.0\n",
      "step: 95740 loss: 45.876566 time elapsed: 121.9809 learning rate: 0.001953, scenario: 0, slope: -0.13789723559214676, fluctuations: 0.0\n",
      "step: 95750 loss: 45.173135 time elapsed: 121.9946 learning rate: 0.001953, scenario: 0, slope: -0.12163713859810955, fluctuations: 0.0\n",
      "step: 95760 loss: 44.531191 time elapsed: 122.0068 learning rate: 0.001953, scenario: 0, slope: -0.10749242504283892, fluctuations: 0.0\n",
      "step: 95770 loss: 43.940886 time elapsed: 122.0188 learning rate: 0.001953, scenario: 0, slope: -0.09544336674375843, fluctuations: 0.0\n",
      "step: 95780 loss: 43.394088 time elapsed: 122.0308 learning rate: 0.001953, scenario: 0, slope: -0.08526362420536261, fluctuations: 0.0\n",
      "step: 95790 loss: 42.884085 time elapsed: 122.0424 learning rate: 0.001953, scenario: 0, slope: -0.07670976687337483, fluctuations: 0.0\n",
      "step: 95800 loss: 42.405317 time elapsed: 122.0538 learning rate: 0.001953, scenario: 0, slope: -0.0701865435979599, fluctuations: 0.0\n",
      "step: 95810 loss: 41.953159 time elapsed: 122.0661 learning rate: 0.001953, scenario: 0, slope: -0.06348133772524196, fluctuations: 0.0\n",
      "step: 95820 loss: 41.523745 time elapsed: 122.0787 learning rate: 0.001953, scenario: 0, slope: -0.05839827470850692, fluctuations: 0.0\n",
      "step: 95830 loss: 41.113821 time elapsed: 122.0916 learning rate: 0.001953, scenario: 0, slope: -0.05411349925827261, fluctuations: 0.0\n",
      "step: 95840 loss: 40.720646 time elapsed: 122.1056 learning rate: 0.001953, scenario: 0, slope: -0.050491914426269595, fluctuations: 0.0\n",
      "step: 95850 loss: 40.341894 time elapsed: 122.1190 learning rate: 0.001953, scenario: 0, slope: -0.047421150019710194, fluctuations: 0.0\n",
      "step: 95860 loss: 39.975591 time elapsed: 122.1319 learning rate: 0.001953, scenario: 0, slope: -0.04480876971219059, fluctuations: 0.0\n",
      "step: 95870 loss: 39.499742 time elapsed: 122.1450 learning rate: 0.004606, scenario: 1, slope: -0.04277796100598274, fluctuations: 0.0\n",
      "step: 95880 loss: 38.352671 time elapsed: 122.1582 learning rate: 0.010860, scenario: 0, slope: -0.043491571560398894, fluctuations: 0.0\n",
      "step: 95890 loss: 36.641237 time elapsed: 122.1719 learning rate: 0.010860, scenario: 0, slope: -0.050502170750639834, fluctuations: 0.0\n",
      "step: 95900 loss: 15812.383170 time elapsed: 122.1853 learning rate: 0.009237, scenario: -1, slope: 9.5270822990598, fluctuations: 0.0\n",
      "step: 95910 loss: 1871.104623 time elapsed: 122.1995 learning rate: 0.003221, scenario: -1, slope: 16.010310429063896, fluctuations: 0.04\n",
      "step: 95920 loss: 537.996613 time elapsed: 122.2125 learning rate: 0.001123, scenario: -1, slope: 15.872667455644612, fluctuations: 0.06\n",
      "step: 95930 loss: 258.622901 time elapsed: 122.2247 learning rate: 0.000392, scenario: -1, slope: 12.954981624342508, fluctuations: 0.08\n",
      "step: 95940 loss: 176.830335 time elapsed: 122.2364 learning rate: 0.000137, scenario: -1, slope: 8.323165454091264, fluctuations: 0.08\n",
      "step: 95950 loss: 171.247437 time elapsed: 122.2478 learning rate: 0.000048, scenario: -1, slope: 3.910156969689316, fluctuations: 0.08\n",
      "step: 95960 loss: 167.402121 time elapsed: 122.2598 learning rate: 0.000022, scenario: 0, slope: -0.44946738067847214, fluctuations: 0.08\n",
      "step: 95970 loss: 165.286334 time elapsed: 122.2719 learning rate: 0.000022, scenario: 0, slope: -5.0152928143529305, fluctuations: 0.08\n",
      "step: 95980 loss: 163.754904 time elapsed: 122.2846 learning rate: 0.000022, scenario: 0, slope: -10.088997536197294, fluctuations: 0.08\n",
      "step: 95990 loss: 162.637433 time elapsed: 122.2981 learning rate: 0.000022, scenario: 0, slope: -16.077124057293332, fluctuations: 0.08\n",
      "step: 96000 loss: 161.728490 time elapsed: 122.3109 learning rate: 0.000022, scenario: 0, slope: -20.954709521794918, fluctuations: 0.07\n",
      "step: 96010 loss: 160.907967 time elapsed: 122.3248 learning rate: 0.000022, scenario: 0, slope: -4.225756035034646, fluctuations: 0.03\n",
      "step: 96020 loss: 160.124018 time elapsed: 122.3381 learning rate: 0.000022, scenario: 0, slope: -1.062585475971812, fluctuations: 0.01\n",
      "step: 96030 loss: 159.358346 time elapsed: 122.3526 learning rate: 0.000022, scenario: 0, slope: -0.3030515045657676, fluctuations: 0.0\n",
      "step: 96040 loss: 158.477019 time elapsed: 122.3663 learning rate: 0.000042, scenario: 1, slope: -0.14427027815868065, fluctuations: 0.0\n",
      "step: 96050 loss: 156.407547 time elapsed: 122.3800 learning rate: 0.000109, scenario: 1, slope: -0.11254674293435626, fluctuations: 0.0\n",
      "step: 96060 loss: 151.357949 time elapsed: 122.3935 learning rate: 0.000284, scenario: 1, slope: -0.11397384863285123, fluctuations: 0.0\n",
      "step: 96070 loss: 140.006396 time elapsed: 122.4081 learning rate: 0.000736, scenario: 1, slope: -0.15989729912882317, fluctuations: 0.0\n",
      "step: 96080 loss: 124.459979 time elapsed: 122.4215 learning rate: 0.000736, scenario: 0, slope: -0.27305372872717604, fluctuations: 0.0\n",
      "step: 96090 loss: 113.125420 time elapsed: 122.4340 learning rate: 0.000736, scenario: 0, slope: -0.4254726749782172, fluctuations: 0.0\n",
      "step: 96100 loss: 104.705080 time elapsed: 122.4456 learning rate: 0.000736, scenario: 0, slope: -0.5668399679102136, fluctuations: 0.0\n",
      "step: 96110 loss: 98.172947 time elapsed: 122.4578 learning rate: 0.000736, scenario: 0, slope: -0.7187525533309994, fluctuations: 0.0\n",
      "step: 96120 loss: 92.902076 time elapsed: 122.4693 learning rate: 0.000736, scenario: 0, slope: -0.8196945828461116, fluctuations: 0.0\n",
      "step: 96130 loss: 88.502905 time elapsed: 122.4808 learning rate: 0.000736, scenario: 0, slope: -0.8730475951053959, fluctuations: 0.0\n",
      "step: 96140 loss: 84.736759 time elapsed: 122.4937 learning rate: 0.000736, scenario: 0, slope: -0.8701780181459279, fluctuations: 0.0\n",
      "step: 96150 loss: 81.456555 time elapsed: 122.5077 learning rate: 0.000736, scenario: 0, slope: -0.8083180210829054, fluctuations: 0.0\n",
      "step: 96160 loss: 78.567607 time elapsed: 122.5214 learning rate: 0.000736, scenario: 0, slope: -0.6962564426468338, fluctuations: 0.0\n",
      "step: 96170 loss: 76.004868 time elapsed: 122.5348 learning rate: 0.000736, scenario: 0, slope: -0.5630978019361218, fluctuations: 0.0\n",
      "step: 96180 loss: 73.719901 time elapsed: 122.5476 learning rate: 0.000736, scenario: 0, slope: -0.4572146664215655, fluctuations: 0.0\n",
      "step: 96190 loss: 71.674111 time elapsed: 122.5607 learning rate: 0.000736, scenario: 0, slope: -0.3826293522004492, fluctuations: 0.0\n",
      "step: 96200 loss: 69.835227 time elapsed: 122.5736 learning rate: 0.000736, scenario: 0, slope: -0.3326591966540203, fluctuations: 0.0\n",
      "step: 96210 loss: 68.175225 time elapsed: 122.5871 learning rate: 0.000736, scenario: 0, slope: -0.2857047252404669, fluctuations: 0.0\n",
      "step: 96220 loss: 66.668907 time elapsed: 122.5998 learning rate: 0.000736, scenario: 0, slope: -0.2519487088105483, fluctuations: 0.0\n",
      "step: 96230 loss: 65.301859 time elapsed: 122.6134 learning rate: 0.000736, scenario: 0, slope: -0.22401309886093723, fluctuations: 0.0\n",
      "step: 96240 loss: 64.052643 time elapsed: 122.6262 learning rate: 0.000736, scenario: 0, slope: -0.2003702364825972, fluctuations: 0.0\n",
      "step: 96250 loss: 62.906042 time elapsed: 122.6380 learning rate: 0.000736, scenario: 0, slope: -0.18009134143303007, fluctuations: 0.0\n",
      "step: 96260 loss: 61.848753 time elapsed: 122.6494 learning rate: 0.000736, scenario: 0, slope: -0.16257070086089856, fluctuations: 0.0\n",
      "step: 96270 loss: 60.869208 time elapsed: 122.6609 learning rate: 0.000736, scenario: 0, slope: -0.1473765831892571, fluctuations: 0.0\n",
      "step: 96280 loss: 59.957655 time elapsed: 122.6731 learning rate: 0.000736, scenario: 0, slope: -0.13417420851179834, fluctuations: 0.0\n",
      "step: 96290 loss: 59.105870 time elapsed: 122.6847 learning rate: 0.000736, scenario: 0, slope: -0.12268749395358111, fluctuations: 0.0\n",
      "step: 96300 loss: 58.306962 time elapsed: 122.6969 learning rate: 0.000736, scenario: 0, slope: -0.1136206852205714, fluctuations: 0.0\n",
      "step: 96310 loss: 57.555175 time elapsed: 122.7111 learning rate: 0.000736, scenario: 0, slope: -0.10394992306008492, fluctuations: 0.0\n",
      "step: 96320 loss: 56.845559 time elapsed: 122.7248 learning rate: 0.000736, scenario: 0, slope: -0.09632568529679558, fluctuations: 0.0\n",
      "step: 96330 loss: 56.168559 time elapsed: 122.7389 learning rate: 0.000736, scenario: 0, slope: -0.08965748260595066, fluctuations: 0.0\n",
      "step: 96340 loss: 55.131521 time elapsed: 122.7537 learning rate: 0.000736, scenario: 0, slope: -0.08463330396256281, fluctuations: 0.0\n",
      "step: 96350 loss: 54.067550 time elapsed: 122.7690 learning rate: 0.000736, scenario: 0, slope: -0.08293396168046561, fluctuations: 0.0\n",
      "step: 96360 loss: 53.242028 time elapsed: 122.7838 learning rate: 0.000736, scenario: 0, slope: -0.08281772516462327, fluctuations: 0.0\n",
      "step: 96370 loss: 52.495406 time elapsed: 122.7985 learning rate: 0.000736, scenario: 0, slope: -0.08308810201288748, fluctuations: 0.0\n",
      "step: 96380 loss: 51.825691 time elapsed: 122.8125 learning rate: 0.000736, scenario: 0, slope: -0.08319134615168645, fluctuations: 0.0\n",
      "step: 96390 loss: 51.212793 time elapsed: 122.8275 learning rate: 0.000736, scenario: 0, slope: -0.08264985398437324, fluctuations: 0.0\n",
      "step: 96400 loss: 50.646306 time elapsed: 122.8401 learning rate: 0.000736, scenario: 0, slope: -0.08130924813290955, fluctuations: 0.0\n",
      "step: 96410 loss: 50.120172 time elapsed: 122.8537 learning rate: 0.000736, scenario: 0, slope: -0.0782709036517502, fluctuations: 0.0\n",
      "step: 96420 loss: 49.629415 time elapsed: 122.8662 learning rate: 0.000736, scenario: 0, slope: -0.07393787002093029, fluctuations: 0.0\n",
      "step: 96430 loss: 49.169689 time elapsed: 122.8786 learning rate: 0.000736, scenario: 0, slope: -0.06793647920348105, fluctuations: 0.0\n",
      "step: 96440 loss: 48.737376 time elapsed: 122.8906 learning rate: 0.000736, scenario: 0, slope: -0.06106332742627988, fluctuations: 0.0\n",
      "step: 96450 loss: 48.334105 time elapsed: 122.9041 learning rate: 0.000736, scenario: 0, slope: -0.05590264241866985, fluctuations: 0.0\n",
      "step: 96460 loss: 47.946307 time elapsed: 122.9178 learning rate: 0.001185, scenario: 1, slope: -0.051806615176881045, fluctuations: 0.0\n",
      "step: 96470 loss: 47.443636 time elapsed: 122.9317 learning rate: 0.003072, scenario: 1, slope: -0.0489655932435563, fluctuations: 0.02\n",
      "step: 96480 loss: 57.408865 time elapsed: 122.9447 learning rate: 0.007969, scenario: 1, slope: -0.029620627539567606, fluctuations: 0.05\n",
      "step: 96490 loss: 463.861237 time elapsed: 122.9584 learning rate: 0.012007, scenario: -1, slope: 0.3710604096588268, fluctuations: 0.08\n",
      "step: 96500 loss: 203.191203 time elapsed: 122.9720 learning rate: 0.004652, scenario: -1, slope: 6.285103718858268, fluctuations: 0.11\n",
      "step: 96510 loss: 74.446460 time elapsed: 122.9862 learning rate: 0.001622, scenario: -1, slope: 5.404722942025139, fluctuations: 0.15\n",
      "step: 96520 loss: 79.529768 time elapsed: 122.9999 learning rate: 0.000566, scenario: -1, slope: 4.038733200002182, fluctuations: 0.17\n",
      "step: 96530 loss: 59.178555 time elapsed: 123.0132 learning rate: 0.000197, scenario: -1, slope: 2.3978457211166107, fluctuations: 0.18\n",
      "step: 96540 loss: 54.432092 time elapsed: 123.0271 learning rate: 0.000069, scenario: -1, slope: 0.7607610512720026, fluctuations: 0.18\n",
      "step: 96550 loss: 53.843390 time elapsed: 123.0395 learning rate: 0.000041, scenario: 0, slope: -0.7785697470717808, fluctuations: 0.19\n",
      "step: 96560 loss: 53.191877 time elapsed: 123.0520 learning rate: 0.000041, scenario: 0, slope: -2.6343518107827313, fluctuations: 0.19\n",
      "step: 96570 loss: 53.011906 time elapsed: 123.0639 learning rate: 0.000041, scenario: 0, slope: -4.373235857321641, fluctuations: 0.17\n",
      "step: 96580 loss: 52.879529 time elapsed: 123.0760 learning rate: 0.000041, scenario: 0, slope: -5.53000120112081, fluctuations: 0.13\n",
      "step: 96590 loss: 52.723931 time elapsed: 123.0881 learning rate: 0.000041, scenario: 0, slope: -6.712615185198991, fluctuations: 0.11\n",
      "step: 96600 loss: 52.584518 time elapsed: 123.0995 learning rate: 0.000041, scenario: 0, slope: -1.1911966524452522, fluctuations: 0.07\n",
      "step: 96610 loss: 52.453772 time elapsed: 123.1128 learning rate: 0.000041, scenario: 0, slope: -0.2738530177980027, fluctuations: 0.04\n",
      "step: 96620 loss: 52.324636 time elapsed: 123.1267 learning rate: 0.000041, scenario: 0, slope: -0.06600977793632057, fluctuations: 0.02\n",
      "step: 96630 loss: 52.166852 time elapsed: 123.1402 learning rate: 0.000087, scenario: 1, slope: -0.024196158510420073, fluctuations: 0.01\n",
      "step: 96640 loss: 51.789326 time elapsed: 123.1535 learning rate: 0.000226, scenario: 1, slope: -0.020175060350545188, fluctuations: 0.0\n",
      "step: 96650 loss: 50.894877 time elapsed: 123.1669 learning rate: 0.000586, scenario: 1, slope: -0.018988812070754837, fluctuations: 0.0\n",
      "step: 96660 loss: 49.045022 time elapsed: 123.1800 learning rate: 0.001519, scenario: 1, slope: -0.02704976677531003, fluctuations: 0.0\n",
      "step: 96670 loss: 46.195976 time elapsed: 123.1933 learning rate: 0.003939, scenario: 1, slope: -0.046656905562466405, fluctuations: 0.0\n",
      "step: 96680 loss: 43.518972 time elapsed: 123.2065 learning rate: 0.004766, scenario: 0, slope: -0.07617845964286583, fluctuations: 0.0\n",
      "step: 96690 loss: 41.843745 time elapsed: 123.2201 learning rate: 0.004766, scenario: 0, slope: -0.10786684564480954, fluctuations: 0.0\n",
      "step: 96700 loss: 40.759494 time elapsed: 123.2337 learning rate: 0.004766, scenario: 0, slope: -0.13285725750071584, fluctuations: 0.0\n",
      "step: 96710 loss: 39.959127 time elapsed: 123.2463 learning rate: 0.004766, scenario: 0, slope: -0.1547647581692312, fluctuations: 0.0\n",
      "step: 96720 loss: 39.300059 time elapsed: 123.2578 learning rate: 0.004766, scenario: 0, slope: -0.16429782519015082, fluctuations: 0.0\n",
      "step: 96730 loss: 38.720126 time elapsed: 123.2694 learning rate: 0.004766, scenario: 0, slope: -0.16266522207660009, fluctuations: 0.0\n",
      "step: 96740 loss: 38.184353 time elapsed: 123.2813 learning rate: 0.004766, scenario: 0, slope: -0.14978610887165275, fluctuations: 0.0\n",
      "step: 96750 loss: 37.665640 time elapsed: 123.2925 learning rate: 0.004766, scenario: 0, slope: -0.12754623038019455, fluctuations: 0.0\n",
      "step: 96760 loss: 37.516202 time elapsed: 123.3043 learning rate: 0.004766, scenario: 0, slope: -0.10150173164362361, fluctuations: 0.02\n",
      "step: 96770 loss: 36.827738 time elapsed: 123.3170 learning rate: 0.004766, scenario: 0, slope: -0.0777079206770105, fluctuations: 0.05\n",
      "step: 96780 loss: 36.384122 time elapsed: 123.3310 learning rate: 0.004766, scenario: 0, slope: -0.0622318821744414, fluctuations: 0.06\n",
      "step: 96790 loss: 35.921144 time elapsed: 123.3446 learning rate: 0.004766, scenario: 0, slope: -0.05413360324938784, fluctuations: 0.06\n",
      "step: 96800 loss: 35.489633 time elapsed: 123.3583 learning rate: 0.004766, scenario: 0, slope: -0.05012098201038593, fluctuations: 0.06\n",
      "step: 96810 loss: 35.048171 time elapsed: 123.3722 learning rate: 0.004766, scenario: 0, slope: -0.0473447870602065, fluctuations: 0.06\n",
      "step: 96820 loss: 34.643743 time elapsed: 123.3858 learning rate: 0.004766, scenario: 0, slope: -0.045792168225421766, fluctuations: 0.06\n",
      "step: 96830 loss: 34.263290 time elapsed: 123.3987 learning rate: 0.004766, scenario: 0, slope: -0.04463570471874331, fluctuations: 0.06\n",
      "step: 96840 loss: 33.894861 time elapsed: 123.4121 learning rate: 0.004766, scenario: 0, slope: -0.043683394997664285, fluctuations: 0.06\n",
      "step: 96850 loss: 33.535797 time elapsed: 123.4254 learning rate: 0.004766, scenario: 0, slope: -0.042882667620418036, fluctuations: 0.06\n",
      "step: 96860 loss: 33.185258 time elapsed: 123.4385 learning rate: 0.004766, scenario: 0, slope: -0.04149990194524357, fluctuations: 0.03\n",
      "step: 96870 loss: 32.842196 time elapsed: 123.4515 learning rate: 0.004766, scenario: 0, slope: -0.03965423960171299, fluctuations: 0.01\n",
      "step: 96880 loss: 32.505963 time elapsed: 123.4638 learning rate: 0.004766, scenario: 0, slope: -0.038304079794595144, fluctuations: 0.0\n",
      "step: 96890 loss: 32.175988 time elapsed: 123.4755 learning rate: 0.004766, scenario: 0, slope: -0.03703541910472446, fluctuations: 0.0\n",
      "step: 96900 loss: 31.851867 time elapsed: 123.4869 learning rate: 0.004766, scenario: 0, slope: -0.03593733088726074, fluctuations: 0.0\n",
      "step: 96910 loss: 31.523396 time elapsed: 123.4991 learning rate: 0.006978, scenario: 1, slope: -0.03486914283980717, fluctuations: 0.0\n",
      "step: 96920 loss: 30.872221 time elapsed: 123.5107 learning rate: 0.013599, scenario: 0, slope: -0.034935826438315165, fluctuations: 0.0\n",
      "step: 96930 loss: 58.096949 time elapsed: 123.5237 learning rate: 0.014959, scenario: 1, slope: -0.01811592219200844, fluctuations: 0.0\n",
      "step: 96940 loss: 5479.509216 time elapsed: 123.5375 learning rate: 0.005532, scenario: -1, slope: 20.886248511272147, fluctuations: 0.02\n",
      "step: 96950 loss: 652.868436 time elapsed: 123.5510 learning rate: 0.001929, scenario: -1, slope: 21.250400187357474, fluctuations: 0.05\n",
      "step: 96960 loss: 393.429311 time elapsed: 123.5639 learning rate: 0.000673, scenario: -1, slope: 18.596919436999073, fluctuations: 0.07\n",
      "step: 96970 loss: 256.223570 time elapsed: 123.5775 learning rate: 0.000234, scenario: -1, slope: 13.481030982257067, fluctuations: 0.08\n",
      "step: 96980 loss: 219.799245 time elapsed: 123.5909 learning rate: 0.000082, scenario: -1, slope: 7.649993716618273, fluctuations: 0.08\n",
      "step: 96990 loss: 215.088789 time elapsed: 123.6043 learning rate: 0.000029, scenario: -1, slope: 2.030857368157441, fluctuations: 0.08\n",
      "step: 97000 loss: 213.728241 time elapsed: 123.6177 learning rate: 0.000021, scenario: 0, slope: -3.079725463377266, fluctuations: 0.08\n",
      "step: 97010 loss: 212.633873 time elapsed: 123.6318 learning rate: 0.000021, scenario: 0, slope: -9.786080653135185, fluctuations: 0.08\n",
      "step: 97020 loss: 211.669754 time elapsed: 123.6457 learning rate: 0.000021, scenario: 0, slope: -16.774250940083352, fluctuations: 0.08\n",
      "step: 97030 loss: 210.798354 time elapsed: 123.6586 learning rate: 0.000021, scenario: 0, slope: -25.216728285087793, fluctuations: 0.08\n",
      "step: 97040 loss: 209.980979 time elapsed: 123.6710 learning rate: 0.000021, scenario: 0, slope: -7.17230418160166, fluctuations: 0.05\n",
      "step: 97050 loss: 209.193219 time elapsed: 123.6829 learning rate: 0.000021, scenario: 0, slope: -1.8090812268421783, fluctuations: 0.03\n",
      "step: 97060 loss: 208.422049 time elapsed: 123.6947 learning rate: 0.000021, scenario: 0, slope: -0.4888506401249688, fluctuations: 0.01\n",
      "step: 97070 loss: 207.637531 time elapsed: 123.7066 learning rate: 0.000030, scenario: 1, slope: -0.1818459239011523, fluctuations: 0.0\n",
      "step: 97080 loss: 206.045239 time elapsed: 123.7186 learning rate: 0.000079, scenario: 1, slope: -0.09813320435925688, fluctuations: 0.0\n",
      "step: 97090 loss: 202.027268 time elapsed: 123.7313 learning rate: 0.000205, scenario: 1, slope: -0.0993774684867784, fluctuations: 0.0\n",
      "step: 97100 loss: 192.251029 time elapsed: 123.7449 learning rate: 0.000483, scenario: 1, slope: -0.13209743430263246, fluctuations: 0.0\n",
      "step: 97110 loss: 172.256492 time elapsed: 123.7593 learning rate: 0.000941, scenario: 0, slope: -0.24681734417069506, fluctuations: 0.0\n",
      "step: 97120 loss: 150.190929 time elapsed: 123.7727 learning rate: 0.000941, scenario: 0, slope: -0.44750349432783615, fluctuations: 0.0\n",
      "step: 97130 loss: 132.685973 time elapsed: 123.7862 learning rate: 0.000941, scenario: 0, slope: -0.6959540045960001, fluctuations: 0.0\n",
      "step: 97140 loss: 118.729588 time elapsed: 123.7998 learning rate: 0.000941, scenario: 0, slope: -0.9480754169172454, fluctuations: 0.0\n",
      "step: 97150 loss: 107.572621 time elapsed: 123.8130 learning rate: 0.000941, scenario: 0, slope: -1.1692284255486027, fluctuations: 0.0\n",
      "step: 97160 loss: 98.563070 time elapsed: 123.8267 learning rate: 0.000941, scenario: 0, slope: -1.3323169654196898, fluctuations: 0.0\n",
      "step: 97170 loss: 91.174953 time elapsed: 123.8400 learning rate: 0.000941, scenario: 0, slope: -1.4163823953657793, fluctuations: 0.0\n",
      "step: 97180 loss: 85.033927 time elapsed: 123.8542 learning rate: 0.000941, scenario: 0, slope: -1.4073161410127475, fluctuations: 0.0\n",
      "step: 97190 loss: 79.877526 time elapsed: 123.8669 learning rate: 0.000941, scenario: 0, slope: -1.303164002972048, fluctuations: 0.0\n",
      "step: 97200 loss: 75.508016 time elapsed: 123.8794 learning rate: 0.000941, scenario: 0, slope: -1.1425500881160184, fluctuations: 0.0\n",
      "step: 97210 loss: 71.767482 time elapsed: 123.8941 learning rate: 0.000941, scenario: 0, slope: -0.9168715471908857, fluctuations: 0.0\n",
      "step: 97220 loss: 68.529391 time elapsed: 123.9061 learning rate: 0.000941, scenario: 0, slope: -0.7487991444762504, fluctuations: 0.0\n",
      "step: 97230 loss: 65.693684 time elapsed: 123.9179 learning rate: 0.000941, scenario: 0, slope: -0.6181201344530917, fluctuations: 0.0\n",
      "step: 97240 loss: 63.178887 time elapsed: 123.9304 learning rate: 0.000941, scenario: 0, slope: -0.5163278650296385, fluctuations: 0.0\n",
      "step: 97250 loss: 60.916933 time elapsed: 123.9442 learning rate: 0.000941, scenario: 0, slope: -0.43661009753518754, fluctuations: 0.0\n",
      "step: 97260 loss: 58.939215 time elapsed: 123.9579 learning rate: 0.000941, scenario: 0, slope: -0.3734429534522883, fluctuations: 0.0\n",
      "step: 97270 loss: 57.183491 time elapsed: 123.9708 learning rate: 0.000941, scenario: 0, slope: -0.32249432573518017, fluctuations: 0.0\n",
      "step: 97280 loss: 55.608317 time elapsed: 123.9844 learning rate: 0.000941, scenario: 0, slope: -0.2809440137971657, fluctuations: 0.0\n",
      "step: 97290 loss: 54.183875 time elapsed: 123.9978 learning rate: 0.000941, scenario: 0, slope: -0.24667932011004334, fluctuations: 0.0\n",
      "step: 97300 loss: 52.894399 time elapsed: 124.0104 learning rate: 0.000941, scenario: 0, slope: -0.22071180018256953, fluctuations: 0.0\n",
      "step: 97310 loss: 51.722835 time elapsed: 124.0241 learning rate: 0.000941, scenario: 0, slope: -0.1938446084303287, fluctuations: 0.0\n",
      "step: 97320 loss: 50.593917 time elapsed: 124.0375 learning rate: 0.000941, scenario: 0, slope: -0.17318101534886113, fluctuations: 0.0\n",
      "step: 97330 loss: 48.997014 time elapsed: 124.0512 learning rate: 0.000941, scenario: 0, slope: -0.15766348183183893, fluctuations: 0.0\n",
      "step: 97340 loss: 47.929209 time elapsed: 124.0650 learning rate: 0.000941, scenario: 0, slope: -0.14595054388313192, fluctuations: 0.01\n",
      "step: 97350 loss: 47.070324 time elapsed: 124.0772 learning rate: 0.000941, scenario: 0, slope: -0.13561150066750235, fluctuations: 0.01\n",
      "step: 97360 loss: 46.333342 time elapsed: 124.0889 learning rate: 0.000941, scenario: 0, slope: -0.12622616621144078, fluctuations: 0.01\n",
      "step: 97370 loss: 45.664517 time elapsed: 124.1007 learning rate: 0.000941, scenario: 0, slope: -0.11708983006401409, fluctuations: 0.01\n",
      "step: 97380 loss: 45.065307 time elapsed: 124.1125 learning rate: 0.000941, scenario: 0, slope: -0.10791517932620653, fluctuations: 0.01\n",
      "step: 97390 loss: 44.518297 time elapsed: 124.1240 learning rate: 0.000941, scenario: 0, slope: -0.09850654846479841, fluctuations: 0.01\n",
      "step: 97400 loss: 44.017172 time elapsed: 124.1355 learning rate: 0.000941, scenario: 0, slope: -0.08972190823141465, fluctuations: 0.01\n",
      "step: 97410 loss: 43.554834 time elapsed: 124.1481 learning rate: 0.000941, scenario: 0, slope: -0.0784554951183194, fluctuations: 0.01\n",
      "step: 97420 loss: 43.125995 time elapsed: 124.1622 learning rate: 0.000941, scenario: 0, slope: -0.06772647588569346, fluctuations: 0.01\n",
      "step: 97430 loss: 42.726262 time elapsed: 124.1763 learning rate: 0.000941, scenario: 0, slope: -0.05965337506745332, fluctuations: 0.0\n",
      "step: 97440 loss: 42.351955 time elapsed: 124.1895 learning rate: 0.000941, scenario: 0, slope: -0.05399910510172247, fluctuations: 0.0\n",
      "step: 97450 loss: 42.000013 time elapsed: 124.2029 learning rate: 0.000941, scenario: 0, slope: -0.04943999833169175, fluctuations: 0.0\n",
      "step: 97460 loss: 41.664625 time elapsed: 124.2164 learning rate: 0.001252, scenario: 1, slope: -0.04556255506176351, fluctuations: 0.0\n",
      "step: 97470 loss: 41.068834 time elapsed: 124.2300 learning rate: 0.003247, scenario: 1, slope: -0.042895670928628116, fluctuations: 0.0\n",
      "step: 97480 loss: 39.728482 time elapsed: 124.2434 learning rate: 0.008423, scenario: 1, slope: -0.04417306981259162, fluctuations: 0.0\n",
      "step: 97490 loss: 37.793488 time elapsed: 124.2571 learning rate: 0.009265, scenario: 0, slope: -0.05303294761150245, fluctuations: 0.0\n",
      "step: 97500 loss: 36.746728 time elapsed: 124.2703 learning rate: 0.009265, scenario: 0, slope: -0.06396479379374062, fluctuations: 0.01\n",
      "step: 97510 loss: 35.756141 time elapsed: 124.2842 learning rate: 0.009265, scenario: 0, slope: -0.07673892685726752, fluctuations: 0.04\n",
      "step: 97520 loss: 34.774144 time elapsed: 124.2964 learning rate: 0.009265, scenario: 0, slope: -0.08952554195854644, fluctuations: 0.05\n",
      "step: 97530 loss: 34.095426 time elapsed: 124.3084 learning rate: 0.009265, scenario: 0, slope: -0.0991922924735436, fluctuations: 0.05\n",
      "step: 97540 loss: 33.492430 time elapsed: 124.3204 learning rate: 0.009265, scenario: 0, slope: -0.10419508043713033, fluctuations: 0.05\n",
      "step: 97550 loss: 32.933261 time elapsed: 124.3324 learning rate: 0.009265, scenario: 0, slope: -0.10417596996957687, fluctuations: 0.05\n",
      "step: 97560 loss: 32.291615 time elapsed: 124.3442 learning rate: 0.009265, scenario: 0, slope: -0.09915753847891716, fluctuations: 0.05\n",
      "step: 97570 loss: 33.654387 time elapsed: 124.3575 learning rate: 0.009265, scenario: 0, slope: -0.08215233403978982, fluctuations: 0.08\n",
      "step: 97580 loss: 32.114740 time elapsed: 124.3716 learning rate: 0.009265, scenario: 0, slope: -0.06703289185318723, fluctuations: 0.12\n",
      "step: 97590 loss: 31.179731 time elapsed: 124.3849 learning rate: 0.009265, scenario: 0, slope: -0.056121412305487846, fluctuations: 0.14\n",
      "step: 97600 loss: 30.562240 time elapsed: 124.3982 learning rate: 0.009265, scenario: 0, slope: -0.05195292997238613, fluctuations: 0.14\n",
      "step: 97610 loss: 29.936352 time elapsed: 124.4122 learning rate: 0.009265, scenario: 0, slope: -0.05032298573794094, fluctuations: 0.11\n",
      "step: 97620 loss: 29.425554 time elapsed: 124.4256 learning rate: 0.009265, scenario: 0, slope: -0.050825231246810425, fluctuations: 0.1\n",
      "step: 97630 loss: 28.927964 time elapsed: 124.4388 learning rate: 0.009265, scenario: 0, slope: -0.05228172868298633, fluctuations: 0.1\n",
      "step: 97640 loss: 40.110392 time elapsed: 124.4522 learning rate: 0.009265, scenario: 0, slope: -0.03402905504817968, fluctuations: 0.1\n",
      "step: 97650 loss: 8766.487659 time elapsed: 124.4656 learning rate: 0.008496, scenario: -1, slope: 9.354727923479418, fluctuations: 0.12\n",
      "step: 97660 loss: 580.525360 time elapsed: 124.4804 learning rate: 0.002962, scenario: -1, slope: 20.553637702881137, fluctuations: 0.15\n",
      "step: 97670 loss: 197.050651 time elapsed: 124.4942 learning rate: 0.001033, scenario: -1, slope: 18.39486049778157, fluctuations: 0.13\n",
      "step: 97680 loss: 157.097065 time elapsed: 124.5066 learning rate: 0.000360, scenario: -1, slope: 13.061320090072796, fluctuations: 0.1\n",
      "step: 97690 loss: 109.653138 time elapsed: 124.5186 learning rate: 0.000126, scenario: -1, slope: 8.101596039910264, fluctuations: 0.1\n",
      "step: 97700 loss: 103.089970 time elapsed: 124.5301 learning rate: 0.000049, scenario: -1, slope: 3.44858293015236, fluctuations: 0.09\n",
      "step: 97710 loss: 101.485684 time elapsed: 124.5421 learning rate: 0.000029, scenario: 0, slope: -2.0750620899530134, fluctuations: 0.09\n",
      "step: 97720 loss: 99.856001 time elapsed: 124.5534 learning rate: 0.000029, scenario: 0, slope: -7.281150824441766, fluctuations: 0.09\n",
      "step: 97730 loss: 98.670883 time elapsed: 124.5662 learning rate: 0.000029, scenario: 0, slope: -13.076829710738263, fluctuations: 0.09\n",
      "step: 97740 loss: 97.846692 time elapsed: 124.5798 learning rate: 0.000029, scenario: 0, slope: -18.906082615017098, fluctuations: 0.08\n",
      "step: 97750 loss: 97.170621 time elapsed: 124.5933 learning rate: 0.000029, scenario: 0, slope: -14.592962898617271, fluctuations: 0.07\n",
      "step: 97760 loss: 96.543772 time elapsed: 124.6069 learning rate: 0.000029, scenario: 0, slope: -3.205452927120126, fluctuations: 0.04\n",
      "step: 97770 loss: 95.941729 time elapsed: 124.6204 learning rate: 0.000029, scenario: 0, slope: -0.8259784526882687, fluctuations: 0.02\n",
      "step: 97780 loss: 95.359525 time elapsed: 124.6340 learning rate: 0.000029, scenario: 0, slope: -0.26481021346915273, fluctuations: 0.01\n",
      "step: 97790 loss: 94.776341 time elapsed: 124.6475 learning rate: 0.000042, scenario: 1, slope: -0.09623910350617272, fluctuations: 0.0\n",
      "step: 97800 loss: 93.615190 time elapsed: 124.6610 learning rate: 0.000099, scenario: 1, slope: -0.08280621455993638, fluctuations: 0.0\n",
      "step: 97810 loss: 91.017503 time elapsed: 124.6749 learning rate: 0.000257, scenario: 1, slope: -0.07924783773255224, fluctuations: 0.0\n",
      "step: 97820 loss: 85.105593 time elapsed: 124.6889 learning rate: 0.000667, scenario: 1, slope: -0.09880622636888385, fluctuations: 0.0\n",
      "step: 97830 loss: 76.689744 time elapsed: 124.7014 learning rate: 0.000667, scenario: 0, slope: -0.15523046447273517, fluctuations: 0.0\n",
      "step: 97840 loss: 70.303988 time elapsed: 124.7130 learning rate: 0.000667, scenario: 0, slope: -0.23463499344730165, fluctuations: 0.0\n",
      "step: 97850 loss: 65.496892 time elapsed: 124.7250 learning rate: 0.000667, scenario: 0, slope: -0.3180643460106509, fluctuations: 0.0\n",
      "step: 97860 loss: 61.786599 time elapsed: 124.7368 learning rate: 0.000667, scenario: 0, slope: -0.39226627533037756, fluctuations: 0.0\n",
      "step: 97870 loss: 58.840688 time elapsed: 124.7488 learning rate: 0.000667, scenario: 0, slope: -0.4479548162122598, fluctuations: 0.0\n",
      "step: 97880 loss: 56.440778 time elapsed: 124.7609 learning rate: 0.000667, scenario: 0, slope: -0.47837986461620796, fluctuations: 0.0\n",
      "step: 97890 loss: 54.441327 time elapsed: 124.7737 learning rate: 0.000667, scenario: 0, slope: -0.4785128926025135, fluctuations: 0.0\n",
      "step: 97900 loss: 52.745238 time elapsed: 124.7876 learning rate: 0.000667, scenario: 0, slope: -0.45078057720426795, fluctuations: 0.0\n",
      "step: 97910 loss: 51.284622 time elapsed: 124.8020 learning rate: 0.000667, scenario: 0, slope: -0.3854511981430398, fluctuations: 0.0\n",
      "step: 97920 loss: 50.010362 time elapsed: 124.8155 learning rate: 0.000667, scenario: 0, slope: -0.3110305512959266, fluctuations: 0.0\n",
      "step: 97930 loss: 48.886060 time elapsed: 124.8288 learning rate: 0.000667, scenario: 0, slope: -0.24934664328009254, fluctuations: 0.0\n",
      "step: 97940 loss: 47.884151 time elapsed: 124.8425 learning rate: 0.000667, scenario: 0, slope: -0.20456846167469653, fluctuations: 0.0\n",
      "step: 97950 loss: 46.983331 time elapsed: 124.8562 learning rate: 0.000667, scenario: 0, slope: -0.17152240659455015, fluctuations: 0.0\n",
      "step: 97960 loss: 46.166796 time elapsed: 124.8687 learning rate: 0.000667, scenario: 0, slope: -0.14649995582650968, fluctuations: 0.0\n",
      "step: 97970 loss: 45.421025 time elapsed: 124.8821 learning rate: 0.000667, scenario: 0, slope: -0.12708393360483974, fluctuations: 0.0\n",
      "step: 97980 loss: 44.734913 time elapsed: 124.8963 learning rate: 0.000667, scenario: 0, slope: -0.1116906228087126, fluctuations: 0.0\n",
      "step: 97990 loss: 44.099106 time elapsed: 124.9093 learning rate: 0.000667, scenario: 0, slope: -0.09927333919505177, fluctuations: 0.0\n",
      "step: 98000 loss: 43.505433 time elapsed: 124.9216 learning rate: 0.000667, scenario: 0, slope: -0.09004958636560328, fluctuations: 0.0\n",
      "step: 98010 loss: 42.946357 time elapsed: 124.9338 learning rate: 0.000667, scenario: 0, slope: -0.08073542465846964, fluctuations: 0.0\n",
      "step: 98020 loss: 42.414326 time elapsed: 124.9455 learning rate: 0.000667, scenario: 0, slope: -0.0737666260973747, fluctuations: 0.0\n",
      "step: 98030 loss: 41.900951 time elapsed: 124.9572 learning rate: 0.000667, scenario: 0, slope: -0.0679711031959857, fluctuations: 0.0\n",
      "step: 98040 loss: 41.396107 time elapsed: 124.9692 learning rate: 0.000667, scenario: 0, slope: -0.06319173817673183, fluctuations: 0.0\n",
      "step: 98050 loss: 40.888428 time elapsed: 124.9823 learning rate: 0.000667, scenario: 0, slope: -0.059344571204483915, fluctuations: 0.0\n",
      "step: 98060 loss: 40.374066 time elapsed: 124.9959 learning rate: 0.000667, scenario: 0, slope: -0.056385830926008025, fluctuations: 0.0\n",
      "step: 98070 loss: 39.880533 time elapsed: 125.0095 learning rate: 0.000667, scenario: 0, slope: -0.0541875149678223, fluctuations: 0.0\n",
      "step: 98080 loss: 39.443462 time elapsed: 125.0230 learning rate: 0.000667, scenario: 0, slope: -0.05240766024312061, fluctuations: 0.0\n",
      "step: 98090 loss: 39.050411 time elapsed: 125.0366 learning rate: 0.000667, scenario: 0, slope: -0.050706014947098255, fluctuations: 0.0\n",
      "step: 98100 loss: 38.687870 time elapsed: 125.0499 learning rate: 0.000667, scenario: 0, slope: -0.0490749231020394, fluctuations: 0.0\n",
      "step: 98110 loss: 38.347797 time elapsed: 125.0638 learning rate: 0.000667, scenario: 0, slope: -0.04683516558197298, fluctuations: 0.0\n",
      "step: 98120 loss: 38.027775 time elapsed: 125.0771 learning rate: 0.000667, scenario: 0, slope: -0.04451561759445864, fluctuations: 0.0\n",
      "step: 98130 loss: 37.726021 time elapsed: 125.0902 learning rate: 0.000667, scenario: 0, slope: -0.0419296926954744, fluctuations: 0.0\n",
      "step: 98140 loss: 37.313673 time elapsed: 125.1034 learning rate: 0.001730, scenario: 1, slope: -0.039381041875825994, fluctuations: 0.0\n",
      "step: 98150 loss: 36.364123 time elapsed: 125.1159 learning rate: 0.004488, scenario: 1, slope: -0.03895165471479134, fluctuations: 0.0\n",
      "step: 98160 loss: 34.652150 time elapsed: 125.1281 learning rate: 0.006572, scenario: 0, slope: -0.044441908890872045, fluctuations: 0.0\n",
      "step: 98170 loss: 33.278660 time elapsed: 125.1394 learning rate: 0.006572, scenario: 0, slope: -0.055647869179113275, fluctuations: 0.0\n",
      "step: 98180 loss: 32.273419 time elapsed: 125.1509 learning rate: 0.006572, scenario: 0, slope: -0.06862188627477007, fluctuations: 0.0\n",
      "step: 98190 loss: 31.482374 time elapsed: 125.1628 learning rate: 0.006572, scenario: 0, slope: -0.08071947653936663, fluctuations: 0.0\n",
      "step: 98200 loss: 30.825246 time elapsed: 125.1740 learning rate: 0.006572, scenario: 0, slope: -0.08945061240953849, fluctuations: 0.0\n",
      "step: 98210 loss: 30.254101 time elapsed: 125.1865 learning rate: 0.006572, scenario: 0, slope: -0.09607968444866392, fluctuations: 0.0\n",
      "step: 98220 loss: 29.732353 time elapsed: 125.2006 learning rate: 0.006572, scenario: 0, slope: -0.09740399733651801, fluctuations: 0.0\n",
      "step: 98230 loss: 29.244577 time elapsed: 125.2142 learning rate: 0.006572, scenario: 0, slope: -0.09365028713180193, fluctuations: 0.0\n",
      "step: 98240 loss: 28.808630 time elapsed: 125.2277 learning rate: 0.006572, scenario: 0, slope: -0.08446909815026787, fluctuations: 0.0\n",
      "step: 98250 loss: 28.410517 time elapsed: 125.2407 learning rate: 0.006572, scenario: 0, slope: -0.07168562540695539, fluctuations: 0.0\n",
      "step: 98260 loss: 28.040124 time elapsed: 125.2537 learning rate: 0.006572, scenario: 0, slope: -0.06023221103018504, fluctuations: 0.0\n",
      "step: 98270 loss: 27.694622 time elapsed: 125.2670 learning rate: 0.006572, scenario: 0, slope: -0.05255993450567391, fluctuations: 0.0\n",
      "step: 98280 loss: 27.370363 time elapsed: 125.2803 learning rate: 0.006572, scenario: 0, slope: -0.04712666708832646, fluctuations: 0.0\n",
      "step: 98290 loss: 27.064685 time elapsed: 125.2939 learning rate: 0.006572, scenario: 0, slope: -0.042979313005275674, fluctuations: 0.0\n",
      "step: 98300 loss: 26.775110 time elapsed: 125.3069 learning rate: 0.006572, scenario: 0, slope: -0.03991215193147144, fluctuations: 0.0\n",
      "step: 98310 loss: 26.499609 time elapsed: 125.3206 learning rate: 0.006572, scenario: 0, slope: -0.036700965418518, fluctuations: 0.0\n",
      "step: 98320 loss: 26.236399 time elapsed: 125.3320 learning rate: 0.006572, scenario: 0, slope: -0.03416820814092163, fluctuations: 0.0\n",
      "step: 98330 loss: 25.983965 time elapsed: 125.3439 learning rate: 0.006572, scenario: 0, slope: -0.03202563789021484, fluctuations: 0.0\n",
      "step: 98340 loss: 25.740994 time elapsed: 125.3556 learning rate: 0.006572, scenario: 0, slope: -0.030210007177645132, fluctuations: 0.0\n",
      "step: 98350 loss: 25.506338 time elapsed: 125.3674 learning rate: 0.006572, scenario: 0, slope: -0.0286349442744639, fluctuations: 0.0\n",
      "step: 98360 loss: 25.223294 time elapsed: 125.3789 learning rate: 0.014087, scenario: 1, slope: -0.02734505539275377, fluctuations: 0.0\n",
      "step: 98370 loss: 99674.462353 time elapsed: 125.3914 learning rate: 0.014208, scenario: -1, slope: 70.24508462861188, fluctuations: 0.01\n",
      "step: 98380 loss: 12907.379595 time elapsed: 125.4054 learning rate: 0.004954, scenario: -1, slope: 194.42114706243632, fluctuations: 0.03\n",
      "step: 98390 loss: 7497.281689 time elapsed: 125.4189 learning rate: 0.001727, scenario: -1, slope: 203.37908522652492, fluctuations: 0.04\n",
      "step: 98400 loss: 5309.358328 time elapsed: 125.4319 learning rate: 0.000669, scenario: -1, slope: 177.8578118241496, fluctuations: 0.04\n",
      "step: 98410 loss: 4803.852265 time elapsed: 125.4461 learning rate: 0.000233, scenario: -1, slope: 135.57339714606655, fluctuations: 0.04\n",
      "step: 98420 loss: 4649.745059 time elapsed: 125.4590 learning rate: 0.000081, scenario: -1, slope: 91.2259441530134, fluctuations: 0.04\n",
      "step: 98430 loss: 4604.012047 time elapsed: 125.4721 learning rate: 0.000028, scenario: -1, slope: 41.68454231099871, fluctuations: 0.04\n",
      "step: 98440 loss: 4588.829166 time elapsed: 125.4858 learning rate: 0.000014, scenario: 0, slope: -14.151081538879405, fluctuations: 0.04\n",
      "step: 98450 loss: 4579.134498 time elapsed: 125.4988 learning rate: 0.000014, scenario: 0, slope: -78.09732192654141, fluctuations: 0.04\n",
      "step: 98460 loss: 4569.610830 time elapsed: 125.5119 learning rate: 0.000014, scenario: 0, slope: -152.70620519922636, fluctuations: 0.04\n",
      "step: 98470 loss: 4560.224868 time elapsed: 125.5249 learning rate: 0.000014, scenario: 0, slope: -150.03360561255116, fluctuations: 0.03\n",
      "step: 98480 loss: 4550.956488 time elapsed: 125.5366 learning rate: 0.000014, scenario: 0, slope: -36.74533121026317, fluctuations: 0.01\n",
      "step: 98490 loss: 4541.792259 time elapsed: 125.5485 learning rate: 0.000014, scenario: 0, slope: -10.272803175671074, fluctuations: 0.0\n",
      "step: 98500 loss: 4531.724702 time elapsed: 125.5599 learning rate: 0.000022, scenario: 1, slope: -3.840066267773578, fluctuations: 0.0\n",
      "step: 98510 loss: 4510.729609 time elapsed: 125.5739 learning rate: 0.000057, scenario: 1, slope: -1.5591797973007349, fluctuations: 0.0\n",
      "step: 98520 loss: 4458.068759 time elapsed: 125.5870 learning rate: 0.000147, scenario: 1, slope: -1.2481742572704806, fluctuations: 0.0\n",
      "step: 98530 loss: 4329.703597 time elapsed: 125.6008 learning rate: 0.000381, scenario: 1, slope: -1.719142811703185, fluctuations: 0.0\n",
      "step: 98540 loss: 4040.776688 time elapsed: 125.6146 learning rate: 0.000989, scenario: 1, slope: -3.235939851765612, fluctuations: 0.0\n",
      "step: 98550 loss: 3534.410807 time elapsed: 125.6283 learning rate: 0.001448, scenario: 0, slope: -6.614236544809524, fluctuations: 0.0\n",
      "step: 98560 loss: 3105.458728 time elapsed: 125.6417 learning rate: 0.001448, scenario: 0, slope: -11.685381108227599, fluctuations: 0.0\n",
      "step: 98570 loss: 2778.337030 time elapsed: 125.6552 learning rate: 0.001448, scenario: 0, slope: -17.25136335842239, fluctuations: 0.0\n",
      "step: 98580 loss: 2507.040684 time elapsed: 125.6686 learning rate: 0.001448, scenario: 0, slope: -22.481953529193962, fluctuations: 0.0\n",
      "step: 98590 loss: 2256.501240 time elapsed: 125.6823 learning rate: 0.001448, scenario: 0, slope: -26.84784528529598, fluctuations: 0.0\n",
      "step: 98600 loss: 2015.220796 time elapsed: 125.6958 learning rate: 0.001448, scenario: 0, slope: -29.739382010993253, fluctuations: 0.0\n",
      "step: 98610 loss: 1799.062927 time elapsed: 125.7100 learning rate: 0.001448, scenario: 0, slope: -31.564008765011955, fluctuations: 0.0\n",
      "step: 98620 loss: 1626.995751 time elapsed: 125.7242 learning rate: 0.001448, scenario: 0, slope: -31.256895750453044, fluctuations: 0.0\n",
      "step: 98630 loss: 1496.095085 time elapsed: 125.7369 learning rate: 0.001448, scenario: 0, slope: -28.986824338641977, fluctuations: 0.0\n",
      "step: 98640 loss: 1395.781982 time elapsed: 125.7490 learning rate: 0.001448, scenario: 0, slope: -25.267739486099448, fluctuations: 0.0\n",
      "step: 98650 loss: 1315.914619 time elapsed: 125.7611 learning rate: 0.001448, scenario: 0, slope: -21.455514783196648, fluctuations: 0.0\n",
      "step: 98660 loss: 1249.375285 time elapsed: 125.7728 learning rate: 0.001448, scenario: 0, slope: -18.332761491548446, fluctuations: 0.0\n",
      "step: 98670 loss: 1191.968909 time elapsed: 125.7849 learning rate: 0.001448, scenario: 0, slope: -15.567103105308478, fluctuations: 0.0\n",
      "step: 98680 loss: 1140.856659 time elapsed: 125.7969 learning rate: 0.001448, scenario: 0, slope: -12.988413776066173, fluctuations: 0.0\n",
      "step: 98690 loss: 1093.762521 time elapsed: 125.8102 learning rate: 0.001448, scenario: 0, slope: -10.620101973451542, fluctuations: 0.0\n",
      "step: 98700 loss: 1030.980563 time elapsed: 125.8235 learning rate: 0.001448, scenario: 0, slope: -8.81540477939079, fluctuations: 0.0\n",
      "step: 98710 loss: 982.557547 time elapsed: 125.8374 learning rate: 0.001448, scenario: 0, slope: -7.241159592834407, fluctuations: 0.0\n",
      "step: 98720 loss: 942.627336 time elapsed: 125.8507 learning rate: 0.001448, scenario: 0, slope: -6.2783832742232635, fluctuations: 0.0\n",
      "step: 98730 loss: 908.671012 time elapsed: 125.8639 learning rate: 0.001448, scenario: 0, slope: -5.609333505449057, fluctuations: 0.0\n",
      "step: 98740 loss: 878.271493 time elapsed: 125.8777 learning rate: 0.001448, scenario: 0, slope: -5.108203962309878, fluctuations: 0.0\n",
      "step: 98750 loss: 850.783538 time elapsed: 125.8907 learning rate: 0.001448, scenario: 0, slope: -4.690482191837677, fluctuations: 0.0\n",
      "step: 98760 loss: 825.338247 time elapsed: 125.9042 learning rate: 0.001448, scenario: 0, slope: -4.306649246532888, fluctuations: 0.0\n",
      "step: 98770 loss: 793.050695 time elapsed: 125.9176 learning rate: 0.001448, scenario: 0, slope: -3.942997631817734, fluctuations: 0.0\n",
      "step: 98780 loss: 763.643077 time elapsed: 125.9314 learning rate: 0.001448, scenario: 0, slope: -3.6372734581133295, fluctuations: 0.02\n",
      "step: 98790 loss: 738.564331 time elapsed: 125.9446 learning rate: 0.001448, scenario: 0, slope: -3.3152678560636906, fluctuations: 0.02\n",
      "step: 98800 loss: 716.397058 time elapsed: 125.9564 learning rate: 0.001448, scenario: 0, slope: -3.039137566530781, fluctuations: 0.02\n",
      "step: 98810 loss: 689.063547 time elapsed: 125.9688 learning rate: 0.001448, scenario: 0, slope: -2.8450060264412325, fluctuations: 0.02\n",
      "step: 98820 loss: 660.232792 time elapsed: 125.9808 learning rate: 0.001448, scenario: 0, slope: -2.771995687384798, fluctuations: 0.02\n",
      "step: 98830 loss: 639.120447 time elapsed: 125.9924 learning rate: 0.001448, scenario: 0, slope: -2.716209941905085, fluctuations: 0.02\n",
      "step: 98840 loss: 617.957029 time elapsed: 126.0037 learning rate: 0.001448, scenario: 0, slope: -2.652700971509148, fluctuations: 0.02\n",
      "step: 98850 loss: 593.977253 time elapsed: 126.0164 learning rate: 0.001448, scenario: 0, slope: -2.583360710063679, fluctuations: 0.02\n",
      "step: 98860 loss: 568.288604 time elapsed: 126.0295 learning rate: 0.001448, scenario: 0, slope: -2.5132718636086007, fluctuations: 0.02\n",
      "step: 98870 loss: 543.539772 time elapsed: 126.0429 learning rate: 0.001448, scenario: 0, slope: -2.450089465998943, fluctuations: 0.01\n",
      "step: 98880 loss: 522.606106 time elapsed: 126.0566 learning rate: 0.001448, scenario: 0, slope: -2.419986509703467, fluctuations: 0.0\n",
      "step: 98890 loss: 504.860729 time elapsed: 126.0700 learning rate: 0.001448, scenario: 0, slope: -2.382441393453417, fluctuations: 0.0\n",
      "step: 98900 loss: 488.935259 time elapsed: 126.0833 learning rate: 0.001448, scenario: 0, slope: -2.312502132401247, fluctuations: 0.0\n",
      "step: 98910 loss: 474.336817 time elapsed: 126.0974 learning rate: 0.001448, scenario: 0, slope: -2.1800189863608472, fluctuations: 0.0\n",
      "step: 98920 loss: 460.705087 time elapsed: 126.1107 learning rate: 0.001448, scenario: 0, slope: -2.0771914538149514, fluctuations: 0.0\n",
      "step: 98930 loss: 447.702143 time elapsed: 126.1238 learning rate: 0.001448, scenario: 0, slope: -1.9525909802831316, fluctuations: 0.0\n",
      "step: 98940 loss: 435.319797 time elapsed: 126.1370 learning rate: 0.001448, scenario: 0, slope: -1.7990010943649528, fluctuations: 0.0\n",
      "step: 98950 loss: 423.702477 time elapsed: 126.1501 learning rate: 0.001448, scenario: 0, slope: -1.634342926450804, fluctuations: 0.0\n",
      "step: 98960 loss: 412.359051 time elapsed: 126.1623 learning rate: 0.001448, scenario: 0, slope: -1.487458884522651, fluctuations: 0.0\n",
      "step: 98970 loss: 402.113815 time elapsed: 126.1742 learning rate: 0.001448, scenario: 0, slope: -1.369714452295699, fluctuations: 0.0\n",
      "step: 98980 loss: 391.702955 time elapsed: 126.1857 learning rate: 0.001448, scenario: 0, slope: -1.2822041662755295, fluctuations: 0.0\n",
      "step: 98990 loss: 382.232199 time elapsed: 126.1972 learning rate: 0.001448, scenario: 0, slope: -1.2111606825773968, fluctuations: 0.0\n",
      "step: 99000 loss: 373.070582 time elapsed: 126.2086 learning rate: 0.001448, scenario: 0, slope: -1.1554568624402621, fluctuations: 0.0\n",
      "step: 99010 loss: 364.287096 time elapsed: 126.2216 learning rate: 0.001448, scenario: 0, slope: -1.0934587188356686, fluctuations: 0.0\n",
      "step: 99020 loss: 355.984333 time elapsed: 126.2352 learning rate: 0.001448, scenario: 0, slope: -1.040045137307121, fluctuations: 0.0\n",
      "step: 99030 loss: 347.812484 time elapsed: 126.2487 learning rate: 0.001448, scenario: 0, slope: -0.9874434345353393, fluctuations: 0.0\n",
      "step: 99040 loss: 339.969255 time elapsed: 126.2625 learning rate: 0.001448, scenario: 0, slope: -0.9414515465469777, fluctuations: 0.0\n",
      "step: 99050 loss: 332.395619 time elapsed: 126.2758 learning rate: 0.001448, scenario: 0, slope: -0.9005235041891707, fluctuations: 0.0\n",
      "step: 99060 loss: 325.013654 time elapsed: 126.2893 learning rate: 0.001448, scenario: 0, slope: -0.8650176993930079, fluctuations: 0.0\n",
      "step: 99070 loss: 317.775163 time elapsed: 126.3027 learning rate: 0.001448, scenario: 0, slope: -0.831712568235285, fluctuations: 0.0\n",
      "step: 99080 loss: 310.754997 time elapsed: 126.3161 learning rate: 0.001448, scenario: 0, slope: -0.8032795902298724, fluctuations: 0.0\n",
      "step: 99090 loss: 303.892202 time elapsed: 126.3297 learning rate: 0.001448, scenario: 0, slope: -0.7782363150671303, fluctuations: 0.0\n",
      "step: 99100 loss: 297.190231 time elapsed: 126.3427 learning rate: 0.001448, scenario: 0, slope: -0.7582574836609416, fluctuations: 0.0\n",
      "step: 99110 loss: 290.645801 time elapsed: 126.3583 learning rate: 0.001448, scenario: 0, slope: -0.7362387522705213, fluctuations: 0.0\n",
      "step: 99120 loss: 284.716960 time elapsed: 126.3730 learning rate: 0.001448, scenario: 0, slope: -0.7174766927502391, fluctuations: 0.0\n",
      "step: 99130 loss: 278.465876 time elapsed: 126.3852 learning rate: 0.001448, scenario: 0, slope: -0.6940728940158062, fluctuations: 0.0\n",
      "step: 99140 loss: 271.857270 time elapsed: 126.3968 learning rate: 0.001448, scenario: 0, slope: -0.6760894797066787, fluctuations: 0.0\n",
      "step: 99150 loss: 265.861866 time elapsed: 126.4093 learning rate: 0.001448, scenario: 0, slope: -0.660649795206335, fluctuations: 0.0\n",
      "step: 99160 loss: 259.732111 time elapsed: 126.4219 learning rate: 0.001448, scenario: 0, slope: -0.6472346637518488, fluctuations: 0.0\n",
      "step: 99170 loss: 252.389512 time elapsed: 126.4352 learning rate: 0.001448, scenario: 0, slope: -0.6379301749079797, fluctuations: 0.0\n",
      "step: 99180 loss: 236.680732 time elapsed: 126.4488 learning rate: 0.001448, scenario: 0, slope: -0.6604905652368024, fluctuations: 0.01\n",
      "step: 99190 loss: 225.450237 time elapsed: 126.4627 learning rate: 0.001448, scenario: 0, slope: -0.7183911714301543, fluctuations: 0.02\n",
      "step: 99200 loss: 216.950842 time elapsed: 126.4762 learning rate: 0.001448, scenario: 0, slope: -0.7761252638259469, fluctuations: 0.03\n",
      "step: 99210 loss: 209.787757 time elapsed: 126.4902 learning rate: 0.001448, scenario: 0, slope: -0.8410379061580933, fluctuations: 0.03\n",
      "step: 99220 loss: 203.074624 time elapsed: 126.5040 learning rate: 0.001448, scenario: 0, slope: -0.8806208287753836, fluctuations: 0.03\n",
      "step: 99230 loss: 197.162562 time elapsed: 126.5174 learning rate: 0.001448, scenario: 0, slope: -0.8946908029527401, fluctuations: 0.03\n",
      "step: 99240 loss: 191.810003 time elapsed: 126.5304 learning rate: 0.001448, scenario: 0, slope: -0.8850617272189407, fluctuations: 0.03\n",
      "step: 99250 loss: 186.903649 time elapsed: 126.5431 learning rate: 0.001448, scenario: 0, slope: -0.8471201867240613, fluctuations: 0.03\n",
      "step: 99260 loss: 182.352509 time elapsed: 126.5579 learning rate: 0.001448, scenario: 0, slope: -0.7781325666366475, fluctuations: 0.03\n",
      "step: 99270 loss: 178.090053 time elapsed: 126.5708 learning rate: 0.001448, scenario: 0, slope: -0.6773763259132256, fluctuations: 0.03\n",
      "step: 99280 loss: 174.080340 time elapsed: 126.5830 learning rate: 0.001448, scenario: 0, slope: -0.5953849170059495, fluctuations: 0.01\n",
      "step: 99290 loss: 170.293188 time elapsed: 126.5952 learning rate: 0.001448, scenario: 0, slope: -0.5326750074126069, fluctuations: 0.01\n",
      "step: 99300 loss: 166.703871 time elapsed: 126.6071 learning rate: 0.001448, scenario: 0, slope: -0.49373522207466924, fluctuations: 0.0\n",
      "step: 99310 loss: 163.290434 time elapsed: 126.6195 learning rate: 0.001448, scenario: 0, slope: -0.45219959548478295, fluctuations: 0.0\n",
      "step: 99320 loss: 160.033555 time elapsed: 126.6313 learning rate: 0.001448, scenario: 0, slope: -0.4214714499259431, fluctuations: 0.0\n",
      "step: 99330 loss: 157.167442 time elapsed: 126.6443 learning rate: 0.001448, scenario: 0, slope: -0.39533496344366653, fluctuations: 0.0\n",
      "step: 99340 loss: 154.507413 time elapsed: 126.6580 learning rate: 0.001448, scenario: 0, slope: -0.37192526543965826, fluctuations: 0.02\n",
      "step: 99350 loss: 151.254034 time elapsed: 126.6717 learning rate: 0.001448, scenario: 0, slope: -0.35136876727594146, fluctuations: 0.02\n",
      "step: 99360 loss: 148.315232 time elapsed: 126.6852 learning rate: 0.001448, scenario: 0, slope: -0.334464140038178, fluctuations: 0.02\n",
      "step: 99370 loss: 145.518545 time elapsed: 126.6988 learning rate: 0.001448, scenario: 0, slope: -0.3202063700153043, fluctuations: 0.02\n",
      "step: 99380 loss: 142.833277 time elapsed: 126.7120 learning rate: 0.001448, scenario: 0, slope: -0.30810948916352615, fluctuations: 0.02\n",
      "step: 99390 loss: 141.638944 time elapsed: 126.7257 learning rate: 0.001448, scenario: 0, slope: -0.29375135616771464, fluctuations: 0.03\n",
      "step: 99400 loss: 138.166922 time elapsed: 126.7388 learning rate: 0.001448, scenario: 0, slope: -0.2861675472624547, fluctuations: 0.06\n",
      "step: 99410 loss: 135.219574 time elapsed: 126.7529 learning rate: 0.001448, scenario: 0, slope: -0.2770758486234131, fluctuations: 0.07\n",
      "step: 99420 loss: 132.806636 time elapsed: 126.7665 learning rate: 0.001448, scenario: 0, slope: -0.2708092418988608, fluctuations: 0.07\n",
      "step: 99430 loss: 131.329407 time elapsed: 126.7796 learning rate: 0.001448, scenario: 0, slope: -0.26468213186811573, fluctuations: 0.07\n",
      "step: 99440 loss: 128.621698 time elapsed: 126.7918 learning rate: 0.001448, scenario: 0, slope: -0.2530500970816959, fluctuations: 0.07\n",
      "step: 99450 loss: 126.086596 time elapsed: 126.8040 learning rate: 0.001448, scenario: 0, slope: -0.24669583571395154, fluctuations: 0.07\n",
      "step: 99460 loss: 123.704834 time elapsed: 126.8165 learning rate: 0.001448, scenario: 0, slope: -0.24281328239619449, fluctuations: 0.07\n",
      "step: 99470 loss: 121.585344 time elapsed: 126.8282 learning rate: 0.001448, scenario: 0, slope: -0.2398294365621419, fluctuations: 0.07\n",
      "step: 99480 loss: 120.530776 time elapsed: 126.8400 learning rate: 0.001448, scenario: 0, slope: -0.2357152711292857, fluctuations: 0.07\n",
      "step: 99490 loss: 117.457851 time elapsed: 126.8527 learning rate: 0.001448, scenario: 0, slope: -0.22556700858703152, fluctuations: 0.07\n",
      "step: 99500 loss: 115.434391 time elapsed: 126.8659 learning rate: 0.001448, scenario: 0, slope: -0.22170808948216322, fluctuations: 0.06\n",
      "step: 99510 loss: 113.596668 time elapsed: 126.8800 learning rate: 0.001448, scenario: 0, slope: -0.217184323820795, fluctuations: 0.06\n",
      "step: 99520 loss: 111.564458 time elapsed: 126.8937 learning rate: 0.001448, scenario: 0, slope: -0.21472703983812455, fluctuations: 0.06\n",
      "step: 99530 loss: 109.974108 time elapsed: 126.9074 learning rate: 0.001448, scenario: 0, slope: -0.21216681457653155, fluctuations: 0.06\n",
      "step: 99540 loss: 108.750492 time elapsed: 126.9208 learning rate: 0.001448, scenario: 0, slope: -0.2004437979680221, fluctuations: 0.05\n",
      "step: 99550 loss: 106.209856 time elapsed: 126.9341 learning rate: 0.001448, scenario: 0, slope: -0.19577496580164538, fluctuations: 0.07\n",
      "step: 99560 loss: 104.216948 time elapsed: 126.9475 learning rate: 0.001448, scenario: 0, slope: -0.1933206569075432, fluctuations: 0.08\n",
      "step: 99570 loss: 102.538676 time elapsed: 126.9614 learning rate: 0.001448, scenario: 0, slope: -0.19220382861075644, fluctuations: 0.08\n",
      "step: 99580 loss: 100.755414 time elapsed: 126.9748 learning rate: 0.001448, scenario: 0, slope: -0.1899484203843888, fluctuations: 0.08\n",
      "step: 99590 loss: 100.023573 time elapsed: 126.9880 learning rate: 0.001448, scenario: 0, slope: -0.18354303455274376, fluctuations: 0.06\n",
      "step: 99600 loss: 98.341125 time elapsed: 127.0004 learning rate: 0.001448, scenario: 0, slope: -0.1750670862205966, fluctuations: 0.05\n",
      "step: 99610 loss: 96.050904 time elapsed: 127.0133 learning rate: 0.001448, scenario: 0, slope: -0.17257358547666707, fluctuations: 0.08\n",
      "step: 99620 loss: 94.416829 time elapsed: 127.0254 learning rate: 0.001448, scenario: 0, slope: -0.17120647318510152, fluctuations: 0.09\n",
      "step: 99630 loss: 92.931533 time elapsed: 127.0369 learning rate: 0.001448, scenario: 0, slope: -0.1702548490272537, fluctuations: 0.09\n",
      "step: 99640 loss: 91.413922 time elapsed: 127.0487 learning rate: 0.001448, scenario: 0, slope: -0.16451212363155812, fluctuations: 0.07\n",
      "step: 99650 loss: 89.959489 time elapsed: 127.0614 learning rate: 0.001448, scenario: 0, slope: -0.1616073975481621, fluctuations: 0.05\n",
      "step: 99660 loss: 88.515222 time elapsed: 127.0752 learning rate: 0.001448, scenario: 0, slope: -0.159354228707776, fluctuations: 0.05\n",
      "step: 99670 loss: 87.220947 time elapsed: 127.0888 learning rate: 0.001448, scenario: 0, slope: -0.15777518548990238, fluctuations: 0.05\n",
      "step: 99680 loss: 86.039087 time elapsed: 127.1021 learning rate: 0.001448, scenario: 0, slope: -0.14914663517868776, fluctuations: 0.06\n",
      "step: 99690 loss: 85.197816 time elapsed: 127.1154 learning rate: 0.001448, scenario: 0, slope: -0.14532141851320415, fluctuations: 0.08\n",
      "step: 99700 loss: 83.302882 time elapsed: 127.1280 learning rate: 0.001448, scenario: 0, slope: -0.13806108382204293, fluctuations: 0.08\n",
      "step: 99710 loss: 81.747723 time elapsed: 127.1414 learning rate: 0.001448, scenario: 0, slope: -0.13617208312177653, fluctuations: 0.08\n",
      "step: 99720 loss: 80.410340 time elapsed: 127.1542 learning rate: 0.001448, scenario: 0, slope: -0.1362237148632559, fluctuations: 0.07\n",
      "step: 99730 loss: 79.073654 time elapsed: 127.1674 learning rate: 0.001448, scenario: 0, slope: -0.13683665984723975, fluctuations: 0.07\n",
      "step: 99740 loss: 77.711867 time elapsed: 127.1808 learning rate: 0.001448, scenario: 0, slope: -0.13819447796370476, fluctuations: 0.07\n",
      "step: 99750 loss: 76.366990 time elapsed: 127.1938 learning rate: 0.001448, scenario: 0, slope: -0.14008742959561743, fluctuations: 0.07\n",
      "step: 99760 loss: 75.089221 time elapsed: 127.2059 learning rate: 0.001448, scenario: 0, slope: -0.14252502643710674, fluctuations: 0.07\n",
      "step: 99770 loss: 75.219274 time elapsed: 127.2180 learning rate: 0.001448, scenario: 0, slope: -0.13704497566885815, fluctuations: 0.08\n",
      "step: 99780 loss: 72.959308 time elapsed: 127.2301 learning rate: 0.001448, scenario: 0, slope: -0.12707403221028393, fluctuations: 0.09\n",
      "step: 99790 loss: 71.302478 time elapsed: 127.2419 learning rate: 0.001448, scenario: 0, slope: -0.12474247257466542, fluctuations: 0.08\n",
      "step: 99800 loss: 70.061312 time elapsed: 127.2534 learning rate: 0.001448, scenario: 0, slope: -0.12474877100163848, fluctuations: 0.07\n",
      "step: 99810 loss: 68.910771 time elapsed: 127.2663 learning rate: 0.001448, scenario: 0, slope: -0.12575392742113833, fluctuations: 0.06\n",
      "step: 99820 loss: 67.725720 time elapsed: 127.2796 learning rate: 0.001448, scenario: 0, slope: -0.12644057467500247, fluctuations: 0.06\n",
      "step: 99830 loss: 66.602108 time elapsed: 127.2930 learning rate: 0.001448, scenario: 0, slope: -0.1268622144182884, fluctuations: 0.06\n",
      "step: 99840 loss: 65.709649 time elapsed: 127.3068 learning rate: 0.001448, scenario: 0, slope: -0.12695626031957735, fluctuations: 0.06\n",
      "step: 99850 loss: 64.596586 time elapsed: 127.3204 learning rate: 0.001448, scenario: 0, slope: -0.11861419902500515, fluctuations: 0.07\n",
      "step: 99860 loss: 63.756158 time elapsed: 127.3339 learning rate: 0.001448, scenario: 0, slope: -0.11716468008394179, fluctuations: 0.09\n",
      "step: 99870 loss: 62.854335 time elapsed: 127.3474 learning rate: 0.001448, scenario: 0, slope: -0.10713864535146185, fluctuations: 0.1\n",
      "step: 99880 loss: 61.810971 time elapsed: 127.3610 learning rate: 0.001448, scenario: 0, slope: -0.1023989629877512, fluctuations: 0.1\n",
      "step: 99890 loss: 60.888721 time elapsed: 127.3750 learning rate: 0.001448, scenario: 0, slope: -0.10062098963310107, fluctuations: 0.08\n",
      "step: 99900 loss: 60.030817 time elapsed: 127.3885 learning rate: 0.001448, scenario: 0, slope: -0.09953365498936538, fluctuations: 0.07\n",
      "step: 99910 loss: 59.205716 time elapsed: 127.4025 learning rate: 0.001448, scenario: 0, slope: -0.09872238040905185, fluctuations: 0.07\n",
      "step: 99920 loss: 67.653328 time elapsed: 127.4147 learning rate: 0.001448, scenario: 0, slope: -0.08446057782189768, fluctuations: 0.07\n",
      "step: 99930 loss: 58.290951 time elapsed: 127.4269 learning rate: 0.001448, scenario: 0, slope: -0.08176238785342375, fluctuations: 0.1\n",
      "step: 99940 loss: 57.764613 time elapsed: 127.4384 learning rate: 0.001448, scenario: 0, slope: -0.08218283489377402, fluctuations: 0.13\n",
      "step: 99950 loss: 56.711936 time elapsed: 127.4502 learning rate: 0.001448, scenario: 0, slope: -0.07299127329762557, fluctuations: 0.14\n",
      "step: 99960 loss: 55.917506 time elapsed: 127.4621 learning rate: 0.001448, scenario: 0, slope: -0.0713844798211031, fluctuations: 0.12\n",
      "step: 99970 loss: 55.272828 time elapsed: 127.4749 learning rate: 0.001448, scenario: 0, slope: -0.07143164058033293, fluctuations: 0.1\n",
      "step: 99980 loss: 54.715753 time elapsed: 127.4886 learning rate: 0.001448, scenario: 0, slope: -0.07223693971066833, fluctuations: 0.09\n",
      "step: 99990 loss: 54.164619 time elapsed: 127.5024 learning rate: 0.001448, scenario: 0, slope: -0.0735968902255049, fluctuations: 0.09\n",
      "Time taken by learn is 127.6835 seconds\n",
      "Time taken by compute_tau_f is 1.2510 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6202559999999999"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTYPE = 'float64'\n",
    "L63_data_path = '../data/L63-trajectories'\n",
    "save_folder='../data/adaptive-rate'\n",
    "log_interval = 100\n",
    "milestones = [10*2**n for n in range(15)]\n",
    "learning_rate = 1e-3\n",
    "drop = 0.7\n",
    "steps = int(1e5)\n",
    "save_interval = 100\n",
    "N = 2000\n",
    "L0 = 0.4\n",
    "L1 = 3.5\n",
    "beta = 4e-5\n",
    "train = np.load(f'{L63_data_path}/train.npy').astype(DTYPE)\n",
    "test = np.load(f'{L63_data_path}/test.npy')[:, :, :1000].astype(DTYPE)\n",
    "\n",
    "model = srnn.SurrogateModel_NN(3, 100, name='nn', save_folder=save_folder)\n",
    "model.learn(train[:, :N], steps, 1e-2, batch_size='GD', log_interval=10, save_interval=100,\\\n",
    "            update_interval=100, del_loss=0.01, fluc_lim=0.2, del_rate=0.1)\n",
    "tau_f_rmse, tau_f_se, rmse, se = model.compute_tau_f(test[:100], error_threshold=0.05)\n",
    "tau_f_1 = tau_f_se.mean()\n",
    "tau_f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5664b0-49e2-4a05-986d-acbcf7319486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x = [15217365, 1918240., 2192992, 1542356.749509, 1e6, 1195660.183959, 854033.622230]\n",
    "y = np.arange(len(x))\n",
    "res = stats.linregress(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdc5137-5f2d-4c17-ba78-d2f16186c1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1633148.0630497143"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67548c09-c9c1-4f2f-96a5-ee608a2a926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "argrelextrema(np.array(x), np.greater)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1a655-5e17-4326-8089-3113abfaeadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
